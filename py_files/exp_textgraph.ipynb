{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/causal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": True,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.4,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"YES_NO\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2dd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_85269/3364762243.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class WordAvgEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str = \"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model.vectors.npy\"):\n",
    "        self.kv = KeyedVectors.load(model_path, mmap='r')\n",
    "        self.dim = self.kv.vector_size\n",
    "        self.token_pat = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        toks = [t.lower() for t in self.token_pat.findall(text)]\n",
    "        vecs = [self.kv[w] for w in toks if w in self.kv]\n",
    "        if not vecs:\n",
    "            return np.zeros(self.dim, dtype=np.float32)\n",
    "        return np.mean(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed_text(text)\n",
    "\n",
    "word_emb = WordAvgEmbeddings(model_path=\"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model\")\n",
    "sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "#emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document], emb) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        return parser.question_to_graph(question)\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        return parser.question_to_causal_graph(question)\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "        related_triples = doc0.page_content.strip()\n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "            f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    # 2) current question + triples (optional)\n",
    "    triples_text = \"\"\n",
    "    if relations and CONFIG.get(\"include_current_triples\", True):\n",
    "        text = build_relationship_text(question, G, relations) \n",
    "        triples_text += text\n",
    "    \n",
    "    sections.append(\n",
    "        \"[GRAPH FORMAT DESCRIPTION]:\\n\"\n",
    "        \"The graph triples are encoded as a JSON object with three fields:\\n\"\n",
    "        \"- \\\"entity_dict\\\": A list of entity names, where the index is the entity ID.\\n\"\n",
    "        \"- \\\"relation_dict\\\": A list of relation types, where the index is the relation ID.\\n\"\n",
    "        \"- \\\"edges\\\": A list of triples [head_id, relation_id, tail_id], meaning:\\n\"\n",
    "        \"  entity_dict[head_id] -- relation_dict[relation_id] --> entity_dict[tail_id].\\n\\n\"\n",
    "        \"Example:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"entity_dict\\\": [\\\"cat\\\", \\\"chases\\\", \\\"mouse\\\"],\\n\"\n",
    "        \"  \\\"relation_dict\\\": [\\\"subj\\\", \\\"obj\\\"],\\n\"\n",
    "        \"  \\\"edges\\\": [[0,0,1], [1,1,2]]\\n\"\n",
    "        \"}\\n\"\n",
    "        \"represents:\\n\"\n",
    "        \"- cat --subj--> chases\\n\"\n",
    "        \"- chases --obj--> mouse\\n\"\n",
    "    )    \n",
    "    \n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    if triples_text.strip():\n",
    "        q_block += f\"\\n[CURRENT QUESTION'S GRAPH TRIPLES]:\\n{triples_text}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    # 3) task instructions (placed at the end)\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    rules = (\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    sections.append(rules)\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db = None,\n",
    "    prompt = None\n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if faiss_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)  # Number of docs to retrieve\n",
    "        _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "        \n",
    "    if prompt == None:\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    # If return_full_text=False → only new content; else trim prefix\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    # Normalize YES/NO case\n",
    "    if CONFIG.get(\"answer_mode\", \"YES_NO\"):\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        a = answer.strip().lower()\n",
    "        if \"yes\" in a and \"no\" not in a:\n",
    "            answer = yes\n",
    "            print(answer)\n",
    "            return answer\n",
    "        elif \"no\" in a and \"yes\" not in a:\n",
    "            answer = no\n",
    "            print(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    \n",
    "    \n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        G, rels = parse_question_to_graph_generic(parser, q)\n",
    "        text = build_relationship_text(q, G, rels)  # Output [QUESTION][GRAPH][TRIPLES]\n",
    "\n",
    "        # Get LLM answer\n",
    "        answer = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": G.number_of_nodes(),\n",
    "            \"num_edges\": G.number_of_edges(),\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            metadata[\"prompt_snapshot\"] = make_graph_qa_prompt(q, G, rels)\n",
    "\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20306aa8",
   "metadata": {},
   "source": [
    "## Text RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295d34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "       # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", \n",
    "\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba0620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\": \"YES\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\": \"YES\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\": \"YES\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\": \"YES\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\": \"YES\",\n",
    "\n",
    "    # --- 原始 NO 类 ---\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\": \"NO\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\": \"NO\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\": \"NO\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\": \"NO\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\": \"NO\",\n",
    "\n",
    "    # --- Follow-up Questions (YES/NO balanced) ---\n",
    "    # Earth shape\n",
    "    \"Despite the Earth's slightly flattened poles, is its shape closer to a sphere than to a flat surface?\": \"YES\",\n",
    "    \"Is the Earth perfectly flat with no curvature anywhere on its surface?\": \"NO\",\n",
    "\n",
    "    # Sun rotation\n",
    "    \"Given the Earth's rotation, is the apparent motion of the Sun consistent with the Sun rising in the east?\": \"YES\",\n",
    "    \"If the Earth did not rotate on its axis, would the Sun still rise and set in the same pattern as it does now?\": \"NO\",\n",
    "\n",
    "    # Paris as capital\n",
    "    \"Considering France's administrative structure, is Paris recognized as the political and economic capital of the nation?\": \"YES\",\n",
    "    \"Is Berlin, rather than Paris, designated as the official capital of France in any historical or legal record?\": \"NO\",\n",
    "\n",
    "    # Oxygen necessity\n",
    "    \"Since oxygen is vital for human life, is it correct to say that humans cannot survive without breathing air containing oxygen?\": \"YES\",\n",
    "    \"Can humans live indefinitely without any access to oxygen in their environment?\": \"NO\",\n",
    "\n",
    "    # Moon as satellite\n",
    "    \"Is the Moon the only large natural body that consistently orbits Earth in the solar system?\": \"YES\",\n",
    "    \"Do humans have multiple moons orbiting the Earth, similar to Jupiter or Saturn?\": \"NO\",\n",
    "\n",
    "    # Sahara Desert\n",
    "    \"Is the Sahara Desert geographically located across multiple countries in northern Africa?\": \"YES\",\n",
    "    \"Is the Sahara Desert primarily located in the continent of South America?\": \"NO\",\n",
    "\n",
    "    # Amazon River\n",
    "    \"Does the Nile River surpass the Amazon River in length when measured by the most widely accepted geographical data?\": \"YES\",\n",
    "    \"Is the Amazon River considered to originate in Europe according to global mapping authorities?\": \"NO\",\n",
    "\n",
    "    # Tokyo\n",
    "    \"Is Tokyo the capital city of Japan and a major economic center in Asia?\": \"YES\",\n",
    "    \"Is Tokyo officially listed as the capital city of South Korea in government documents?\": \"NO\",\n",
    "\n",
    "    # Penguins\n",
    "    \"Do penguins naturally inhabit regions in the Southern Hemisphere, particularly Antarctica?\": \"YES\",\n",
    "    \"Do penguins live alongside polar bears in the Arctic region as part of their natural habitat?\": \"NO\",\n",
    "\n",
    "    # Gold vs Lead\n",
    "    \"Is gold denser than most metals but still slightly less dense than lead?\": \"NO\",\n",
    "    \"Is gold classified as a metal due to its physical and chemical properties?\": \"YES\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399a983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text RAG: 仅用问题文本入库 ===\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_text_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    生成仅文本 RAG 的文档，并使 metadata 字段与图 RAG 对齐：\n",
    "    - graph_id / question / num_nodes / num_edges / llm_model / llm_answer / created_at / prompt_snapshot(可选)\n",
    "    - 其中 num_nodes/num_edges 统一置 0，保持同名键方便评测与对比\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        page_content = f\"{q}\"\n",
    "\n",
    "        answer = answer_with_llm_text(q, gen_pipe, text_db=text_db)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": 0,                    \n",
    "            \"num_edges\": 0,                    \n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            prompt_snapshot = make_text_qa_prompt(q, None if not text_db else similarity_search_text_docs(q, text_db, k=CONFIG.get(\"faiss_search_k\",3))[1])\n",
    "            metadata[\"prompt_snapshot\"] = prompt_snapshot\n",
    "\n",
    "        docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_docs_text_only(questions: List[str]) -> List[Document]:\n",
    "    return [build_text_doc(q) for q in questions]\n",
    "\n",
    "def build_text_faiss_index(questions: List[str]) -> FAISS:\n",
    "    docs = build_docs_text_only(questions)\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "def similarity_search_text_docs(\n",
    "    user_question: str,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "):\n",
    "    query_text = f\"{user_question}\"\n",
    "    results = vectordb.similarity_search_with_score(query_text, k=k)\n",
    "    return query_text, results\n",
    "\n",
    "\n",
    "def make_text_qa_prompt(\n",
    "    question: str,\n",
    "    retrieved_docs=None\n",
    ") -> str:\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, _ = retrieved_docs[0]\n",
    "        related_q_txt = doc0.page_content.strip()\n",
    "        related_answer = (doc0.metadata or {}).get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION TEXT]:\\n{related_q_txt}\\n\"\n",
    "            f\"[RELATED ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    sections.append(f\"[CURRENT QUESTION]: {question}\")\n",
    "\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    sections.append(\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    return \"\\n\\n\".join(sections)\n",
    "\n",
    "def answer_with_llm_text(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if text_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)\n",
    "        _, hits = similarity_search_text_docs(question, text_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "\n",
    "    # Prompt\n",
    "    prompt = make_text_qa_prompt(question, retrieved_docs)\n",
    "\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    if CONFIG.get(\"answer_mode\", \"YES_NO\"):\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        a = answer.strip().lower()\n",
    "        if \"yes\" in a and \"no\" not in a:\n",
    "            answer = yes\n",
    "        elif \"no\" in a and \"yes\" not in a:\n",
    "            answer = no\n",
    "        else:\n",
    "            answer = no  \n",
    "    return answer\n",
    "\n",
    "def build_text_faiss_index_with_answers(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    bootstrap_db: Optional[FAISS] = None\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    用文本 RAG 路线生成答案并入库，然后返回 FAISS 向量库。\n",
    "    bootstrap_db: 若传入，文本检索会优先引用该库的历史问答作为 retrieved context（冷启动可传 None）。\n",
    "    \"\"\"\n",
    "    docs = build_text_docs_with_answer(\n",
    "        questions=questions,\n",
    "        gen_pipe=gen_pipe,\n",
    "        add_prompt_snapshot=add_prompt_snapshot,\n",
    "        text_db=bootstrap_db,\n",
    "    )\n",
    "    print(docs)\n",
    "    return build_faiss_index(docs, sentence_emb)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3966e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_once_mode(\n",
    "    question: str,\n",
    "    mode: str,                 # \"text\" or \"graph\"\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser=None,\n",
    "    text_db: Optional[FAISS] = None,\n",
    "    graph_db: Optional[FAISS] = None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    assert mode in (\"text\", \"graph\")\n",
    "\n",
    "    retrieved_docs = None\n",
    "    retrieval_latency = 0.0\n",
    "    retrieved_count = 0\n",
    "\n",
    "    # ---- 1) 检索 + 计时（仅检索耗时）----\n",
    "    if mode == \"text\":\n",
    "        if text_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_text_docs(\n",
    "                question, text_db, k=CONFIG.get(\"faiss_search_k\", 3)\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        prompt = make_text_qa_prompt(question, retrieved_docs=retrieved_docs)\n",
    "    else:\n",
    "        if graph_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_graph_docs(\n",
    "                question, parser, graph_db, k=CONFIG.get(\"faiss_search_k\", 3)\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    # 小工具：token 计数\n",
    "    def _count_tokens(tokenizer, text: str) -> int:\n",
    "        return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # ---- 2) 推理计时（生成部分）----\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_g0 = time.perf_counter()\n",
    "    out = gen_pipe(prompt)\n",
    "    gen_latency = time.perf_counter() - t_g0  # 仅生成耗时\n",
    "\n",
    "    text = out[0][\"generated_text\"]\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # ---- 3) 汇总 ----\n",
    "    return {\n",
    "        \"label\": label or f\"{mode}_rag\",\n",
    "        \"mode\": mode,\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": retrieval_latency + gen_latency,  \n",
    "        \"gen_latency_sec\": gen_latency,                   \n",
    "        \"retrieval_latency_sec\": retrieval_latency,       \n",
    "        \"retrieved_count\": retrieved_count,               \n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "        \"used_retrieval\": bool(retrieved_docs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compare_text_vs_graph(\n",
    "    questions: List[str],\n",
    "    gen_pipe, tokenizer, parser,\n",
    "    text_db: Optional[FAISS],\n",
    "    graph_db: Optional[FAISS],\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q in questions:\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"text\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"text_rag\")\n",
    "        )\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"graph\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"graph_rag\")\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _normalize_yesno(text: str) -> str:\n",
    "    if text is None: return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\" or (\"yes\" in t and \"no\" not in t): return \"YES\"\n",
    "    if t == \"no\"  or (\"no\"  in t and \"yes\" not in t): return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def _norm_q(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"把 gold label 合并到 df，并生成 pred / correct 列。一定要 return DataFrame。\"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"attach_gold: input df is None or not a DataFrame\")\n",
    "\n",
    "    g = pd.DataFrame(list(gold_map.items()), columns=[\"question\",\"gold\"])\n",
    "    g[\"question_norm\"] = g[\"question\"].map(_norm_q)\n",
    "    g[\"gold\"] = g[\"gold\"].map(lambda x: \"YES\" if str(x).upper()==\"YES\" else \"NO\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"question_norm\"] = out[\"question\"].map(_norm_q)\n",
    "    out = out.merge(g[[\"question_norm\",\"gold\"]], on=\"question_norm\", how=\"left\")\n",
    "\n",
    "    out[\"pred\"] = out[\"answer\"].map(_normalize_yesno)\n",
    "    out[\"correct\"] = (out[\"pred\"] == out[\"gold\"]).astype(int)\n",
    "\n",
    "    # 便于排错：提示没有 gold 命中的题\n",
    "    miss = out[out[\"gold\"].isna()]\n",
    "    if len(miss):\n",
    "        print(f\"⚠️ {len(miss)} questions had no gold match. Showing a few:\")\n",
    "        print(miss[[\"question\",\"label\"]].head(5))\n",
    "\n",
    "    return out  # ←←← 关键：确保返回\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame):\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for k, sub in df_with_gold.groupby(\"label\"):\n",
    "        n = len(sub[sub[\"gold\"].notna()])\n",
    "        acc = sub[\"correct\"].mean() if n else float(\"nan\")\n",
    "        print(f\"{k:<10s} acc={acc:.3f} (n={n})\")\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    A = df[df[\"label\"] == base_label]\n",
    "    B = df[df[\"label\"] == target_label]\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b - a) / max(1e-9, a)\n",
    "\n",
    "    print(\"\\n== Cost (avg) ==\")\n",
    "    for col in [\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"latency_sec\",             \n",
    "        \"retrieval_latency_sec\",   \n",
    "        \"gen_latency_sec\",        \n",
    "        \"retrieved_count\",         \n",
    "        \"peak_vram_MiB\",\n",
    "        \"prompt_chars\",\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            a, b, d = avg(col)\n",
    "            print(f\"{col:>22s} | {base_label}: {a:8.3f} | {target_label}: {b:8.3f} | Δ%: {d*100:7.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9916527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_size_bytes(path: str) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            fp = os.path.join(root, f)\n",
    "            try: total += os.path.getsize(fp)\n",
    "            except OSError: pass\n",
    "    return total\n",
    "\n",
    "def save_and_report_sizes(text_db: FAISS, graph_db: FAISS, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\"):\n",
    "    text_db.save_local(text_dir)\n",
    "    graph_db.save_local(graph_dir)\n",
    "    def human(n):\n",
    "        u=[\"B\",\"KB\",\"MB\",\"GB\"]; i=0; x=float(n)\n",
    "        while x>=1024 and i<len(u)-1: x/=1024.0; i+=1\n",
    "        return f\"{x:.2f} {u[i]}\"\n",
    "    s_text  = dir_size_bytes(text_dir)\n",
    "    s_graph = dir_size_bytes(graph_dir)\n",
    "    print(f\"[Index size] text_rag  = {human(s_text)}  ({text_dir})\")\n",
    "    print(f\"[Index size] graph_rag = {human(s_graph)}  ({graph_dir})\")\n",
    "    return s_text, s_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4e9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?', 'num_nodes': 11, 'num_edges': 9, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534049}, page_content='{\"e\":[\"set\",\"perspective\",\"west\",\"east\",\"complete\",\"rotation\",\"cause\",\"Sun\",\"appear\",\"rise\",\"Earth\"],\"r\":[\"prep_from\",\"prep_in\",\"subj\",\"obj\"],\"questions([[e,r,e], ...])\":[[0,0,1],[0,1,2],[3,2,0],[4,3,5],[5,2,6],[7,2,8],[7,2,9],[9,1,3],[10,2,4]]}'), Document(metadata={'graph_id': 'Q2', 'question': \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", 'num_nodes': 6, 'num_edges': 4, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534050}, page_content='{\"e\":[\"Moon\",\"complete\",\"orbit\",\"classified\",\"classify\",\"Earth \\'s only natural satellite\"],\"r\":[\"subj\",\"obj\",\"prep_as\"],\"questions([[e,r,e], ...])\":[[0,0,1],[1,1,2],[3,0,4],[4,2,5]]}'), Document(metadata={'graph_id': 'Q3', 'question': 'Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?', 'num_nodes': 6, 'num_edges': 4, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534051}, page_content='{\"e\":[\"live\",\"Arctic region\",\"locate\",\"the Southern Hemisphere\",\"penguins\",\"located\"],\"r\":[\"prep_in\",\"subj\"],\"questions([[e,r,e], ...])\":[[0,0,1],[2,0,3],[4,1,0],[5,1,2]]}'), Document(metadata={'graph_id': 'Q4', 'question': 'Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?', 'num_nodes': 6, 'num_edges': 4, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534051}, page_content='{\"e\":[\"it\",\"official capital city\",\"accord\",\"its constitution\",\"Paris\",\"consider\"],\"r\":[\"isa\",\"prep_to\",\"subj\"],\"questions([[e,r,e], ...])\":[[0,0,1],[2,1,3],[4,2,5],[4,2,2]]}'), Document(metadata={'graph_id': 'Q5', 'question': 'Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?', 'num_nodes': 6, 'num_edges': 4, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534052}, page_content='{\"e\":[\"Earth\",\"round in shape\",\"consider\",\"orbits\",\"orbit\",\"Sun\"],\"r\":[\"property\",\"subj\",\"obj\"],\"questions([[e,r,e], ...])\":[[0,0,1],[0,1,2],[3,1,4],[4,2,5]]}'), Document(metadata={'graph_id': 'Q6', 'question': 'Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?', 'num_nodes': 9, 'num_edges': 7, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534053}, page_content='{\"e\":[\"humans\",\"need\",\"oxygen\",\"survive\",\"normal conditions\",\"require\",\"base\",\"basic human biology\",\"requires\"],\"r\":[\"subj\",\"obj\",\"prep_under\",\"prep_on\"],\"questions([[e,r,e], ...])\":[[0,0,1],[1,1,2],[2,0,3],[3,2,4],[5,1,2],[6,3,7],[8,0,5]]}'), Document(metadata={'graph_id': 'Q7', 'question': 'Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534053}, page_content='{\"e\":[\"it\",\"capital city\"],\"r\":[\"isa\"],\"questions([[e,r,e], ...])\":[[0,0,1]]}'), Document(metadata={'graph_id': 'Q8', 'question': 'Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534054}, page_content='{\"e\":[\"gold\",\"make\"],\"r\":[\"subj\"],\"questions([[e,r,e], ...])\":[[0,0,1]]}'), Document(metadata={'graph_id': 'Q9', 'question': 'Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534054}, page_content='__EMPTY_JSON__'), Document(metadata={'graph_id': 'Q10', 'question': 'Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534055}, page_content='{\"e\":[\"located\",\"locate\",\"South American continent\"],\"r\":[\"subj\",\"prep_in\"],\"questions([[e,r,e], ...])\":[[0,0,1],[1,1,2]]}')]\n",
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534059}, page_content='Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?'), Document(metadata={'graph_id': 'Q2', 'question': \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534059}, page_content=\"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\"), Document(metadata={'graph_id': 'Q3', 'question': 'Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534060}, page_content='Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?'), Document(metadata={'graph_id': 'Q4', 'question': 'Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534060}, page_content='Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?'), Document(metadata={'graph_id': 'Q5', 'question': 'Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1756534060}, page_content='Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?'), Document(metadata={'graph_id': 'Q6', 'question': 'Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534060}, page_content='Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?'), Document(metadata={'graph_id': 'Q7', 'question': 'Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534061}, page_content='Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?'), Document(metadata={'graph_id': 'Q8', 'question': 'Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534061}, page_content='Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?'), Document(metadata={'graph_id': 'Q9', 'question': 'Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534061}, page_content='Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?'), Document(metadata={'graph_id': 'Q10', 'question': 'Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1756534062}, page_content='Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?')]\n",
      "[Index size] text_rag  = 18.15 KB  (faiss_text_idx)\n",
      "[Index size] graph_rag = 8.49 KB  (faiss_graph_idx)\n",
      "\n",
      "== Accuracy by config ==\n",
      "graph_rag  acc=0.900 (n=10)\n",
      "text_rag   acc=0.800 (n=10)\n",
      "\n",
      "== Cost (avg) ==\n",
      "          input_tokens | text_rag:  183.200 | graph_rag:  449.100 | Δ%:  145.14%\n",
      "         output_tokens | text_rag:    2.400 | graph_rag:   27.900 | Δ%: 1062.50%\n",
      "          total_tokens | text_rag:  185.600 | graph_rag:  477.000 | Δ%:  157.00%\n",
      "           latency_sec | text_rag:    0.547 | graph_rag:    3.244 | Δ%:  493.48%\n",
      " retrieval_latency_sec | text_rag:    0.018 | graph_rag:    0.005 | Δ%:  -73.63%\n",
      "       gen_latency_sec | text_rag:    0.529 | graph_rag:    3.239 | Δ%:  512.81%\n",
      "       retrieved_count | text_rag:    3.000 | graph_rag:    3.000 | Δ%:    0.00%\n",
      "         peak_vram_MiB | text_rag:      nan | graph_rag:      nan | Δ%:     nan%\n",
      "          prompt_chars | text_rag:  844.000 | graph_rag: 1622.300 | Δ%:   92.22%\n"
     ]
    }
   ],
   "source": [
    "gen_pipe, tokenizer = load_llm_pipeline()\n",
    "parser = RelationshipGraphParser()\n",
    "\n",
    "graph_docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    faiss_db=None \n",
    ")\n",
    "print(graph_docs)\n",
    "graph_db = build_faiss_index(graph_docs, word_emb)  \n",
    "\n",
    "text_db = build_text_faiss_index_with_answers(\n",
    "    questions,\n",
    "    gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    bootstrap_db=None  \n",
    ")\n",
    "\n",
    "save_and_report_sizes(text_db, graph_db, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\")\n",
    "\n",
    "eval_questions = list(GOLD_LABELS.keys())[:10]\n",
    "df = batch_compare_text_vs_graph(\n",
    "    eval_questions, gen_pipe, tokenizer, parser, text_db, graph_db\n",
    ")\n",
    "df_ab_gold = attach_gold(df, GOLD_LABELS)\n",
    "evaluate_accuracy(df_ab_gold)\n",
    "summarize_cost(df_ab_gold, base_label=\"text_rag\", target_label=\"graph_rag\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
