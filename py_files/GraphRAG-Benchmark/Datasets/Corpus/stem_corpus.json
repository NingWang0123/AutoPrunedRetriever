[
  {
    "corpus_name": "STEM",
    "context": "Brown bear The brown bear (\"Ursus arctos\") is a large bear with the widest distribution of any living ursid.  The species is distributed across much of northern Eurasia and North America.  It is one of the two largest terrestrial carnivorans alive today, rivaled in body size only by its close cousin, the polar bear (\"Ursus maritimus\"), which is much less variable in size and averages larger due to this.  There are several recognized subspecies, many of which are quite well-known within their native ranges, found in the brown bear species. The brown bear's principal range includes parts of Russia, Central Asia, China, Canada, the United States (mostly Alaska), Scandinavia, and the Carpathian region (especially Romania), Anatolia, and Caucasus.  The brown bear is recognized as a national and state animal in several European countries. While the brown bear's range has shrunk and it has faced local extinctions, it remains listed as a least concern species by the International Union for Conservation of Nature (IUCN) with a total population of approximately 200,000.  As of 2012, this and the American black bear are the only bear species not classified as threatened by the IUCN.  However, the Californian, North African (Atlas bear), and Mexican subspecies were hunted to extinction in the 19th and early 20th centuries, and many of the southern Asian subspecies are highly endangered.  One of the smaller-bodied subspecies, the Himalayan brown bear, is critically endangered, occupying only 2% of its former range and threatened by uncontrolled poaching for its parts.  The Marsican brown bear, one of several currently isolated populations of the main Eurasian brown bear race, in central Italy is believed to have a population of just 30 to 40 bears. The brown bear is sometimes referred to as the \"bruin\", from Middle English.  This name originated in the fable, \"History of Reynard the Fox\", translated by William Caxton, from Middle Dutch \"bruun\" or \"bruyn\", meaning \"brown\" (the color).  In the mid-19th century United States, the brown bear was termed \"Old Ephraim\" and sometimes as \"Moccasin Joe\". The scientific name of the brown bear, \"Ursus arctos\", comes from the Latin \"ursus\", meaning \"bear\", and Άρκτος \"arctos\", from the Greek word for bear. Brown bears are thought to have evolved from \"Ursus etruscus\" in Asia.  The brown bear, per Kurten (1976), has been stated as \"clearly derived from the Asian population of \"Ursus savini\" about 800,000 years ago; spread into Europe, to the New World.\"  A genetic analysis indicated that the brown bear lineage diverged from the cave bear species complex approximately 1.2-1.4 million years ago but did not clarify if \"U. savini\" persisted as a paraspecies for the brown bear before perishing.  The oldest fossils positively identified as from this species occur in China from about 0.5 million years ago.  Brown bears entered Europe about 250,000 years ago, and North Africa shortly after.  Brown bear remains from the Pleistocene period are common in the British Isles, where it is thought they might have outcompeted cave bears (\"Ursus spelaeus\").  The species entered Alaska 100,000 years ago, though they did not move south until 13,000 years ago.  It is speculated that brown bears were unable to migrate south until the extinction of the much larger \"Arctodus simus\". Several paleontologists suggest the possibility of two separate brown bear migrations: inland brown bears, also known as grizzlies, are thought to stem from narrow-skulled bears which migrated from northern Siberia to central Alaska and the rest of the continent, while Kodiak bears descend from broad-skulled bears from Kamchatka, which colonized the Alaskan peninsula.  Brown bear fossils discovered in Ontario, Ohio, Kentucky and Labrador show the species occurred farther east than indicated in historic records.  In North America, two types of the subspecies \"Ursus arctos horribilis\" are generally recognized—the coastal brown bear and the inland grizzly bear; these two types broadly define the range of sizes of all brown bear subspecies. There are many methods used by scientists to define bear species and subspecies as no one method is always effective.  Brown bear taxonomy and subspecies classification has been described as \"formidable and confusing\" with few authorities listing the same specific set of subspecies.  Genetic testing is now perhaps the most important way to scientifically define brown bear relationships and names.  Generally genetic testing uses the word clade rather than species because a genetic test alone cannot define a biological species.  Most genetic studies report on how closely related the bears are (or their Genetic distance).  There are hundreds of obsolete brown bear subspecies, each with its own name, and this can become confusing; Hall (1981) lists 86 different types and even as many as 90 have been proposed.  However, recent DNA analysis has identified as few as five main clades which contain all extant brown bears.  As of 2005, 16 extant or recently extinct subspecies are recognized by the general scientific community. As well as the exact number of overall brown bear subspecies, its precise relationship to the polar bear also remains in debate.  The polar bear is a recent offshoot of the brown bear.  The point at which the polar bear diverged from the brown bear is unclear with estimations based on genetics and fossils ranging from 400,000 to 70,000 years ago, but most recent analysis has indicated that the polar split somewhere between 250,000 and 130,000 years ago.  Under some definitions, the brown bear can be construed as the paraspecies for the polar bear.  DNA analysis recently revealed that the brown bears in North America are genetically quite homogeneous except for the one subspecies Kodiak bear and the enigmatic ABC Islands bears that have the mtDNA of the polar bear (\"Ursus maritimus\"). The subspecies have been listed as follows: A grizzly–polar bear hybrid (known as a \"pizzly bear\" or \"grolar bear\") is a rare ursid hybrid resulting from a union of a brown bear and a polar bear.  It has occurred both in captivity and in the wild.  In 2006, the occurrence of this hybrid in nature was confirmed by testing the DNA of a strange-looking bear that had been shot in the Canadian Arctic.  Previously, the hybrid had been produced in zoos, and was considered a \"cryptid\" (a hypothesized animal for which there is no scientific proof of existence in the wild). Analyses of the genomes of bears have recovered multiple instances of introgressive hybridization between various bear species, including introgression of polar bear DNA intro brown bears during the Pleistocene. Brown bears are often not fully brown.  Brown bears have long, thick fur, with a moderately long mane at the back of the neck which varies somewhat across the races.  In India, brown bears can be reddish with silver tips, while in China, brown bears are bicolored with a yellow-brown or whitish cape across the neck, chest, and shoulders.  North American grizzlies can be dark brown (almost black) to cream (almost white) or yellowish brown and often have darker colored legs.  The common name grizzly stems from their typical coloration, with the hairs on their back usually being brownish-black at the base and whitish-cream at the tips, giving them their distinctive \"grizzled\" color.  The winter fur is very thick and long, especially in northern subspecies, and can reach 11 to at the withers.  The winter hairs are thin, yet rough to the touch.  The summer fur is much shorter and sparser, and its length and density varies geographically. Brown bears have very large and curved claws, those present on the forelimbs being longer than those on the hind limbs.  They may reach 5 to and may measure 7 to along the curve.  They are generally dark with a light tip, with some forms having completely light claws.  Brown bear claws are longer and straighter than those of American black bears (\"Ursus americanus\").  The claws are blunt, while those of a black bear are sharp.  Due to their claw structure, in addition to their excessive weight, adult brown bears cannot typically climb trees as can both species of black bear, although in rare cases adult female brown bears have been seen in trees.  The claws of a polar bear are also quite different, being notably shorter but broader with a strong curve and sharper point, presumably both as an aid to traveling over ice (sometimes nearly vertically) and procuring active prey.  The paws of the brown bear are quite large.  The rear feet of adult bears have been found to typically measure 21 to long, while the forefeet tend to measure about 40% less in length.  All four feet in average sized brown bears tend to be about 17.5 to in width.  In large coastal or Kodiak bear males, the hindfoot may measure up to 40 cm in length, 28.5 cm in width, while outsized Kodiak bears having had confirmed measurements of up to 46 cm along their rear foot.  Brown bears are the only extant bears with a hump at the top of their shoulder, which are made entirely of muscle, this feature having developed presumably for imparting more force in digging, which is habitual during foraging for most bears of the species and also used heavily in den construction prior to hibernation. Adults have massive, heavily built concave skulls, which are large in proportion to the body.  The forehead is high and rises steeply.  The projections of the skull are well developed when compared to those of Asian black bears (\"Ursus thibetanus\"): the latter have sagittal crests not exceeding more than 19–20% of the total length of the skull, while the former have sagittal crests comprising up to 40–41% of the skull's length.  Skull projections are more weakly developed in females than in males.  The braincase is relatively small and elongated.  There is a great deal of geographical variation in the skull, and presents itself chiefly in dimensions.  Grizzlies, for example, tend to have flatter profiles than European and coastal American brown bears.  Skull lengths of Russian bears tend to be 31.5 to for males, and 27.5 to for females.  The width of the zygomatic arches in males is 17.5 to , and 14.7 to in females.  Brown bears have very strong teeth: the incisors are relatively big and the canine teeth are large, the lower ones being strongly curved.  The first three molars of the upper jaw are underdeveloped and single crowned with one root.  The second upper molar is smaller than the others, and is usually absent in adults.  It is usually lost at an early age, leaving no trace of the alveolus in the jaw.  The first three molars of the lower jaw are very weak, and are often lost at an early age.  The teeth of brown bears reflect their dietary plasticity and are broadly similar to other bears excluding the two most herbivorous living bears, the giant panda (\"Ailuropoda melanoleuca\") and spectacled bear (\"Tremarctos ornatus\"), which have blunt, small premolars (ideal for grinding down fibrous plants) compared to the jagged premolars of ursid bears that at least seasonally often rely on flesh as a food source.  The teeth are reliably larger than American black bears but average smaller in molar length than polar bears. Brown bears have the broadest skull of any extant ursine bear, only the afforementioned most herbivorous living bears exceed them in relative breadth of the skull.  Another extant ursine bear, the sloth bear (\"Melursus ursinus\"), has a proportionately longer skull than the brown bear and can match the skull length of even large brown bear races, presumably as an aid for foraging heavily on insect colonies for which a long muzzle is helpful as an evolved feature in several unrelated mammalian groups. The size of brown bears is the most variable of modern bears.  The typical size depends upon which population it is from, and most accepted races vary widely in size.  This is in part due to sexual dimorphism, as male brown bears average at least 30% larger in most races.  Individual bears also vary in size seasonally, weighing the least in spring due to lack of foraging during hibernation, and the most in late fall, after a period of hyperphagia to put on additional weight to prepare for hibernation.  Therefore, a bear may need to be weighed in both spring and fall to get an idea of its mean annual weight. The normal range of physical dimensions for a brown bear is a head-and-body length of 1.4 to and a shoulder height of 70 to .  The tail is relatively short, as in all bears, ranging from 6 to in length.  The smallest brown bears, females during spring among barren-ground populations, can weigh so little as to roughly match the body mass of males of the smallest living bear species, the sun bear (\"Helarctos malayanus\"), while the largest coastal races attain sizes broadly similar to those of the largest living bear species, the polar bear.  Interior brown bears are generally smaller than is often perceived, being around the same weight as an average Southern African lion (Kalahari, Katanga or Transvaal lion), at an estimate average of 180 kg in males and 135 kg in females, whereas adults of the coastal races weigh about twice as much.  The average weight of adult male bears from 19 populations, from around the world and various subspecies (including both large and small bodied subspecies), was found to 217 kg while adult females from 24 populations were found to average 152 kg . Brown bear size, most often measured in body mass, is highly variable and is correlated to extent of food access.  Therefore, bears who range in ecozones that include have access to openings, cover and moisture or water tend to average larger whereas those bears that range into ecozones with enclosed forested areas or arid, sparsely vegetated regions, both of which tend to be sub-optimal foraging habitat for brown bears, average smaller.  The brown bear in northern Europe (i.e. Scandinavia, eastern Europe, western Russia), Yellowstone National Park or interior Alaska seasonally weigh on average between 115 and , from mean low adult female weights in spring to male bear mean high weights in fall.  Bears from the Yukon Delta, interior British Columbia, Jasper National Park and southern Europe (i.e. Spain, the Balkans) can weigh from 55 to on average.  These mass variations represent only two widespread subspecies, the grizzly bear in North America, and the Eurasian brown bear in Europe.  Due to the lack of genetic variation within subspecies, the environmental conditions in a given area likely plays the largest part in such weight variations. The grizzly is especially variable in size, as grizzlies from the largest populations, i.e. interior Alaska, with the heaviest weights recorded in Nelchina, Alaska, nearly three times heavier in males than the smallest grizzlies from Alberta, Canada's Jasper National Park.  Between the sexes, the grizzlies of Nelchina average around 207 kg , whereas the Jasper grizzlies averaged about 74 kg .  The enclosed taiga habitat of Jasper presumably is sub-optimal foraging habitat for grizzlies, requiring them to range widely and feed sparsely, thus reducing body weights and putting bears at risk of starvation, while in surfaces areas in the tundra and prairie are apparently ideal for feeding.  Even elsewhere in the Alberta province, weights averaging more than twice those of Jasper grizzlies have been recorded.  A gradual diminishment in body size is noted in grizzly bears from the sub-Arctic zone, from the Brooks Range to the Mackenzie Mountains, presumably because food becomes much sparser in such regions, although perhaps the most northerly recorded grizzly bears ever, in the Northwest Territories, was a large and healthy male weighing 320 kg , more than twice as much as an average male weighs near the Arctic Circle.  Data from Eurasia similarly indicates a diminished body mass in sub-Arctic brown bears, based on the weights of bears from northern Finland and Yakutia. Head-and-body length in grizzly bears averages from 1.8 to while in Eurasian brown bears it similarly averages from 1.7 to .  Adult shoulder height averaged 95.2 cm in Yellowstone (for any bear measured five or more years old) and a median of 98.5 cm (for adults only 10 or more years old) in Slovakia.  Standing on its hindlegs, a posture only assumed occasionally, typically sized brown bears can reportedly range from 1.83 to in standing height.  Exceptionally large inland specimens have been reported in several parts of North America, Europe, Russia and even Hokkaido.  The largest recorded grizzlies from Yellowstone and Washington state both weighed approximately 500 kg and eastern European bears have been weighed in Slovakia and Bulgaria of up to 400 kg , about double the average weight for male bears in these regions.  Among the grizzly and Eurasian brown bear subspecies, the largest reportedly shot from each being 680 kg and 481 kg , respectively.  The latter bear, from western Russia, reportedly measured just under 2.5 m in head-and-body length. In Eurasia, the size of bears roughly increases from the west to the east, with the largest bears there native to eastern Russia.  Even in the nominate subspecies size increases in the eastern limits, with mature male bears in Arkhangelsk Oblast and Bashkortostan commonly exceeding 300 kg .  Other bears of intermediate size may occur in inland races of Russia.  Much like the grizzly and Eurasian brown bear, populations of the Ussuri brown bear (\"U. a. lasiotus\") and the East Siberian brown bear (\"U. a. collaris\") may vary widely in size.  In some cases, the big adult males of these races may have matched the Kodiak bear in size.  East Siberian brown bears from outside the sub-Arctic and mainland Ussuri brown bears average about the same size as the largest-bodied populations of grizzly bear, i.e. those of similar latitude in Alaska, and have been credited with weights ranging from 100 to throughout the seasons.  On the other hand, the Ussuri brown bears found in the insular population of Hokkaido are usually quite small, usually weighing less than 150 kg , exactly half the weight reported for male Ussuri brown bears from Khabarovsk Krai.  This is due presumably to the enclosed mixed forest habitat of Hokkaido.  A similarly diminished size has been reported in east Siberian brown bears from Yakutia, as even adult males average around 145 kg , thus about 40% less than the average weight of male bears of this race from central Siberia and the Chukchi Peninsula. In linear measurements and mean body mass, several subspecies may vie for the title of smallest race, although thus far their reported body masses broadly overlaps with those of the smaller-bodied populations of Eurasian brown and grizzly bears.  Leopold (1959) described the now extinct Mexican grizzly that, according to Rausch (1963), as the smallest race of grizzly bear in North America although the exact parameters of its body size are not known today.  Bears from the Syrian (\"U. a. syriacus\") subspecies will reportedly weigh around 100 to in adulthood.  The Himalayan race (\"U. a. isabellinus\") is another rival for smallest subspecies, in Pakistan this race averages about 70 kg in females and 135 kg in males.  Himalayan brown bear females were cited with an average head-and-body length of merely 1.4 m .  Brown bears of the compact Gobi Desert population, which is not usually listed as a distinct subspecies in recent decades, weigh around 90 to between the sexes so are similar in weight to bears from the Himalayas and even heavier than grizzlies from Jasper National Park.  However, the Gobi bear has been reported to measure as small as 1 m in head-and-body length, which if accurate would make them the smallest known brown bear in linear dimensions.  These smallest brown bear races are characteristically found in \"barren ground\" type habitats, i.e. sub-desert in bears from the Syrian and Gobi races and arid alpine meadow in Himalayan bears. The largest subspecies are the Kodiak bear (\"U. a. middendorffi\") and the questionably distinct peninsular or coastal brown bear (\"U. a. gyas\").  Also the extinct California grizzly (\"U. a. californicus\") was rather large.  Once mature, the typical female Kodiak bear can range in body mass from 120 to and while from sexual maturity onward male ranges from 168 to .  According to Guinness Records the average male Kodiak bear is 2.44 m in total length (head-to-tail) and has a shoulder height of 1.33 m .  When averaged between their spring low and fall high weights from both localities, males from Kodiak island and coastal Alaska weighed from 312 to with a mean body mass of 357 kg while the same figures in females were 202 to with a mean body mass of 224 kg .  By the time they reach or exceed eight to nine years of age, male Kodiak bears tend to be much larger than newly mature 6-year-old males, potentially tripling their average weight within three years’ time, and can expect to average between 360 and .  The reported mean adult body masses for both sexes of polar bear are very similar to the peninsular and Kodiak bears.  Due to their roughly corresponding body sizes, the two races and the species can both legitimately be considered the largest living member of the bear family Ursidae and largest extant terrestrial carnivores. The largest variety of brown bear from Eurasia is the Kamchatka brown bear (\"U. a. beringianus\").  In the Kamchatka brown bear in past decades, old males have been known to reach body mass of 500 - by fall, putting the race well within Kodiak bear sizes and leading it to be considered the largest of the extant Russian races.  However, a diminishment in body size of \"U.a. berigianus\" has been noted, mostly likely in correlation with overhunting.  In the 1960s and 1970s, most adult Kamchatka brown bears weighed merely between 150 and , however mean weights of mature male bears has been reported as averaging 350 to in 2005. Brown bears were once native to much of Asia, some parts of the Atlas Mountains of Africa and perhaps most of Europe and North America, but are now extinct in some areas, and their populations have greatly decreased in other areas.  There are approximately 200,000 brown bears left in the world.  The largest populations are in Russia with 120,000, the United States with 32,500, and Canada with around 25,000. The brown bear currently occurs in the countries of Afghanistan, Albania, Andorra (recently reoccupied), Armenia, Azerbaijan, Belarus, Bhutan (possibly extinct), Bosnia and Herzegovina, Bulgaria, Canada, China, Croatia, Czech Republic (possibly only vagrants), Estonia, Finland, France, Georgia, Greece, India, Iran, Iraq, Italy, Japan, Kazakhstan, Kyrgyzstan, Latvia (extinct before World War II; possibly vagrants from Estonia or Russia after World War II), Republic of Macedonia, Mongolia, Montenegro, Nepal, North Korea, Norway, Pakistan, Poland, Romania, Russia, Serbia, Slovakia, Slovenia, Spain, Sweden, Tajikistan, Turkey, Turkmenistan, Ukraine, the United States, and Uzbekistan. The brown bear is usually called the grizzly bear in North America.  It once ranged throughout much of the continent. As many as 20,000 brown bears range throughout the Yukon, Northwest Territories, and British Columbia, and in the majority of Alberta.  Canada has one of the most stable brown bear populations today.  They reach their current eastern limits of their distribution in North America in a majority of Nunavut, northeastern Saskatchewan and northern Manitoba, where they range as far east as the west coast of the Hudson Bay from around Rankin Inlet south to Southern Indian Lake. The brown bear has lost 98% of its habitat in the lower 48 states.  About 95% of the brown bear population in the United States is in Alaska, though in the lower 48 states, they are repopulating gradually but steadily along the Rockies and the western Great Plains.  The Alaskan population is estimated at 32,000 individuals.  The largest populations of brown bears in the lower 48 states are found in the 23,300-km2 Greater Yellowstone Ecosystem and the 24,800-km2 Northern Continental Divide Ecosystem.  The Greater Yellowstone Ecosystem of northwest Wyoming is estimated to hold about 674–839 grizzly bears, followed slightly the Northern Continental Divide Ecosystem of northwest Montana with about 765 animals, the Cabinet-Yaak Ecosystem of northwest Montana and northeast Idaho with about 42-65 bears of the species, the Selkirk Ecosystem of northeast Washington and northwest Idaho with only about 40–50 animals and even less the North Cascades Ecosystem of northcentral Washington with about 5–10 grizzlies.  These five ecosystems combine for a total of a maximum 1,729 wild grizzlies still persisting in the contiguous United States.  Unfortunately, these populations are isolated from each other, inhibiting any genetic flow between ecosystems and creating low genetic diversity in remaining populations which can have negative long-time effects.  This isolation poses one of the greatest threats to the future survival of the grizzly bear in the contiguous United States.  Although there is no record of their existence in the United States east of the Rocky Mountain and Great Plain regions in human history, fossil records from Kentucky and the Ungava Peninsula do in fact show that grizzly bears once roamed in eastern North America. Although many people hold the belief some brown bears may be present in Mexico they are almost certainly extinct.  The last Mexican grizzly bear was shot in 1976.  None had been seen since 1960 prior. In Europe, there are 14,000 brown bears in ten fragmented populations.  They are extinct in the British Isles, extremely threatened in France and Spain, and in trouble over most of Central Europe. Brown bears reach their western limits in Spain.  In the Cantabrian mountains of northwest Spain, some 210 bears were found to dwell in Asturias, Cantabria, Galicia and León, in the Picos de Europa and adjacent areas in 2013.  As of 2015, this population was estimated at around 250 individuals, but only due it being a more extensive survey and their numbers may be declining rather than increasing.  However the population of brown bears in the Pyrenees mountains, in a range shared between France, Spain and Andorra, is much lower, estimated at 14 to 25, with a shortage of breeding females.  Their rarity in this area has led biologists to release bears, mostly female, from Slovenia in spring 2006 to reduce the imbalance and preserve the species' presence in the area.  The bears were released despite protests from French farmers.  By 2017 the bears in the Pyrenean region had increased to 39 including 10 cubs. A small population of brown bears (formerly assigned to the race \"Ursus arctos marsicanus\" which is now considered part of the nominate race) still lives in central Italy (Apennine Mountains, Abruzzo and Latium), with no more than 70 individuals, protected by strong laws, but endangered by the human presence in the area. In eastern and northern Europe, the range of brown bear currently extends more broadly.  Among the most populous countries for brown bears in the eastern region are Romania which has approximately 4,000–5,000 brown bears, Bulgaria with 900–1,200, Slovakia at about 600–800 bears, Slovenia at approximately 500–700 animals and Greece holds about 200 animals in the south.  The Carpathian brown bear population of Romania is the largest in Europe outside of Russia.  Despite the relatively large size of the country's bear population, the species’ numbers there were declining alarmingly due to overhunting before Romania's EU membership (which also depended on the protection of the brown bear in the country).  In July 2017 the Romanian Ministry of Environment released an order for the hunting of 175 bears that year because of either increasing bear population or changes in animal behavior because of destruction of habitat by deforestation causing an increase in attacks on humans and damage caused by bears to local communities.  There is also a smaller brown bear population in the Carpathian Mountains in Ukraine (estimated at about 200 in 2005), Slovakia and Poland (estimated at about 100 in 2009 in the latter country).  The total Carpathian population is estimated at about 8,000. Northern Europe is home to a large bear population, with an estimated 2,500 (range 2,350–2,900) in Sweden, about 1,600 in Finland, about 700 in Estonia and 70 in Norway, totaling to nearly 5,000 individuals in the wild.  Another large and relatively stable population of brown bears in Europe, consisting of 2,500–3,000 individuals, is the Dinaric-Pindos (Balkans) population, with contiguous distribution in northeast Italy, Slovenia, Croatia, Bosnia and Herzegovina, Serbia, Montenegro, Macedonia, Albania, Bulgaria and Greece.  Brown bears inhabited the mountains of Austria until as recently as 2011, after a reintroduction effort failed and the species became extinct again.  There is currently no effort to reintroduce the species into Austria.  The entire alpine population of brown bears includes about 50 individuals, most of them living in the Adamello Brento nature park in Italy.  Reintroduction of 10 Slovenian brown bears to the Trentino area in 1998 and 2002 produced occasional visitors to the South Tirol, the Swiss Eastern Alps, Bavaria and isolated sightings in the Central Alps.  The small group of bears living in the Slovenian Alps is connected to the larger Dinaric-Pindos population. In this part of the world, the brown bear occurs from Georgia, Armenia and Azerbaijan southbound spottily through Turkey, northernmost Iraq, western and northern Iran, thence discontinuously in northeastern Kazakhstan, southeastern Uzbekistan and north to Kyrgyzstan.  The populations in these countries are generally very small and fragmented, thus at high risk of genetic isolation, and they occupy only small segments of their former range here.  At least 20-30 were present in northern Iran as of 2015. In Asia, brown bears are found in nearly every part of Russia, thence to the southeast in a small area of Northeast China, western China, and parts of North Korea.  Further west, they reach the southern limits of their worldwide distribution, dwelling spottily in northern Pakistan, Afghanistan and the northernmost point of India, i.e. Jammu and Kashmir.  They possibly persist in northern Nepal, northern Bhutan and northern Myanmar but are not confirmed residents today in these nations. Three distinct lineages of the Hokkaido brown bear (\"Ursus arctos yesoensis L.\") can also be found on the Japanese island of Hokkaidō.  Hokkaido has the largest number of non-Russian brown bears in eastern Asia with about 2,000–3,000 animals, although, in 2015, the Biodiversity Division of the Hokkaido government estimated the population as being as high as 10,600. Many people hold the belief some brown bears may be present in the Atlas Mountains of Morocco but there have been none sighted in the last century.  In addition to the native Atlas bear the Romans apparently imported bears from Spain for spectacles with some escaping and founding a population in Africa, though it is doubtful that they still persist today. This species inhabits the broadest range of habitats of any living bear species.  They seem to have no altitudinal preferences and have been recorded from sea-level to an elevation of 5000 m (the latter in the Himalayas).  In most of their range, brown bears generally seems to prefer semiopen country, with a scattering of vegetation that can allow them a resting spot during the day.  However, they have been recorded as inhabiting every variety of northern temperate forest known to occur.  North American brown bears, or grizzly bears, generally seem to prefer open or semi-open landscapes, with the species once having been common on the Great Plains and continues to occur in sizeable numbers in tundra and coastal estuaries and islands. Variable numbers still occur in prairie areas of the northern Rocky Mountains (mostly in Canada but some in the contiguous United States).  Where continuous and protected, such as the Greater Yellowstone Ecosystem, the prairie is near ideal interior habitat for the species. In western Eurasia, they inhabit mostly mountainous woodlands, in ranges such as the Alps, the Pyrenees and the Caucasus, though they may have been driven into more wooded, precipitous habitats due to the prior extensive persecution of the species in some regions. In Central Asia, human disturbances are minimal as this area has a harsher environment and is more sparsely populated.  In this part of the world, bears may be found in steppe, which is sparser and more desert-like than grassland habitats in North America that occur at similar latitudes, and some bears may live out their lives even in desert edge, such as those that live in the Middle East (Syrian bears) and the rare Gobi bear which is native only to the Chinese-Mongolian desert of its name and isolated from other populations.  Alpine meadows are the typical habitat in the Himalayan and Tibetan populations of brown bear.  In Siberia, the species seems well-adapted to living in almost all parts of the extensive pine forests, usually coming to waterways or poorly drained openings and bogs while feeding and sheltering in broad roots and trunks in the interior.  Eastern Russian forests hold arguably the largest number of brown bears in the world outside of possibly Alaska and northwestern Canada.  The brown bears of Hokkaido are also largely forest dwelling, but dwell in mixed forests dominated by broadleaf trees such as beech. It is thought the Eurasian bears which colonized America were tundra-adapted (as are many grizzlies are today in North America) and the species is sometimes found around sub-Arctic ice fields.  This is indicated by brown bears in the Chukotka Peninsula on the Asian side of Bering Strait, which are the only Asian brown bears to live year-round in lowland tundra like their North American cousins.  Genetics relay that two separate radiations led to today's North American brown bears one a coastal form that lead to Kodiak bear (from \"U. a. beringianus\" or a common ancestor) and one an interior form that lead to the grizzly bear (from \"U. a. lasiotus\" or a common ancestor).  In Arctic areas, the potential habitat of the brown bear is increasing.  The warming of that region has allowed the species to move farther north into what was once exclusively the domain of the polar bear (potentially another offshoot of a radiation of coastal brown bears).  In non-Arctic areas, habitat loss is blamed as the leading cause of endangerment, followed by hunting. While the brown bear's range has shrunk and it has faced local extinctions, it remains listed as a least concern species by the IUCN with a total population of approximately 200,000.  As of 2012, this and the American black bear are the only bear species not classified as threatened by the IUCN.  However, the Californian, North African (Atlas bear), and Mexican subspecies, as well as brown bear populations in the Pacific Northwest were hunted to extinction in the nineteenth and twentieth centuries, and many of the southern Asian subspecies are highly endangered.  The Syrian brown bear (\"Ursus arctos syriacus\") is very rare and it has been extirpated from more than half of its historic range.  One of the smallest-bodied subspecies, the Himalayan brown bear, is critically endangered, occupying only 2% of its former range and threatened by uncontrolled poaching for its parts.  The Marsican brown bear in central Italy is believed to have a population of just 30 to 40 bears. The brown bear is extinct in: Algeria, Belgium, Denmark, Egypt; Germany; Hungary; Ireland, Israel; Jordan, Lebanon; Liechtenstein; Lithuania, Luxembourg, Mexico; Moldova; Monaco, Morocco; Netherlands; Portugal; San Marino; Switzerland; Syria, Tunisia, United Kingdom, Vatican and possibly extinct in Bhutan. The brown bear is often described as nocturnal.  However, it frequently seems to peak in activity in the morning and early evening hours. <ref name=\"Brown / Grizzly Bear Facts\"> </ref> Studies have shown that activity throughout the range can occur at nearly any time of night or day, with bears who dwell in areas with more extensive human contact being more likely to be fully nocturnal.  Furthermore, yearling and newly independent bears are more likely to be active diurnally and many adult bears in low-disturbance areas are largely crepuscular.  In summer through autumn, a brown bear can double its weight from the spring, gaining up to 180 kg of fat, on which it relies to make it through winter, when it becomes very lethargic.  Although they are not full hibernators and can be woken easily, both sexes like to den in a protected spot during the winter months.  Hibernation dens may consist of any spot that provides cover from the elements and that can accommodate their bodies, such as a cave, crevice, cavernous tree roots, or hollow logs. Brown bears have one of the largest brains of any extant carnivoran relative to their body size and have been shown to engage in tool-use (e.g. using a barnacle-covered rock to scratch its neck), which requires advanced cognitive abilities.  This species is mostly solitary, although bears may gather in large numbers at major food sources (e.g., moth colonies, open garbage dumps or rivers holding spawning salmon) and form social hierarchies based on age and size.  Adult male bears are particularly aggressive and are avoided by adolescent and subadult males both at concentrated feeding opportunities and chance encounters.  Female bears with cubs rival adult males in aggression, and are much more intolerant of other bears than single females.  Young adolescent males tend to be least aggressive, and have been observed in nonantagonistic interactions with each other.  Dominance between bears is asserted by making a frontal orientation, showing off canines, muzzle twisting and neck stretching to which a subordinate will respond with a lateral orientation, by turning away and dropping the head and by sitting or lying down.  During combat, bears use their paws to strike their opponents in the chest or shoulders and bite the head or neck.  In his \"Great Bear Almanac\", Gary Brown lists 11 different sounds bears produce in 9 different contexts.  Sounds expressing anger or aggravation include growls, roars, woofs, champs and smacks, while sounds expressing nervousness or pain include woofs, grunts, and bawls.  Sows will bleat or hum when communicating with their cubs. Brown bears usually occur over vast home ranges, however they are not highly territorial.  Several adult bears often roam freely over the same vicinity without issue unless rights to a fertile female or food sources are being contested.  Males always cover more area than females each year.  Despite their lack of traditional territorial behavior, adult males can seem to have a \"personal zone\" in which other bears are not tolerated if they are seen.  Males always wander further than females due to both increasing access to females and food sources while females are advantaged by smaller territories in part since it decreases the likelihood of encounters with male bears who may endanger their cubs.  In areas where food is abundant and concentrated, such as coastal Alaska, home ranges for females are up to 24 km2 and for males are up to 89 km2 .  Similarly, in British Columbia, bears of the two sexes travel relatively compact home ranges of 115 km2 and 318 km2 .  In Yellowstone National Park, home ranges for females are up to 281 km2 and up to 874 km2 for males.  In the central Arctic of Canada, where food sources are quite sparse, home ranges range up to 2434 km2 in females and 8171 km2 in males. A study of male-inherited Y chromosome DNA sequence found that brown bears, over the past few 10,000 years, have shown strong male-biased dispersal That study found surprisingly similar Y chromosomes in brown bear populations as far apart as Norway and coastal Alaska, indicating extensive gene flow across Eurasia and North America.  Notably, this contrasts with genetic signals from female-inherited mitochondrial DNA (mtDNA), where brown bears of different geographic regions typically show strong differences in their mtDNA, a result of female philopatry. The mating season is from mid-May to early July, shifting later the further north the bears are found.  Being serially monogamous, brown bears remain with the same mate from a couple of days to a couple of weeks.  Outside of this narrow time frame, adult male and female brown bears show no sexual interest in each other.  Females mature sexually between the age of 4 and 8 years of age, with an average age at sexual maturity of 5.2-5.5 years old, while males first mate about a year later on average, when they are large and strong enough to successfully compete with other males for mating rights.  Males will try to mate with as many females as they can, usually a successful one mates with two females in a span of one to three weeks.  The adult female brown bear is similarly promiscuous, mating with up to four, rarely even eight, males while in heat and potentially breeding with two males in a single day.  Females come into oestrus on average every three to four years, with a full range of 2.4 to 5.7 years.  The urine markings of a female in oestrus can attract several males via scent.  Paternity DNA tests have shown that up to 29% of cubs in a litter will be from two to three different males.  Dominant males may try to sequester a female for her entire oestrus period of approximately two weeks but usually are unable to retain her for the entire time.  Copulation is vigorous and prolonged and can last up to an hour, although the mean time is about 23–24 minutes. Males take no part in raising their cubs – parenting is left entirely to the females.  Through the process of delayed implantation, a female's fertilized egg divides and floats freely in the uterus for six months.  During winter dormancy, the fetus attaches to the uterine wall.  The cubs are born eight weeks later, while the mother sleeps.  If the mother does not gain enough weight to survive through the winter, the embryo does not implant and is reabsorbed into the body.  There have been cases of bears with as many as six cubs, although the average litter size is 1-3, with more than four being considered uncommon.  There are records of females sometimes adopting stray cubs or even trading or kidnapping cubs when they emerge from hibernation (a larger female may claim cubs away from a smaller one).  Older and larger females within a population tend to give birth to larger litters The size of a litter also depends on factors such as geographic location and food supply.  At birth, the cubs are blind, toothless, hairless, and may weigh from 350 to , again reportedly based on the age and condition of the mother.  They feed on their mother's milk until spring or even early summer, depending on climate conditions.  At this time, the cubs weigh 7 to and have developed enough to follow her over long distances and begin to forage for solid food. The cubs are fully dependent on the mother and a close bond is formed.  During the dependency stage, the cubs learn (rather than inherit as instincts from birth) survival techniques, such as which foods have the highest nutritional value and where to obtain them; how to hunt, fish, and defend themselves; and where to den.  Increased brain size in large carnivores has been positively linked to whether a given species is solitary, as is the brown bear, or raises their offspring communally, thus female brown bears have relatively large, well-developed brains presumably key in teaching behavior.  The cubs learn by following and imitating their mother's actions during the period they are with her.  Cubs remain with their mother for an average of 2.5 years in North America, uncommonly being independent as early as 1.5 years of age or as late as 4.5 years of age.  The stage at which independence is attained may generally be earlier in some parts of Eurasia, as the latest date which mother cubs were together was 2.3 years and most families separated in under two years in a study from Hokkaido and in Sweden most cubs on their own were still yearlings.  Brown bears practice infanticide, as an adult male bear may kill the cubs of another bear.  When adult male brown bears kill a cub it is usually because he is bringing the female into oestrus, as she will enter that state within 2–4 days after the death of her cubs.  Cubs flee up a tree, if available, when they see a strange male bear, and the mother often successfully defends them, even though the male may be twice as heavy as she, although females have been known to lose their lives in these confrontations. The brown bear is a naturally long-lived animal.  Wild females have been observed reproducing up to 28 years of age, which is the oldest known age for reproduction of any ursid in the wild.  The peak reproductive age for females ranges from 4 to 20 years old.  The lifespan of brown bears of both sexes within minimally hunted populations is estimated at an average of 25 years.  The oldest wild brown bear on record was nearly 37 years old.  The oldest recorded female in captivity was nearly 40 years old, while males in captivity have been verified to live up to 47 years, with one captive male possibly attaining 50 years of age. While male bears potentially live longer in captivity, female grizzly bears have a greater annual survival rate than males within wild populations per a study done in the Greater Yellowstone Ecosystem.  Annual mortality for bears of any age is estimated at around 10% in most protected areas; however, the average annual mortality rate rises to an estimated 38% in hunted populations.  Around 13% to 44% of cubs die within their first year even in well-protected areas.  Mortality rates of 75-100% among the cubs of any given year are not uncommon.  Beyond predation by large predators including wolves, Siberian tigers (\"Panthera tigris altaica\") and other brown bears, starvation and accidents also claim the lives of cubs.  Studies have indicated that the most prevalent source of mortality for first-year cubs is malnutrition.  By the second and third years of their lives, the annual mortality rate among cubs in the care of their mothers drops to 10-15%. Even in populations living in protected areas, humans are still the leading cause of mortality for brown bears.  The largest amount of legalized brown bear hunting occurs in Canada, Finland, Russia, Slovakia and Alaska.  Hunting is unregulated in many areas within the range of the brown bear.  Even where hunting is legally permitted most biologists feel that the numbers hunted are excessive considering the low reproduction rate and sparse distribution of the species.  Brown bears are also killed in collisions with automobiles, which is a significant cause of mortality in the United States and Europe. The brown bear is one of the most omnivorous animals in the world and has been recorded consuming the greatest variety of foods of any bear.  Throughout life, this species is regularly curious about the potential of eating virtually any organism or object that they encounter.  Certainly no other animal in their given ecosystems, short perhaps of other bear species and humans, can claim to feed on as broad a range of dietary opportunities.  Food that is both abundant and easily obtained is preferred.  Their jaw structure has evolved to fit their dietary habits.  Their diet varies enormously throughout their differing areas based on opportunity.  In spring, winter-provided carrion, grasses, shoots, sedges and forbs are the dietary mainstays for brown bears from almost every part of their distribution.  Fruits, including berries, become increasingly important during summer and early autumn.  Roots and bulbs become critical in autumn for some inland bear populations if fruit crops are poor.  The dietary variability is illustrated in the western United States, as meat made up 51% of the average year-around diet for grizzly bears from Yellowstone National Park while it made up only 11% of the year-around diet for grizzlies from Glacier National Park a few hundred miles to the north. Despite their reputation, most brown bears are not highly carnivorous, as they derive up to 90% of their dietary food energy from vegetable matter.  Brown bears often feed on a variety of plant life, including berries, grasses, flowers, acorns (\"Quercus\" ssp.)  and pine cones as well as mosses and fungi such as mushrooms.  In total, over 200 plant species have been identified in their foods.  Arguably the most herbivorous diets have come from the warmer temperate parts of Eurasia as more than 90% of the diet may be herbivorous.  These include countries and regions such as Spain, Slovakia, most of the Balkans including Greece, Turkey, the Himalayas, and presumably the Middle East.  In many inland portions of North America, the diet of grizzly bears is between 80 and 90% plant-based but animal meat can be much more important in some areas.  It has been found that being restricted to a largely vegetarian diet puts constraints on the growth and size of bears who live off of them, largely because their digestive systems do not process plants as well as animal fats and proteins. Among all living bears, brown bears are uniquely equipped to dig for tough foods such as roots and shoots.  They use their long, strong claws to dig out earth to reach the roots and their powerful jaws to bite through them.  For the most part, the consumed plant life in spring, predominantly roots immediately post-hibernation and grasses later in spring, is not highly nutritious for bears and mainly staves off hunger and starvation until more nutritious food is available.  Brown bears have difficulty digesting large quantities of tough, fibrous foods.  Hedysarum roots are among the most commonly eaten foods from throughout the range and can become important substitutes if stable foods such as fruits become unavailable.  Corms and bulbs are important when available as they are one of the greater sources of protein in plant life as are hard masts such as acorns. Brown bears are restricted in their access to hard masts compared to black bears because of the limited climbing abilities of grown bears and therefore are confined largely to masts fallen to the ground, pirated from other creatures or within a reach of about 3 m that the bears can stretch to with their paws extended and standing on their hindlegs.  Hard masts can become the most important food (although consumed mainly in late summer and fall) where available in large quantities such as on Hokkaido, Italy and Spain.  One of the most important foods in the Rocky Mountain region of the United States is the Whitebark pine nut (\"Pinus albicaulis\"), which is attained perhaps a majority of the time by raiding the once abundant caches of American red squirrels (\"Tamiasciurus hudsonicus\") rather than direct foraging.  The decline of whitebark pine nut due to the inadvertent introduction by man of the invasive, virulent fungi \"Cronartium ribicola\" has in turn required grizzlies to seek alternate food sources, many of which are carnivorous.  In a Greek food study, soft mast were found to outrank hard masts as a food source, with about a quarter of the year-around diet consisting of the legume \"Medicago\". Fruits and berries are indispensable for brown bears in most areas as a high-energy food stuff for bears which is necessary to survive the hibernation cycle.  The variety of fruits consumed is high, with most of the well-known, wild fruiting plants in temperate North America and Eurasia attracting brown bears in late summer and fall.  Among the most prominent fruits found in their foods from through the range include many \"Prunus\" species including prunes and cherries, crowberries (\"Empetrum nigrum\"), pears (\"Pyrus\" ssp.) , crabapples (\"Malus\" ssp.) , brambles (\"Rubus fruticosus\"), raspberries (\"Rubus idaeus\"), bearberries (\"Arctostaphylos\" ssp.)  (reportedly named for bears’ fondness for them), blueberries (\"Vaccinium\" ssp.) , lingonberries (\"Vaccinium vitis-idaea\") and huckleberries (\"Vaccinium parvifolium\").  Fruit appears to become more secondary in the diet in areas where hard masts and animal protein are abundant in late summer and fall, as these more protein-rich foods appear to be more nutritious for bears than carbohydrate-rich fruits are, despite their fondness for fruit.  Even where fruit are commonly eaten, other foods must be eaten to meet nutritional requirements.  It is estimated that a small female brown bear may need to eat nearly 30,000 berries each day in late summer/fall in order to subsist on a purely fruit-based diet. Brown bears will also commonly consume animal matter, which in summer and autumn may regularly be in the form of insects, larvae and grubs, including beehives. Most insects eaten are of the highly social variety found in colonial nests, which provide a likely greater quantity of food, although they will also tear apart rotten logs on the forest floor, turn over rocks or simply dig in soft earth in attempts to consume individual invertebrates such as bugs, beetles and earthworms. Honey bees and wasps are important supplemental foods in Eurasia from the furthest west of their range, in Spain, to the furthest east, in Hokkaido.  Bears in Yellowstone and Montana eat an enormous number of moths during the summer, sometimes consuming as many as 40,000 army cutworm moths (\"Euxoa auxiliaris\") in a single day, and may derive up to half of their annual food energy from these insects.  In Europe, a variety of species of ants have been found to factor heavily into the diet in some areas such as Scandinavia and eastern Europe.  In Slovenia, for example, up to 25% of the dry mass consumed by bears was ants.  Locally heavy consumption of ants has been reported in North America as well, as in west-central Alberta, 49% of scat contained ants.  This species mainly feed on ants with a passive response to the colony being dug out and low levels of formic acid, therefore carpenter ants (\"Camponotus\" ssp.) , which are accessed through rotten logs rather than underground colonies, are preferred where available.  Other important insect aggregations that brown bears feed heavily on in some regions include ladybirds and caddisfly.  Brown bears living near coastal regions will regularly eat crabs and clams.  In Alaska's Katmai National Park and Preserve, bears along the beaches of estuaries regularly dig through the sand for soft-shell clam (\"Mya arenaria\") and Pacific razor clam (\"Siliqua patula\"), providing a more nutritious source of dietary energy in spring than plant life before fish become available there.  The zarigani (\"Cambaroides japonicus\"), a type of crayfish, of Hokkaido is also an important, protein-rich dietary supplement for bears there. By far the closest dietary relationship between brown bears and fish occurs between the salmon and trout of the \"Oncorhynchus\" genus, particularly in coastal areas but also in some inland areas of North America.  In the Kamchatka peninsula and several parts of coastal Alaska, including Kodiak Island, brown bears feed largely on spawning salmon, whose nutrition and abundance explain the enormous size of the bears in these areas.  Sockeye salmon (\"O. nerka\") and pink salmon (\"O. gorbuscha\") are the two most commonly predated, but many coho (\"O. kisutch\"), Chinook (\"O. tshawytscha\"), masu (\"O. masou\") and chum salmon (\"O. keta\") are also taken.  Even in the coastal ranges of the Pacific, a diverse omnivorous diet is eaten, with the salmon spawning reliably providing food only in late summer and early fall.  Exceptionally, salmon may come to inland rivers as early as June in the Brooks River when other coastal Alaskan bears are in their dietary \"lean period\" and provide food for bears sooner than normal.  On Kodiak island, it appears the availability of alternate food sources is high, as berry crops are often profuse, marine organisms often wash up and ungulates both wild and domesticated are available.  The fishing techniques of bears are well-documented.  They often congregate around falls when the salmon are forced to breach the water, at which point the bears will try to catch the fish in mid-air (often with their mouths).  They will also wade into shallow waters, hoping to pin a slippery salmon with their claws.  While they may eat almost all the parts of the fish, bears at the peak of spawning, when there is usually a glut of fish to feed on, may eat only the most nutritious parts of the salmon (including the eggs and head) and then indifferently leave the rest of the carcass to scavengers, which can include red foxes (\"Vulpes vulpes\"), bald eagles (\"Haliaeetus leucocephalus\"), common ravens (\"Corvus corax\") and gulls. Despite their normally solitary habits, Brown bears will gather rather closely in numbers at good spawning sites.  The largest and most powerful males claim the most fruitful fishing spots and bears (especially males) will sometimes fight over the rights to a prime fishing spot.  Despite their aggressive defensive abilities, female brown bears usually select sub-optimal fishing spots to avoid male bears that could potentially threaten their cubs. One other key relationship occurs between brown bears and \"Oncorhynchus\" species occurs with the grizzly bear and the cutthroat trout (\"O. clarki\") in the Rockies such as around Yellowstone.  Here this species was consumed in considerable numbers although, like the whitebark pine nut, this food source has declined due to invasive species introduced by man, i.e. invasive trout which are outcompeting cutthroat trout.  The now extinct California grizzly bear was also a fairly specialized \"Onocorhynchus\" predator in California's mountain streams and rivers, principally of rainbow trout (\"O. mykiss\").  Outside of Pacific-based salmonids, predatory relationships between brown bears and fish are uncommon.  Predation on broad whitefish (\"Coregonus nasus\") and longnose suckers (\"Catostomus catostomus\") has been reported in sub-Arctic Canada and northern pike (\"Esox lucius\") and grayling (\"Thymallus thymallus\") in Siberia, plus other older records of brown bears hunting miscellaneous freshwater fish in Eurasia. Beyond the regular predation of salmon, most brown bears are not particularly active predators.  Nonetheless, brown bears are capable of obtaining practically all forms of the mammals that they encounter: from mouse-like rodents to those as fearsome as a tiger or as large as a bison.  Over 100 species of mammal have been recorded either in the scats of brown bears or have been observed as being killed or consumed by the species, although much of this consumption probably represents merely scavenging on carrion. A perhaps surprisingly high balance of mammalian foods consists of rodents or similar smallish mammals, as about half of the species consumed by brown bears weigh less than 10 kg on average.  These may include hares (\"Lepus\" ssp.) , pikas (\"Ochotona\" ssp.)  marmots (\"Marmota\" ssp.) , ground squirrels, chipmunks, mice, rats, lemmings and voles. Due to their propensity for digging, brown bears are able to smell out active subterranean burrows of these small mammals and either wait quietly or furiously dig away until the animals are either displaced and lunged at or are cornered in their burrows.  Not only do they consume the small mammals, but they also feed on their caches, as has been recorded in grizzly bears attacking voles and northern pocket gophers (\"Thomomys talpoides\").  In some areas, caches may be the primary target when bears dig at these animal's burrows, as may be the case with Siberian chipmunks (\"Eutamias sibiricus\"), whose hoards can contain up to 20 kg of food, with the chipmunks themselves only being caught occasionally.  With particular regularity, tundra-dwelling grizzlies will wait at burrows of Arctic ground squirrels (\"Spermophilus parryii\") hoping to pick off a few of the 1 kg rodents.  Ground squirrel hunting is most successful in September and October, when early snow may impede the rodents’ rocky escape routes.  In Denali National Park, Arctic ground squirrels represent about 8% of the year-round diet of grizzly bears and are the most consistent source of animal protein for grizzlies there.  An even more important dietary relationship with a small mammal occurs in the Tibetan blue bear, which is apparently the most completely carnivorous brown bear race, foraging most regularly for plateau pikas (\"Ochotona curzoniae\"), a species about one-sixth the weight of an Arctic ground squirrel.  As many as 25 pikas have been found in a single bear's stomach and in Changtang, 60% of the diet consisted of pikas.  Where plateau pikas are absent, as in the Mustang region of Nepal, Himalayan marmots (\"Marmota himalayana\") become the dietary staple of the bear, occurring in about half of nearly a thousand scats.  Large rodents such as beavers (\"Castor\" spp.)  and North American porcupines (\"Erethizon dorsatum\") are rare prey items, mostly due to differing habitat preferences, as well as the obvious defenses of the latter.  Up to five species of cetacean have been recorded as a food source in the coastal regions of Alaska, the Central Arctic and (formerly) California when beached. In most of their range, brown bears regularly feed on ungulates.  In many cases, this important food source is obtained as carrion.  Carrion is mostly eaten in spring, when winter snow and ice conditions (including snow-slides) and starvation claim many ungulate lives.  As carcasses are often solidly frozen when encountered, brown bears may sit on them to thaw them sufficiently for consumption.  While perhaps a majority of bears of the species will charge at ungulates at some point in their lives, many predation attempts start with the bear clumsily and half-heartedly pursuing the prey, and end with the prey escaping alive.  On the other hand, some brown bears are quite self-assured predators who habitually pursue and catch large prey items which are mainly ungulates.  Such bears are usually taught how to hunt by their mothers from an early age.  They are the most regular predator of ungulates among extant bear species.  The extent of hunting behavior differs by region.  For example, in Slovenia, ungulate meat was four times more likely to be obtained as carrion than through hunting, while on the contrary in east-central Alaska, live hunting of ungulates was four times more likely than scavenging of carrion.  The extent of carnivory in brown bears has been proven to increase at northern latitudes.  When brown bears attack these large animals, they usually target young or infirm ones, as they are easier to catch.  Successful hunts usually occur after a short rush and ambush but they may chase down prey in the open and will try to separate mother and young.  Prey is usually killed when the bear grabs the rib cage over the back and delivers a bite to the back of the head, neck, face or nose.  The bear may also pin its prey (usually young) to the ground and then immediately tear and eat it alive.  Despite being characterized as unskilled predators with minimally-refined hunting skills, most individual bears who are routine ungulate predators have shown the ability to vary their hunting strategy and have hunting success rates comparable to other large, solitary carnivorans.  Brown bears will on occasion bite or swipe at some prey in order to stun it enough to knock it over for consumption.  To pick out young or infirm individuals, bears will charge at herds so the slower-moving and more vulnerable individuals will be made apparent.  Brown bears may also ambush young animals by finding them via scent.  Despite being characterized as a danger primarily to young, spring neonatal ungulates in the first couple of days of life, when they have undeveloped legs and cannot run at full speed, young ungulates may be pursued well into summer or fall after they have developed running abilities.  Most attacks on adult ungulates occur when the prey has some variety of physical disadvantage.  When emerging from hibernation, brown bears, whose broad paws allow them to walk over most ice and snow, may pursue large prey such as moose, whose hooves cannot support them on encrusted snow.  Similarly, predatory attacks on large prey sometimes occur at riverbeds, when it is more difficult for the prey specimen to run away due to muddy or slippery soil.  On rare occasions, most importantly when confronting unusually large, fully-grown and dangerous prey, bears kill them by hitting with their powerful forearms, which can break the necks and backs of large creatures such as adult moose and adult bison. The leading ungulate prey for brown bears is normally deer.  Up to a dozen species have been eaten by brown bears but the main prey species are the larger species they encounter: elk (\"Cervus canadensis\"), moose (\"Alces alces\") and caribou (\"Rangifer tarandus\").  Larger deer are preferred because they tend to be less agile and swift than small or medium-sized deer (although a caribou can handily outpace a grizzly bear in the open), they are found in large quantities in several areas inhabited by brown bears and provide a larger meal per carcass.  Moose may be preferred where found in large numbers because of their solitary habits and tendency to dwell in wooded areas, both of which makes them easier to ambush.  Despite its diminished reputation as a predator, the brown bear is the most dangerous solitary predator of moose, with only packs of wolves a more regular predator; even Siberian tigers take other prey, primarily (elk and boar), in areas where they co-exist with the giant deer.  Brown bears normally avoid the potential risks of hunting large deer, which can potentially fight back but usually escape bears by running, by picking out young calves or sickly adults from deer herds.  In northeastern Norway, it was found that moose were the most important single food item (present in up to 45% of scats and locally comprising more than 70% of the bear's dietary energy) for local brown bears and several local bears appear to be specialized moose hunters, most often picking off sickly yearling moose and pregnant but healthy cows.  In Yellowstone National Park, grizzly bears who derived much of their food energy from ungulates were studied, and 30% of the ungulates consumed were through predation, the remaining amount from scavenging of carcasses.  Elk, bison and moose (the three largest native ungulates in the region) each constituted nearly a quarter of the overall ungulate diet.  13% of the total of ungulates actively hunted and killed per that study in Yellowstone were elk calves, while 8% of the actively and successfully hunted prey there were adult cow elk.  Despite their lack of preference for smaller deer, other species including red deer (\"Cervus elaphus\"), sika deer (\"Cervus nippon\" ), axis deer (\"Axis axis\"), European roe deer (\"Capreolus capreolus\"), Siberian roe deer (\"Capreolus pygargus\"), fallow deer (\"Dama dama\"), mule deer (\"Odocoileus hemionus\") and white-tailed deer (\"Odocoileus virginianus\") have turned up in their diet. As many as 20 species of bovids are also potential prey, including various sheep, goats, antelope, bison (\"Bison\" ssp.)  and muskoxen (\"Ovibos moschatus\").  Bovids are mostly taken in random encounters when bears come across a vulnerable, usually young or sickly individual, as smaller species are extremely agile (and often live in rocky environments) and larger varieties are potentially dangerous, especially if aware of the bear's presence.  In some parts of eastern Europe and Russia, wild boar (\"Sus scrofa\") may be taken in surprisingly large quantities considering the mostly herbivorous reputation of bears in these regions.  One study from the Amur territory of Russia found that brown bears were actually more prolific killers of wild boars than both tigers and gray wolves, but these results are probably biased due to the scarcity of tigers in the region because of overhunting of the big cat.  In rare cases, brown bears are capable of killing bulls of the largest ungulates in regions they inhabit, reportedly including moose, muskox, yak (\"Bos grunniens\") and both American and European bison (\"Bison bison\" & \"bonasus\").  Remarkably, such attacks are sometimes carried out by bears that were not particularly large, including interior sow grizzlies or small-bodied bears from the Central Arctic, and some exceptional ungulates taken may be up to two to three times the weight of the attacking bear.  However, most of the bears who took adult moose in east-central Alaska and Scandinavia were large, mature males. This species may eat birds and their eggs, including almost entirely ground- or rock-nesting species.  Although not typically able to capture a healthy grown bird, eggs, nestlings and fledglings of large bird species can be very attractive to brown bears.  Species attacked have ranged can be any size available from Aleutian terns (\"Onychoprion aleuticus\") to trumpeter and whooper swans (\"Cygnus buccinator\" & \"cygnus\").  Most recorded avian prey have consisted of geese and sea ducks nesting in the lower Arctic Circle, followed by coveys of galliforms, as these birds place their nests in shallow water and on the ground as well as raise their chicks in such areas, so are relatively more vulnerable.  Large birds of prey including sea eagles, gyrfalcons (\"Falco rusticolus\") and golden eagles (\"Aquila chrysaetos\") are sometimes exploited as prey if nesting in rock formations accessible on foot and eagles and falcons may furiously dive at bears near their nests.  Due to their inhabitance of cooler temperate areas, reptiles and amphibians are rarely a food source and have been verified as prey only in a few cases: frogs in the Italian Alps, rat snakes in Hokkaido, grass lizards in the Amur territory and tortoises in Greece. When forced to live in close proximity with humans and their domesticated animals, bears may potentially predate any type of domestic animal.  Most type of livestock have been domesticated for millennia and have little to no anti-predator defenses.  Therefore, brown bears are somewhat more likely to attack healthy adults than they are of healthy adult wild animals.  Among domestic and farm animals, domestic cattle (\"Bos primigenius taurus\") are sometimes exploited as prey.  Cattle are bitten on the neck, back or head and then the abdominal cavity is opened for eating.  In Norway, free-ranging domestic sheep (\"Ovis aries\") are numerous and the local brown bears derive 65–87% of their dietary energy in late summer from sheep.  Because of the aforementioned vulnerability, examination of Norwegian sheep remains suggest many of the sheep consumed there are adults that were killed by the bears rather than merely scavenged, and thus some local farmers received partial compensation for their stock losses.  In nearby northern Sweden, free-ranging sheep are not present and the bear derive their food predominantly from natural sources.  Horses (\"Equus ferus caballus\"), goats (\"Capra aegagrus hircus\"), pigs (\"Sus scrofa domesticus\"), chickens (\"Gallus gallus domesticus\") and dogs (\"Canis lupus familaris\") may be opportunistically killed in several parts of the brown bear's range as well.  Plants and fruit farmed by humans are readily consumed as well, including corn (\"Zea mays \"), wheat (\"Triticum\" spp.) , apple (\"Malus pumila\"), sorghum (\"Sorghum\" ssp.) , melons and any form of berries.  They will also feed at domestic bee farms, readily consuming both honey and the contents of the honey bee (\"Apis mellifera\") colony.  Human foods and trash or refuse is eaten when possible.  When an open garbage dump was kept in Yellowstone, brown bears were one of the most voracious and regular scavengers.  The dump was closed after both brown and American black bears came to associate humans with food and lost their natural fear of them.  In other areas, such as Alaska, dumps may continue to be an attractant for brown bears. While feeding on carrion, brown bears use their size to intimidate other predators, such as wolves, cougars (\"Puma concolor\"), tigers (\"Panthera tigris\") and black bears from their kills.  Owing to their formidable size and aggressive disposition, predation by wild animals outside of their own species is rare for brown bears of any age, even cubs are often safe due to their watchful mother.  There are two records of golden eagles (\"Aquila chrysaetos\") predating brown bear cubs. Adult bears are generally immune to predatory attacks except from tigers and other bears.  Siberian tigers (\"Panthera tigris altaica\") prefer preying on young bears but smaller, fully grown adult female brown bears outside their dens may also be taken.  Successful predatory attacks by tigers on adult brown bears are usually on females, with or without cubs, in their dens.  Of 44 recorded encounters between the two predators, 20 resulted in confrontations; in 50% of these, the bears in general (not necessarily brown bears) were killed, in 27% the tigers were killed, and 23% of the cases ended with both animals surviving and parting ways despite injuries sustained in the conflict.  Some bears emerging from hibernation seek out tigers in order to steal their kills.  Despite the possibility of tiger predation, some large brown bears may actually benefit from the tiger's presence by appropriating tiger kills that the bears may not be able to successfully hunt themselves, and follow tiger tracks.  Geptner et al. (1972) stated that bears are generally afraid of tigers and change their path after coming across tiger trails.  In the winters of 1970–1973, Yudakov and Nikolaev recorded 1 case of a brown bear showing no fear of the tigers and another case of a brown bear changing path upon crossing tiger tracks.  Other researchers have observed bears following tiger tracks to scavenge tigers' kills or to prey on tigers.  Bears frequently track down tigers to usurp their kills, with occasional fatal outcomes for the tiger.  A report from 1973 describes twelve known cases of brown bears killing tigers, including adult males; in all cases the tigers were subsequently eaten by the bears.  There are reports of brown bears specifically targeting Amur leopards and tigers to appropriate their kills.  In the Sikhote-Alin reserve, 35% of tiger kills were stolen by bears, with the tigers either departing entirely or leaving part of the kill for the bear. Brown bears regularly intimidate gray wolves (\"Canis lupus\") away from their kills, with wolves occurring in most of the brown bear's worldwide distribution.  In Yellowstone National Park, brown bears pirate wolf kills so often, Yellowstone's Wolf Project director Doug Smith wrote, \"It's not a matter of if the bears will come calling after a kill, but when.\"  Similarly, in Denali National Park, grizzly bears routinely rob wolf packs of their kills.  On the contrary, in Katmai National Park and Preserve, wolves, even lone wolves, may manage to displace brown bears at carrion sites.  Despite the high animosity between the two species, most confrontations at kill sites or large carcasses end without bloodshed on either side.  Though conflict over carcasses is common, on rare occasions the two predators tolerate each other on the same kill.  To date, there are only a few cases of fully-grown wolves being killed by brown bears and none of wolves killing healthy adult brown bears.  Given the opportunity, however, both species will prey on the other's cubs.  Conclusively, the individual power of the bear against the collective strength of the wolf pack usually results in a long battle for kills or domination. In some areas, the grizzly bear also regularly displaces cougars (\"Puma concolor\") from their kills with some estimates showing cougars locally lose up to a third of their dietary energy to grizzly bears.  Cougars kill small bear cubs on rare occasions, but there was one report of a bear killing a cougar of unknown age and condition between 1993 and 1996.  Eurasian lynx (\"Lynx lynx\"), the largest type of lynx and the only one to regularly take large prey, is similarly an habitual victim of kleptoparasitism to brown bears throughout Eurasia.  Brown bears also co-exist with leopards (\"Panthera pardus\") (in very small remnant wild parts of the Middle East, Jammu and Kashmir, northeastern China and Primorsky Krai) and snow leopards (\"Panthera uncia\"), in several areas of northern central Asia and the Tibetan Plateau).  Although the brown bears’ interactions with these big cats are little known, they probably have similar relationships as grizzly bears do with cougars in North America.  Snow leopards and Tibetan blue bears are verified, however, to be a threat to one another's cubs. Smaller carnivorous animals are dominated by brown bears and generally avoid direct interactions with them, unless attempting to steal scraps of food.  Species which utilize underground or rock dens tend to be more vulnerable to predatory attacks by brown bears.  Several mustelids, including badgers, are not infrequently preyed upon, and seemingly even arboreal martens may be attacked (especially if unhealthy or caught in furbearer traps).  In North America, both species of otter have been known to be ambushed by brown bears when on land.  On the contrary, wolverines (\"Gulo gulo\") are known to have been persistent enough to fend off a grizzly bear as much as ten times their weight from a kill.  In some rare cases, wolverines have lost their lives to grizzly bears, and wolverines in Denali National Park will reportedly try to avoid encounters with grizzlies.  Beyond wolves, other canids may occasionally be killed around their den, most likely pups or kits, or adults if overly incautious near a carrion site, including coyotes (\"Canis latrans\"), multiple species of fox and raccoon dogs (\"Nyctereutes procyonoides\").  Medium-sized cats may also be rarely killed by brown bears.  Seals are on rare occasions killed by brown bears, including eye-witness accounts of Russian bears ambushing spotted (\"Phoca largha\") and harbor seals (\"Phoca vitulina\").  Consumption of ringed (\"Pusa hispida\" ) and bearded seal (\"Erignathus barbatus\") has been reported in the Mackenzie river delta, presumably via predation or scavenging of polar bear kills, as pinnipeds are not usually encountered as carrion from land. Brown bears usually dominate other bear species in areas where they coexist.  Due to their smaller size, American black bears are at a competitive disadvantage to brown bears in open, unforested areas.  Although displacement of black bears by brown bears has been documented, actual interspecific killing of black bears by brown bears has only occasionally been reported.  Confrontation is mostly avoided due to the black bear's diurnal habits and preference for heavily forested areas, as opposed to the brown bear's largely nocturnal habits and preference for open spaces.  Where they do not live in close proximity to grizzly bears, and especially where found near human habitations, American black bears may become to a larger extent nocturnal.  Brown bears may also kill Asian black bears, though the latter species probably largely avoids conflicts with the brown bear due to similar habits and habitat preferences to the American black species.  Brown bears will eat the fruit dropped from trees by the Asian black bear, as they themselves are too large and cumbersome to climb.  Improbably, in the Himalayas, brown bears are reportedly intimidated by Asian black bears in confrontations.  However, the Himalayan black bears are reportedly more aggressive towards humans than the region's Himalayan brown bear, and the latter is one of the smaller races of brown bear, though still somewhat larger than the black bear.  In Siberia, the opposite is true, and black bears are not known to attack people, but brown bears are.  Both black bears seem to be most vulnerable to predatory attacks by brown bears when the latter species leaves hibernation sooner in early spring and ambushes the smaller ursids in their dens. There has been a recent increase in interactions between brown bears and polar bears, theorized to be caused by climate change.  Brown bears have been seen moving increasingly northward into territories formerly claimed by polar bears.  Despite averaging somewhat smaller sizes, brown bears tend to dominate polar bears in disputes over carcasses, and dead polar bear cubs have been found in brown bear dens. Large herbivores, such as moose, buffalo and muskox may have an intolerance of brown bears due to their possible threat to vulnerable members of their herds or themselves; moose regularly charge grizzly bears in their calf's defense, but seldom are the bears killed.  Bison have been known to fatally injure lone grizzly bears in battles, and even a mountain goat (\"Oreamnos americanus\") was observed to do so with its horns, although herbivores are rarely a serious danger to brown bears. In the roughly 0.9 million years of its existence, brown bears have had to contend with multiple competing species, a majority of which went extinct at the end of the Pleistocene era.  In its much longer history in Eurasia, brown bears diverged from the two species of cave bear, the giant (\"U. spelaeus\") and small cave bears (\"U. rossicus\"), two species that it existed alongside in what is now Europe, for the larger form, and Central Asia and Siberia for the smaller cave bear.  The cave bears were similar in size to the larger forms of brown bear and polar bear alive today in terms of length, but were bulkier, with much higher-density skeletal remains, and presumably rather heavier than the modern brown bear, with the giant cave bear about the same length as a modern Kodiak bear but projected to be some 30% heavier.  Pleistocene-era brown bears appear to have been somewhat larger and more carnivorous than most modern forms based on skull dimensions.  The cave bears are usually deemed to have been highly herbivorous, to a greater extent than the brown bear, based on examinations of stable isotopes and dental morphology.  Recent studies, however, have shown that cave bears could have opportunistically adapted to a fairly omnivorous diet and consumed many herbivore carcasses.  Despite this, the dietary differences and differing habitat preferences (caves of course being much more habitually used in the cave species than the brown bear) allowed the three \"Ursus\" to persist at the same time in different parts of Eurasia. A more dangerous competitor was encountered by brown bears upon crossing the Bering Land Bridge about 100,000 years ago in the form of the short-faced bear, a long-legged bear that was estimated to have weighed about twice as much on average than a modern Kodiak bear.  Although at one time deemed a hyper-carnivorous predator that used its long legs to run down large prey, more recent studies have indicated that, like the modern brown bear, it was an omnivorous opportunist that probably scavenged many meals and presumably used its size and great height to intimidate large predators from kills.  Until it declined along with its food sources, dwindling into extinction, the short-faced bear presumably prevented the brown bear from spreading south, mainly through competition but also, to some extent, predation.  A single fossil ulna some 70,000-years-old found in Great Britain, deemed to come from an extremely large sub-adult Pleistocene polar bear (\"Ursus maritimus tyrannus\"), was projected to be the largest known form of the \"Ursus\" genus.  However, this cannot be ruled out as an extinct population of brown bear, based on the inconclusiveness of the only known fossil.  Other carnivorans during the Pleistocene that, based on their large size, were possibly occasional predators of early brown bears (at least females and cubs) were primarily big cats, including the cave lion (\"Panthera leo spelaea\") and the saber-toothed \"Machairodus\" in Eurasia and, perhaps rarely in southern Beringia, the American lion (\"Panthera leo atrox\") and the saber-toothed \"Smilodon\" and, in North America, \"Homotherium\".  Interactions with these large cats were probably not highly disparate from those that continue today between brown bears and the Siberian tiger.  Unlike most of these species, the brown bear was able to survive the Quaternary extinction event that concluded the ice age.  Presumably, this was due to its greater dietary and habitat plasticity and its adoption of a more extensively herbivorous diet, as most temperate-zone herbivores that may have provided meals for more specialized large land carnivores went extinct as the warming climate killed off their food source. It is likely that humans have caused the extinction and fragmentation of bear populations and their habitats since prehistorical times.  It has been shown, for instance, that bear populations from the Greater Caucasus and the Lesser Caucasus Mountains, separated by the densely populated Transcaucasian Depression, have been matrilineally isolated since the early Holocene era, i.e., after permanent human settlements appeared throughout the area.  While hunting by early humans was a (previously underestimated) factor in many of the Quartenary extinction events, a perhaps stronger factor in the survival of this species compared to many other northern Pleistocene bears is the brown bear's stronger genetic diversity.  In comparison, the giant cave bear appeared to enter a genetic bottleneck that started a population decline some 25,000 years before the species’ extinction. Brown bears usually avoid areas where extensive development or urbanization has occurred, unlike the smaller, more inoffensive American black bear which can adapt to peri-urban regions.  Under many circumstances, extensive human development may cause brown bears to alter their home ranges.  However, bears can easily lose their natural cautiousness upon being attracted to human-created food sources, such as garbage dumps, litter bins, and dumpsters.  Brown bears may even venture into human dwellings or barns in search of food as humans encroach into bear habitats.  In different parts of their distribution, brown bears sometimes kill and eat domesticated animals.  The saying \"a fed bear is a dead bear\" has come into use to popularize the idea that allowing bears to scavenge human garbage, such as trash cans and campers' backpacks, pet food, or other food sources that draw the bear into contact with humans, can result in a bear's death.  Where attractive bear food and concentrated human settlements overlap, human-bear conflict can create an ecological trap resulting in a lower apparent survival rate for brown bears caught in the trap, as well as attracting additional bears and thereby causing an overall population decline in the area, based on a 20062013 study performed in a southeastern British Columbian valley. When bears come to associate human activity with a \"food reward\", they are likely to continue to become emboldened; the likelihood of human-bear encounters increases, as they may return to the same location despite relocation.  Relocation of the bear has been used to separate the bear from the human environment, but it does not address the problem of the bear's newly learned association of humans with food or the environmental situations which created the human-habituated bear.  \"Placing a bear in habitat used by other bears may lead to competition and social conflict, and result in the injury or death of the less dominant bear.\"  Yellowstone National Park, a reserve located in the western United States, contains prime habitat for the grizzly bear (\"Ursus arctos horribilis\"), and due to the enormous number of visitors, human-bear encounters are common.  The scenic beauty of the area has led to an influx of people moving into the area.  In addition, because there are so many bear relocations to the same remote areas of Yellowstone, and because male bears tend to dominate the center of the relocation zone, female bears tend to be pushed to the boundaries of the region and beyond.  As a result, a large proportion of repeat offenders, bears that are killed for public safety, are females.  This creates a further depressive effect on an already endangered species.  The grizzly bear is officially described as \"threatened\" in the U.S. Though the problem is most significant with regard to grizzlies, these issues affect the other types of brown bears as well. In Europe, part of the problem lies with shepherds; over the past two centuries, many sheep and goat herders have gradually abandoned the more traditional practice of using dogs to guard flocks, which have concurrently grown larger.  Typically, they allow the herds to graze freely over sizeable tracts of land.  As bears reclaim parts of their range, they may eat livestock as sheep and goat are relatively easy for a bear to kill.  In some cases, the shepherds shoot the bear, thinking their livelihood is under threat.  Many are now better informed about the ample compensation available, and will make a claim when they lose livestock to a bear.  Another issue in several parts of their range in Europe is supplemental feeding stations where various kind of animal carrion is offered, which are set up mainly in Scandinavia and eastern Europe both to support the locally threatened species and so humans can enjoy watching bears that may otherwise prove evasive.  Despite that most stations were cautiously set in remote areas far from human habitations, some brown bears in such areas have become conditioned to associate humans with food and become excessively bold \"problem bears\".  Also supplemental feeding appears to cause no decrease in livestock predation. Native American tribes sympatric with brown bears often view them with a mixture of awe and fear.  North American brown bears have at times been so feared by the natives, that they were rarely hunted, especially alone.  At traditional grizzly hunts in some western tribes such as the Gwich’in the expedition was conducted with the same preparation and ceremoniality as intertribal warfare, and was never done except with a company of 4–10 warriors.  The tribe members who dealt the killing blow were highly esteemed among their compatriots.  Californian natives actively avoided prime bear habitat, and would not allow their young men to hunt alone, for fear of bear attacks.  During the Spanish colonial period, some tribes, instead of hunting grizzlies themselves, would seek aid from European colonists to deal with problem bears.  Many authors in the American west wrote of natives or voyageurs with lacerated faces and missing noses or eyes due to attacks from grizzlies. Many Native American tribes both respect and fear the brown bear.  In Kwakiutl mythology, black and brown bears became enemies when Grizzly Bear Woman killed Black Bear Woman for being lazy.  Black Bear Woman's children, in turn, killed Grizzly Bear Woman's own cubs.  Sleeping Bear Dunes is named after a Native American legend, where a female bear and her cub swam across Lake Michigan.  Exhausted from their journey, the bears rested on the shoreline and fell sound asleep.  Over the years, the sand covered them up, creating a huge sand dune. There are an average of two fatal attacks by bears per year in North America.  In Scandinavia, there are only four known cases since 1902 of bear encounters which have resulted in death.  The two most common causes for bear attack are surprise and curiosity.  Some types of bears, such as polar bears, are more likely to attack humans when searching for food, while American black bears are much less likely to attack.  Despite their boldness and potential for predation if the bear is hungry, polar bears rarely attack humans because they are infrequently encountered in the Arctic sea. The Alaska Science Center ranks the following as the most likely reasons for bear attacks: Aggressive behavior in brown bears is favored by numerous selection variables.  Unlike the smaller black bears, adult brown bears are too large and have improperly shaped claws to escape danger by climbing trees, so they respond to danger by standing their ground and warding off their attackers.  Increased aggressiveness also assists female brown bears in better ensuring the survival of their young to reproductive age.  Mothers defending cubs are the most prone to attacking, being responsible for 70% of brown bear-caused human fatalities in North America. Brown bears seldom attack humans on sight, and usually avoid people.  In Russia, it estimated that one in a thousand on-foot encounters with brown bears results in attack.  They are, however, unpredictable in temperament, and may attack if they are surprised or feel threatened.  Despite the low rate of attack, they appear to be the most dangerous northern carnivoran to humans, attacking more people on average annually than American black bears or northern races of Asian black bear and Siberian tigers, cougars or gray wolves.  Several large carnivorans from tropical regions of Africa and Asia may be more dangerous to humans than brown bears, including other ursids such as the sloth bear and Indian races of the Asian black bear.  Each of the latter species of bear may kill up to a dozen people annually in certain Indian districts, about as many in a relatively small area as all of the world's brown bears combined.  The high levels of human-directed aggression in these ursine bears is reportedly their evolution with once large numbers of predatory Bengal tigers (arguably itself the world's most dangerous carnivoran to humans even with its extreme decline in modern times, being statistically much more likely to attack than their northern Siberian cousins) and a very large human population, increasing the risk of surprising and angering a defensive black or sloth bear, the latter species often charging aggressively rather than fleeing if a surprise encounter occurs as do most other bear species. Sows with cubs account for many attacks on humans by brown bears in North America.  Habituated or food-conditioned bears can also be dangerous, as their long-term exposure to humans causes them to lose their natural shyness, and, in some cases, to associate humans with food.  Small parties of one or two people are more often attacked by brown bears than large groups, with only one known case of an attack on a group of six or more.  In that instance, it is thought that due to surprise, the grizzly bear may not have recognized the size of the group.  In the majority of attacks resulting in injury, brown bears precede the attack with a growl or huffing sound.  In contrast to injuries caused by American black bears, which are usually minor, brown bear attacks more often tend to result in serious injury and, in some cases, death.  Brown bears seem to confront humans as they would when fighting other bears: they rise up on their hind legs, and attempt to \"disarm\" their victims by biting and holding on to the lower jaw to avoid being bitten in turn.  Due to the bears' enormous physical strength, even a single bite or swipe can be deadly, as in tigers, with some human victims having had their heads completely crushed by a bear bite.  Most attacks occur in the months of July, August, and September, the time when the number of outdoor recreationalists, such as hikers or hunters, is higher.  People who assert their presence through noises tend to be less vulnerable, as they alert bears to their presence.  In direct confrontations, people who run are statistically more likely to be attacked than those who stand their ground.  Violent encounters with brown bears usually last only a few minutes, though they can be prolonged if the victims fight back.  In Alberta, two common behaviors by human hunters, imitating the calls of deer to attract them and carrying ungulate carcasses, seem to court aggressive behavior and lead to a higher rate of attack from grizzly bears. Attacks on humans are considered extremely rare in the former Soviet Union, though exceptions exist in districts where they are not as often pursued by hunters.  Siberian bears, for example, tend to be much bolder toward humans than their shyer, more persecuted European counterparts.  The delineation in Eurasia between areas where aggressiveness of bears tends to increase is the Ural Mountains, although the bears of eastern Europe are somewhat more aggressive than those of western Europe.  In 2008, a platinum mining compound in the Olyotorsky district of northern Kamchatka was besieged by a group of 30 bears, who killed two guards and prevented workers from leaving their homes.  Ten people a year on average are killed by brown bears in Russia, more than all the other parts of brown bear's international range combined, although Russia also holds more brown bears than all other parts of the world combined.  In Scandinavia, only three fatal attacks were recorded in the 20th century. In Japan, a large brown bear nicknamed \"\"Kesagake\"\" (袈裟懸け, \"kesa-style slasher\") made history for causing the worst bear attack in Japanese history at Tomamae, Hokkaidō during numerous encounters during December 1915.  It killed seven people and wounded three others (with possibly another three previous fatalities to its credit) before being gunned down after a large-scale beast-hunt.  Today, there is still a shrine at Rokusensawa (六線沢), where the event took place, in memory of the victims of the incident. Within Yellowstone National Park, injuries caused by grizzly attacks in developed areas averaged approximately one per year during the 1930s through to the 1950s, though it increased to four per year during the 1960s.  They then decreased to one injury every two years during the 1970s.  Between 1980 and 2002, there have been only two human injuries caused by grizzly bears in a developed area.  Though grizzly attacks were rare in the backcountry before 1970, the number of attacks increased to an average of approximately one per year during the 1970s, 1980s, and 1990s.  In Alberta, from 1960 to 1998, the number of attacks by grizzly bears ending in injury were nearly three times more common than attacks ending in injury by American black bears despite the black bear being an estimated 38 times more numerous in the province than grizzlies. A study by US and Canadian researchers has found pepper spray to be more effective at stopping aggressive bear behavior than guns, working in 92% of studied incidents versus 67% for guns.  Carrying pepper spray is highly recommended by many authorities when traveling in bear country; however, carrying two means of deterrent, one of which is a large caliber gun, is also advised.  Solid shotgun slugs, or three buckshot rounds, or a pistol of .44 caliber or more is suggested if a heavy hunting rifle is not available.  Guns remain a viable, last resort option to be used in defense of life from aggressive bears.  Too often, people do not carry a proper caliber weapon to neutralize the bear.  According to the Alaska Science Center, a 12 gauge shotgun with slugs has been the most effective weapon.  There have been fewer injuries as a result of only carrying lethal loads in the shotgun, as opposed to deterrent rounds.  State of Alaska Defense of Life or Property (DLP) laws require one to report the kill to authorities, and salvage the hide, skull, and claws.  A page at the State of Alaska Department of Natural Resources website offers information about how to \"select a gun that will stop a bear (12-gauge shotgun or .300 mag rifle).\" Campers are often told to wear bright colored red ribbons and bells, and carry whistles to ward off bears.  They are told to look for grizzly scat in camping areas, and be careful to carry the bells and whistles in those areas.  Grizzly scat is difficult to differentiate from black bear scat, as diet is in a constant state of flux depending on the availability of seasonal food items.  If a bear is killed near camp, the bear's carcass must be adequately disposed of, including entrails and blood, if possible.  Failure to move the carcass has often resulted in it attracting other bears and further exacerbating a bad situation.  Moving camps immediately is another recommended method. Brown bears often figure into the literature of Europe and North America, in particular that which is written for children. \" The Brown Bear of Norway\" is a Scottish fairy tale telling the adventures of a girl who married a prince magically turned into a bear, and who managed to get him back into a human form by the force of her love and after many trials and difficulties.  With \"Goldilocks and the Three Bears\", a story from England, the three bears are usually depicted as brown bears.  In German speaking countries, children are often told the fairytale of Snow White and Rose Red; the handsome prince in this tale has been transfigured into a brown bear.  In the United States, parents often read their preschool age children the book Brown Bear, Brown Bear, What Do You See?  to teach them their colors and how they are associated with different animals. The Russian bear is a common national personification for Russia (as well as the former Soviet Union) despite the country having no appointed national animal.  The brown bear is Finland's national animal. The grizzly bear is the state animal of Montana.  The California golden bear is the state animal of California.  Both animals are sub-species of the brown bear, and the species was extirpated from the latter state. The coat of arms of Madrid depicts a bear reaching up into a \"madroño\" or strawberry tree (\"Arbutus unedo\") to eat some of its fruit, whereas the Swiss city of Bern's coat of arms also depicts a bear and the city's name is popularly thought to derive from the German word for bear.  The brown bear is depicted on the reverse of the Croatian 5 kuna coin, minted since 1993. The Bundesliga club Bayern Munich has a brown bear mascot named Berni.  The National Football League (NFL) franchise in Chicago, Illinois, is named the Bears.  In this context, no differentiation between black and brown bears is needed.  The school mascot for George Fox University, Brown University, the University of California, Los Angeles, the University of California, Berkeley, the University of California, Riverside, and the University of Alberta is the brown bear. In the town of Prats de Molló, in Vallespir, southern France, a \"bear festival\" (\"festa de l'ós\") is celebrated annually at the beginning of spring, in which the locals dress up as bears, cover themselves with soot or coal and oil, and \"attack\" the onlookers, attempting to get everyone dirty.  The festival ends with the \"ball de l'os\" (bear dance).\n\nIndustrial Revolution The Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840.  This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power, the development of machine tools and the rise of the factory system. Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested; the textile industry was also the first to use modern production methods. The Industrial Revolution began in Great Britain and many of the technological innovations were British.  By the mid-18th century Britain controlled a global trading empire with colonies in North America and political influence over the Indian subcontinent by the East India Company.  The development of trade and the rise of business were major causes of the Industrial Revolution. The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way.  In particular, average income and population began to exhibit unprecedented sustained growth.  Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries. GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies.  Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants. The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes.  Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830.  Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800.  Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France. An economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured.  Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth.  Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution.  These new innovations included new steel making processes, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories. The earliest recorded use of the term \"Industrial Revolution\" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise.  In his 1976 book \"\", Raymond Williams states in the entry for \"Industry\": \"The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century.\"  The term \"Industrial Revolution\" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of \"la révolution industrielle\".  Friedrich Engels in \"The Condition of the Working Class in England\" in 1844 spoke of \"an industrial revolution, a revolution which at the same time changed the whole of civil society\".  However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then.  Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term. Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term \"revolution\" is a misnomer.  This is still a subject of debate among some historians. The commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century.  By the 1830s the following gains had been made in important technologies: The share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831.  Value added by the British woollen industry was 14.1% in 1801.  Cotton factories in Britain numbered approximately 900 in 1797.  In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800.  In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800.  In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain.  In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years. Wages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about one-sixth those in India in 1770, when overall productivity in Britain was about three times higher than in India. Parts of India, China and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD.  In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption.  In the 15th century China began to require households to pay part of their taxes in cotton cloth.  By the 17th century almost all Chinese wore cotton clothing.  Almost everywhere cotton cloth could be used as a medium of exchange.  In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers.  Some merchants also owned small weaving workshops.  India produced a variety of cotton cloth, some of exceptionally fine quality. The Age of Discovery was followed by a period of colonialism beginning around the 16th century.  Following the opening of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr.  VOC) or Dutch East India Company and the British founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe.  One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe.  By the mid 1760s cloth was over three-quarters of the East India Company's exports.  Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century. By 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat.  Earlier European attempts at cotton spinning and weaving were in 12th century Italy and 15th century southern Germany, but these industries eventually ended when the supply of cotton was cut off.  The Moors in Spain grew, spun and wove cotton beginning around the 10th century. British cloth could not compete with Indian cloth because India's labor cost was approximately one-fifth that of Britain's.  In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India. The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft.  Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew. On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system.  Occasionally the work was done in the workshop of a master weaver.  Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials.  In the off season the women, typically farmers' wives, did the spinning and the men did the weaving.  Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.  The flying shuttle patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving.  It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors. Lewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness.  The technology was developed with the help of John Wyatt of Birmingham.  Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey.  In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines.  This operated until about 1764.  A similar mill was built by Daniel Bourn in Leominster, but this burnt down.  Both Lewis Paul and Daniel Bourn patented carding machines in 1748.  Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill.  Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule. In 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770.  It was the first practical spinning frame with multiple spindles.  The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting.  It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners.  The jenny produced a lightly twisted yarn only suitable for weft, not warp. The spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769.  The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright.  For each spindle, the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle.  The roller spacing was slightly longer than the fibre length.  Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread.  The top rollers were leather-covered and loading on the rollers was applied by a weight.  The weights kept the twist from backing up before the rollers.  The bottom rollers were wood and metal, with fluting along the length.  The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain.  A horse powered the first factory to use the spinning frame.  Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name. Samuel Crompton's Spinning Mule was introduced in 1779.  Mule implies \"hybrid\" because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating.  Crompton's mule was able to produce finer thread than hand spinning and at a lower cost.  Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities. Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785.  In 1776 he patented a two-man operated loom which was more conventional.  Cartwright built two factories; the first burned down and the second was sabotaged by his workers.  Cartwright's loom design had several flaws, the most serious being thread breakage.  Samuel Horrocks patented a fairly successful loom in 1813.  Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co. The demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed.  Eli Whitney responded to the challenge by inventing the inexpensive cotton gin.  With a cotton gin a man could remove seed from as much upland cotton in one day as would have previously taken a woman working two months to process at one pound per day. These advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright.  He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines.  He created the cotton mill which brought the production processes together in a factory, and he developed the use of power – first horse power and then water powerwhich made cotton manufacture a mechanised industry.  Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly.  Before long steam power was applied to drive textile machinery.  Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories. Although mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine woven cloth still could not equal the quality of hand woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high tread counts.  However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand spun and woven fabric in low-wage India, eventually destroying the industry. The earliest attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton.  Productivity improvement in wool spinning during the Industrial Revolution was significant, but was far less that that of cotton. Arguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721.  Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, since the silk industry there was a closely guarded secret, the state of the industry there is unknown.  Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition.  In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London. A major change in the metal industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal.  For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was more abundant than wood. In the smelting and refining of iron, coal and to a lesser degree coke produced inferior iron than charcoal because the impurities in coal, at least until solutions to contamination were developed.  Use of coal in smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas.  These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal.  This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal.  This technology was applied to lead from 1678 and to copper from 1687.  It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace.  (The foundry cupola is a different (and later) innovation.) This was followed by Abraham Darby, who made great strides using coke to fuel his blast furnaces at Coalbrookdale in 1709.  However, the coke pig iron he made was used mostly for the production of cast iron goods, such as pots and kettles.  He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.  Coke pig iron was hardly used to produce wrought iron in forges until the mid-1750s, when his son Abraham Darby II built Horsehay and Ketley furnaces (not far from Coalbrookdale).  By then, coke pig iron was cheaper than charcoal pig iron.  Since cast iron was becoming cheaper and more plentiful, it began being a structural material following the building of the innovative Iron Bridge in 1778 by Abraham Darby III. Wrought iron for smiths to forge into consumer goods was still made in finery forges, as it long had been.  However, new processes were adopted in the ensuing years.  The first is referred to today as potting and stamping, but this was superseded by Henry Cort's puddling process.  Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784.  Rolling replaced hammering for consolidating wrought iron and expelling some of the dross.  Rolling was 15 times faster than hammering with a trip hammer.  Roller mills were first used for making sheets, but later were developed for rolling structural shapes such as angles and rails. Puddling produced a structural grade iron at a relatively low cost.  Puddling was a means of decarburizing pig iron by slow oxidation as the iron was manually stirred using a long rod.  The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler.  When the glob was large enough the puddler would remove it.  Puddling was backbreaking and extremely hot work.  Few puddlers lived to be 40.  Puddling was done in a reverberatory furnace, allowing coal or coke to be used as fuel.  The puddling process continued to be used until the late 19th century when iron was being displaced by steel.  Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Up to that time, British iron manufacturers had used considerable amounts of imported iron to supplement native supplies.  This came principally from Sweden from the mid-17th century and later also from Russia from the end of the 1720s.  However, from 1785, imports decreased because of the new iron making technology, and Britain became an exporter of bar iron as well as manufactured wrought iron consumer goods. Hot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron.  By using waste exhaust heat to preheat combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coal or two-thirds using coke; however, the efficiency gains continued as the technology improved.  Hot blast also raised the operating temperature of furnaces, increasing their capacity.  Using less coal or coke meant introducing fewer impurities into the pig iron.  This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably. Two decades before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs.  Benjamin Huntsman developed his crucible steel technique in the 1740s.  The raw material for this was blister steel, made by the cementation process. The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items.  The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries. The development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind.  In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam.  By 1815 steam power had grown to 210,000 hp. The first commercially successful industrial use of steam power was due to Thomas Savery in 1698.  He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its \"brand name\", \"The Miner's Friend\").  Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes.  Savery pumps continued to be produced until the late 18th century. The first successful piston steam engine was introduced by Thomas Newcomen before 1712.  A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a lot of capital to build, and produced about 5 hp .  They were also used to power municipal water supply pumps.  They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper.  Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century.  By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden.  A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad.  In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements.  A total of 1,454 engines had been built by 1800. A fundamental change in working principles was brought about by Scotsman James Watt.  With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber.  The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam.  Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency.  These improvements increased engine efficiency so that Boulton & Watts engines used only 20–25% as much coal per horsepower-hour as Newcomen's.  Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795. By 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill.  Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from 5 to . Until about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine.  Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere.  High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats. The development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines. Small industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century.  These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery. Pre-industrial machinery was built by various craftsmen – millwrights built water and wind mills, carpenters made wooden framing, and smiths and turners made metal parts.  Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time.  As the Industrial Revolution progressed, machines with metal parts and frames became more common.  Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts.  There was also the need for precision in making parts.  Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners. The demand for metal parts led to the development of several machine tools.  They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms. Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels.  Consequently, the use of metal was kept to a minimum.  Hand methods of production were very laborious and costly and precision was difficult to achieve. The first large machine tool was the cylinder boring machine used for boring the large-diameter cylinders on early steam engines.  The planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century.  Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century. Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich.  He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen.  In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size Lathe in the UK.  He was hired away by Joseph Bramah for the production of high security metal locks that required precision craftsmanship.  Bramah patented a lathe that had similarities to the slide rest lathe.  Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw.  Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template.  The slide rest lathe was called one of history's most important inventions, although not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears. Maudslay left Bramah's employment and set up his own shop.  He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills.  These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability.  The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth. James Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds.  Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement. The impact of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts.  The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century. In the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added. The large scale production of chemicals was an important development during the Industrial Revolution.  The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746.  He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead.  Instead of making a small amount each time, he was able to make around 100 lb in each of the chambers, at least a tenfold increase. The production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate.  The Leblanc process was a reaction of sulphuric acid with sodium chloride to give sodium sulphate and hydrochloric acid.  The sodium sulphate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulphide.  Adding water separated the soluble sodium carbonate from the calcium sulphide.  The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulphide was a useless waste product).  Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes. These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes.  Sodium carbonate had many uses in the glass, textile, soap, and paper industries.  Early uses for sulphuric acid included pickling (removing rust) iron and steel, and for bleaching cloth. The development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk.  Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world. After 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry.  Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques.  British scientists by contrast, lacked research universities and did not train advanced students; instead the practice was to hire German-trained chemists. In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades.  This process involves sintering a mixture of clay and limestone to about 1400 °C , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete.  Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel.  Cement was used on a large scale in the construction of the London sewerage system a generation later. Another major industry of the later Industrial Revolution was gas lighting.  Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers.  The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution.  The first gas lighting utilities were established in London between 1812 and 1820.  They soon became one of the major consumers of coal in the UK.  Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil.  Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before. A new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century.  In 1832 this process was used by the Chance Brothers to create sheet glass.  They became the leading producers of window and plate glass.  This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings.  The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure. A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France.  The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London.  Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today. The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes. The British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy.  However, per-capita food supply in Europe was stagnant or delining and did not improve in some parts of Europe until the late 18th century. Industrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine. Jethro Tull invented an improved seed drill in 1701.  It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth.  This was important because the yield of seeds harvested to seeds planted at that time was around four or five.  Tull's seed drill was very expensive and not very reliable and therefore did not have much of an impact.  Good quality seed drills were not produced until the mid 18th century. Joseph Foljambe's \"Rotherham plough\" of 1730 was the first commercially successful iron plough.  The threshing machine, invented by Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour.  It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots. Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters. Coal mining in Britain, particularly in South Wales, started early.  Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted.  In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill.  Shaft mining was done in some areas, but the limiting factor was the problem of removing water.  It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine).  In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity.  The introduction of the steam pump by Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted.  These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable.  The Cornish engine, developed in the 1810s, was much more efficient than the Watt Steam engine. Coal mining was very dangerous owing to the presence of firedamp in many coal seams.  Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson.  However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light.  Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century.  Conditions of work were very poor, with a high casualty rate from rock falls. Other developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1300 years. At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea.  Wagon ways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed.  Animals supplied all of the motive power on land, with sails providing the motive power on the sea.  The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network.  Raw materials and finished products could be moved more quickly and cheaply than before.  Improved transportation also allowed new ideas to spread quickly. Canals were the first technology to allow bulk materials to be economically transported long distances inland.  This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart. Building of canals dates to ancient times.  The Grand Canal in China, \"the world's largest artificial waterway and oldest canal still in existence,\" parts of which were started between the 6th and 4th centuries BC, is 1121 mi long and links Hangzhou with Beijing. In the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country.  Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater.  From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ as of 2013 ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half.  This success helped inspire a period of intense canal building, known as Canal Mania.  New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively. By the 1820s a national network was in existence.  Canal construction served as a model for the organisation and methods later used to construct the railways.  They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on.  The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port.  However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper. Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain. Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads.  Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust.  New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816.  The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country.  Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses.  Lighter goods were conveyed by smaller carts or by teams of pack horse.  Stage coaches carried the rich, and the less wealthy could pay to ride on carriers carts. Reducing friction was one of the major reasons for the success of railroads compared to wagons.  This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England. “ A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton.  A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration.  Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together.  A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.” Railways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high pressure steam engine also around 1800. Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal.  These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline.  The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used).  Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs.  See: Metallurgy Steam locomotives began being built after the introduction of high pressure steam engines after the expiration of the Boulton and Watt patent in 1800.  High pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water.  They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines.  A few of these early locomotives were used in mines.  Steam-hauled public railways began with the Stockton and Darlington Railway in 1825. The rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of Hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity the blast furnace. On 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington.  The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool.  The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight.  The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania. Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution.  After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories. Prior to the Industrial Revolution most of the workforce was employed in agriculture, either as self-employed farmers as land owners or tenants, or as landless agricultural labourers.  It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing.  Households also spun and wove for market production.  At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods. In Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as \"cottage industry\", was being practiced.  Typical putting out system goods included spinning and weaving.  Merchant capitalist typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods.  Embezzlement of supplies by workers and poor quality were common problems.  The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system. Some early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers.  Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories. The majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans.  They typically worked for 12 to 14 hours per day with only Sundays off.  It was common for women take factory jobs seasonally during slack periods of farm work.  Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers.  Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity.  (See: British Agricultural Revolution, Threshing machine) The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx, however, he recognized the increase in productivity made possible by technology. Women's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women.  Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th century England, it lowered the status of women as they lost much of their economic importance.  Clark argues that in 16th century England, women were engaged in many aspects of industry and agriculture.  The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates.  Their useful economic roles gave them a sort of equality with their husbands.  However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work.  Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs.  Capitalism, therefore, had a negative effect on powerful women. In a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation.  Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history.  In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households.  The second stage was the \"family wage economy\" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children.  The third or modern stage is the \"family consumer economy,\" in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption. Some economists, such as Robert E. Lucas, Jr., say that the real impact of the Industrial Revolution was that \"for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility.\"  Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. The effects on living conditions the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s.  A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards.  During 1813–1913, there was a significant increase in worker wages. Chronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century.  Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain.  The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined a few years by the mid 18th century. The initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices.  In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus.  This condition is called the Malthusian trap, and it finally started to overcome by transportation improvements, such as canals, improved roads and steamships.  Railroads and steamships were introduced near the end of the Industrial Revolution. The very rapid growth in population in the 19th century in the cities included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London.  The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms.  Private renting from housing landlords was the dominant tenure.  P. Kemp says this was usually of advantage to tenants.  People moved in so rapidly that there was not enough capital to build adequate housing for everyone, so low income newcomers squeezed into increasingly overcrowded slums.  Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults.  Cholera from polluted water and typhoid were endemic.  Unlike rural areas, there were no famines such as devastated Ireland in the 1840s. A large exposé literature grew up condemning the unhealthy conditions.  By far the most famous publication was by one of the founders of the Socialist movement, \"The Condition of the Working Class in England\" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors.  These shanty towns had narrow walkways between irregularly shaped lots and dwellings.  There were no sanitary facilities.  Population density was extremely high.  Not everyone lived in such poor conditions.  The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions. Conditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction.  In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved.  For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house. Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating.  Meeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, Wedgwood created goods such as tableware, which was starting to become a common feature on dining tables. According to Robert Hughes in \"The Fatal Shore\", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740.  The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million.  Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s.  Europe's population increased from about 100 million in 1700 to 400 million by 1900. The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income. In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry.  Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines.  As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children.  Also, harsh working conditions were prevalent long before the Industrial Revolution took place.  Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution. Industrialisation led to the creation of the factory.  The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories.  Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed \"Cottonopolis\", and the world's first industrial city.  Manchester experienced a six-times increase in its population between 1771 and 1831.  Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there. For much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs.  Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler. In other industries the transition to factory production was not so divisive.  Some industrialists themselves tried to improve factory and living conditions for their workers.  One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement. By 1746 an integrated brass mill was working at Warmley near Bristol.  Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods.  Housing was provided for workers on site.  Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system. The Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although \"infant\" mortality rates were reduced markedly.  There was still limited opportunity for education and children were expected to work.  Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers.  This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries.  In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children. Child labour existed before the Industrial Revolution but with the increase in population and education it became more visible.  Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage.  Children as young as four were employed.  Beatings and long hours were common, with some child coal miners and hurriers working from 4 am until 5 pm.  Conditions were dangerous, with some children killed when they dozed off and fell into the path of the carts, while others died from gas explosions.  Many children developed lung cancer and other diseases and died before the age of 25.  Workhouses would sell orphans and abandoned children as \"pauper apprentices\", working without wages for board and lodging.  Those who ran away would be whipped and returned to their masters, with some masters shackling them to prevent escape.  Children employed as mule scavengers by cotton mills would crawl under machinery to pick up cotton, working 14 hours a day, six days a week.  Some lost hands or limbs, others were crushed under the machines, and some were decapitated.  Young girls worked at match factories, where phosphorus fumes would cause many to develop phossy jaw.  Children employed at glassworks were regularly burned and blinded, and those working at potteries were vulnerable to poisonous clay dust. Reports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight.  The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare. Politicians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour.  In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours.  Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult.  About ten years later, the employment of children and women in mining was forbidden.  Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century. The Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of \"combinations\" or trade unions to help advance the interests of working people.  The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production.  Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production.  Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining. The main method the unions used to effect change was strike action.  Many strikes were painful events for both sides, the unions and the management.  In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824.  Even after this, unions were still severely restricted. In 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage.  That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s.  They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six.  In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done.  James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia.  They became known as the Tolpuddle Martyrs.  In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice.  Its \"Charter\" of reforms received over three million signatures but was rejected by Parliament without consideration. Working people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship.  Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class. Unions slowly overcame the legal restrictions on the right to strike.  In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain. Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party. The rapid industrialisation of the English economy cost many craft workers their jobs.  The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation.  Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver.  Many such unemployed workers, weavers and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery.  These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure.  The first attacks of the Luddite movement began in 1811.  The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry.  Those rioters who were caught were tried and hanged, or transported for life. Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances.  Threshing machines were a particular target, and hayrick burning was a popular activity.  However, the riots led to the first formation of trade unions, and further pressure for reform. The traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved. Cheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in regions where it was grown, with little raw cotton available for export.  Consequently, prices of raw cotton rose.  At the beginning of the Industrial Revolution, cotton was grown in small plots in the Old World — the uncrowned Americas were far better able to recruit available land with the potential for new cotton production.  Some cotton had been grown in the West Indies, particularly in Haiti, but Haitian cotton production was halted by the Haitian Revolution in 1791.  The invention of the cotton gin in 1793 allowed Georgia green seeded cotton to be profitable, leading to widespread growth of cotton plantations in the United States and Brazil.  (A strain of cotton seed brought to Natchez in 1806, \"Gossypium hirsutum\", would become the parent genetic material for over 90% of world cotton production to come: it produced bolls that were three to four times faster to pick.)  The Americas, particularly the U.S., had labor shortages and high priced labor, which made slavery attractive.  America's cotton plantations were highly efficient and profitable, and able to keep up with demand.  The U.S. Civil war created a \"cotton famine\" that lead to increased production in other areas of the world, including new colonies in Africa. The origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution.  The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste.  The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash.  An Alkali inspector and four sub-inspectors were appointed to curb this pollution.  The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision. The manufactured gas industry began in British cities in 1812–1820.  The technique used produced highly toxic effluent that was dumped into sewers and rivers.  The gas companies were repeatedly sued in nuisance lawsuits.  They usually lost and modified the worst practices.  The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish.  Finally, Parliament wrote company charters to regulate toxicity.  The industry reached the US around 1850 causing pollution and lawsuits. In industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms.  Typically the highest priority went to water and air pollution.  The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs.  It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke.  Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke.  It also provided for sanctions against factories that emitted large amounts of black smoke.  The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash and gritty particles and to empower local authorities to impose their own regulations. The application of steam power to the industrial processes of printing supported a massive expansion of newspaper and popular book publishing, which reinforced rising literacy and demands for mass political participation. During the Industrial Revolution, the life expectancy of children increased dramatically.  The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829. The growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas.  In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century).  Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million. The Industrial Revolution on Continental Europe came a little later than in Great Britain.  In many industries, this involved the application of technology developed in Britain in new places.  Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities.  By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England.  The German, Russian and Belgian governments all provided state funding to the new industries.  In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted. Belgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French speaking southern Belgium) was the first region to follow the British model successfully.  Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi.  The leader was a transplanted Englishman John Cockerill.  His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825. Wallonia exemplified the radical evolution of industrial expansion.  Thanks to coal (the French word \"houille\" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain.  But it is also pointed out by many researchers, with its \"Sillon industriel\", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, (...) there was a huge industrial development based on coal-mining and iron-making...'.  Philippe Raxhon wrote about the period after 1830: \"It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain.\"  \"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth making town of Ghent.\"  Michel De Coster, Professor at the Université de Liège wrote also: \"The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory (...) But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated.\" Wallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape.  At the left, the \"Sillon industriel\", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920).  Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different.  In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say: The industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium.  During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres (...) at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world.  By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain.  Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent (...) Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910.  Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast.  During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region.  Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory.  Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows. The industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries.  Notably, most French historians argue France did not go through a clear \"take-off\".  Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries.  However, some stages were identified by Maurice Lévy-Leboyer: Based on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century.  At first the production of dyes based on aniline was critical. Germany's political disunity – with three dozen states – and a pervasive conservatism made it difficult to build railways in the 1830s.  However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders.  Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways.  In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry.  Observers found that even as late as 1890, their engineering was inferior to Britain's.  However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth.  Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen.  By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France During the period 1790–1815 Sweden experienced two parallel economic movements: an \"agricultural revolution\" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a \"protoindustrialisation\", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter.  This led to economic growth benefiting large sections of the population and leading up to a \"consumption revolution\" starting in the 1820s. During 1815–1850 the protoindustries developed into more specialised and larger industries.  This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland.  Several important institutional changes took place in this period, such as free and mandatory schooling introduced 1842 (as first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848. During 1850–1890, Sweden experienced a veritable explosion in export, dominated by crops, wood and steel.  Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873. During 1890–1930, Sweden experienced the second industrial revolution.  New industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile. The industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West.  The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development.  It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan). In 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways.  The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up.  The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories.  Education was expanded and Japanese students were sent to study in the west. Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas. During the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy.  The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period. Important American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US.  The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century. Oliver Evans invented an automated flour mill in the mid 1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon.  This is considered to be the first modern materials handling system an important advance in the progress toward mass production. The United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s.  As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers.  The newer water-powered production lines proved more economical than horse-drawn production.  In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest. Thomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future.  This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years.  Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill. In 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island.  He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge.  After founding Slater's Mill, he went on to own 13 textile mills.  Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.)  The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of \"America's Hardest-Working River', the Blackstone.  The Blackstone River and its tributaries, which cover more than 45 mi from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution.  At its peak over 1100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development. Merchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810.  Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company.  Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory.  After his death in 1817, his associates built America's first planned factory town, which they named after him.  This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States.  Lowell, Massachusetts, using 5.6 mi of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution.  The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain.  However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour. A major U.S. contribution to industrialization was the development of techniques to make interchangeable parts from metal.  Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms.  The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory.  Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy.  The milling machine, a fundamental machine tool, is believed to have been invented by Ely Whitney, who was a government contractor who built firearms as part of this program.  Another important invention was the Blanchard lathe, invented by Thomas Blanchard.  The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks.  The use of machinery and the techniques for producing standardized and interchangeable parts became known as the American system of manufacturing Precision manufacturing techniques made it possible to build machines that mechanized the shoe industry.  and the watch industry.  The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches. Steel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a \"Second Industrial Revolution\", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities.  However, it only became widely available in the 1870s after the process was modified to produce more uniform quality.  Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century. This Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany. The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation. A new revolution began with electricity and electrification in the electrical industries.  The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s. By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets. The causes of the Industrial Revolution were complicated and remain a topic for debate, with some historians believing the Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century.  The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories.  The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century.  A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development. Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine.  However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution. Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates.  He explains that the model for standardised mass production was the printing press and that \"the archetypal model for the industrial era was the clock\".  He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine. The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain.  In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them.  Internal tariffs were abolished by Henry VIII of England, they survived in Russia till 1753, 1789 in France and 1839 in Spain. Governments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor.  The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology.  In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development.  However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors.  Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power. One question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East, or at other times like in Classical Antiquity or the Middle Ages.  Numerous factors have been suggested, including education, technological changes (see Scientific Revolution in Europe), \"modern\" government, \"modern\" work attitudes, ecology, and culture. China was the world's most technological advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Exploration, by which time China banned imports and denied entry to foreigners.  China was also a totalitarian society.  Modern estimates of per capita income on Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars.  India was essentially feudal, politically fragmented and not as economically advanced as Western Europe. Historians such as David Landes and Max Weber credit the different belief systems in Asia and Europe with dictating where the revolution occurred.  The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought.  Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews.  Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea. Regarding India, the Marxist historian Rajani Palme Dutt said: \"The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain.\"  In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore.  In addition, the economy was highly dependent on two sectors – agriculture of subsistence and cotton, and there appears to have been little technical innovation.  It is believed that the vast amounts of wealth were largely stored away in palace treasuries by totalitarian monarchs prior to the British take over. Economic historian Joel Mokyr has argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities.  This is what set Europe apart from the technologically advanced, large unitary empires such as China and India.  China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the industrial revolution would occur in Europe, not China or India.  In Europe, political fragmentation was coupled with an \"integrated market for ideas\" where Europe's intellectuals used the lingua franca of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters. Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the industrial revolution.  Key factors fostering this environment were: (1) The period of peace and stability which followed the unification of England and Scotland; (2) no trade barriers between England and Scotland; (3) the rule of law (enforcing property rights and respecting the sanctity of contracts); (4) a straightforward legal system that allowed the formation of joint-stock companies (corporations); (5) absence of tolls, which had largely disappeared from Britain by the 15th century, but were an extreme burden on goods elsewhere in the world, and (6) a free market (capitalism). Geographical and natural resource advantages of Great Britain were the fact that it had extensive coast lines and many navigable rivers in an age where water was the easiest means of transportation and having the highest quality coal in Europe. There were two main values that really drove the Industrial Revolution in Britain.  These values were self-interest and an entrepreneurial spirit.  Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution.  These advancements also greatly benefitted the British society as a whole.  Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions. The debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries.  Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment.  However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution.  Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output. Instead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia.  Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy).  Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods.  The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe.  This was further aided by Britain's geographical position – an island separated from the rest of mainland Europe. Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed.  It had a dense population for its small geographical size.  Enclosure of common land and the related agricultural revolution made a supply of this labour readily available.  There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands.  Local supplies of coal, iron, lead, copper, tin, limestone and water power, resulted in excellent conditions for the development and expansion of industry.  Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry. The stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution.  Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism.  (This point is also made in Hilaire Belloc's \"The Servile State\".) The French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, \"Letters on the English\" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours.  \"Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind.  There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts.  There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker’s word.  If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another’s throats; but as there are such a multitude, they all live happy and in peace.\" Britain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%.  Seventy percent of European urbanisation happened in Britain 1750–1800.  By 1800, only the Netherlands was more urbanised than Britain.  This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch.  The latter compete with land grown to feed people while mined materials do not.  Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised.  A workhorse needs 3 to for fodder while even early steam engines produced four times more mechanical energy. In 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise.  In the 18th century, it was the only European country whose cities and population shrank.  Without coal, Britain would have run out of suitable river sites for mills by the 1830s. Economic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur.  These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. Knowledge of innovation was spread by several means.  Workers who were trained in the technique might move to another employer or might be poached.  A common method was for someone to make a study tour, gathering information where he could.  During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy.  In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods.  Study tours were common then, as now, as was the keeping of travel diaries.  Records made by industrialists and technicians of the period are an incomparable source of information about their methods. Another means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' (\"i.e.\" science) and often its application to manufacturing.  The Lunar Society flourished from 1765 to 1809, and it has been said of them, \"They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution\".  Other such societies published volumes of proceedings and transactions.  For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual \"Transactions\". There were publications describing technology.  Encyclopaedias such as Harris's \"Lexicon Technicum\" (1704) and Abraham Rees's \"Cyclopaedia\" (1802–1819) contain much of value.  \"Cyclopaedia\" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings.  Foreign printed sources such as the \"Descriptions des Arts et Métiers\" and Diderot's \"Encyclopédie\" explained foreign methods with fine engraved plates. Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents.  Foreign periodicals, such as the \"Annales des Mines\", published accounts of travels made by French engineers who observed British methods on study tours. Another theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work.  The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War.  Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures. Dissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities).  When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education.  The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies. Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved.  While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen.  Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century. During the Industrial Revolution an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement.  Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes.  Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley.  The movement stressed the importance of \"nature\" in art and language, in contrast to \"monstrous\" machines and factories; the \"Dark satanic mills\" of Blake's poem \"And did those feet in ancient time\".  Mary Shelley's novel \"Frankenstein\" reflected concerns that scientific progress might be two-edged.  French Romanticism likewise was highly critical of industry.\n\nSpacetime In physics, spacetime is any mathematical model that fuses the three dimensions of space and the one dimension of time into a single 4‑dimensional continuum.  Spacetime diagrams are useful in visualizing and understanding relativistic effects such as how different observers perceive \"where\" and \"when\" events occur. Until the turn of the 20th century, the assumption had been that the three-dimensional geometry of the universe (its description in terms of locations, shapes, distances, and directions) was distinct from time (the measurement of when events occur within the universe).  However, Albert Einstein's 1905 \"special theory of relativity\" postulated that the speed of light through empty space has one definite value—a constant—that is independent of the motion of the light source.  Einstein's equations described important consequences of this fact: The distances and times between pairs of events vary when measured in different \"inertial frames of reference\" (separate vantage points that aren’t being subjected to g‑forces but have different velocities). Einstein's theory was framed in terms of kinematics (the study of moving bodies), and showed how quantification of distances and times varied for measurements made in different reference frames.  His theory was a breakthrough advance over Lorentz's 1904 theory of electromagnetic phenomena and Poincaré's electrodynamic theory.  Although these theories included equations identical to those that Einstein introduced (i.e. the Lorentz transformation), they were essentially ad hoc models proposed to explain the results of various experiments—including the famous Michelson–Morley interferometer experiment—that were extremely difficult to fit into existing paradigms. In 1908, Hermann Minkowski—once one of the math professors of a young Einstein in Zurich—presented a geometric interpretation of special relativity that fused time and the three spatial dimensions of space into a single four-dimensional continuum now known as Minkowski space.  A key feature of this interpretation is the definition of a \"spacetime interval\" that combines distance and time.  Although measurements of distance and time between events differ for measurements made in different reference frames, the spacetime interval is independent of the inertial frame of reference in which they are recorded. Minkowski's geometric interpretation of relativity was to prove vital to Einstein's development of his 1915 general theory of relativity, wherein he showed that spacetime becomes curved in the presence of mass or energy. <a href=\"%23Summary%20Definitions\">Click here for a brief section summary </a> Non-relativistic classical mechanics treats time as a universal quantity of measurement which is uniform throughout space and which is separate from space.  Classical mechanics assumes that time has a constant rate of passage that is independent of the state of motion of an observer, or indeed of anything external.  Furthermore, it assumes that space is Euclidean, which is to say, it assumes that space follows the geometry of common sense. In the context of special relativity, time cannot be separated from the three dimensions of space, because the observed rate at which time passes for an object depends on the object's velocity relative to the observer.  General relativity, in addition, provides an explanation of how gravitational fields can slow the passage of time for an object as seen by an observer outside the field. In ordinary space, a position is specified by three numbers, known as dimensions.  In the Cartesian coordinate system, these are called x, y, and z.  A position in spacetime is called an \"event\", and requires four numbers to be specified: the three-dimensional location in space, plus the position in time (Fig. 1).  Spacetime is thus four dimensional.  An event is something that happens instantaneously at a single point in spacetime, represented by a set of coordinates \"x\", \"y\", \"z\" and \"t\". The word \"event\" used in relativity should not be confused with the use of the word \"event\" in normal conversation, where it might refer to an \"event\" as something such as a concert, sporting event, or a battle.  These are not mathematical \"events\" in the way the word is used in relativity, because they have finite durations and extents.  Unlike the analogies used to explain events, such as firecrackers or lightning bolts, mathematical events have zero duration and represent a single point in space. The path of a particle through spacetime can be considered to be a succession of events.  The series of events can be linked together to form a line which represents a particle's progress through spacetime.  That line is called the particle's \"world line\". Mathematically, spacetime is a \"manifold\", which is to say, it appears locally \"flat\" near each point in the same way that, at small enough scales, a globe appears flat.  An extremely large scale factor, formula_1 (conventionally called the \"speed of light\") relates distances measured in space with distances measured in time.  The magnitude of this scale factor (nearly in space being equivalent to 1 second in time), along with the fact that spacetime is a manifold, implies that at ordinary, non-relativistic speeds and at ordinary, human-scale distances, there is little that humans might observe which is noticeably different from what they might observe if the world were Euclidean.  It was only with the advent of sensitive scientific measurements in the mid-1800s, such as the Fizeau experiment and the Michelson–Morley experiment, that puzzling discrepancies began to be noted between observation versus predictions based on the implicit assumption of Euclidean space. In special relativity, an \"observer\" will, in most cases, mean a frame of reference from which a set of objects or events are being measured.  This usage differs significantly from the ordinary English meaning of the term.  Reference frames are inherently nonlocal constructs, and according to this usage of the term, it does not make sense to speak of an observer as having a location.  In Fig. 1‑1, imagine that a scientist is in control of a dense lattice of clocks, synchronized within her reference frame, that extends indefinitely throughout the three dimensions of space.  Her location within the lattice is not important.  She uses her latticework of clocks to determine the time and position of events taking place within its reach.  The term \"observer\" refers to the entire ensemble of clocks associated with one inertial frame of reference.  In this idealized case, every point in space has a clock associated with it, and thus the clocks register each event instantly, with no time delay between an event and its recording.  A real observer, however, will see a delay between the emission of a signal and its detection due to the speed of light.  To synchronize the clocks, in the data reduction following an experiment, the time when a signal is received will be corrected to reflect its actual time were it to have been recorded by an idealized lattice of clocks. In many books on special relativity, especially older ones, the word \"observer\" is used in the more ordinary sense of the word.  It is usually clear from context which meaning has been adopted. Physicists distinguish between what one \"measures\" or \"observes\" (after one has factored out signal propagation delays), versus what one visually sees without such corrections.  Failure to understand the difference between what one measures/observes versus what one sees is the source of much error among beginning students of relativity. \"Return to Introduction\" Click here for a brief section summary By the mid-1800s, various experiments such as the observation of the Arago spot (a bright point at the center of a circular object's shadow due to diffraction) and differential measurements of the speed of light in air versus water were considered to have proven the wave nature of light as opposed to a corpuscular theory.  Propagation of waves was then (wrongly) assumed to require the existence of a medium which \"waved\": in the case of light waves, this was considered to be a hypothetical luminiferous aether.  However, the various attempts to establish the properties of this hypothetical medium yielded contradictory results.  For example, the Fizeau experiment of 1851 demonstrated that the speed of light in flowing water was less than the sum of the speed of light in air plus the speed of the water by an amount dependent on the water's index of refraction.  Among other issues, the dependence of the partial aether-dragging implied by this experiment on the index of refraction (which is dependent on wavelength) led to the unpalatable conclusion that aether \"simultaneously\" flows at different speeds for different colors of light.  The famous Michelson–Morley experiment of 1887 (Fig. 1‑2) showed no differential influence of Earth's motions through the hypothetical aether on the speed of light, and the most likely explanation, complete aether dragging, was in conflict with the observation of stellar aberration. George Francis FitzGerald in 1889 and Hendrik Lorentz in 1892 independently proposed that material bodies traveling through the fixed aether were physically affected by their passage, contracting in the direction of motion by an amount that was exactly what was necessary to explain the negative results of the Michelson-Morley experiment.  (No length changes occur in directions transverse to the direction of motion.) By 1904, Lorentz had expanded his theory such that he had arrived at equations formally identical with those that Einstein were to derive later (i.e. the Lorentz transform), but with a fundamentally different interpretation.  As a theory of dynamics (the study of forces and torques and their effect on motion), his theory assumed actual physical deformations of the physical constituents of matter.  Lorentz's equations predicted a quantity that he called \"local time\", with which he could explain the aberration of light, the Fizeau experiment and other phenomena.  However, Lorentz considered local time to be only an auxiliary mathematical tool, a trick as it were, to simplify the transformation from one system into another. Other physicists and mathematicians at the turn of the century came close to arriving at what is currently known as spacetime.  Einstein himself noted, that with so many people unraveling separate pieces of the puzzle, \"the special theory of relativity, if we regard its development in retrospect, was ripe for discovery in 1905.\" An important example is Henri Poincaré, who in 1898 argued that the simultaneity of two events is a matter of convention.  In 1900, he recognized that Lorentz's \"local time\" is actually what is indicated by moving clocks by applying an explicitly \"operational definition\" of clock synchronization assuming constant light speed.  In 1900 and 1904, he suggested the inherent undetectability of the aether by emphasizing the validity of what he called the principle of relativity, and in 1905/1906 he mathematically perfected Lorentz's theory of electrons in order to bring it into accordance with the postulate of relativity.  While discussing various hypotheses on Lorentz invariant gravitation, he introduced the innovative concept of a 4-dimensional space-time by defining various four vectors, namely four-position, four-velocity, and four-force.  He did not pursue the 4-dimensional formalism in subsequent papers, however, stating that this line of research seemed to \"entail great pain for limited profit\", ultimately concluding \"that three-dimensional language seems the best suited to the description of our world\".  Furthermore, even as late as 1909, Poincaré continued to believe in the dynamical interpretation of the Lorentz transform.  For these and other reasons, most historians of science argue that Poincaré did not invent what is now called special relativity. In 1905, Einstein introduced special relativity (even though without using the techniques of the spacetime formalism) in its modern understanding as a theory of space and time.  While his results are mathematically equivalent to those of Lorentz and Poincaré, it was Einstein who showed that the Lorentz transformations are not the result of interactions between matter and aether, but rather concern the nature of space and time itself.  Einstein performed his analyses in terms of kinematics (the study of moving bodies without reference to forces) rather than dynamics.  He obtained all of his results by recognizing that the entire theory can be built upon two postulates: The principle of relativity and the principle of the constancy of light speed.  In addition, Einstein in 1905 superseded previous attempts of an electromagnetic mass-energy relation by introducing the general equivalence of mass and energy, which was instrumental for his subsequent formulation of the equivalence principle in 1907, which declares the equivalence of inertial and gravitational mass.  By using the mass-energy equivalence, Einstein showed, in addition, that the gravitational mass of a body is proportional to its energy content, which was one of early results in developing general relativity.  While it would appear that he did not at first think geometrically about spacetime, in the further development of general relativity Einstein fully incorporated the spacetime formalism. When Einstein published in 1905, another of his competitors, his former mathematics professor Hermann Minkowski, had also arrived at most of the basic elements of special relativity.  Max Born recounted a meeting he had made with Minkowski, seeking to be Minkowski's student/collaborator: Minkowski had been concerned with the state of electrodynamics after Michelson's disruptive experiments at least since the summer of 1905, when Minkowski and David Hilbert led an advanced seminar attended by notable physicists of the time to study the papers of Lorentz, Poincaré et al.  However, it is not at all clear when Minkowski began to formulate the geometric formulation of special relativity that was to bear his name, or to which extent he was influenced by Poincaré's four-dimensional interpretation of the Lorentz transformation.  Nor is it clear if he ever fully appreciated Einstein's critical contribution to the understanding of the Lorentz transformations, thinking of Einstein's work as being an extension of Lorentz's work. A little more than a year before his death, Minkowski introduced his geometric interpretation of spacetime to the public on November 5, 1907 in a lecture to the Göttingen Mathematical society with the title, \"The Relativity Principle\" (\"Das Relativitätsprinzip\").  In the original version of this lecture, Minkowski continued to use such obsolescent terms as the ether, but the posthumous publication in 1915 of this lecture in the \"Annals of Physics\" (\"Annalen der Physik\") was edited by Sommerfeld to remove this term.  Sommerfeld also edited the published form of this lecture to revise Minkowski's judgement of Einstein from being a mere clarifier of the principle of relativity, to being its chief expositor.  On December 21, 1907, Minkowski spoke again to the Göttingen scientific society, and on September 21, 1908, Minkowski presented his famous talk, \"Space and Time\" (\"Raum und Zeit\"), to the German Society of Scientists and Physicians. The opening words of \"Space and Time\" include Minkowski's famous statement that \"Henceforth, space for itself, and time for itself shall completely reduce to a mere shadow, and only some sort of union of the two shall preserve independence.\"  \"Space and Time\" included the first public presentation of spacetime diagrams (Fig. 1‑4), and included a remarkable demonstration that the concept of the \"invariant interval\" (discussed below), along with the empirical observation that the speed of light is finite, allows derivation of the entirety of special relativity. Einstein, for his part, was initially dismissive of Minkowski's geometric interpretation of special relativity, regarding it as \"überflüssige Gelehrsamkeit\" (superfluous learnedness).  However, in order to complete his search for general relativity that started in 1907, the geometric interpretation of relativity proved to be vital, and in 1916, Einstein fully acknowledged his indebtedness to Minkowski, whose interpretation greatly facilitated the transition to general relativity.  Since there are other types of spacetime, such as the curved spacetime of general relativity, the spacetime of special relativity is today known as \"Minkowski spacetime.\" \"Return to Introduction\" <a href=\"%23Summary%20Spacetime%20interval\">Click here for a brief section summary </a> In three-dimensions, the \"distance\" between two points can be defined using the Pythagorean theorem: Although two viewers may measure the x,y, and z position of the two points using different coordinate systems, the distance between the points will be the same for both (assuming that they are measuring using the same units).  The distance is \"invariant\". In special relativity, however, the distance between two points is no longer the same if it measured by two different observers when one of the observers is moving, because of the Lorentz contraction.  The situation is ever more complicated if the two points are separated in time as well as in space.  For example, if one observer sees two events occur at the same place, but at different times, a person moving with respect to the first observer will see the two events occurring at different places, because (from their point of view) they are stationary, and the position of the event is receding or approaching.  Thus, a different measure must be used to measure the effective \"distance\" between two events. In four-dimensional spacetime, the analog to distance is the \"interval\".  Although time comes in as a fourth dimension, it is treated differently than the spatial dimensions.  Minkowski space hence differs in important respects from four-dimensional Euclidean space.  The fundamental reason for merging space and time into spacetime is that space and time are separately not invariant, which is to say that, under the proper conditions, different observers will disagree on the length of time between two \"events\" (because of time dilation) or the distance between the two events (because of length contraction).  But special relativity provides a new invariant, called the \"spacetime interval\", which combines distances in space and in time.  All observers who measure time and distance carefully will find the same spacetime interval between any two events.  Suppose an observer measures two events as being separated by a time formula_3 and a spatial distance formula_4.  Then the spacetime interval formula_5 between the two events that are separated by a distance formula_6 in space and a duration formula_7 in time is: The constant formula_10, the speed of light, converts the units used to measure time (seconds) into units used to measure distance (meters). Note on nomenclature: Although for brevity, one frequently sees interval expressions expressed without deltas, including in most of the following discussion, it should be understood that in general, formula_11 means formula_6, etc.  We are always concerned with \"differences\" of spatial or temporal coordinate values belonging to two events, and since there is no preferred origin, single coordinate values have no essential meaning. The equation above is similar to the Pythagorean theorem, except with a minus sign between the formula_13 and the formula_14 terms.  Note also that the spacetime interval is the quantity formula_15, not formula_16 itself.  The reason is that unlike distances in Euclidean geometry, intervals in Minkowski spacetime can be negative.  Rather than deal with square roots of negative numbers, physicists customarily regard formula_15 as a distinct symbol in itself, rather than the square of something. Because of the minus sign, the spacetime interval between two distinct events can be zero.  If formula_15 is positive, the spacetime interval is \"timelike\", meaning that two events are separated by more time than space.  If formula_15 is negative, the spacetime interval is \"spacelike\", meaning that two events are separated by more space than time.  Spacetime intervals are zero when formula_20.  In other words, the spacetime interval between two events on the world line of something moving at the speed of light is zero.  Such an interval is termed \"lightlike\" or \"null\".  A photon arriving in our eye from a distant star will not have aged, despite having (from our perspective) spent years in its passage. A spacetime diagram is typically drawn with only a single space and a single time coordinate.  Fig. 2‑1 presents a spacetime diagram illustrating the \"world lines\" (i.e. paths in spacetime) of two photons, A and B, originating from the same event and going in opposite directions.  In addition, C illustrates the world line of a slower-than-light-speed object.  The vertical time coordinate is scaled by formula_10 so that it has the same units (meters) as the horizontal space coordinate.  Since photons travel at the speed of light, their world lines have a slope of ±1.  In other words, every meter that a photon travels to the left or right requires approximately 3.3 nanoseconds of time. Note on nomenclature: There are two sign conventions in use in the relativity literature: These sign conventions are associated with the \"metric signatures\" (+ − − −) and (− + + +).  A minor variation is to place the time coordinate last rather than first.  Both conventions are widely used within the field of study. \"Return to Introduction\" Click here for a brief section summary In comparing measurements made by relatively moving observers in different reference frames, it is useful to work with the frames in a standard configuration.  In Fig. 2‑2, two Galilean reference frames (i.e. conventional 3-space frames) are displayed in relative motion.  Frame S belongs to a first observer O, and frame S′ (pronounced \"S prime\") belongs to a second observer O′. Fig. 2‑3a redraws Fig. 2‑2 in a different orientation.  Fig. 2‑3b illustrates a spacetime diagram from the viewpoint of observer O.  Since S and S′ are in standard configuration, their origins coincide at times \"t\" = 0 in frame S and \"t\"′ = 0 in frame S'.  The \"ct\"′ axis passes through the events in frame S′ which have \"x\"′ = 0.  But the points with \"x\"′ = 0 are moving in the \"x\"-direction of frame S with velocity \"v\", so that they are not coincident with the \"ct\" axis at any time other than zero.  Therefore, the \"ct\"′ axis is tilted with respect to the \"ct\" axis by an angle \"θ\" given by The \"x\"′ axis is also tilted with respect to the \"x\" axis.  To determine the angle of this tilt, we recall that the slope of the world line of a light pulse is always ±1.  Fig. 2‑3c presents a spacetime diagram from the viewpoint of observer O′.  Event P represents the emission of a light pulse at \"x\"′ = 0, \"ct\"′ = −\"a\".  The pulse is reflected from a mirror situated a distance \"a\" from the light source (event Q), and returns to the light source at \"x\"′ = 0, \"ct\"′ = \"a\" (event R). The same events P, Q, R are plotted in Fig. 2‑3b in the frame of observer O.  The light paths have slopes = 1 and −1 so that ΔPQR forms a right triangle.  Since OP = OQ = OR, the angle between \"x\"′ and \"x\" must also be \"θ\". While the rest frame has space and time axes that meet at right angles, the moving frame is drawn with axes that meet at an acute angle.  The frames are actually equivalent.  The asymmetry is due to unavoidable distortions in how spacetime coordinates can map onto a Cartesian plane, and should be considered no stranger than the manner in which, on a Mercator projection of the Earth, the relative sizes of land masses near the poles (Greenland and Antarctica) are highly exaggerated relative to land masses near the Equator. \"Return to Introduction\" Click here for a brief section summary In Fig. 2-4, event O is at the origin of a spacetime diagram, and the two diagonal lines represent all events that have zero spacetime interval with respect to the origin event.  These two lines form what is called the \"light cone\" of the event O, since adding a second spatial dimension (Fig. 2‑5) makes the appearance that of two right circular cones meeting with their apices at O.  One cone extends into the future (t>0), the other into the past (t<0). A light (double) cone divides spacetime into separate regions with respect to its apex.  The interior of the future light cone consists of all events that are separated from the apex by more \"time\" (temporal distance) than necessary to cross their \"spatial distance\" at lightspeed; these events comprise the \"timelike future\" of the event O. Likewise, the \"timelike past\" comprises the interior events of the past light cone.  So in \"timelike intervals\" Δ\"ct\" is greater than Δ\"x\", making timelike intervals positive.  The region exterior to the light cone consists of events that are separated from the event O by more \"space\" than can be crossed at lightspeed in the given \"time\".  These events comprise the so-called \"spacelike\" region of the event O, denoted \"Elsewhere\" in Fig. 2‑4.  Events on the light cone itself are said to be \"lightlike\" (or \"null separated\") from O. Because of the invariance of the spacetime interval, all observers will assign the same light cone to any given event, and thus will agree on this division of spacetime. The light cone has an essential role within the concept of causality.  It is possible for a not-faster-than-light-speed signal to travel from the position and time of O to the position and time of D (Fig. 2‑4).  It is hence possible for event O to have a causal influence on event D.  The future light cone contains all the events that could be causally influenced by O. Likewise, it is possible for a not-faster-than-light-speed signal to travel from the position and time of A, to the position and time of O.  The past light cone contains all the events that could have a causal influence on O.  In contrast, assuming that signals cannot travel faster than the speed of light, any event, like e.g. B or C, in the spacelike region (Elsewhere), cannot either effect event O, nor can they be affected by event O employing such signalling.  Under this assumption any causal relationship between event O and any events in the spacelike region of a light cone is excluded. \"Return to Introduction\" Click here for a brief section summary All observers will agree that for any given event, an event within the given event's future light cone occurs \"after\" the given event.  Likewise, for any given event, an event within the given event's past light cone occurs \"before\" the given event.  The before-after relationship observed for timelike-separated events remains unchanged no matter what the reference frame of the observer, i.e. no matter how the observer may be moving.  The situation is quite different for spacelike-separated events.  Fig. 2‑4 was drawn from the reference frame of an observer moving at \"v\" = 0.  From this reference frame, event C is observed to occur after event O, and event B is observed to occur before event O. From a different reference frame, the orderings of these non-causally-related events can be reversed.  In particular, one notes that if two events are simultaneous in a particular reference frame, they are \"necessarily\" separated by a spacelike interval and thus are noncausally related.  The observation that simultaneity is not absolute, but depends on the observer's reference frame, is termed the relativity of simultaneity. Fig. 2-6 illustrates the use of spacetime diagrams in the analysis of the relativity of simultaneity.  The events in spacetime are invariant, but the coordinate frames transform as discussed above for Fig. 2‑3.  The three events (A, B, C) are simultaneous from the reference frame of an observer moving at \"v\" = 0.  From the reference frame of an observer moving at \"v\" = 0.3 \"c\", the events appear to occur in the order C, B, A. From the reference frame of an observer moving at \"v\" = −0.5 \"c\" , the events appear to occur in the order A, B, C .  The white line represents a \"plane of simultaneity\" being moved from the past of the observer to the future of the observer, highlighting events residing on it.  The gray area is the light cone of the observer, which remains invariant. A spacelike spacetime interval gives the same distance that an observer would measure if the events being measured were simultaneous to the observer.  A spacelike spacetime interval hence provides a measure of \"proper distance\", i.e. the true distance = formula_25 Likewise, a timelike spacetime interval gives the same measure of time as would be presented by the cumulative ticking of a clock that moves along a given world line.  A timelike spacetime interval hence provides a measure of the \"proper time\" = formula_26. \"Return to Introduction\" Click here for a brief section summary In Euclidean space (having spatial dimensions only), the set of points equidistant (using the Euclidean metric) from some point form a circle (in two dimensions) or a sphere (in three dimensions).  In (1+1)-dimensional Minkowski spacetime (having one temporal and one spatial dimension), the points at some constant spacetime interval away from the origin (using the Minkowski metric) form curves given by the two equations These equations describe two families of hyperbolae in an \"x\"–\"ct\" spacetime diagram, which are termed \"invariant hyperbolae\". In Fig. 2‑7a, each magenta hyperbola connects all events having some fixed \"spacelike\" separation from the origin, while the green hyperbolae connect events of equal \"timelike\" separation. Fig. 2‑7b reflects the situation in (1+2)-dimensional Minkowski spacetime (one temporal and two spatial dimensions) with the corresponding hyperboloids.  Each \"timelike\" interval generates a hyperboloid of one sheet, while each spacelike interval generates a hyperboloid of two sheets. The (1+2)-dimensional boundary between space- and timelike hyperboloids, established by the events forming a zero spacetime interval to the origin, is made up by degenerating the hyperboloids to the light cone.  In (1+1)-dimensions the hyperbolae degenerate to the two grey 45°-lines depicted in Fig. 2‑7a. Note on nomenclature: The magenta hyperbolae, which cross the \"x\" axis, are termed \"timelike\" (in contrast to \"spacelike\") hyperbolae because all \"distances\" to the origin \"along\" the hyperbola are timelike intervals.  Because of that, these hyperbolae represent actual paths that can be traversed by (constantly accelerating) particles in spacetime: between any two events on one hyperbola a causality relation is possible, because the inverse of the slope –representing the necessary speed– for all secants is less than formula_1.  On the other hand, the green hyperbolae, which cross the \"ct\" axis, are termed \"spacelike\", because all intervals \"along\" these hyperbolae are spacelike intervals: no causality is possible between any two points on one of these hyperbolae, because all secants represent speeds larger than formula_1. \"Return to Introduction\" Click here for a brief section summary Fig. 2-8 illustrates the invariant hyperbola for all events that can be reached from the origin in a proper time of 5 meters (approximately ).  Different world lines represent clocks moving at different speeds.  A clock that is stationary with respect to the observer has a world line that is vertical, and the elapsed time measured by the observer is the same as the proper time.  For a clock traveling at 0.3\"c\", the elapsed time measured by the observer is 5.24 meters ( ), while for a clock traveling at 0.7\"c\", the elapsed time measured by the observer is 7.00 meters ( ).  This illustrates the phenomenon known as \"time dilation\".  Clocks that travel faster take longer (in the observer frame) to tick out the same amount of proper time, and they travel further along the x–axis than they would have without time dilation.  The measurement of time dilation by two observers in different inertial reference frames is mutual.  If observer O measures the clocks of observer O′ as running slower in his frame, observer O′ in turn will measure the clocks of observer O as running slower. Length contraction, like time dilation, is a manifestation of the relativity of simultaneity.  Measurement of length requires measurement of the spacetime interval between two events that are simultaneous in one's frame of reference.  But events that are simultaneous in one frame of reference are, in general, not simultaneous in other frames of reference. Fig. 2-9 illustrates the motions of a 1 m rod that is traveling at 0.5 \"c\" along the \"x\" axis.  The edges of the blue band represent the world lines of the rod's two endpoints.  The invariant hyperbola illustrates events separated from the origin by a spacelike interval of 1 m.  The endpoints O and B measured when \"t\"′  = 0 are simultaneous events in the S′ frame.  But to an observer in frame S, events O and B are not simultaneous.  To measure length, the observer in frame S measures the endpoints of the rod as projected onto the \"x\"-axis along their world lines.  The projection of the rod's \"world sheet\" onto the \"x\" axis yields the foreshortened length OC. (not illustrated) Drawing a vertical line through A so that it intersects the \"x\"' axis demonstrates that, even as OB is foreshortened from the point of view of observer O, OA is likewise foreshortened from the point of view of observer O′.  In the same way that each observer measures the other's clocks as running slow, each observer measures the other's rulers as being contracted. \"Return to Introduction\" Click here for a brief section summary Mutual time dilation and length contraction tend to strike beginners as inherently self-contradictory concepts.  The worry is that if observer A measures observer B's clocks as running slowly, simply because B is moving at speed \"v\" relative to A, then the principle of relativity requires that observer B likewise measures A's clocks as running slowly.  This is an important question that \"goes to the heart of understanding special relativity.\" Basically, A and B are performing two different measurements. In order to measure the rate of ticking of one of B's clocks, A must use \"two\" of his own clocks, the first to record the time where B's clock first ticked \"at the first location of B\", and second to record the time where B's clock emitted its second tick \"at the next location of B\".  Observer A needs two clocks because B is moving, so a grand total of three clocks are involved in the measurement.  A's two clocks must be synchronized in A's frame.  Conversely, B requires two clocks synchronized in \"her\" frame to record the ticks of A's clocks at the locations where A's clocks emitted their ticks.  Therefore, A and B are performing their measurements with different sets of three clocks each.  Since they are not doing the same measurement with the same clocks, there is no inherent necessity that the measurements be reciprocally \"consistent\" such that, if one observer measures the other's clock to be slow, the other observer measures the one's clock to be fast. In regards to mutual length contraction, Fig. 2‑9 illustrates that the primed and unprimed frames are mutually rotated by a hyperbolic angle (analogous to ordinary angles in Euclidean geometry).  Because of this rotation, the projection of a primed meter-stick onto the unprimed x-axis is foreshortened, while the projection of an unprimed meter-stick onto the primed x′-axis is likewise foreshortened. Fig. 2-10 reinforces previous discussions about mutual time dilation.  In this figure, Events A and C are separated from event O by equal timelike intervals.  From the unprimed frame, events A and B are measured as simultaneous, but more time has passed for the unprimed observer than has passed for the primed observer.  From the primed frame, events C and D are measured as simultaneous, but more time has passed for the primed observer than has passed for the unprimed observer.  Each observer measures the clocks of the other observer as running more slowly. Please note the importance of the word \"measure\".  An observer's state of motion cannot affect an observed object, but it \"can\" affect the observer's \"observations\" of the object. In Fig. 2-10, each line drawn parallel to the \"x\" axis represents a line of simultaneity for the unprimed observer.  All events on that line have the same time value of \"ct\".  Likewise, each line drawn parallel to the \"x\"′ axis represents a line of simultaneity for the primed observer.  All events on that line have the same time value of \"ct\"′ . \"Return to Introduction\" Click here for a brief section summary Elementary introductions to special relativity often illustrate the differences between Galilean relativity and special relativity by posing a series of supposed \"paradoxes\".  All paradoxes are, in reality, merely ill-posed or misunderstood problems, resulting from our unfamiliarity with velocities comparable to the speed of light.  The remedy is to solve many problems in special relativity and to become familiar with its so-called counter-intuitive predictions.  The geometrical approach to studying spacetime is considered one of the best methods for developing a modern intuition. The twin paradox is a thought experiment involving identical twins, one of whom makes a journey into space in a high-speed rocket, returning home to find that the twin who remained on Earth has aged more.  This result appears puzzling because each twin observes the other twin as moving, and so at first glance, it would appear that each should find the other to have aged less.  The twin paradox sidesteps the justification for mutual time dilation presented above by avoiding the requirement for a third clock.  Nevertheless, the \"twin paradox\" is not a true paradox because it is easily understood within the context of special relativity. The impression that a paradox exists stems from a misunderstanding of what special relativity states.  Special relativity does not declare all frames of reference to be equivalent, only inertial frames.  The traveling twin's frame is not inertial during periods when she is accelerating.  Furthermore, the difference between the twins is observationally detectable: the traveling twin needs to fire her rockets to be able to return home, while the stay-at-home twin does not. Deeper analysis is needed before we can understand why these distinctions should result in a difference in the twins' ages.  Consider the spacetime diagram of Fig. 2‑11.  This presents the simple case of a twin going straight out along the x axis and immediately turning back.  From the standpoint of the stay-at-home twin, there is nothing puzzling about the twin paradox at all.  The proper time measured along the traveling twin's world line from O to C, plus the proper time measured from C to B, is less than the stay-at-home twin's proper time measured from O to A to B.  More complex trajectories require integrating the proper time between the respective events along the curve (i.e. the path integral) to calculate the total amount of proper time experienced by the traveling twin. Complications arise if the twin paradox is analyzed from the traveling twin's point of view. For the rest of this discussion, we adopt Weiss's nomenclature, designating the stay-at-home twin as Terence and the traveling twin as Stella. We had previously noted that Stella is not in an inertial frame.  Given this fact, it is sometimes stated that full resolution of the twin paradox requires general relativity.  This is not true. A pure SR analysis would be as follows: Analyzed in Stella's rest frame, she is motionless for the entire trip.  When she fires her rockets for the turnaround, she experiences a pseudo force which resembles a gravitational force.  Figs. 2‑6 and 2‑11 illustrate the concept of lines (planes) of simultaneity: Lines parallel to the observer's x-axis (xy-plane) represent sets of events that are simultaneous in the observer frame.  In Fig. 2‑11, the blue lines connect events on Terence's world line which, \"from Stella's point of view\", are simultaneous with events on her world line.  (Terence, in turn, would observe a set of horizontal lines of simultaneity.)  Throughout both the outbound and the inbound legs of Stella's journey, she measures Terence's clocks as running slower than her own.  \"But during the turnaround\" (i.e. between the bold blue lines in the figure), a shift takes place in the angle of her lines of simultaneity, corresponding to a rapid skip-over of the events in Terence's world line that Stella considers to be simultaneous with her own.  Therefore, at the end of her trip, Stella finds that Terence has aged more than she has. Although general relativity is not \"required\" to analyze the twin paradox, application of the Equivalence Principle of general relativity does provide some additional insight into the subject.  We had previously noted that Stella is not stationary in an inertial frame.  Analyzed in Stella's rest frame, she is motionless for the entire trip.  When she is coasting her rest frame is inertial, and Terence's clock will appear to run slow.  But when she fires her rockets for the turnaround, her rest frame is an accelerated frame and she experiences a force which is pushing her as if she were in a gravitational field.  Terence will appear to be high up in that field and because of gravitational time dilation, his clock will appear to run fast, so much so that the net result will be that Terence has aged more than Stella when they are back together.  As will be discussed in the forthcoming section Curvature of time, the theoretical arguments predicting gravitational time dilation are not exclusive to general relativity.  \"Any\" theory of gravity will predict gravitational time dilation if it respects the principle of equivalence, including Newton's theory. \"Return to Introduction\" Click here for a brief section summary This introductory section has focused on the spacetime of special relativity, since it is the easiest to describe.  Minkowski spacetime is flat, takes no account of gravity, is uniform throughout, and serves as nothing more than a static background for the events that take place in it.  The presence of gravity greatly complicates the description of spacetime.  In general relativity, spacetime is no longer a static background, but actively interacts with the physical systems that it contains.  Spacetime curves in the presence of matter, can propagate waves, bends light, and exhibits a host of other phenomena.  A few of these phenomena are described in the later sections of this article. \"Return to Introduction\" <a href=\"%23Summary%20Galilean%20transformations\">Click here for a brief section summary </a> A basic goal is to be able to compare measurements made by observers in relative motion.  Say we have an observer O in frame S who has measured the time and space coordinates of an event, assigning this event three Cartesian coordinates and the time as measured on his lattice of synchronized clocks (\"x\", \"y\", \"z\", \"t\") (see Fig. 1‑1).  A second observer O′ in a different frame S′ measures the same event in \"her\" coordinate system and \"her\" lattice of synchronized clocks (\"x\"′ , \"y\"′ , \"z\"′ , \"t\"′ ) .  Since we are dealing with inertial frames, neither observer is under acceleration, and a simple set of equations allows us to relate coordinates (\"x\", \"y\", \"z\", \"t\") to (\"x\"′ , \"y\"′ , \"z\"′ , \"t\"′ ) .  Given that the two coordinate systems are in standard configuration, meaning that they are aligned with parallel (\"x\", \"y\", \"z\") coordinates and that \"t\" = 0 when \"t\"′ = 0 , the coordinate transformation is as follows: Fig. 3-1 illustrates that in Newton's theory, time is universal, not the velocity of light.  Consider the following thought experiment: The red arrow illustrates a train that is moving at 0.4 c with respect to the platform.  Within the train, a passenger shoots a bullet with a speed of 0.4 c in the frame of the train.  The blue arrow illustrates that a person standing on the train tracks measures the bullet as traveling at 0.8 c.  This is in accordance with our naive expectations. More generally, assume that frame S′ is moving at velocity \"v\" with respect to frame S. Within frame S′, observer O′ measures an object moving with velocity \"u\"′ .  What is its velocity \"u\" with respect to frame S?  Since \"x\" = \"ut\" , \"x\"′ = \"x\" − \"vt\" , and \"t\" = \"t\"′ , we can write \"x\"′ = \"ut\" − \"vt\" = (\"u\" − \"v\")\"t\" = (\"u\" − \"v\")\"t\"′ .  This leads to \"u\"′ = \"x\"′ /\"t\"′ and ultimately which is the common-sense Galilean law for the addition of velocities. \"Return to Introduction\" Click here for a brief section summary The composition of velocities is quite different in relativistic spacetime.  To reduce the complexity of the equations slightly, we introduce a common shorthand for the ratio of the speed of an object relative to light, Fig. 3-2a illustrates a red train that is moving forward at a speed given by \"v\"/\"c\" = \"β\" = \"s\"/\"a\" .  From the primed frame of the train, a passenger shoots a bullet with a speed given by \"u\"′ /\"c\" = \"β\"′ = \"n\"/\"m\" , where the distance is measured along a line parallel to the red \"x\"′ axis rather than parallel to the black \"x\" axis.  What is the composite velocity \"u\" of the bullet relative to the platform, as represented by the blue arrow?  Referring to Fig. 3‑2b: The relativistic formula for addition of velocities presented above exhibits several important features: \"Return to Introduction\" Click here for a brief section summary We had previously discussed, in qualitative terms, time dilation and length contraction.  It is straightforward to obtain quantitative expressions for these effects.  Fig. 3‑3 is a composite image containing individual frames taken from two previous animations, simplified and relabeled for the purposes of this section. To reduce the complexity of the equations slightly, we see in the literature a variety of different shorthand notations for \"ct\" : In Fig. 3-3a, segments \"OA\" and \"OK\" represent equal spacetime intervals.  Time dilation is represented by the ratio \"OB\"/\"OK\".  The invariant hyperbola has the equation where \"k\" = \"OK\", and the red line representing the world line of a particle in motion has the equation \"w\" = \"x\"/\"β\" = \"xc\"/\"v\".  A bit of algebraic manipulation yields formula_42 The expression involving the square root symbol appears very frequently in relativity, and one over the expression is called the Lorentz factor, denoted by the Greek letter gamma formula_43: We note that if \"v\" is greater than or equal to \"c\", the expression for formula_43 becomes physically meaningless, implying that \"c\" is the maximum possible speed in nature.  Next, we note that for any \"v\" greater than zero, the Lorentz factor will be greater than one, although the shape of the curve is such that for low speeds, the Lorentz factor is extremely close to one. In Fig. 3-3b, segments \"OA\" and \"OK\" represent equal spacetime intervals.  Length contraction is represented by the ratio \"OB\"/\"OK\".  The invariant hyperbola has the equation , where \"k\" = \"OK\", and the edges of the blue band representing the world lines of the endpoints of a rod in motion have slope 1/\"β\" = \"c\"/\"v\".  Event A has coordinates (\"x\", \"w\") = (\"γk\", \"γβk\").  Since the tangent line through A and B has the equation \"w\" = (\"x\" − \"OB\")/\"β\", we have \"γβk\" = (\"γk\" − \"OB\")/\"β\" and \"Return to Introduction\" Click here for a brief section summary The Galilean transformations and their consequent commonsense law of addition of velocities work well in our ordinary low-speed world of planes, cars and balls.  Beginning in the mid-1800s, however, sensitive scientific instrumentation began finding anomalies that did not fit well with the ordinary addition of velocities. To transform the coordinates of an event from one frame to another in special relativity, we use the Lorentz transformations. The Lorentz factor appears in the Lorentz transformations: The inverse Lorentz transformations are: When \"v\" ≪ \"c\", the \"v\"/c and \"vx\"/\"c\" terms approach zero, and the Lorentz transformations approximate to the Galilean transformations. As noted before, when we write formula_49 formula_50 and so forth, we most often \"really\" mean formula_51 formula_52 and so forth.  Although, for brevity, we write the Lorentz transformation equations without deltas, it should be understood that \"x\" means Δ\"x\", etc.  We are, in general, always concerned with the space and time \"differences\" between events. Note on nomenclature: Calling one set of transformations the normal Lorentz transformations and the other the inverse transformations is misleading, since there is no intrinsic difference between the frames.  Different authors call one or the other set of transformations the \"inverse\" set.  The forwards and inverse transformations are trivially related to each other, since the \"S\" frame can only be moving forwards or reverse with respect to \"S\"′ .  So inverting the equations simply entails switching the primed and unprimed variables and replacing \"v\" with −\"v\". 1). \"Return to Introduction\" There have been many dozens of derivations of the Lorentz transformations since Einstein's original work in 1905, each with its particular focus.  Although Einstein's derivation was based on the invariance of the speed of light, there are other physical principles that may serve as starting points.  Ultimately, these alternative starting points can be considered different expressions of the underlying principle of locality, which states that the influence that one particle exerts on another can not be transmitted instantaneously. The derivation given here and illustrated in Fig. 3‑5 is based on one presented by Bais and makes use of previous results from the Relativistic Composition of Velocities, Time Dilation, and Length Contraction sections.  Event P has coordinates (\"w\", \"x\") in the black \"rest system\" and coordinates (\"w\"′ , \"x\"′ ) in the red frame that is moving with velocity parameter \"β\" = \"v\"/\"c\".  How do we determine \"w\"′ and \"x\"′ in terms of \"w\" and \"x\"?  (Or the other way around, of course.) It is easier at first to derive the \"inverse\" Lorentz transformation. The above equations are alternate expressions for the t and x equations of the inverse Lorentz transformation, as can be seen by substituting \"ct\" for \"w\", \"ct\"′ for \"w\"′ , and \"v\"/\"c\" for \"β\".  From the inverse transformation, the equations of the forwards transformation can be derived by solving for \"t\"′ and \"x\"′ . \"Return to Introduction\" The Lorentz transformations have a mathematical property called linearity, since \"x' \" and \"t' \" are obtained as linear combinations of \"x\" and \"t\", with no higher powers involved.  The linearity of the transformation reflects a fundamental property of spacetime that we tacitly assumed while performing the derivation, namely, that the properties of inertial frames of reference are independent of location and time.  In the absence of gravity, spacetime looks the same everywhere.  All inertial observers will agree on what constitutes accelerating and non-accelerating motion.  Any one observer can use her own measurements of space and time, but there is nothing absolute about them.  Another observer's conventions will do just as well. A result of linearity is that if two Lorentz transformations are applied sequentially, the result is also a Lorentz transformation. \"Return to Introduction\" Click here for a brief section summary The Doppler effect is the change in frequency or wavelength of a wave for a receiver and source in relative motion.  For simplicity, we consider here two basic scenarios: (1) The motions of the source and/or receiver are exactly along the line connecting them (longitudinal Doppler effect), and (2) the motions are at right angles to the said line (transverse Doppler effect).  We are ignoring scenarios where they move along intermediate angles. The classical Doppler analysis deals with waves that are propagating in a medium, such as sound waves or water ripples, and which are transmitted between sources and receivers that are moving towards or away from each other.  The analysis of such waves depends on whether the source, the receiver, or both are moving relative to the medium.  Given the scenario where the receiver is stationary with respect to the medium, and the source is moving directly away from the receiver at a speed of \"v\" for a velocity parameter of \"β\", the wavelength is increased, and the observed frequency \"f\" is given by On the other hand, given the scenario where source is stationary, and the receiver is moving directly away from the source at a speed of \"v\" for a velocity parameter of \"β\", the wavelength is \"not\" changed, but the transmission velocity of the waves relative to the receiver is decreased, and the observed frequency \"f\" is given by Light, unlike sound or water ripples, does not propagate through a medium, and there is no distinction between a source moving away from the receiver or a receiver moving away from the source.  Fig. 3‑6 illustrates a relativistic spacetime diagram showing a source separating from the receiver with a velocity parameter \"β\", so that the separation between source and receiver at time \"w\" is \"βw\".  Because of time dilation, \"w = γw' \".  Since the slope of the green light ray is −1, T = \"w+βw\" = \"γw' \"(1\"+β\").  Hence, the relativistic Doppler effect is given by \"Return to Introduction\" Suppose that a source, moving in a straight line, is at its closest point to the receiver.  It would appear that the classical analysis predicts that the receiver detects no Doppler shift.  Due to subtleties in the analysis, that expectation is not necessarily true.  Nevertheless, when appropriately defined, transverse Doppler shift is a relativistic effect that has no classical analog.  The subtleties are these: In scenario (a), when the source is closest to the receiver, the light hitting the receiver actually comes from a direction where the source had been some time back, and it has a significant longitudinal component, making an analysis from the frame of the receiver tricky.  It is easier to make the analysis from S', the frame of the source.  The point of closest approach is frame-independent and represents the moment where there is no change in distance versus time (i.e. dr/dt = 0 where r is the distance between receiver and source) and hence no longitudinal Doppler shift.  The source observes the receiver as being illuminated by light of frequency \"f' \", but also observes the receiver as having a time-dilated clock.  In frame S, the receiver is therefore illuminated by blueshifted light of frequency Scenario (b) is best analyzed from S, the frame of the receiver.  The illustration shows the receiver being illuminated by light from when the source was closest to the receiver, even though the source has moved on.  Because the source's clocks are time dilated, and since dr/dt was equal to zero at this point, the light from the source, emitted from this closest point, is redshifted with frequency Scenarios (c) and (d) can be analyzed by simple time dilation arguments.  In (c), the receiver observes light from the source as being blueshifted by a factor of formula_43, and in (d), the light is redshifted.  The only seeming complication is that the orbiting objects are in accelerated motion.  However, if an inertial observer looks at an accelerating clock, only the clock's instantaneous speed is important when computing time dilation.  (The converse, however, is not true.)  Most reports of transverse Doppler shift refer to the effect as a redshift and analyze the effect in terms of scenarios (b) or (d). \"Return to Introduction\" Click here for a brief section summary In classical mechanics, the state of motion of a particle is characterized by its mass and its velocity.  Linear momentum, the product of a particle's mass and velocity, is a vector quantity, possessing the same direction as the velocity: p = \"mv\".  It is a \"conserved\" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum cannot change. In relativistic mechanics, the momentum vector is extended to four dimensions.  Added to the momentum vector is a time component that allows the spacetime momentum vector to transform like the spacetime position vector \"(x, t)\".  In exploring the properties of the spacetime momentum, we start, in Fig. 3‑8a, by examining what a particle looks like at rest.  In the rest frame, the spatial component of the momentum is zero, i.e. \"p = 0\", but the time component equals \"mc\". We can obtain the transformed components of this vector in the moving frame by using the Lorentz transformations, or we can read it directly from the figure because we know that \"(mc)'  = γmc\" and \"p'  = −βγmc\", since the red axes are rescaled by gamma.  Fig. 3‑8b illustrates the situation as it appears in the moving frame.  It is apparent that the space and time components of the four-momentum go to infinity as the velocity of the moving frame approaches \"c\". We will use this information shortly to obtain an expression for the four-momentum. \"Return to Introduction\" Light particles, or photons, travel at the speed of \"c\", the constant that is conventionally known as the \"speed of light\".  This statement is not a tautology, since many modern formulations of relativity do not start with constant speed of light as a postulate.  Photons therefore propagate along a light-like world line and, in appropriate units, have equal space and time components for every observer. A consequence of Maxwell's theory of electromagnetism is that light carries energy and momentum, and that their ratio is a constant: \"E/p = c\".  Rearranging, \"E/c\" = \"p\", and since for photons, the space and time components are equal, \"E/c\" must therefore be equated with the time component of the spacetime momentum vector. Photons travel at the speed of light, yet have finite momentum and energy.  For this to be so, the mass term in \"γmc\" must be zero, meaning that photons are massless particles. Infinity times zero is an ill-defined quantity, but \"E/c\" is well-defined. By this analysis, if the energy of a photon equals \"E\" in the rest frame, it equals \"E' = (1 − β)γE\" in a moving frame.  This result can by derived by inspection of Fig. 3‑9 or by application of the Lorentz transformations, and is consistent with the analysis of Doppler effect given previously. \"Return to Introduction\" Consideration of the interrelationships between the various components of the relativistic momentum vector led Einstein to several famous conclusions. Another way of looking at the relationship between mass and energy is to consider a series expansion of \"γmc\" at low velocity: The second term is just an expression for the kinetic energy of the particle.  Mass indeed appears to be another form of energy. The concept of relativistic mass that Einstein introduced in 1905, \"m\", although amply validated every day in particle accelerators around the globe (or indeed in any instrumentation whose use depends on high velocity particles, such as electron microscopes, old-fashioned color television sets, etc.), has nevertheless not proven to be a \"fruitful\" concept in physics in the sense that it is not a concept that has served as a basis for other theoretical development.  Relativistic mass, for instance, plays no role in general relativity. For this reason, as well as for pedagogical concerns, most physicists currently prefer a different terminology when referring to the relationship between mass and energy.  \"Relativistic mass\" is a deprecated term.  The term \"mass\" by itself refers to the rest mass or invariant mass, and is equal to the invariant length of the relativistic momentum vector.  Expressed as a formula, This formula applies to all particles, massless as well as massive.  For massless photons, it yields the same relationship that we had earlier established, \"E = ±pc\" . \"Return to Introduction\" Because of the close relationship between mass and energy, the four-momentum (also called 4‑momentum) is also called the energy-momentum 4‑vector.  Using an uppercase \"P\" to represent the four-momentum and a lowercase p to denote the spatial momentum, the four-momentum may be written as \"Return to Introduction\" Click here for a brief section summary In physics, conservation laws state that certain particular measurable properties of an isolated physical system do not change as the system evolves over time.  In 1915, Emmy Noether discovered that underlying each conservation law is a fundamental symmetry of nature.  The fact that physical processes don't care \"where\" in space they take place (space translation symmetry) yields conservation of momentum, the fact that such processes don't care \"when\" they take place (time translation symmetry) yields conservation of energy, and so on.  In this section, we examine the Newtonian views of conservation of mass, momentum and energy from a relativistic perspective. To understand how the Newtonian view of conservation of momentum needs to be modified in a relativistic context, we examine the problem of two colliding bodies limited to a single dimension. In Newtonian mechanics, two extreme cases of this problem may be distinguished yielding mathematics of minimum complexity: (1) The two bodies rebound from each other in a completely elastic collision.  (2) The two bodies stick together and continue moving as a single particle.  This second case is the case of completely inelastic collision.  For both cases (1) and (2), momentum, mass, and total energy are conserved.  However, kinetic energy is not conserved in cases of inelastic collision.  A certain fraction of the initial kinetic energy is converted to heat. In case (2), two masses with momentums p = mv and p = mv collide to produce a single particle of conserved mass \"m = m + m\" traveling at the center of mass velocity of the original system, v = (mv + mv)/(m + m\") .  The total momentum p = p + p\" is conserved. Fig. 3‑10 illustrates the inelastic collision of two particles from a relativistic perspective.  The time components \"E/c\" and \"E/c\" add up to total \"E/c\" of the resultant vector, meaning that energy is conserved.  Likewise, the space components \"p\" and \"p\" add up to form \"p\" of the resultant vector.  The four-momentum is, as expected, a conserved quantity.  However, the invariant mass of the fused particle, given by the point where the invariant hyperbola of the total momentum intersects the energy axis, is not equal to the sum of the invariant masses of the individual particles that collided.  Indeed, it is larger than the sum of the individual masses: \"m > m + m\" . Looking at the events of this scenario in reverse sequence, we see that non-conservation of mass is a common occurrence: when an unstable elementary particle spontaneously decays into two lighter particles, total energy is conserved, but the mass is not.  Part of the mass is converted into kinetic energy. \"Return to Introduction\" The freedom to choose any frame in which to perform an analysis allows us to pick one which may be particularly convenient.  For analysis of momentum and energy problems, the most convenient frame is usually the \"center-of-momentum frame\" (also called the zero-momentum frame, or COM frame).  This is the frame in which the space component of the system's total momentum is zero.  Fig. 3‑11 illustrates the breakup of a high speed particle into two daughter particles.  In the lab frame, the daughter particles are preferentially emitted in a direction oriented along the original particle's trajectory.  In the COM frame, however, the two daughter particles are emitted in opposite directions, although their masses and the magnitude of their velocities are generally not the same. \"Return to Introduction\" In a Newtonian analysis of interacting particles, transformation between frames is simple because all that is necessary is to apply the Galilean transformation to all velocities.  Since \"v' = v − u\" , the momentum \"p' = p − mu\" .  If the total momentum of an interacting system of particles is observed to be conserved in one frame, it will likewise be observed to be conserved in any other frame. Conservation of momentum in the COM frame amounts to the requirement that \"p\" = 0 both before and after collision.  In the Newtonian analysis, conservation of mass dictates that \"m = m + m\" .  In the simplified, one-dimensional scenarios that we have been considering, only one additional constraint is necessary before the outgoing momenta of the particles can be determined—an energy condition.  In the one-dimensional case of a completely elastic collision with no loss of kinetic energy, the outgoing velocities of the rebounding particles in the COM frame will be precisely equal and opposite to their incoming velocities.  In the case of a completely inelastic collision with total loss of kinetic energy, the outgoing velocities of the rebounding particles will be zero. Newtonian momenta, calculated as \"p = mv\" , fail to behave properly under Lorentzian transformation.  The linear transformation of velocities \"v' = v − u\" is replaced by the highly nonlinear \"v' = (v − u)/(1 − vu/c)\", so that a calculation demonstrating conservation of momentum in one frame will be invalid in other frames.  Einstein was faced with either having to give up conservation of momentum, or to change the definition of momentum.  As we have discussed in the previous section on four-momentum, this second option was what he chose. The relativistic conservation law for energy and momentum replaces the three classical conservation laws for energy, momentum and mass.  Mass is no longer conserved independently, because it has been subsumed into the total relativistic energy.  This makes the relativistic conservation of energy a simpler concept than in nonrelativistic mechanics, because the total energy is conserved without any qualifications.  Kinetic energy converted into heat or internal potential energy shows up as an increase in mass. \"Return to Introduction\" The topics in this section are of significantly greater technical difficulty than those in the preceding sections and are not essential for understanding \"Introduction to curved spacetime. \" <a href=\"%23Summary%20Rapidity\">Click here for a brief section summary </a> Lorentz transformations relate coordinates of events in one reference frame to those of another frame.  Relativistic composition of velocities is used to add two velocities together.  The formulas to perform the latter computations are nonlinear, making them more complex than the corresponding Galilean formulas. This nonlinearity is an artifact of our choice of parameters.  We have previously noted that in an x–ct spacetime diagram, the points at some constant spacetime interval from the origin form an invariant hyperbola.  We have also noted that the coordinate systems of two spacetime reference frames in standard configuration are hyperbolically rotated with respect to each other. The natural functions for expressing these relationships are the hyperbolic analogs of the trigonometric functions.  Fig. 4‑1a shows a unit circle with sin(\"a\") and cos(\"a\"), the only difference between this diagram and the familiar unit circle of elementary trigonometry being that \"a\" is interpreted, not as the angle between the ray and the \"x\"-axis , but as twice the area of the sector swept out by the ray from the \"x\"-axis .  (Numerically, the angle and 2 × area measures for the unit circle are identical.)  Fig. 4‑1b shows a unit hyperbola with sinh(\"a\") and cosh(\"a\"), where \"a\" is likewise interpreted as twice the tinted area.  Fig. 4‑2 presents plots of the sinh, cosh, and tanh functions. For the unit circle, the slope of the ray is given by In the Cartesian plane, rotation of point (\"x, y\") into point (\"x' , y' \") by angle \"θ\" is given by In a spacetime diagram, the velocity parameter formula_70 is the analog of slope.  The \"rapidity\", \"φ\", is defined by where The rapidity defined above is very useful in special relativity because many expressions take on a considerably simpler form when expressed in terms of it.  For example, rapidity is simply additive in the collinear velocity-addition formula; or in other words, formula_76 The Lorentz transformations take a simple form when expressed in terms of rapidity.  The \"γ\" factor can be written as Transformations describing relative motion with uniform velocity and without rotation of the space coordinate axes are called \"boosts\". Substituting \"γ\" and \"γβ\" into the transformations as previously presented and rewriting in matrix form, the Lorentz boost in the \"x\" direction may be written as and the inverse Lorentz boost in the \"x\" direction may be written as In other words, Lorentz boosts represent hyperbolic rotations in Minkowski spacetime. The advantages of using hyperbolic functions are such that some textbooks such as the classic ones by Taylor and Wheeler introduce their use at a very early stage. \"Return to Introduction\" Click here for a brief section summary Four‑vectors have been mentioned above in context of the energy-momentum 4‑vector , but without any great emphasis.  Indeed, none of the elementary derivations of special relativity require them.  But once understood, 4‑vectors , and more generally tensors, greatly simplify the mathematics and conceptual understanding of special relativity.  Working exclusively with such objects leads to formulas that are \"manifestly\" relativistically invariant, which is a considerable advantage in non-trivial contexts.  For instance, demonstrating relativistic invariance of Maxwell's equations in their usual form is not trivial, while it is merely a routine calculation (really no more than an observation) using the field strength tensor formulation.  On the other hand, general relativity, from the outset, relies heavily on 4‑vectors , and more generally tensors, representing physically relevant entities.  Relating these via equations that do not rely on specific coordinates requires tensors, capable of connecting such 4‑vectors even within a \"curved\" spacetime, and not just within a \"flat\" one as in special relativity.  The study of tensors is outside the scope of this article, which provides only a basic discussion of spacetime. A 4-tuple, \"A = (A, A, A, A\") is a \"4-vector\" if its component \"A\" transform between frames according the Lorentz transformation. If using \"(ct, x, y, z)\" coordinates, \"A\" is a 4–vector if it transforms (in the \"x\"-direction ) according to which comes from simply replacing \"ct\" with \"A\" and \"x\" with \"A\" in the earlier presentation of the Lorentz transformation.  As usual, when we write \"x\", \"t\", etc. we generally mean \"Δx\", \"Δt\" etc. The last three components of a 4–vector must be a standard vector in three-dimensional space.  Therefore, a 4–vector must transform like \"(c Δt, Δx, Δy, Δz)\" under Lorentz transformations as well as rotations. \"Return to Introduction\" \"Return to Introduction\" As expected, the final components of the above 4-vectors are all standard 3-vectors corresponding to spatial 3-momentum , 3-force etc. \"Return to Introduction\" The first postulate of special relativity declares the equivalency of all inertial frames.  A physical law holding in one frame must apply in all frames, since otherwise it would be possible to differentiate between frames.  As noted in the previous discussion of energy and momentum conservation, Newtonian momenta fail to behave properly under Lorentzian transformation, and Einstein preferred to change the definition of momentum to one involving 4-vectors rather than give up on conservation of momentum. Physical laws must be based on constructs that are frame independent.  This means that physical laws may take the form of equations connecting scalars, which are always frame independent.  However, equations involving 4-vectors require the use of tensors with appropriate rank, which themselves can be thought of as being built up from 4-vectors . \"Return to Introduction\" Click here for a brief section summary It is a common misconception that special relativity is applicable only to inertial frames, and that it is unable to handle accelerating objects or accelerating reference frames.  Actually, accelerating objects can generally be analyzed without needing to deal with accelerating frames at all.  It is only when gravitation is significant that general relativity is required. Properly handling accelerating frames does requires some care, however.  The difference between special and general relativity is that (1) In special relativity, all velocities are relative, but acceleration is absolute.  (2) In general relativity, all motion is relative, whether inertial, accelerating, or rotating.  To accommodate this difference, general relativity uses curved spacetime. In this section, we analyze several scenarios involving accelerated reference frames. \"Return to Introduction\" The Dewan–Beran–Bell spaceship paradox (Bell's spaceship paradox) is a good example of a problem where intuitive reasoning unassisted by the geometric insight of the spacetime approach can lead to issues. In Fig. 4‑4, two identical spaceships float in space and are at rest relative to each other.  They are connected by a string which is capable of only a limited amount of stretching before breaking.  At a given instant in our frame, the observer frame, both spaceships accelerate in the same direction along the line between them with the same constant proper acceleration.  Will the string break? The main article for this section recounts how, when the paradox was new and relatively unknown, even professional physicists had difficulty working out the solution.  Two lines of reasoning lead to opposite conclusions.  Both arguments, which are presented below, are flawed even though one of them yields the correct answer. The problem with the first argument is that there is no \"frame of the spaceships.\"  There cannot be, because the two spaceships measure a growing distance between the two.  Because there is no common frame of the spaceships, the length of the string is ill-defined.  Nevertheless, the conclusion is correct, and the argument is mostly right.  The second argument, however, completely ignores the relativity of simultaneity. A spacetime diagram (Fig. 4‑5) makes the correct solution to this paradox almost immediately evident.  Two observers in Minkowski spacetime accelerate with constant magnitude formula_111 acceleration for proper time formula_112 (acceleration and elapsed time measured by the observers themselves, not some inertial observer).  They are comoving and inertial before and after this phase.  In Minkowski geometry, the length of the spacelike line segment formula_113 turns out to be greater than the length of the spacelike line segment formula_114. The length increase can be calculated with the help of the Lorentz transformation.  If, as illustrated in Fig. 4‑5, the acceleration is finished, the ships will remain at a constant offset in some frame formula_115 If formula_116 and formula_117 are the ships' positions in formula_118 the positions in frame formula_119 are: The \"paradox\", as it were, comes from the way that Bell constructed his example.  In the usual discussion of Lorentz contraction, the rest length is fixed and the moving length shortens as measured in frame formula_121.  As shown in Fig. 4‑5, Bell's example asserts the moving lengths formula_114 and formula_123 measured in frame formula_121 to be fixed, thereby forcing the rest frame length formula_113 in frame formula_119 to increase. \"Return to Introduction\" Certain special relativity problem setups can lead to insight about phenomena normally associated with general relativity, such as event horizons.  In the text accompanying Fig. 2‑7, we had noted that the magenta hyperbolae represented actual paths that are tracked by a constantly accelerating traveler in spacetime.  During periods of positive acceleration, the traveler's velocity just \"approaches\" the speed of light, while, measured in our frame, the traveler's acceleration constantly decreases. Fig. 4‑6 details various features of the traveler's motions with more specificity.  At any given moment, her space axis is formed by a line passing through the origin and her current position on the hyperbola, while her time axis is the tangent to the hyperbola at her position.  The velocity parameter formula_70 approaches a limit of one as formula_128 increases.  Likewise, formula_43 approaches infinity. The shape of the invariant hyperbola corresponds to a path of constant proper acceleration.  This is demonstrable as follows: Fig. 4‑6 illustrates a specific calculated scenario.  Terence (A) and Stella (B) initially stand together 100 light hours from the origin.  Stella lifts off at time 0, her spacecraft accelerating at 0.01 c per hour.  Every twenty hours, Terence radios updates to Stella about the situation at home (solid green lines).  Stella receives these regular transmissions, but the increasing distance (offset in part by time dilation) causes her to receive Terence's communications later and later as measured on her clock, and she \"never\" receives any communications from Terence after 100 hours on his clock (dashed green lines). After 100 hours according to Terence's clock, Stella enters a dark region.  She has traveled outside Terence's timelike future.  On the other hand, Terence can continue to receive Stella's messages to him indefinitely.  He just has to wait long enough.  Spacetime has been divided into distinct regions separated by an \"apparent\" event horizon.  So long as Stella continues to accelerate, she can never know what takes place behind this horizon. \"Return to Introduction\" <a href=\"%23Summary%20Basic%20propositions\">Click here for a brief section summary </a> Newton's theories assumed that motion takes place against the backdrop of a rigid Euclidean reference frame that extends throughout all space and all time.  Gravity is mediated by a mysterious force, acting instantaneously across a distance, whose actions are independent of the intervening space.  In contrast, Einstein denied that there is any background Euclidean reference frame that extends throughout space.  Nor is there any such thing as a force of gravitation, only the structure of spacetime itself. In spacetime terms, the path of a satellite orbiting the Earth is not dictated by the distant influences of the Earth, Moon and Sun.  Instead, the satellite moves through space only in response to local conditions.  Since spacetime is everywhere locally flat when considered on a sufficiently small scale, the satellite is always following a straight line in its local inertial frame.  We say that the satellite always follows along the path of a geodesic.  No evidence of gravitation can be discovered following alongside the motions of a single particle. In any analysis of spacetime, evidence of gravitation requires that one observe the relative accelerations of \"two\" bodies or two separated particles.  In Fig. 5‑1, two separated particles, free-falling in the gravitational field of the Earth, exhibit tidal accelerations due to local inhomogeneities in the gravitational field such that each particle follows a different path through spacetime.  The tidal accelerations that these particles exhibit with respect to each other do not require forces for their explanation.  Rather, Einstein described them in terms of the geometry of spacetime, i.e. the curvature of spacetime.  These tidal accelerations are strictly local.  It is the cumulative total effect of many local manifestations of curvature that result in the \"appearance\" of a gravitational force acting at a long range from Earth. Two central propositions underlie general relativity. To go from the elementary description above of curved spacetime to a complete description of gravitation requires tensor calculus and differential geometry, topics both requiring considerable study.  Without these mathematical tools, it is possible to write \"about\" general relativity, but it is not possible to demonstrate any non-trivial derivations. Rather than this section attempting to offer a (yet another) relatively non-mathematical presentation \"about\" general relativity, the reader is referred to the featured Wikipedia articles Introduction to general relativity and General relativity. Instead, the focus in this section will be to explore a handful of elementary scenarios that serve to give somewhat of the flavor of general relativity. \"Return to Introduction\" Click here for a brief section summary In the discussion of special relativity, forces played no more than a background role.  Special relativity assumes the ability to define inertial frames that fill all of spacetime, all of whose clocks run at the same rate as the clock at the origin.  Is this really possible?  In a nonuniform gravitational field, experiment dictates that the answer is no.  Gravitational fields make it impossible to construct a \"global\" inertial frame.  In small enough regions of spacetime, \"local\" inertial frames are still possible.  General relativity involves the systematic stitching together of these local frames into a more general picture of spacetime. Shortly after the publication of the general theory in 1916, a number of scientists pointed out that general relativity predicts the existence of gravitational redshift.  Einstein himself suggested the following thought experiment: (i) Assume that a tower of height \"h\" (Fig. 5‑3) has been constructed.  (ii) Drop a particle of rest mass \"m\" from the top of the tower.  It falls freely with acceleration \"g\", reaching the ground with velocity \"v\" = (2\"gh\") , so that its total energy \"E\", as measured by an observer on the ground, is \"m\" = ½\"mv\"/\"c\" = \"m + mgh/c\".  (iii) A mass-energy converter transforms the total energy of the particle into a single high energy photon, which it directs upward.  (iv) At the top of the tower, an energy-mass converter transforms the energy of the photon \"E' \" back into a particle of rest mass \"m' \". It must be that \"m\" = \"m' \" , since otherwise one would be able to construct a perpetual motion device.  We therefore predict that \"E' \" = \"m\" , so that A photon climbing in Earth's gravitational field loses energy and is redshifted.  Early attempts to measure this redshift through astronomical observations were somewhat inconclusive, but definitive laboratory observations were performed by Pound & Rebka (1959) and later by Pound & Snider (1964). Light has an associated frequency, and this frequency may be used to drive the workings of a clock.  The gravitational redshift leads to an important conclusion about time itself: Gravity makes time run slower.  Suppose we build two identical clocks whose rates are controlled by some stable atomic transition.  Place one clock on top of the tower, while the other clock remains on the ground.  An experimenter on top of the tower observes that signals from the ground clock are lower in frequency than those of the clock next to her on the tower.  Light going up the tower is a just a wave, and it is impossible for wave crests to disappear on the way up.  Exactly as many oscillations of light arrive at the top of the tower as were emitted at the bottom.  The experimenter concludes that the ground clock is running slow, and can confirm this by bringing the tower clock down to compare side-by-side with the ground clock.  For a 1 km tower, the discrepancy would amount to about 9.4 nanoseconds per day, easily measurable with modern instrumentation. Clocks in a gravitational field do not all run at the same rate.  Experiments such as the Pound–Rebka experiment have firmly established curvature of the time component of spacetime.  The Pound–Rebka experiment says nothing about curvature of the \"space\" component of spacetime.  But note that the theoretical arguments predicting gravitational time dilation do not depend on the details of general relativity at all.  \"Any\" theory of gravity will predict gravitational time dilation if it respects the principle of equivalence.  This includes Newtonian gravitation.  A standard demonstration in general relativity is to show how, in the \"Newtonian limit\" (i.e. the particles are moving slowly, the gravitational field is weak, and the field is static), curvature of time alone is sufficient to derive Newton's law of gravity. Newtonian gravitation is a theory of curved time.  General relativity is a theory of curved time \"and\" curved space.  Given \"G\" as the gravitational constant, \"M\" as the mass of a Newtonian star, and orbiting bodies of insignificant mass at distance \"r\" from the star, the spacetime interval for Newtonian gravitation is one for which only the time coefficient is variable: \"Return to Introduction\" Click here for a brief section summary The formula_146 coefficient in front of formula_147 describes the curvature of time in Newtonian gravitation, and this curvature completely accounts for all Newtonian gravitational effects.  As expected, this correction factor is directly proportional to formula_148 and formula_149, and because of the formula_150 in the denominator, the correction factor increases as one approaches the gravitating body, meaning that time is curved. But general relativity is a theory of curved space \"and\" curved time, so if there are terms modifying the spatial components of the spacetime interval presented above, shouldn't their effects be seen on, say, planetary and satellite orbits due to curvature correction factors applied to the spatial terms? The answer is that they \"are\" seen, but the effects are tiny.  The reason is that planetary velocities are extremely small compared to the speed of light, so that for planets and satellites of the solar system, the formula_147 term dwarfs the spatial terms. Despite the minuteness of the spatial terms, the first indications that something was wrong with Newtonian gravitation were discovered over a century-and-a-half ago.  In 1859, Urbain Le Verrier, in an analysis of available timed observations of transits of Mercury over the Sun's disk from 1697 to 1848, reported that known physics could not explain the orbit of Mercury, unless there possibly existed a planet or asteroid belt within the orbit of Mercury.  The perihelion of Mercury's orbit exhibited an excess rate of precession over that which could be explained by the tugs of the other planets.  The ability to detect and accurately measure the minute value of this anomalous precession (only 43 arc seconds per tropical century) is testimony to the sophistication of 19th century astrometry. As the famous astronomer who had earlier discovered the existence of Neptune \"at the tip of his pen\" by analyzing wobbles in the orbit of Uranus, Le Verrier's announcement triggered a two-decades long period of \"Vulcan-mania\", as professional and amateur astronomers alike hunted for the hypothetical new planet.  This search included several false sightings of Vulcan.  It was ultimately established that no such planet or asteroid belt existed. In 1916, Einstein was to show that this anomalous precession of Mercury is explained by the spatial terms in the curvature of spacetime.  Curvature in the temporal term, being simply an expression of Newtonian gravitation, has no part in explaining this anomalous precession.  The success of his calculation was a powerful indication to Einstein's peers that the general theory of relativity could be correct. The most spectacular of Einstein's predictions was his calculation that the curvature terms in the spatial components of the spacetime interval could be measured in the bending of light around a massive body.  Light has a slope of ±1 on a spacetime diagram.  Its movement in space is equal to its movement in time.  For the weak field expression of the invariant interval, Einstein calculated an exactly equal but opposite sign curvature in its spatial components. In Newton's gravitation, the formula_146 coefficient in front of formula_147 predicts bending of light around a star.  In general relativity, the formula_156 coefficient in front of formula_157 predicts a \"doubling\" of the total bending. The story of the 1919 Eddington eclipse expedition and Einstein's rise to fame is well told elsewhere. \"Return to Introduction\" Click here for a brief section summary In Newton's theory of gravitation, the only source of gravitational force is mass. In contrast, general relativity identifies several sources of spacetime curvature in addition to mass.  In the Einstein field equations, the sources of gravity are presented on the right-hand side in formula_158 the stress–energy tensor. Fig. 5‑5 classifies the various sources of gravity in the stress-energy tensor: One important conclusion to be derived from the equations is that, colloquially speaking, \"gravity itself creates gravity\".  Energy has mass.  Even in Newtonian gravity, the gravitational field is associated with an energy, \"E = mgh\", called the gravitational potential energy.  In general relativity, the energy of the gravitational field feeds back into creation of the gravitational field.  This makes the equations nonlinear and hard to solve in anything other than weak field cases.  Numerical relativity is a branch of general relativity using numerical methods to solve and analyze problems, often employing supercomputers to study black holes, gravitational waves, neutron stars and other phenomena in the strong field regime. \"Return to Introduction\" In special relativity, mass-energy is closely connected to momentum.  As we have discussed earlier in the section on Energy and momentum, just as space and time are different aspects of a more comprehensive entity called spacetime, mass-energy and momentum are merely different aspects of a unified, four-dimensional quantity called four-momentum.  In consequence, if mass-energy is a source of gravity, momentum must also be a source.  The inclusion of momentum as a source of gravity leads to the prediction that moving or rotating masses can generate fields analogous to the magnetic fields generated by moving charges, a phenomenon known as gravitomagnetism. It is well known that the force of magnetism can be deduced by applying the rules of special relativity to moving charges.  (An eloquent demonstration of this was presented by Feynman in volume II, chapter 13–6 of his \"Lectures on Physics\", available online.)  Analogous logic can be used to demonstrate the origin of gravitomagnetism.  In Fig. 5‑7a, two parallel, infinitely long streams of massive particles have equal and opposite velocities \"−v\" and \"+v\" relative to a test particle at rest and centered between the two.  Because of the symmetry of the setup, the net force on the central particle is zero.  Assume \"v\" « \"c\" so that velocities are simply additive.  Fig. 5‑7b shows exactly the same setup, but in the frame of the upper stream.  The test particle has a velocity of \"+v\", and the bottom stream has a velocity of +2\"v\".  Since the physical situation has not changed, only the frame in which things are observed, the test particle should not be attracted towards either stream.  But it is not at all clear that the forces exerted on the test particle are equal.  (1) Since the bottom stream is moving faster than the top, each particle in the bottom stream has a larger mass energy than a particle in the top.  (2) Because of Lorentz contraction, there are more particles per unit length in the bottom stream than in the top stream.  (3) Another contribution to the active gravitational mass of the bottom stream comes from an additional pressure term which, at this point, we do not have sufficient background to discuss.  All of these effects together would seemingly demand that the test particle be drawn towards the bottom stream. The test particle is not drawn to the bottom stream because of a velocity-dependent force that serves to repel a particle \"that is moving in the same direction as the bottom stream.\"  This velocity-dependent gravitational effect is gravitomagnetism. Matter in motion through a gravitomagnetic field is hence subject to so-called \"frame-dragging\" effects analogous to electromagnetic induction.  It has been proposed that such gravitomagnetic forces underlie the generation of the relativistic jets (Fig. 5‑8) ejected by some rotating supermassive black holes. \"Return to Introduction\" Quantities that are directly related to energy and momentum should be sources of gravity as well, namely internal pressure and stress.  Taken together, mass-energy , momentum, pressure and stress all serve as sources of gravity: Collectively, they are what tells spacetime how to curve. General relativity predicts that pressure acts as a gravitational source with exactly the same strength as mass-energy density.  The inclusion of pressure as a source of gravity leads to dramatic differences between the predictions of general relativity versus those of Newtonian gravitation.  For example, the pressure term sets a maximum limit to the mass of a neutron star.  The more massive a neutron star, the more pressure is required to support its weight against gravity.  The increased pressure, however, adds to the gravity acting on the star's mass.  Above a certain mass determined by the Tolman–Oppenheimer–Volkoff limit, the process becomes runaway and the neutron star collapses to a black hole. The stress terms become highly significant when performing calculations such as hydrodynamic simulations of core-collapse supernovae. \"Return to Introduction\" These predictions for the roles of pressure, momentum and stress as sources of spacetime curvature are elegant and play an important role in theory.  In regards to pressure, the early universe was radiation dominated, and it is highly unlikely that any of the relevant cosmological data (e.g. nucleosynthesis abundances, etc.) could be reproduced if pressure did not contribute to gravity, or if it did not have the same strength as a source of gravity as mass-energy .  Likewise, the mathematical consistency of the Einstein field equations would be broken if the stress terms didn't contribute as a source of gravity. All that is well and good, but are there any direct, quantitative experimental or observational measurements that confirm that these terms contribute to gravity with the correct strength? Before discussing the experimental evidence regarding these other sources of gravity, we need first to discuss Bondi's distinctions between different possible types of mass: (1) active mass (formula_163) is the mass which acts as the source of a gravitational field; (2) passive mass (formula_164) is the mass which reacts to a gravitational field; (3) inertial mass (formula_165) is the mass which reacts to acceleration. In Newtonian theory, In general relativity, \"Return to Introduction\" The classic experiment to measure the strength of a gravitational source (i.e. its active mass) was first conducted in 1797 by Henry Cavendish (Fig. 5‑9a).  Two small but dense balls are suspended on a fine wire, making a torsion balance.  Bringing two large test masses close to the balls introduces a detectable torque.  Given the dimensions of the apparatus and the measurable spring constant of the torsion wire, the gravitational constant \"G\" can be determined. To study pressure effects by compressing the test masses is hopeless, because attainable laboratory pressures are insignificant in comparison with the mass-energy of a metal ball. However, the repulsive electromagnetic pressures resulting from protons being tightly squeezed inside atomic nuclei are typically on the order of 10 atm ≈ 10 Pa ≈ 10 kg·sm.  This amounts to about 1% of the nuclear mass density of approximately 10kg/m (after factoring in c ≈ 9×10ms). If pressure does not act as a gravitational source, then the ratio formula_176 should be lower for nuclei with higher atomic number \"Z\", in which the electrostatic pressures are higher.  L. B. Kreuzer (1968) did a Cavendish experiment using a Teflon mass suspended in a mixture of the liquids trichloroethylene and dibromoethane having the same buoyant density as the Teflon (Fig. 5‑9b).  Fluorine has atomic number \"Z\" = 9 , while bromine has \"Z\" = 35 .  Kreuzer found that repositioning the Teflon mass caused no differential deflection of the torsion bar, hence establishing active mass and passive mass to be equivalent to a precision of 5×10. Although Kreuzer originally considered this experiment merely to be a test of the ratio of active mass to passive mass, Clifford Will (1976) reinterpreted the experiment as a fundamental test of the coupling of sources to gravitational fields. In 1986, Bartlett and Van Buren noted that lunar laser ranging had detected a 2-km offset between the moon’s center of figure and its center of mass.  This indicates an asymmetry in the distribution of Fe (abundant in the Moon's core) and Al (abundant in its crust and mantle).  If pressure did not contribute equally to spacetime curvature as does mass-energy, the moon would not be in the orbit predicted by classical mechanics.  They used their measurements to tighten the limits on any discrepancies between active and passive mass to about 1×10. \"Return to Introduction\" The existence of gravitomagnetism was proven by Gravity Probe B (GP-B) , a satellite-based mission which launched on 20 April 2004.  The spaceflight phase lasted until <time>2005</time>.  The mission aim was to measure spacetime curvature near Earth, with particular emphasis on gravitomagnetism. Initial results confirmed the relatively large geodetic effect (which is due to simple spacetime curvature, and is also known as de Sitter precession) to an accuracy of about 1%.  The much smaller frame-dragging effect (which is due to gravitomagnetism, and is also known as Lense–Thirring precession) was difficult to measure because of unexpected charge effects causing variable drift in the gyroscopes.  Nevertheless, by <time datetime=\"2008-08\">August 2008</time>, the frame-dragging effect had been confirmed to within 15% of the expected result, while the geodetic effect was confirmed to better than 0.5%. Subsequent measurements of frame dragging by laser-ranging observations of the LARES, LAGEOS-1 and LAGEOS-2 satellites has improved on the GP-B measurement, with results (as of 2016) demonstrating the effect to within 5% of its theoretical value, although there has been some disagreement on the accuracy of this result. Another effort, the Gyroscopes in General Relativity (GINGER) experiment, seeks to use three 6 m ring lasers mounted at right angles to each other 1400 m below the Earth's surface to measure this effect. \"Return to Introduction\" \"Return to Introduction\" For physical reasons, a spacetime continuum is mathematically defined as a four-dimensional, smooth, connected Lorentzian manifold formula_177.  This means the smooth Lorentz metric formula_178 has signature formula_179.  The metric determines the \"\", as well as determining the geodesics of particles and light beams.  About each point (event) on this manifold, coordinate charts are used to represent observers in reference frames.  Usually, Cartesian coordinates formula_180 are used.  Moreover, for simplicity's sake, units of measurement are usually chosen such that the speed of light formula_1 is equal to 1. A reference frame (observer) can be identified with one of these coordinate charts; any such observer can describe any event formula_182.  Another reference frame may be identified by a second coordinate chart about formula_182.  Two observers (one in each reference frame) may describe the same event formula_182 but obtain different descriptions. Usually, many overlapping coordinate charts are needed to cover a manifold.  Given two coordinate charts, one containing formula_182 (representing an observer) and another containing formula_186 (representing another observer), the intersection of the charts represents the region of spacetime in which both observers can measure physical quantities and hence compare results.  The relation between the two sets of measurements is given by a non-singular coordinate transformation on this intersection.  The idea of coordinate charts as local observers who can perform measurements in their vicinity also makes good physical sense, as this is how one actually collects physical data—locally. For example, two observers, one of whom is on Earth, but the other one who is on a fast rocket to Jupiter, may observe a comet crashing into Jupiter (this is the event formula_182).  In general, they will disagree about the exact location and timing of this impact, i.e., they will have different 4-tuples formula_180 (as they are using different coordinate systems).  Although their kinematic descriptions will differ, dynamical (physical) laws, such as momentum conservation and the first law of thermodynamics, will still hold.  In fact, relativity theory requires more than this in the sense that it stipulates these (and all other physical) laws must take the same form in all coordinate systems.  This introduces tensors into relativity, by which all physical quantities are represented. Geodesics are said to be time-like, null, or space-like if the tangent vector to one point of the geodesic is of this nature.  Paths of particles and light beams in spacetime are represented by time-like and null (light-like) geodesics, respectively. \"Return to Introduction\" \"Return to Introduction\" ^Definitions (click here to return to main) ^History (click here to return to main) \"Return to Introduction\" ^Spacetime interval (click here to return to main) ^Reference frames (click here to return to main) ^Light cone (click here to return to main) ^Relativity of simultaneity (click here to return to main) ^Invariant hyperbola (click here to return to main) ^Time dilation and length contraction (click here to return to main) ^Mutual time dilation and the twin paradox (click here to return to main) ^Mutual time dilation (click here to return to main) ^Twin paradox (click here to return to main) ^Gravitation (click here to return to main) \"Return to Introduction\" ^Galilean transformations (click here to return to main) ^Relativistic composition of velocities (click here to return to main) ^Time dilation and length contraction revisited (click here to return to main) ^Lorentz transformations (click here to return to main) ^Doppler effect (click here to return to main) ^Energy and momentum (click here to return to main) ^Conservation laws (click here to return to main) \"Return to Introduction\" ^Rapidity (click here to return to main) ^4‑vectors (click here to return to main) ^Acceleration (click here to return to main) \"Return to Introduction\" ^Basic propositions (click here to return to main) ^Curvature of time (click here to return to main) ^Curvature of space (click here to return to main) ^Sources of spacetime curvature (click here to return to main) \"Return to Introduction\"\n\nBeta distribution In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by \"α\" and \"β\", that appear as exponents of the random variable and control the shape of the distribution. The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines. In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions.  For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission.  The beta distribution is a suitable model for the random behavior of percentages and proportions. The usual formulation of the beta distribution is also known as the beta distribution of the first kind, whereas \"beta distribution of the second kind\" is an alternative name for the beta prime distribution. The probability density function (pdf) of the beta distribution, for 0 ≤ \"x\" ≤ 1 , and shape parameters \"α\", \"β\" > 0, is a power function of the variable \"x\" and of its reflection (1 − \"x\") as follows: where Γ(\"z\") is the gamma function.  The beta function, formula_16, is a normalization constant to ensure that the total probability integrates to 1.  In the above equations \"x\" is a realization—an observed value that actually occurred—of a random process \"X\". This definition includes both ends \"x\" = 0 and \"x\" = 1 , which is consistent with definitions for other continuous distributions supported on a bounded interval which are special cases of the beta distribution, for example the arcsine distribution, and consistent with several authors, like N. L. Johnson and S. Kotz.  However, the inclusion of \"x\" = 0 and \"x\" = 1 does not work for \"α\", \"β\" < 1 ; accordingly, several other authors, including W. Feller, choose to exclude the ends \"x\" = 0 and \"x\" = 1 , (so that the two ends are not actually part of the domain of the density function) and consider instead 0 < \"x\" < 1 . Several authors, including N. L. Johnson and S. Kotz, use the symbols \"p\" and \"q\" (instead of \"α\" and \"β\") for the shape parameters of the beta distribution, reminiscent of the symbols traditionally used for the parameters of the Bernoulli distribution, because the beta distribution approaches the Bernoulli distribution in the limit when both shape parameters \"α\" and \"β\" approach the value of zero. In the following, a random variable \"X\" beta-distributed with parameters \"α\" and \"β\" will be denoted by: Other notations for beta-distributed random variables used in the statistical literature are formula_18 and formula_19. The cumulative distribution function is where formula_21 is the incomplete beta function and formula_22 is the regularized incomplete beta function. The mode of a Beta distributed random variable \"X\" with \"α\", \"β\" > 1 is the most likely value of the distribution (corresponding to the peak in the PDF), and is given by the following expression: When both parameters are less than one (\"α\", \"β\" < 1), this is the anti-mode: the lowest point of the probability density curve. Letting \"α\" = \"β\", the expression for the mode simplifies to 1/2, showing that for \"α\" = \"β\" > 1 the mode (resp.  anti-mode when \"α\", \"β\" < 1 ), is at the center of the distribution: it is symmetric in those cases.  See Shapes section in this article for a full list of mode cases, for arbitrary values of \"α\" and \"β\".  For several of these cases, the maximum value of the density function occurs at one or both ends.  In some cases the (maximum) value of the density function occurring at the end is finite.  For example, in the case of \"α\" = 2, \"β\" = 1 (or \"α\" = 1, \"β\" = 2), the density function becomes a right-triangle distribution which is finite at both ends.  In several other cases there is a singularity at one end, where the value of the density function approaches infinity.  For example, in the case \"α\" = \"β\" = 1/2, the Beta distribution simplifies to become the arcsine distribution.  There is debate among mathematicians about some of these cases and whether the ends (\"x\" = 0, and \"x\" = 1) can be called \"modes\" or not. The median of the beta distribution is the unique real number formula_24 for which the regularized incomplete beta function formula_25.  There is no general closed-form expression for the median of the beta distribution for arbitrary values of \"α\" and \"β\".  Closed-form expressions for particular values of the parameters \"α\" and \"β\" follow: The following are the limits with one parameter finite (non-zero) and the other approaching these limits: A reasonable approximation of the value of the median of the beta distribution, for both α and β greater or equal to one, is given by the formula When α, β ≥ 1, the relative error (the absolute error divided by the median) in this approximation is less than 4% and for both α ≥ 2 and β ≥ 2 it is less than 1%.  The absolute error divided by the difference between the mean and the mode is similarly small: The expected value (mean) (\"μ\") of a Beta distribution random variable \"X\" with two parameters \"α\" and \"β\" is a function of only the ratio \"β\"/\"α\" of these parameters: Letting \"α\" = \"β\" in the above expression one obtains \"μ\" = 1/2 , showing that for \"α\" = \"β\" the mean is at the center of the distribution: it is symmetric.  Also, the following limits can be obtained from the above expression: Therefore, for \"β\"/\"α\" → 0, or for \"α\"/\"β\" → ∞, the mean is located at the right end, \"x\" = 1 .  For these limit ratios, the beta distribution becomes a one-point degenerate distribution with a Dirac delta function spike at the right end, \"x\" = 1 , with probability 1, and zero probability everywhere else.  There is 100% probability (absolute certainty) concentrated at the right end, \"x\" = 1 . Similarly, for \"β\"/\"α\" → ∞, or for \"α\"/\"β\" → 0, the mean is located at the left end, \"x\" = 0 .  The beta distribution becomes a 1-point Degenerate distribution with a Dirac delta function spike at the left end, \"x\" = 0, with probability 1, and zero probability everywhere else.  There is 100% probability (absolute certainty) concentrated at the left end, \"x\" = 0.  Following are the limits with one parameter finite (non-zero) and the other approaching these limits: While for typical unimodal distributions (with centrally located modes, inflexion points at both sides of the mode, and longer tails) (with Beta(\"α\", \"β\") such that \"α\", \"β\" > 2 ) it is known that the sample mean (as an estimate of location) is not as robust as the sample median, the opposite is the case for uniform or \"U-shaped\" bimodal distributions (with Beta(\"α\", \"β\") such that \"α\", \"β\" ≤ 1 ), with the modes located at the ends of the distribution.  As Mosteller and Tukey remark ( p. 207) \"the average of the two extreme observations uses all the sample information.  This illustrates how, for short-tailed distributions, the extreme observations should get more weight.\"  By contrast, it follows that the median of \"U-shaped\" bimodal distributions with modes at the edge of the distribution (with Beta(\"α\", \"β\") such that \"α\", \"β\" ≤ 1 ) is not robust, as the sample median drops the extreme sample observations from consideration.  A practical application of this occurs for example for random walks, since the probability for the time of the last visit to the origin in a random walk is distributed as the arcsine distribution Beta(1/2, 1/2): the mean of a number of realizations of a random walk is a much more robust estimator than the median (which is an inappropriate sample measure estimate in this case). The logarithm of the geometric mean \"G\" of a distribution with random variable \"X\" is the arithmetic mean of ln(\"X\"), or, equivalently, its expected value: For a beta distribution, the expected value integral gives: where \"ψ\" is the digamma function. Therefore, the geometric mean of a beta distribution with shape parameters \"α\" and \"β\" is the exponential of the digamma functions of \"α\" and \"β\" as follows: While for a beta distribution with equal shape parameters α = β, it follows that skewness = 0 and mode = mean = median = 1/2, the geometric mean is less than 1/2: 0 < \"G\" < 1/2 .  The reason for this is that the logarithmic transformation strongly weights the values of \"X\" close to zero, as ln(\"X\") strongly tends towards negative infinity as \"X\" approaches zero, while ln(\"X\") flattens towards zero as \"X\" → 1 . Along a line \"α\" = \"β\" , the following limits apply: Following are the limits with one parameter finite (non-zero) and the other approaching these limits: The accompanying plot shows the difference between the mean and the geometric mean for shape parameters α and β from zero to 2.  Besides the fact that the difference between them approaches zero as α and β approach infinity and that the difference becomes large for values of α and β approaching zero, one can observe an evident asymmetry of the geometric mean with respect to the shape parameters α and β.  The difference between the geometric mean and the mean is larger for small values of α in relation to β than when exchanging the magnitudes of β and α. N. L.Johnson and S. Kotz suggest the logarithmic approximation to the digamma function \"ψ\"(\"α\") ≈ ln(\"α\" − 1/2) which results in the following approximation to the geometric mean: Numerical values for the relative error in this approximation follow: [(\"α\" = \"β\" = 1): 9.39% ]; [(\"α\" = \"β\" = 2): 1.29% ]; [(\"α\" = 2, \"β\" = 3): 1.51% ]; [(\"α\" = 3, \"β\" = 2): 0.44% ]; [(\"α\" = \"β\" = 3): 0.51% ]; [(\"α\" = \"β\" = 4): 0.26% ]; [(\"α\" = 3, \"β\" = 4): 0.55% ]; [(\"α\" = 4, \"β\" = 3): 0.24% ]. Similarly, one can calculate the value of shape parameters required for the geometric mean to equal 1/2.  Given the value of the parameter \"β\", what would be the value of the other parameter, \"α\", required for the geometric mean to equal 1/2? .  The answer is that (for \"β\" > 1 ), the value of \"α\" required tends towards \"β\" + 1/2 as \"β\" → ∞ .  For example, all these couples have the same geometric mean of 1/2: [\"β\" = 1, \"α\" = 1.4427 ], [\"β\" = 2, \"α\" = 2.46958 ], [\"β\" = 3, \"α\" = 3.47943 ], [\"β\" = 4, \"α\" = 4.48449 ], [\"β\" = 5, \"α\" = 5.48756 ], [\"β\" = 10, \"α\" = 10.4938 ], [\"β\" = 100, \"α\" = 100.499 ]. The fundamental property of the geometric mean, which can be proven to be false for any other mean, is This makes the geometric mean the only correct mean when averaging \"normalized\" results, that is results that are presented as ratios to reference values.  This is relevant because the beta distribution is a suitable model for the random behavior of percentages and it is particularly suitable to the statistical modelling of proportions.  The geometric mean plays a central role in maximum likelihood estimation, see section \"Parameter estimation, maximum likelihood.\"  Actually, when performing maximum likelihood estimation, besides the geometric mean \"G\" based on the random variable X, also another geometric mean appears naturally: the geometric mean based on the linear transformation ––(1 − \"X\") , the mirror-image of \"X\", denoted by \"G\": Along a line \"α\" = \"β\" , the following limits apply: Following are the limits with one parameter finite (non-zero) and the other approaching these limits: It has the following approximate value: Although both \"G\" and \"G\" are asymmetric, in the case that both shape parameters are equal \"α\" = \"β\" , the geometric means are equal: \"G\" = \"G\".  This equality follows from the following symmetry displayed between both geometric means: The inverse of the harmonic mean (\"H\") of a distribution with random variable \"X\" is the arithmetic mean of 1/\"X\", or, equivalently, its expected value.  Therefore, the harmonic mean (\"H\") of a beta distribution with shape parameters \"α\" and \"β\" is: The harmonic mean (\"H\") of a Beta distribution with α < 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter α less than unity. Letting \"α\" = \"β\" in the above expression one obtains showing that for \"α\" = \"β\" the harmonic mean ranges from 0, for \"α\" = \"β\" = 1, to 1/2, for \"α\" = \"β\" → ∞. Following are the limits with one parameter finite (non-zero) and the other approaching these limits: The harmonic mean plays a role in maximum likelihood estimation for the four parameter case, in addition to the geometric mean.  Actually, when performing maximum likelihood estimation for the four parameter case, besides the harmonic mean \"H\" based on the random variable \"X\", also another harmonic mean appears naturally: the harmonic mean based on the linear transformation (1 − \"X\"), the mirror-image of \"X\", denoted by \"H\": The harmonic mean (\"H\") of a Beta distribution with \"β\" < 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter \"β\" less than unity. Letting \"α\" = \"β\" in the above expression one obtains showing that for \"α\" = \"β\" the harmonic mean ranges from 0, for \"α\" = \"β\" = 1, to 1/2, for \"α\" = \"β\" → ∞. Following are the limits with one parameter finite (non-zero) and the other approaching these limits: Although both \"H\" and \"H\" are asymmetric, in the case that both shape parameters are equal \"α\" = \"β\", the harmonic means are equal: \"H\" = \"H\".  This equality follows from the following symmetry displayed between both harmonic means: The variance (the second moment centered on the mean) of a Beta distribution random variable \"X\" with parameters α and β is: Letting α = β in the above expression one obtains showing that for \"α\" = \"β\" the variance decreases monotonically as \"α\" = \"β\" increases.  Setting \"α\" = \"β\" = 0 in this expression, one finds the maximum variance var(\"X\") = 1/4 which only occurs approaching the limit, at \"α\" = \"β\" = 0 . The beta distribution may also be parametrized in terms of its mean \"μ\" (0 < \"μ\" < 1) and sample size \"ν\" = \"α\" + \"β\" (\"ν\" > 0 ) (see section below titled \"Mean and sample size\"): Using this parametrization, one can express the variance in terms of the mean \"μ\" and the sample size \"ν\" as follows: Since \"ν\" = (\"α\" + \"β\") > 0 , it must follow that var(\"X\") < \"μ\"(1 − \"μ\") For a symmetric distribution, the mean is at the middle of the distribution, \"μ\" = 1/2 , and therefore: Also, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions: The logarithm of the geometric variance, ln(var), of a distribution with random variable \"X\" is the second moment of the logarithm of \"X\" centered on the geometric mean of \"X\", (ln(\"G\"): and therefore, the geometric variance is: In the Fisher information matrix, and the curvature of the log likelihood function, the logarithm of the geometric variance of the reflected variable (1 − \"X\") and the logarithm of the geometric covariance between \"X\" and (1 − \"X\") appear: For a beta distribution, higher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral.  They can be expressed in terms of higher order poly-gamma functions.  See the section titled \"Other moments, Moments of transformed random variables, Moments of logarithmically transformed random variables\".  The variance of the logarithmic variables and covariance of ln\"X\" and ln(1−\"X\") are: where the trigamma function, denoted ψ(α), is the second of the polygamma functions, and is defined as the derivative of the digamma function: Therefore, The accompanying plots show the log geometric variances and log geometric covariance versus the shape parameters \"α\" and \"β\".  The plots show that the log geometric variances and log geometric covariance are close to zero for shape parameters α and β greater than 2, and that the log geometric variances rapidly rise in value for shape parameter values \"α\" and \"β\" less than unity.  The log geometric variances are positive for all values of the shape parameters.  The log geometric covariance is negative for all values of the shape parameters, and it reaches large negative values for \"α\" and \"β\" less than unity. Following are the limits with one parameter finite (non-zero) and the other approaching these limits: Limits with two parameters varying: Although both ln(var) and ln(var) are asymmetric, when the shape parameters are equal, α = β, one has: ln(var) = ln(var).  This equality follows from the following symmetry displayed between both log geometric variances: The log geometric covariance is symmetric: The mean absolute deviation around the mean for the beta distribution with shape parameters α and β is: The mean absolute deviation around the mean is a more robust estimator of statistical dispersion than the standard deviation for beta distributions with tails and inflection points at each side of the mode, Beta(α, β) distributions with α,β > 2, as it depends on the linear (absolute) deviations rather than the square deviations from the mean.  Therefore, the effect of very large deviations from the mean are not as overly weighted. Using Stirling's approximation to the Gamma function, N.L.Johnson and S.Kotz derived the following approximation for values of the shape parameters greater than unity (the relative error for this approximation is only −3.5% for α = β = 1, and it decreases to zero as α → ∞, β → ∞): At the limit α → ∞, β → ∞, the ratio of the mean absolute deviation to the standard deviation (for the beta distribution) becomes equal to the ratio of the same measures for the normal distribution: formula_74.  For α = β = 1 this ratio equals formula_75, so that from α = β = 1 to α, β → ∞ the ratio decreases by 8.5%.  For α = β = 0 the standard deviation is exactly equal to the mean absolute deviation around the mean.  Therefore, this ratio decreases by 15% from α = β = 0 to α = β = 1, and by 25% from α = β = 0 to α, β → ∞ .  However, for skewed beta distributions such that α → 0 or β → 0, the ratio of the standard deviation to the mean absolute deviation approaches infinity (although each of them, individually, approaches zero) because the mean absolute deviation approaches zero faster than the standard deviation. Using the parametrization in terms of mean μ and sample size ν = α + β > 0: one can express the mean absolute deviation around the mean in terms of the mean μ and the sample size ν as follows: For a symmetric distribution, the mean is at the middle of the distribution, μ = 1/2, and therefore: Also, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions: The mean absolute difference for the Beta distribution is: The Gini coefficient for the Beta distribution is half of the relative mean absolute difference: The skewness (the third moment centered on the mean, normalized by the 3/2 power of the variance) of the beta distribution is Letting α = β in the above expression one obtains γ = 0, showing once again that for α = β the distribution is symmetric and hence the skewness is zero.  Positive skew (right-tailed) for α < β, negative skew (left-tailed) for α > β. Using the parametrization in terms of mean μ and sample size ν = α + β: one can express the skewness in terms of the mean μ and the sample size ν as follows: The skewness can also be expressed just in terms of the variance \"var\" and the mean μ as follows: The accompanying plot of skewness as a function of variance and mean shows that maximum variance (1/4) is coupled with zero skewness and the symmetry condition (μ = 1/2), and that maximum skewness (positive or negative infinity) occurs when the mean is located at one end or the other, so that the \"mass\" of the probability distribution is concentrated at the ends (minimum variance). The following expression for the square of the skewness, in terms of the sample size ν = α + β and the variance \"var\", is useful for the method of moments estimation of four parameters: This expression correctly gives a skewness of zero for α = β, since in that case (see section titled \"Variance\"): formula_86. For the symmetric case (α = β), skewness = 0 over the whole range, and the following limits apply: For the asymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions: The beta distribution has been applied in acoustic analysis to assess damage to gears, as the kurtosis of the beta distribution has been reported to be a good indicator of the condition of a gear.  Kurtosis has also been used to distinguish the seismic signal generated by a person's footsteps from other signals.  As persons or other targets moving on the ground generate continuous signals in the form of seismic waves, one can separate different targets based on the seismic waves they generate.  Kurtosis is sensitive to impulsive signals, so it's much more sensitive to the signal generated by human footsteps than other signals generated by vehicles, winds, noise, etc.  Unfortunately, the notation for kurtosis has not been standardized.  Kenney and Keeping use the symbol γ for the excess kurtosis, but Abramowitz and Stegun use different terminology.  To prevent confusion between kurtosis (the fourth moment centered on the mean, normalized by the square of the variance) and excess kurtosis, when using symbols, they will be spelled out as follows: Letting α = β in the above expression one obtains Therefore, for symmetric beta distributions, the excess kurtosis is negative, increasing from a minimum value of −2 at the limit as {α = β} → 0, and approaching a maximum value of zero as {α = β} → ∞.  The value of −2 is the minimum value of excess kurtosis that any distribution (not just beta distributions, but any distribution of any possible kind) can ever achieve.  This minimum value is reached when all the probability density is entirely concentrated at each end \"x\" = 0 and \"x\" = 1, with nothing in between: a 2-point Bernoulli distribution with equal probability 1/2 at each end (a coin toss: see section below \"Kurtosis bounded by the square of the skewness\" for further discussion).  The description of kurtosis as a measure of the \"potential outliers\" (or \"potential rare, extreme values\") of the probability distribution, is correct for all distributions including the beta distribution.  When rare, extreme values can occur in the beta distribution, the higher its kurtosis; otherwise, the kurtosis is lower.  For α ≠ β, skewed beta distributions, the excess kurtosis can reach unlimited positive values (particularly for α → 0 for finite β, or for β → 0 for finite α) because the side away from the mode will produce occasional extreme values.  Minimum kurtosis takes place when the mass density is concentrated equally at each end (and therefore the mean is at the center), and there is no probability mass density in between the ends. Using the parametrization in terms of mean μ and sample size ν = α + β: one can express the excess kurtosis in terms of the mean μ and the sample size ν as follows: The excess kurtosis can also be expressed in terms of just the following two parameters: the variance \"var\", and the sample size ν as follows: and, in terms of the variance \"var\" and the mean μ as follows: The plot of excess kurtosis as a function of the variance and the mean shows that the minimum value of the excess kurtosis (−2, which is the minimum possible value for excess kurtosis for any distribution) is intimately coupled with the maximum value of variance (1/4) and the symmetry condition: the mean occurring at the midpoint (μ = 1/2).  This occurs for the symmetric case of α = β = 0, with zero skewness.  At the limit, this is the 2 point Bernoulli distribution with equal probability 1/2 at each Dirac delta function end \"x\" = 0 and \"x\" = 1 and zero probability everywhere else.  (A coin toss: one face of the coin being \"x\" = 0 and the other face being \"x\" = 1.)  Variance is maximum because the distribution is bimodal with nothing in between the two modes (spikes) at each end.  Excess kurtosis is minimum: the probability density \"mass\" is zero at the mean and it is concentrated at the two peaks at each end.  Excess kurtosis reaches the minimum possible value (for any distribution) when the probability density function has two spikes at each end: it is bi-\"peaky\" with nothing in between them. On the other hand, the plot shows that for extreme skewed cases, where the mean is located near one or the other end (μ = 0 or μ = 1), the variance is close to zero, and the excess kurtosis rapidly approaches infinity when the mean of the distribution approaches either end. Alternatively, the excess kurtosis can also be expressed in terms of just the following two parameters: the square of the skewness, and the sample size ν as follows: From this last expression, one can obtain the same limits published practically a century ago by Karl Pearson in his paper, for the beta distribution (see section below titled \"Kurtosis bounded by the square of the skewness\").  Setting α + β= ν = 0 in the above expression, one obtains Pearson's lower boundary (values for the skewness and excess kurtosis below the boundary (excess kurtosis + 2 − skewness = 0) cannot occur for any distribution, and hence Karl Pearson appropriately called the region below this boundary the \"impossible region\").  The limit of α + β = ν → ∞ determines Pearson's upper boundary. therefore: Values of ν = α + β such that ν ranges from zero to infinity, 0 < ν < ∞, span the whole region of the beta distribution in the plane of excess kurtosis versus squared skewness. For the symmetric case (α = β), the following limits apply: For the unsymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions: The characteristic function is the Fourier transform of the probability density function.  The characteristic function of the beta distribution is Kummer's confluent hypergeometric function (of the first kind): where is the rising factorial, also called the \"Pochhammer symbol\".  The value of the characteristic function for \"t\" = 0, is one: Also, the real and imaginary parts of the characteristic function enjoy the following symmetries with respect to the origin of variable \"t\": The symmetric case α = β simplifies the characteristic function of the beta distribution to a Bessel function, since in the special case α + β = 2α the confluent hypergeometric function (of the first kind) reduces to a Bessel function (the modified Bessel function of the first kind formula_105 ) using Kummer's second transformation as follows: In the accompanying plots, the real part (Re) of the characteristic function of the beta distribution is displayed for symmetric (α = β) and skewed (α ≠ β) cases. It also follows that the moment generating function is In particular \"M\"(α; β; 0) = 1. Using the moment generating function, the \"k\"-th raw moment is given by the factor multiplying the (exponential series) term formula_109 in the series of the moment generating function where (\"x\") is a Pochhammer symbol representing rising factorial.  It can also be written in a recursive form as Since the moment generating function formula_112 has a positive radius of convergence, the beta distribution is determined by its moments. One can also show the following expectations for a transformed random variable, where the random variable \"X\" is Beta-distributed with parameters α and β: \"X\" ~ Beta(α, β).  The expected value of the variable \"(1−X)\" is the mirror-symmetry of the expected value based on \"X\": Due to the mirror-symmetry of the probability density function of the beta distribution, the variances based on variables \"X\" and \"(1−X)\" are identical, and the covariance on \"X(1-X)\" is the negative of the variance: These are the expected values for inverted variables, (these are related to the harmonic means, see section titled \"Harmonic mean\"): The following transformation by dividing the variable \"X\" by its mirror-image \"X\"/(1−\"X\") results in the expected value of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI): Variances of these transformed variables can be obtained by integration, as the expected values of the second moments centered on the corresponding variables: The following variance of the variable \"X\" divided by its mirror-image (\"X\"/(1−\"X\") results in the variance of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI): The covariances are: These expectations and variances appear in the four-parameter Fisher information matrix (section titled \"Fisher information,\" \"four parameters\") Expected values for logarithmic transformations (useful for maximum likelihood estimates, see section titled \"Parameter estimation, Maximum likelihood\" below) are discussed in this section.  The following logarithmic linear transformations are related to the geometric means \"G\" and \"G\" (see section titled \"Geometric mean\"): Where the digamma function ψ(α) is defined as the logarithmic derivative of the gamma function: Logit transformations are interesting, as they usually transform various shapes (including J-shapes) into (usually skewed) bell-shaped densities over the logit variable, and they may remove the end singularities over the original variable: Johnson considered the distribution of the logit - transformed variable ln(\"X\"/1−\"X\"), including its moment generating function and approximations for large values of the shape parameters.  This transformation extends the finite support [0, 1] based on the original variable \"X\" to infinite support in both directions of the real line (−∞, +∞). Higher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral.  They can be expressed in terms of higher order poly-gamma functions as follows: therefore the variance of the logarithmic variables and covariance of ln(\"X\") and ln(1−\"X\") are: where the trigamma function, denoted ψ(α), is the second of the polygamma functions, and is defined as the derivative of the digamma function: The variances and covariance of the logarithmically transformed variables \"X\" and (1−\"X\") are different, in general, because the logarithmic transformation destroys the mirror-symmetry of the original variables \"X\" and (1−\"X\"), as the logarithm approaches negative infinity for the variable approaching zero. These logarithmic variances and covariance are the elements of the Fisher information matrix for the beta distribution.  They are also a measure of the curvature of the log likelihood function (see section on Maximum likelihood estimation). The variances of the log inverse variables are identical to the variances of the log variables: It also follows that the variances of the logit transformed variables are: Given a beta distributed random variable, \"X\" ~ Beta(α, β), the differential entropy of \"X\" is(measured in nats), the expected value of the negative of the logarithm of the probability density function: where \"f\"(\"x\"; α, β) is the probability density function of the beta distribution: The digamma function ψ appears in the formula for the differential entropy as a consequence of Euler's integral formula for the harmonic numbers which follows from the integral: The differential entropy of the beta distribution is negative for all values of α and β greater than zero, except at α = β = 1 (for which values the beta distribution is the same as the uniform distribution), where the differential entropy reaches its maximum value of zero.  It is to be expected that the maximum entropy should take place when the beta distribution becomes equal to the uniform distribution, since uncertainty is maximal when all possible events are equiprobable. For α or β approaching zero, the differential entropy approaches its minimum value of negative infinity.  For (either or both) α or β approaching zero, there is a maximum amount of order: all the probability density is concentrated at the ends, and there is zero probability density at points located between the ends.  Similarly for (either or both) α or β approaching infinity, the differential entropy approaches its minimum value of negative infinity, and a maximum amount of order.  If either α or β approaches infinity (and the other is finite) all the probability density is concentrated at an end, and the probability density is zero everywhere else.  If both shape parameters are equal (the symmetric case), α = β, and they approach infinity simultaneously, the probability density becomes a spike (Dirac delta function) concentrated at the middle \"x\" = 1/2, and hence there is 100% probability at the middle \"x\" = 1/2 and zero probability everywhere else. The (continuous case) differential entropy was introduced by Shannon in his original paper (where he named it the \"entropy of a continuous distribution\"), as the concluding part of the same paper where he defined the discrete entropy.  It is known since then that the differential entropy may differ from the infinitesimal limit of the discrete entropy by an infinite offset, therefore the differential entropy can be negative (as it is for the beta distribution).  What really matters is the relative value of entropy. Given two beta distributed random variables, \"X\" ~ Beta(α, β) and \"X\" ~ Beta(α', β'), the cross entropy is (measured in nats) The cross entropy has been used as an error metric to measure the distance between two hypotheses.  Its absolute value is minimum when the two distributions are identical.  It is the information measure most closely related to the log maximum likelihood (see section on \"Parameter estimation.  Maximum likelihood estimation\")). The relative entropy, or Kullback–Leibler divergence \"D\"(\"X\", \"X\"), is a measure of the inefficiency of assuming that the distribution is \"X\" ~ Beta(α', β') when the distribution is really \"X\" ~ Beta(α, β).  It is defined as follows (measured in nats). The relative entropy, or Kullback–Leibler divergence, is always non-negative.  A few numerical examples follow: The Kullback–Leibler divergence is not symmetric \"D\"(\"X\", \"X\") ≠ \"D\"(\"X\", \"X\") for the case in which the individual beta distributions Beta(1, 1) and Beta(3, 3) are symmetric, but have different entropies \"h\"(\"X\") ≠ \"h\"(\"X\").  The value of the Kullback divergence depends on the direction traveled: whether going from a higher (differential) entropy to a lower (differential) entropy or the other way around.  In the numerical example above, the Kullback divergence measures the inefficiency of assuming that the distribution is (bell-shaped) Beta(3, 3), rather than (uniform) Beta(1, 1).  The \"h\" entropy of Beta(1, 1) is higher than the \"h\" entropy of Beta(3, 3) because the uniform distribution Beta(1, 1) has a maximum amount of disorder.  The Kullback divergence is more than two times higher (0.598803 instead of 0.267864) when measured in the direction of decreasing entropy: the direction that assumes that the (uniform) Beta(1, 1) distribution is (bell-shaped) Beta(3, 3) rather than the other way around.  In this restricted sense, the Kullback divergence is consistent with the second law of thermodynamics. The Kullback–Leibler divergence is symmetric \"D\"(\"X\", \"X\") = \"D\"(\"X\", \"X\") for the skewed cases Beta(3, 0.5) and Beta(0.5, 3) that have equal differential entropy \"h\"(\"X\") = \"h\"(\"X\"). The symmetry condition: follows from the above definitions and the mirror-symmetry \"f\"(\"x\"; α, β) = \"f\"(1−\"x\"; α, β) enjoyed by the beta distribution. If 1 < α < β then mode ≤ median ≤ mean.  Expressing the mode (only for α, β > 1), and the mean in terms of α and β: If 1 < β < α then the order of the inequalities are reversed.  For α, β > 1 the absolute distance between the mean and the median is less than 5% of the distance between the maximum and minimum values of \"x\".  On the other hand, the absolute distance between the mean and the mode can reach 50% of the distance between the maximum and minimum values of \"x\", for the (pathological) case of α = 1 and β = 1 (for which values the beta distribution approaches the uniform distribution and the differential entropy approaches its maximum value, and hence maximum \"disorder\"). For example, for α = 1.0001 and β = 1.00000001: It is known from the inequality of arithmetic and geometric means that the geometric mean is lower than the mean.  Similarly, the harmonic mean is lower than the geometric mean.  The accompanying plot shows that for α = β, both the mean and the median are exactly equal to 1/2, regardless of the value of α = β, and the mode is also equal to 1/2 for α = β > 1, however the geometric and harmonic means are lower than 1/2 and they only approach this value asymptotically as α = β → ∞. As remarked by Feller, in the Pearson system the beta probability density appears as type I (any difference between the beta distribution and Pearson's type I distribution is only superficial and it makes no difference for the following discussion regarding the relationship between kurtosis and skewness).  Karl Pearson showed, in Plate 1 of his paper published in 1916, a graph with the kurtosis as the vertical axis (ordinate) and the square of the skewness as the horizontal axis (abscissa), in which a number of distributions were displayed.  The region occupied by the beta distribution is bounded by the following two lines in the (skewness,kurtosis) plane, or the (skewness,excess kurtosis) plane: or, equivalently, (At a time when there were no powerful digital computers), Karl Pearson accurately computed further boundaries, for example, separating the \"U-shaped\" from the \"J-shaped\" distributions.  The lower boundary line (excess kurtosis + 2 − skewness = 0) is produced by skewed \"U-shaped\" beta distributions with both values of shape parameters α and β close to zero.  The upper boundary line (excess kurtosis − (3/2) skewness = 0) is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter.  Karl Pearson showed that this upper boundary line (excess kurtosis − (3/2) skewness = 0) is also the intersection with Pearson's distribution III, which has unlimited support in one direction (towards positive infinity), and can be bell-shaped or J-shaped.  His son, Egon Pearson, showed that the region (in the kurtosis/squared-skewness plane) occupied by the beta distribution (equivalently, Pearson's distribution I) as it approaches this boundary (excess kurtosis − (3/2) skewness = 0) is shared with the noncentral chi-squared distribution.  Karl Pearson (Pearson 1895, pp. 357, 360, 373–376) also showed that the gamma distribution is a Pearson type III distribution.  Hence this boundary line for Pearson's type III distribution is known as the gamma line.  (This can be shown from the fact that the excess kurtosis of the gamma distribution is 6/\"k\" and the square of the skewness is 4/\"k\", hence (excess kurtosis − (3/2) skewness = 0) is identically satisfied by the gamma distribution regardless of the value of the parameter \"k\").  Pearson later noted that the chi-squared distribution is a special case of Pearson's type III and also shares this boundary line (as it is apparent from the fact that for the chi-squared distribution the excess kurtosis is 12/\"k\" and the square of the skewness is 8/\"k\", hence (excess kurtosis − (3/2) skewness = 0) is identically satisfied regardless of the value of the parameter \"k\").  This is to be expected, since the chi-squared distribution \"X\" ~ χ(\"k\") is a special case of the gamma distribution, with parametrization X ~ Γ(k/2, 1/2) where k is a positive integer that specifies the \"number of degrees of freedom\" of the chi-squared distribution. An example of a beta distribution near the upper boundary (excess kurtosis − (3/2) skewness = 0) is given by α = 0.1, β = 1000, for which the ratio (excess kurtosis)/(skewness) = 1.49835 approaches the upper limit of 1.5 from below.  An example of a beta distribution near the lower boundary (excess kurtosis + 2 − skewness = 0) is given by α= 0.0001, β = 0.1, for which values the expression (excess kurtosis + 2)/(skewness) = 1.01621 approaches the lower limit of 1 from above.  In the infinitesimal limit for both α and β approaching zero symmetrically, the excess kurtosis reaches its minimum value at −2.  This minimum value occurs at the point at which the lower boundary line intersects the vertical axis (ordinate).  (However, in Pearson's original chart, the ordinate is kurtosis, instead of excess kurtosis, and it increases downwards rather than upwards). Values for the skewness and excess kurtosis below the lower boundary (excess kurtosis + 2 − skewness = 0) cannot occur for any distribution, and hence Karl Pearson appropriately called the region below this boundary the \"impossible region.\"  The boundary for this \"impossible region\" is determined by (symmetric or skewed) bimodal \"U\"-shaped distributions for which parameters α and β approach zero and hence all the probability density is concentrated at the ends: \"x\" = 0, 1 with practically nothing in between them.  Since for α ≈ β ≈ 0 the probability density is concentrated at the two ends \"x\" = 0 and \"x\" = 1, this \"impossible boundary\" is determined by a 2-point distribution: the probability can only take 2 values (Bernoulli distribution), one value with probability p and the other with probability \"q\" = 1−\"p\".  For cases approaching this limit boundary with symmetry α = β, skewness ≈ 0, excess kurtosis ≈ −2 (this is the lowest excess kurtosis possible for any distribution), and the probabilities are \"p\" ≈ \"q\" ≈ 1/2.  For cases approaching this limit boundary with skewness, excess kurtosis ≈ −2 + skewness, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities formula_139 at the left end \"x\" = 0 and formula_140 at the right end \"x\" = 1. All statements are conditional on α, β > 0 For certain values of the shape parameters α and β, the probability density function has inflection points, at which the curvature changes sign.  The position of these inflection points can be useful as a measure of the dispersion or spread of the distribution. Defining the following quantity: Points of inflection occur, depending on the value of the shape parameters α and β, as follows: There are no inflection points in the remaining (symmetric and skewed) regions: U-shaped: (α, β < 1) upside-down-U-shaped: (1 < α < 2, 1 < β < 2), reverse-J-shaped (α < 1, β > 2) or J-shaped: (α > 2, β < 1) The accompanying plots show the inflection point locations (shown vertically, ranging from 0 to 1) versus α and β (the horizontal axes ranging from 0 to 5).  There are large cuts at surfaces intersecting the lines α = 1, β = 1, α = 2, and β = 2 because at these values the beta distribution change from 2 modes, to 1 mode to no mode. The beta density function can take a wide variety of different shapes depending on the values of the two parameters \"α\" and \"β\".  The ability of the beta distribution to take this great diversity of shapes (using only two parameters) is partly responsible for finding wide application for modeling actual measurements: The density function is skewed.  An interchange of parameter values yields the mirror image (the reverse) of the initial curve, some more specific cases: Two unknown parameters (formula_184 of a beta distribution supported in the [0,1] interval) can be estimated, using the method of moments, with the first two moments (sample mean and sample variance) as follows.  Let: be the sample mean estimate and be the sample variance estimate.  The method-of-moments estimates of the parameters are When the distribution is required over known interval other than [0, 1] with random variable \"X\", say [\"a\", \"c\"] with random variable \"Y\", then replace formula_191 with formula_192 and formula_193 with formula_194 in the above couple of equations for the shape parameters (see \"Alternative parametrizations, four parameters\" section below). , where: All four parameters (formula_197 of a beta distribution supported in the [\"a\", \"c\"] interval -see section \"Alternative parametrizations, Four parameters\"-) can be estimated, using the method of moments developed by Karl Pearson, by equating sample and population values of the first four central moments (mean, variance, skewness and excess kurtosis).  The excess kurtosis was expressed in terms of the square of the skewness, and the sample size ν = α + β, (see previous section \"Kurtosis\") as follows: One can use this equation to solve for the sample size ν= α + β in terms of the square of the skewness and the excess kurtosis as follows: This is the ratio (multiplied by a factor of 3) between the previously derived limit boundaries for the beta distribution in a space (as originally done by Karl Pearson) defined with coordinates of the square of the skewness in one axis and the excess kurtosis in the other axis (see previous section titled \"Kurtosis bounded by the square of the skewness\"): The case of zero skewness, can be immediately solved because for zero skewness, α = β and hence ν = 2α = 2β, therefore α = β = ν/2 (Excess kurtosis is negative for the beta distribution with zero skewness, ranging from -2 to 0, so that formula_203 -and therefore the sample shape parameters- is positive, ranging from zero when the shape parameters approach zero and the excess kurtosis approaches -2, to infinity when the shape parameters approach infinity and the excess kurtosis approaches zero). For non-zero sample skewness one needs to solve a system of two coupled equations.  Since the skewness and the excess kurtosis are independent of the parameters formula_204, the parameters formula_205 can be uniquely determined from the sample skewness and the sample excess kurtosis, by solving the coupled equations with two known variables (sample skewness and sample excess kurtosis) and two unknowns (the shape parameters): resulting in the following solution: Where one should take the solutions as follows: formula_211 for (negative) sample skewness < 0, and formula_212 for (positive) sample skewness > 0. The accompanying plot shows these two solutions as surfaces in a space with horizontal axes of (sample excess kurtosis) and (sample squared skewness) and the shape parameters as the vertical axis.  The surfaces are constrained by the condition that the sample excess kurtosis must be bounded by the sample squared skewness as stipulated in the above equation.  The two surfaces meet at the right edge defined by zero skewness.  Along this right edge, both parameters are equal and the distribution is symmetric U-shaped for α = β < 1, uniform for α = β = 1, upside-down-U-shaped for 1 < α = β < 2 and bell-shaped for α = β > 2.  The surfaces also meet at the front (lower) edge defined by \"the impossible boundary\" line (excess kurtosis + 2 - skewness = 0).  Along this front (lower) boundary both shape parameters approach zero, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities formula_213 at the left end \"x\" = 0 and formula_214 at the right end \"x\" = 1.  The two surfaces become further apart towards the rear edge.  At this rear edge the surface parameters are quite different from each other.  As remarked, for example, by Bowman and Shenton, sampling in the neighborhood of the line (sample excess kurtosis - (3/2)(sample skewness) = 0) (the just-J-shaped portion of the rear edge where blue meets beige), \"is dangerously near to chaos\", because at that line the denominator of the expression above for the estimate ν = α + β becomes zero and hence ν approaches infinity as that line is approached.  Bowman and Shenton write that \"the higher moment parameters (kurtosis and skewness) are extremely fragile (near that line).  However the mean and standard deviation are fairly reliable.\"  Therefore, the problem is for the case of four parameter estimation for very skewed distributions such that the excess kurtosis approaches (3/2) times the square of the skewness.  This boundary line is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter.  See section titled \"Kurtosis bounded by the square of the skewness\" for a numerical example and further comments about this rear edge boundary line (sample excess kurtosis - (3/2)(sample skewness) = 0).  As remarked by Karl Pearson himself this issue may not be of much practical importance as this trouble arises only for very skewed J-shaped (or mirror-image J-shaped) distributions with very different values of shape parameters that are unlikely to occur much in practice).  The usual skewed skewed-bell-shape distributions that occur in practice do not have this parameter estimation problem. The remaining two parameters formula_204 can be determined using the sample mean and the sample variance using a variety of equations.  One alternative is to calculate the support interval range formula_216 based on the sample variance and the sample kurtosis.  For this purpose one can solve, in terms of the range formula_217, the equation expressing the excess kurtosis in terms of the sample variance, and the sample size ν (see section titled \"Kurtosis\" and \"Alternative parametrizations, four parameters\"): to obtain: Another alternative is to calculate the support interval range formula_216 based on the sample variance and the sample skewness.  For this purpose one can solve, in terms of the range formula_216, the equation expressing the squared skewness in terms of the sample variance, and the sample size ν (see section titled \"Skewness\" and \"Alternative parametrizations, four parameters\"): to obtain: The remaining parameter can be determined from the sample mean and the previously obtained parameters: formula_224: and finally, of course, formula_226. In the above formulas one may take, for example, as estimates of the sample moments: The estimators \"G\" for sample skewness and \"G\" for sample kurtosis are used by DAP/SAS, PSPP/SPSS, and Excel.  However, they are not used by BMDP and (according to ) they were not used by MINITAB in 1998.  Actually, Joanes and Gill in their 1998 study concluded that the skewness and kurtosis estimators used in BMDP and in MINITAB (at that time) had smaller variance and mean-squared error in normal samples, but the skewness and kurtosis estimators used in DAP/SAS, PSPP/SPSS, namely \"G\" and \"G\", had smaller mean-squared error in samples from a very skewed distribution.  It is for this reason that we have spelled out \"sample skewness\", etc., in the above formulas, to make it explicit that the user should choose the best estimator according to the problem at hand, as the best estimator for skewness and kurtosis depends on the amount of skewness (as shown by Joanes and Gill). As it is also the case for maximum likelihood estimates for the gamma distribution, the maximum likelihood estimates for the beta distribution do not have a general closed form solution for arbitrary values of the shape parameters.  If \"X\", ..., \"X\" are independent random variables each having a beta distribution, the joint log likelihood function for \"N\" iid observations is: Finding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the maximum likelihood estimator of the shape parameters: where: since the digamma function denoted ψ(α) is defined as the logarithmic derivative of the gamma function: To ensure that the values with zero tangent slope are indeed a maximum (instead of a saddle-point or a minimum) one has to also satisfy the condition that the curvature is negative.  This amounts to satisfying that the second partial derivative with respect to the shape parameters is negative using the previous equations, this is equivalent to: where the trigamma function, denoted \"ψ\"(\"α\"), is the second of the polygamma functions, and is defined as the derivative of the digamma function: These conditions are equivalent to stating that the variances of the logarithmically transformed variables are positive, since: Therefore, the condition of negative curvature at a maximum is equivalent to the statements: Alternatively, the condition of negative curvature at a maximum is also equivalent to stating that the following logarithmic derivatives of the geometric means \"G\" and \"G\" are positive, since: While these slopes are indeed positive, the other slopes are negative: The slopes of the mean and the median with respect to \"α\" and \"β\" display similar sign behavior. From the condition that at a maximum, the partial derivative with respect to the shape parameter equals zero, we obtain the following system of coupled maximum likelihood estimate equations (for the average log-likelihoods) that needs to be inverted to obtain the (unknown) shape parameter estimates formula_246 in terms of the (known) average of logarithms of the samples \"X\", ..., \"X\": where we recognize formula_248 as the logarithm of the sample geometric mean and formula_249 as the logarithm of the sample geometric mean based on (1 − \"X\"), the mirror-image of \"X\".  For formula_250, it follows that formula_251. These coupled equations containing digamma functions of the shape parameter estimates formula_246 must be solved by numerical methods as done, for example, by Beckman et al.  Gnanadesikan et al. give numerical solutions for a few cases.  N.L.Johnson and S.Kotz suggest that for \"not too small\" shape parameter estimates formula_246, the logarithmic approximation to the digamma function formula_255 may be used to obtain initial values for an iterative solution, since the equations resulting from this approximation can be solved exactly: which leads to the following solution for the initial values (of the estimate shape parameters in terms of the sample geometric means) for an iterative solution: Alternatively, the estimates provided by the method of moments can instead be used as initial values for an iterative solution of the maximum likelihood coupled equations in terms of the digamma functions. When the distribution is required over a known interval other than [0, 1] with random variable \"X\", say [\"a\", \"c\"] with random variable \"Y\", then replace ln(\"X\") in the first equation with and replace ln(1−\"X\") in the second equation with (see \"Alternative parametrizations, four parameters\" section below). If one of the shape parameters is known, the problem is considerably simplified.  The following logit transformation can be used to solve for the unknown shape parameter (for skewed cases such that formula_262, otherwise, if symmetric, both -equal- parameters are known when one is known): This logit transformation is the logarithm of the transformation that divides the variable \"X\" by its mirror-image (\"X\"/(1 - \"X\") resulting in the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI) with support [0, +∞).  As previously discussed in the section \"Moments of logarithmically transformed random variables,\" the logit transformation formula_264, studied by Johnson, extends the finite support [0, 1] based on the original variable \"X\" to infinite support in both directions of the real line (−∞, +∞). If, for example, formula_265 is known, the unknown parameter formula_266 can be obtained in terms of the inverse digamma function of the right hand side of this equation: In particular, if one of the shape parameters has a value of unity, for example for formula_269 (the power function distribution with bounded support [0,1]), using the identity ψ(\"x\" + 1) = ψ(\"x\") + 1/\"x\" in the equation formula_270, the maximum likelihood estimator for the unknown parameter formula_266 is, exactly: The beta has support [0, 1], therefore formula_273, and hence formula_274, and therefore formula_275 . In conclusion, the maximum likelihood estimates of the shape parameters of a beta distribution are (in general) a complicated function of the sample geometric mean, and of the sample geometric mean based on \"(1−X)\", the mirror-image of \"X\".  One may ask, if the variance (in addition to the mean) is necessary to estimate two shape parameters with the method of moments, why is the (logarithmic or geometric) variance not necessary to estimate two shape parameters with the maximum likelihood method, for which only the geometric means suffice?  The answer is because the mean does not provide as much information as the geometric mean.  For a beta distribution with equal shape parameters α = β, the mean is exactly 1/2, regardless of the value of the shape parameters, and therefore regardless of the value of the statistical dispersion (the variance).  On the other hand, the geometric mean of a beta distribution with equal shape parameters α = β, depends on the value of the shape parameters, and therefore it contains more information.  Also, the geometric mean of a beta distribution does not satisfy the symmetry conditions satisfied by the mean, therefore, by employing both the geometric mean based on \"X\" and geometric mean based on \"(1−X)\", the maximum likelihood method is able to provide best estimates for both parameters α = β, without need of employing the variance. One can express the joint log likelihood per \"N\" iid observations in terms of the \"sufficient statistics\" (the sample geometric means) as follows: We can plot the joint log likelihood per \"N\" observations for fixed values of the sample geometric means to see the behavior of the likelihood function as a function of the shape parameters α and β.  In such a plot, the shape parameter estimators formula_246 correspond to the maxima of the likelihood function.  See the accompanying graph that shows that all the likelihood functions intersect at α = β = 1, which corresponds to the values of the shape parameters that give the maximum entropy (the maximum entropy occurs for shape parameters equal to unity: the uniform distribution).  It is evident from the plot that the likelihood function gives sharp peaks for values of the shape parameter estimators close to zero, but that for values of the shape parameters estimators greater than one, the likelihood function becomes quite flat, with less defined peaks.  Obviously, the maximum likelihood parameter estimation method for the beta distribution becomes less acceptable for larger values of the shape parameter estimators, as the uncertainty in the peak definition increases with the value of the shape parameter estimators.  One can arrive at the same conclusion by noticing that the expression for the curvature of the likelihood function is in terms of the geometric variances These variances (and therefore the curvatures) are much larger for small values of the shape parameter α and β.  However, for shape parameter values α, β > 1, the variances (and therefore the curvatures) flatten out.  Equivalently, this result follows from the Cramér–Rao bound, since the Fisher information matrix components for the beta distribution are these logarithmic variances.  The Cramér–Rao bound states that the variance of any \"unbiased\" estimator formula_266 of α is bounded by the reciprocal of the Fisher information: so the variance of the estimators increases with increasing α and β, as the logarithmic variances decrease. Also one can express the joint log likelihood per \"N\" iid observations in terms of the digamma function expressions for the logarithms of the sample geometric means as follows: this expression is identical to the negative of the cross-entropy (see section on \"Quantities of information (entropy)\").  Therefore, finding the maximum of the joint log likelihood of the shape parameters, per \"N\" iid observations, is identical to finding the minimum of the cross-entropy for the beta distribution, as a function of the shape parameters. with the cross-entropy defined as follows: The procedure is similar to the one followed in the two unknown parameter case.  If \"Y\", ..., \"Y\" are independent random variables each having a beta distribution with four parameters, the joint log likelihood function for \"N\" iid observations is: Finding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the maximum likelihood estimator of the shape parameters: these equations can be re-arranged as the following system of four coupled equations (the first two equations are geometric means and the second two equations are the harmonic means) in terms of the maximum likelihood estimates for the four parameters formula_197: with sample geometric means: The parameters formula_204 are embedded inside the geometric mean expressions in a nonlinear way (to the power 1/\"N\").  This precludes, in general, a closed form solution, even for an initial value approximation for iteration purposes.  One alternative is to use as initial values for iteration the values obtained from the method of moments solution for the four parameter case.  Furthermore, the expressions for the harmonic means are well-defined only for formula_299, which precludes a maximum likelihood solution for shape parameters less than unity in the four-parameter case.  Fisher's information matrix for the four parameter case is positive-definite only for α, β > 2 (for further discussion, see section on Fisher information matrix, four parameter case), for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode.  The following Fisher information components (that represent the expectations of the curvature of the log likelihood function) have singularities at the following values: (for further discussion see section on Fisher information matrix).  Thus, it is not possible to strictly carry on the maximum likelihood estimation for some well known distributions belonging to the four-parameter beta distribution family, like the uniform distribution (Beta(1, 1, \"a\", \"c\")), and the arcsine distribution (Beta(1/2, 1/2, \"a\", \"c\")).  N.L.Johnson and S.Kotz ignore the equations for the harmonic means and instead suggest \"If a and c are unknown, and maximum likelihood estimators of \"a\", \"c\", α and β are required, the above procedure (for the two unknown parameter case, with \"X\" transformed as \"X\" = (\"Y\" − \"a\")/(\"c\" − \"a\")) can be repeated using a succession of trial values of \"a\" and \"c\", until the pair (\"a\", \"c\") for which maximum likelihood (given \"a\" and \"c\") is as great as possible, is attained\" (where, for the purpose of clarity, their notation for the parameters has been translated into the present notation). Let a random variable X have a probability density f(x;α).  The partial derivative with respect to the (unknown, and to be estimated) parameter α of the log likelihood function is called the score.  The second moment of the score is called the Fisher information: The expectation of the score is zero, therefore the Fisher information is also the second moment centered on the mean of the score: the variance of the score. If the log likelihood function is twice differentiable with respect to the parameter α, and under certain regularity conditions, then the Fisher information may also be written as follows (which is often a more convenient form for calculation purposes): Thus, the Fisher information is the negative of the expectation of the second derivative with respect to the parameter α of the log likelihood function.  Therefore, Fisher information is a measure of the curvature of the log likelihood function of α.  A low curvature (and therefore high radius of curvature), flatter log likelihood function curve has low Fisher information; while a log likelihood function curve with large curvature (and therefore low radius of curvature) has high Fisher information.  When the Fisher information matrix is computed at the evaluates of the parameters (\"the observed Fisher information matrix\") it is equivalent to the replacement of the true log likelihood surface by a Taylor's series approximation, taken as far as the quadratic terms.  The word information, in the context of Fisher information, refers to information about the parameters.  Information such as: estimation, sufficiency and properties of variances of estimators.  The Cramér–Rao bound states that the inverse of the Fisher information is a lower bound on the variance of any estimator of a parameter α: The precision to which one can estimate the estimator of a parameter α is limited by the Fisher Information of the log likelihood function.  The Fisher information is a measure of the minimum error involved in estimating a parameter of a distribution and it can be viewed as a measure of the resolving power of an experiment needed to discriminate between two alternative hypothesis of a parameter. When there are \"N\" parameters then the Fisher information takes the form of an \"N\"×\"N\" positive semidefinite symmetric matrix, the Fisher Information Matrix, with typical element: Under certain regularity conditions, the Fisher Information Matrix may also be written in the following form, which is often more convenient for computation: With \"X\", ..., \"X\" iid random variables, an \"N\"-dimensional \"box\" can be constructed with sides \"X\", ..., \"X\".  Costa and Cover show that the (Shannon) differential entropy \"h\"(\"X\") is related to the volume of the typical set (having the sample entropy close to the true entropy), while the Fisher information is related to the surface of this typical set. For \"X\", ..., \"X\" independent random variables each having a beta distribution parametrized with shape parameters α and β, the joint log likelihood function for \"N\" iid observations is: therefore the joint log likelihood function per \"N\" iid observations is: For the two parameter case, the Fisher information has 4 components: 2 diagonal and 2 off-diagonal.  Since the Fisher information matrix is symmetric, one of these off diagonal components is independent.  Therefore, the Fisher information matrix has 3 independent components (2 diagonal and 1 off diagonal). Aryal and Nadarajah calculated Fisher's information matrix for the four-parameter case, from which the two parameter case can be obtained as follows: Since the Fisher information matrix is symmetric The Fisher information components are equal to the log geometric variances and log geometric covariance.  Therefore, they can be expressed as trigamma functions, denoted ψ(α), the second of the polygamma functions, defined as the derivative of the digamma function: These derivatives are also derived in the section titled \"Parameter estimation\", \"Maximum likelihood\", \"Two unknown parameters,\" and plots of the log likelihood function are also shown in that section.  The section titled \"Geometric variance and covariance\" contains plots and further discussion of the Fisher information matrix components: the log geometric variances and log geometric covariance as a function of the shape parameters α and β.  The section titled \"Other moments\", \"Moments of transformed random variables\", \"Moments of logarithmically transformed random variables\" contains formulas for moments of logarithmically transformed random variables.  Images for the Fisher information components formula_317 and formula_318 are shown in the section titled \"Geometric variance\". The determinant of Fisher's information matrix is of interest (for example for the calculation of Jeffreys prior probability).  From the expressions for the individual components of the Fisher information matrix, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution is: From Sylvester's criterion (checking whether the diagonal elements are all positive), it follows that the Fisher information matrix for the two parameter case is positive-definite (under the standard condition that the shape parameters are positive \"α\" > 0 and \"β\" > 0). If \"Y\", ..., \"Y\" are independent random variables each having a beta distribution with four parameters: the exponents α and β, as well as \"a\" (the minimum of the distribution range), and \"c\" (the maximum of the distribution range) (section titled \"Alternative parametrizations\", \"Four parameters\"), with probability density function: the joint log likelihood function per \"N\" iid observations is: For the four parameter case, the Fisher information has 4*4=16 components.  It has 12 off-diagonal components = (4*4 total - 4 diagonal).  Since the Fisher information matrix is symmetric, half of these components (12/2=6) are independent.  Therefore, the Fisher information matrix has 6 independent off-diagonal + 4 diagonal = 10 independent components.  Aryal and Nadarajah calculated Fisher's information matrix for the four parameter case as follows: In the above expressions, the use of \"X\" instead of \"Y\" in the expressions var[ln(\"X\")] = ln(var) is \"not an error\".  The expressions in terms of the log geometric variances and log geometric covariance occur as functions of the two parameter \"X\" ~ Beta(α, β) parametrization because when taking the partial derivatives with respect to the exponents (α, β) in the four parameter case, one obtains the identical expressions as for the two parameter case: these terms of the four parameter Fisher information matrix are independent of the minimum \"a\" and maximum \"c\" of the distribution's range.  The only non-zero term upon double differentiation of the log likelihood function with respect to the exponents α and β is the second derivative of the log of the beta function: ln(B(α, β)).  This term is independent of the minimum \"a\" and maximum \"c\" of the distribution's range.  Double differentiation of this term results in trigamma functions.  The sections titled \"Maximum likelihood\", \"Two unknown parameters\" and \"Four unknown parameters\" also show this fact. The Fisher information for \"N\" i.i.d. samples is \"N\" times the individual Fisher information (eq.  11.279, page 394 of Cover and Thomas).  (Aryal and Nadarajah take a single observation, \"N\" = 1, to calculate the following components of the Fisher information, which leads to the same result as considering the derivatives of the log likelihood per \"N\" observations.  Moreover, below the erroneous expression for formula_325 in Aryal and Nadarajah has been corrected.) The lower two diagonal entries of the Fisher information matrix, with respect to the parameter \"a\" (the minimum of the distribution's range): formula_327, and with respect to the parameter \"c\" (the maximum of the distribution's range): formula_328 are only defined for exponents α > 2 and β > 2 respectively.  The Fisher information matrix component formula_327 for the minimum \"a\" approaches infinity for exponent α approaching 2 from above, and the Fisher information matrix component formula_328 for the maximum \"c\" approaches infinity for exponent β approaching 2 from above. The Fisher information matrix for the four parameter case does not depend on the individual values of the minimum \"a\" and the maximum \"c\", but only on the total range (\"c\"−\"a\").  Moreover, the components of the Fisher information matrix that depend on the range (\"c\"−\"a\"), depend only through its inverse (or the square of the inverse), such that the Fisher information decreases for increasing range (\"c\"−\"a\"). The accompanying images show the Fisher information components formula_327 and formula_332.  Images for the Fisher information components formula_333 and formula_334 are shown in the section titled \"Geometric variance\".  All these Fisher information components look like a basin, with the \"walls\" of the basin being located at low values of the parameters. The following four-parameter-beta-distribution Fisher information components can be expressed in terms of the two-parameter: \"X\" ~ Beta(α, β) expectations of the transformed ratio ((1-\"X\")/\"X\") and of its mirror image (\"X\"/(1-\"X\")), scaled by the range (\"c\"−\"a\"), which may be helpful for interpretation: These are also the expected values of the \"inverted beta distribution\" or beta prime distribution (also known as beta distribution of the second kind or Pearson's Type VI) and its mirror image, scaled by the range (\"c\"−\"a\"). Also, the following Fisher information components can be expressed in terms of the harmonic (1/X) variances or of variances based on the ratio transformed variables ((1-X)/X) as follows: See section \"Moments of linearly transformed, product and inverted random variables\" for these expectations. The determinant of Fisher's information matrix is of interest (for example for the calculation of Jeffreys prior probability).  From the expressions for the individual components, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution with four parameters is: Using Sylvester's criterion (checking whether the diagonal elements are all positive), and since diagonal components formula_325 and formula_340 have singularities at α=2 and β=2 it follows that the Fisher information matrix for the four parameter case is positive-definite for α>2 and β>2.  Since for α > 2 and β > 2 the beta distribution is (symmetric or unsymmetric) bell shaped, it follows that the Fisher information matrix is positive-definite only for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode.  Thus, important well known distributions belonging to the four-parameter beta distribution family, like the parabolic distribution (Beta(2,2,a,c)) and the uniform distribution (Beta(1,1,a,c)) have Fisher information components (formula_341) that blow up (approach infinity) in the four-parameter case (although their Fisher information components are all defined for the two parameter case).  The four-parameter Wigner semicircle distribution (Beta(3/2,3/2,a,c)) and arcsine distribution (Beta(1/2,1/2,a,c)) have negative Fisher information determinants for the four-parameter case. If \"X\" and \"Y\" are independent, with formula_342 and formula_343 then So one algorithm for generating beta variates is to generate \"X\"/(\"X\" + \"Y\"), where \"X\" is a gamma variate with parameters (α, 1) and \"Y\" is an independent gamma variate with parameters (β, 1). Also, the \"k\"th order statistic of \"n\" uniformly distributed variates is formula_345, so an alternative if α and β are small integers is to generate α + β − 1 uniform variates and choose the α-th smallest. Another way to generate the Beta distribution is by Pólya urn model.  According to this method, one start with an \"urn\" with α \"black\" balls and β \"white\" balls and draw uniformly with replacement.  Every trial an additional ball is added according to the color of the last ball which was drawn.  Asymptotically, the proportion of black and white balls will be distributed according to the Beta distribution, where each repetition of the experiment will produce a different value. The beta distribution has an important application in the theory of order statistics.  A basic result is that the distribution of the \"k\"th smallest of a sample of size \"n\" from a continuous uniform distribution has a beta distribution.  This result is summarized as: From this, and application of the theory related to the probability integral transform, the distribution of any individual order statistic from any continuous distribution can be derived. A classic application of the beta distribution is the rule of succession, introduced in the 18th century by Pierre-Simon Laplace in the course of treating the sunrise problem.  It states that, given \"s\" successes in \"n\" conditionally independent Bernoulli trials with probability \"p,\" that the estimate of the expected value in the next trial is formula_358.  This estimate is the expected value of the posterior distribution over \"p,\" namely Beta(\"s\"+1, \"n\"−\"s\"+1), which is given by Bayes' rule if one assumes a uniform prior probability over \"p\" (i.e., Beta(1, 1)) and then observes that \"p\" generated \"s\" successes in \"n\" trials.  Laplace's rule of succession has been criticized by prominent scientists.  R. T. Cox described Laplace's application of the rule of succession to the sunrise problem ( p. 89) as \"a travesty of the proper use of the principle.\"  Keynes remarks ( Ch.XXX, p. 382) \"indeed this is so foolish a theorem that to entertain it is discreditable.\"  Karl Pearson showed that the probability that the next (\"n\" + 1) trials will be successes, after n successes in n trials, is only 50%, which has been considered too low by scientists like Jeffreys and unacceptable as a representation of the scientific process of experimentation to test a proposed scientific law.  As pointed out by Jeffreys ( p. 128) (crediting C. D. Broad ) Laplace's rule of succession establishes a high probability of success ((n+1)/(n+2)) in the next trial, but only a moderate probability (50%) that a further sample (n+1) comparable in size will be equally successful.  As pointed out by Perks, \"The rule of succession itself is hard to accept.  It assigns a probability to the next trial which implies the assumption that the actual run observed is an average run and that we are always at the end of an average run.  It would, one would think, be more reasonable to assume that we were in the middle of an average run.  Clearly a higher value for both probabilities is necessary if they are to accord with reasonable belief.\"  These problems with Laplace's rule of succession motivated Haldane, Perks, Jeffreys and others to search for other forms of prior probability (see the next section titled \"Bayesian inference\").  According to Jaynes, the main problem with the rule of succession is that it is not valid when s=0 or s=n (see rule of succession, for an analysis of its validity). The use of Beta distributions in Bayesian inference is due to the fact that they provide a family of conjugate prior probability distributions for binomial (including Bernoulli) and geometric distributions.  The domain of the beta distribution can be viewed as a probability, and in fact the beta distribution is often used to describe the distribution of a probability value \"p\": Examples of beta distributions used as prior probabilities to represent ignorance of prior parameter values in Bayesian inference are Beta(1,1), Beta(0,0) and Beta(1/2,1/2). The beta distribution achieves maximum differential entropy for Beta(1,1): the uniform probability density, for which all values in the domain of the distribution have equal density.  This uniform distribution Beta(1,1) was suggested (\"with a great deal of doubt\") by Thomas Bayes as the prior probability distribution to express ignorance about the correct prior distribution.  This prior distribution was adopted (apparently, from his writings, with little sign of doubt) by Pierre-Simon Laplace, and hence it was also known as the \"Bayes-Laplace rule\" or the \"Laplace rule\" of \"inverse probability\" in publications of the first half of the 20th century.  In the later part of the 19th century and early part of the 20th century, scientists realized that the assumption of uniform \"equal\" probability density depended on the actual functions (for example whether a linear or a logarithmic scale was most appropriate) and parametrizations used.  In particular, the behavior near the ends of distributions with finite support (for example near \"x\" = 0, for a distribution with initial support at \"x\" = 0) required particular attention.  Keynes ( Ch.XXX, p. 381) criticized the use of Bayes's uniform prior probability (Beta(1,1)) that all values between zero and one are equiprobable, as follows: \"Thus experience, if it shows anything, shows that there is a very marked clustering of statistical ratios in the neighborhoods of zero and unity, of those for positive theories and for correlations between positive qualities in the neighborhood of zero, and of those for negative theories and for correlations between negative qualities in the neighborhood of unity. \" The Beta(0,0) distribution was proposed by J.B.S. Haldane, who suggested that the prior probability representing complete uncertainty should be proportional to \"p\"(1−\"p\").  The function \"p\"(1−\"p\") can be viewed as the limit of the numerator of the beta distribution as both shape parameters approach zero: α, β → 0.  The Beta function (in the denominator of the beta distribution) approaches infinity, for both parameters approaching zero, α, β → 0.  Therefore, \"p\"(1−\"p\") divided by the Beta function approaches a 2-point Bernoulli distribution with equal probability 1/2 at each Dirac delta function end, at 0 and 1, and nothing in between, as α, β → 0.  A coin-toss: one face of the coin being at 0 and the other face being at 1.  The Haldane prior probability distribution Beta(0,0) is an \"improper prior\" because its integration (from 0 to 1) fails to strictly converge to 1 due to the Dirac delta function singularities at each end.  However, this is not an issue for computing posterior probabilities unless the sample size is very small.  Furthermore, Zellner points out that on the log-odds scale, (the logit transformation ln(\"p\"/1−\"p\")), the Haldane prior is the uniformly flat prior.  The fact that a uniform prior probability on the logit transformed variable ln(\"p\"/1−\"p\") (with domain (-∞, ∞)) is equivalent to the Haldane prior on the domain [0, 1] was pointed out by Harold Jeffreys in the first edition (1939) of his book Theory of Probability ( p. 123).  Jeffreys writes \"Certainly if we take the Bayes-Laplace rule right up to the extremes we are led to results that do not correspond to anybody's way of thinking.  The (Haldane) rule d\"x\"/(\"x\"(1−\"x\")) goes too far the other way.  It would lead to the conclusion that if a sample is of one type with respect to some property there is a probability 1 that the whole population is of that type.\"  The fact that \"uniform\" depends on the parametrization, led Jeffreys to seek a form of prior that would be invariant under different parametrizations. Harold Jeffreys proposed to use an uninformative prior probability measure that should be invariant under reparameterization: proportional to the square root of the determinant of Fisher's information matrix.  For the Bernoulli distribution, this can be shown as follows: for a coin that is \"heads\" with probability \"p\" ∈ [0, 1] and is \"tails\" with probability 1 − \"p\", for a given (H,T) ∈ {(0,1), (1,0)} the probability is \"p\"(1 − \"p\").  Since \"T\" = 1 − \"H\", the Bernoulli distribution is \"p\"(1 − \"p\").  Considering \"p\" as the only parameter, it follows that the log likelihood for the Bernoulli distribution is The Fisher information matrix has only one component (it is a scalar, because there is only one parameter: \"p\"), therefore: Similarly, for the Binomial distribution with \"n\" Bernoulli trials, it can be shown that Thus, for the Bernoulli, and Binomial distributions, Jeffreys prior is proportional to formula_363, which happens to be proportional to a beta distribution with domain variable \"x\" = \"p\", and shape parameters α = β = 1/2, the arcsine distribution: It will be shown in the next section that the normalizing constant for Jeffreys prior is immaterial to the final result because the normalizing constant cancels out in Bayes theorem for the posterior probability.  Hence Beta(1/2,1/2) is used as the Jeffreys prior for both Bernoulli and binomial distributions.  As shown in the next section, when using this expression as a prior probability times the likelihood in Bayes theorem, the posterior probability turns out to be a beta distribution.  It is important to realize, however, that Jeffreys prior is proportional to formula_365 for the Bernoulli and binomial distribution, but not for the beta distribution.  Jeffreys prior for the beta distribution is given by the determinant of Fisher's information for the beta distribution, which, as shown in the section titled \"Fisher information\" is a function of the trigamma function ψ of shape parameters α and β as follows: As previously discussed, Jeffreys prior for the Bernoulli and binomial distributions is proportional to the arcsine distribution Beta(1/2,1/2), a one-dimensional \"curve\" that looks like a basin as a function of the parameter \"p\" of the Bernoulli and binomial distributions.  The walls of the basin are formed by \"p\" approaching the singularities at the ends \"p\" → 0 and \"p\" → 1, where Beta(1/2,1/2) approaches infinity.  Jeffreys prior for the beta distribution is a \"2-dimensional surface\" (embedded in a three-dimensional space) that looks like a basin with only two of its walls meeting at the corner α = β = 0 (and missing the other two walls) as a function of the shape parameters α and β of the beta distribution.  The two adjoining walls of this 2-dimensional surface are formed by the shape parameters α and β approaching the singularities (of the trigamma function) at α, β → 0.  It has no walls for α, β → ∞ because in this case the determinant of Fisher's information matrix for the beta distribution approaches zero. It will be shown in the next section that Jeffreys prior probability results in posterior probabilities (when multiplied by the binomial likelihood function) that are intermediate between the posterior probability results of the Haldane and Bayes prior probabilities. Jeffreys prior may be difficult to obtain analytically, and for some cases it just doesn't exist (even for simple distribution functions like the asymmetric triangular distribution).  Berger, Bernardo and Sun, in a 2009 paper defined a reference prior probability distribution that (unlike Jeffreys prior) exists for the asymmetric triangular distribution.  They cannot obtain a closed-form expression for their reference prior, but numerical calculations show it to be nearly perfectly ﬁtted by the (proper) prior where θ is the vertex variable for the asymmetric triangular distribution with support [0, 1] (corresponding to the following parameter values in Wikipedia's article on the triangular distribution: vertex \"c\" = \"θ\", left end \"a\" = 0,and right end \"b\" = 1).  Berger et al. also give a heuristic argument that Beta(1/2,1/2) could indeed be the exact Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution.  Therefore, Beta(1/2,1/2) not only is Jeffreys prior for the Bernoulli and binomial distributions, but also seems to be the Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution (for which the Jeffreys prior does not exist), a distribution used in project management and PERT analysis to describe the cost and duration of project tasks. Clarke and Barron prove that, among continuous positive priors, Jeffreys prior (when it exists) asymptotically maximizes Shannon's mutual information between a sample of size n and the parameter, and therefore \"Jeffreys prior is the most uninformative prior\" (measuring information as Shannon information).  The proof rests on an examination of the Kullback–Leibler distance between probability density functions for iid random variables. If samples are drawn from the population of a random variable \"X\" that result in \"s\" successes and \"f\" failures in \"n\" Bernoulli trials \"n\" = \"s\" + \"f\", then the likelihood function for parameters \"s\" and \"f\" given \"x\" = \"p\" (the notation \"x\" = \"p\" in the expressions below will emphasize that the domain \"x\" stands for the value of the parameter \"p\" in the binomial distribution), is the following binomial distribution: If beliefs about prior probability information are reasonably well approximated by a beta distribution with parameters \"α\" Prior and \"β\" Prior, then: According to Bayes' theorem for a continuous event space, the posterior probability is given by the product of the prior probability and the likelihood function (given the evidence \"s\" and \"f\" = \"n\" − \"s\"), normalized so that the area under the curve equals one, as follows: The binomial coefficient appears both in the numerator and the denominator of the posterior probability, and it does not depend on the integration variable \"x\", hence it cancels out, and it is irrelevant to the final result.  Similarly the normalizing factor for the prior probability, the beta function B(αPrior,βPrior) cancels out and it is immaterial to the final result.  The same posterior probability result can be obtained if one uses an un-normalized prior because the normalizing factors all cancel out.  Several authors (including Jeffreys himself) thus use an un-normalized prior formula since the normalization constant cancels out.  The numerator of the posterior probability ends up being just the (un-normalized) product of the prior probability and the likelihood function, and the denominator is its integral from zero to one.  The beta function in the denominator, B(\"s\" + \"α\" Prior, \"n\" − \"s\" + \"β\" Prior), appears as a normalization constant to ensure that the total posterior probability integrates to unity. The ratio \"s\"/\"n\" of the number of successes to the total number of trials is a sufficient statistic in the binomial case, which is relevant for the following results. For the Bayes' prior probability (Beta(1,1)), the posterior probability is: For the Jeffreys' prior probability (Beta(1/2,1/2)), the posterior probability is: and for the Haldane prior probability (Beta(0,0)), the posterior probability is: From the above expressions it follows that for \"s\"/\"n\" = 1/2) all the above three prior probabilities result in the identical location for the posterior probability mean = mode = 1/2.  For \"s\"/\"n\" < 1/2, the mean of the posterior probabilities, using the following priors, are such that: mean for Bayes prior > mean for Jeffreys prior > mean for Haldane prior.  For \"s\"/\"n\" > 1/2 the order of these inequalities is reversed such that the Haldane prior probability results in the largest posterior mean.  The \"Haldane\" prior probability Beta(0,0) results in a posterior probability density with \"mean\" (the expected value for the probability of success in the \"next\" trial) identical to the ratio \"s\"/\"n\" of the number of successes to the total number of trials.  Therefore, the Haldane prior results in a posterior probability with expected value in the next trial equal to the maximum likelihood.  The \"Bayes\" prior probability Beta(1,1) results in a posterior probability density with \"mode\" identical to the ratio \"s\"/\"n\" (the maximum likelihood). In the case that 100% of the trials have been successful \"s\" = \"n\", the \"Bayes\" prior probability Beta(1,1) results in a posterior expected value equal to the rule of succession (\"n\" + 1)/(\"n\" + 2), while the Haldane prior Beta(0,0) results in a posterior expected value of 1 (absolute certainty of success in the next trial).  Jeffreys prior probability results in a posterior expected value equal to ('n\" + 1/2)/(\"n\" + 1), Perks (p. 303) points out: \"This provides a new rule of succession and expresses a 'reasonable' position to take up, namely, that after an unbroken run of n successes we assume a probability for the next trial equivalent to the assumption that we are about half-way through an average run, i.e. that we expect a failure once in (2\"n\" + 2) trials.  The Bayes–Laplace rule implies that we are about at the end of an average run or that we expect a failure once in (\"n\" + 2) trials.  The comparison clearly favours the new result (what is now called Jeffreys prior) from the point of view of 'reasonableness'.\" Conversely, in the case that 100% of the trials have resulted in failure (\"s\" = 0), the \"Bayes\" prior probability Beta(1,1) results in a posterior expected value for success in the next trial equal to 1/(\"n\" + 2), while the Haldane prior Beta(0,0) results in a posterior expected value of success in the next trial of 0 (absolute certainty of failure in the next trial).  Jeffreys prior probability results in a posterior expected value for success in the next trial equal to (1/2)/(\"n\" + 1), which Perks (p. 303) points out: \"is a much more reasonably remote result than the Bayes-Laplace result 1/(\"n\" + 2)\". Jaynes questions (for the uniform prior Beta(1,1)) the use of these formulas for the cases \"s\" = 0 or \"s\" = \"n\" because the integrals do not converge (Beta(1,1) is an improper prior for \"s\" = 0 or \"s\" = \"n\").  In practice, the conditions 0 (p. 303) shows that, for what is now known as the Jeffreys prior, this probability is ((\"n\" + 1/2)/(\"n\" + 1))((\"n\" + 3/2)/(\"n\" + 2))...(2\"n\" − 1/2)/(2\"n\"), which for \"n\" = 1, 2, 3 gives 3/4, 35/48, 693/960; rapidly approaching a limiting value of formula_376 as n tends to infinity.  Perks remarks that what is now known as the Jeffreys prior: \"is clearly more 'reasonable' than either the Bayes-Laplace result or the result on the (Haldane) alternative rule rejected by Jeffreys which gives certainty as the probability.  It clearly provides a very much better correspondence with the process of induction.  Whether it is 'absolutely' reasonable for the purpose, i.e. whether it is yet large enough, without the absurdity of reaching unity, is a matter for others to decide.  But it must be realized that the result depends on the assumption of complete indifference and absence of knowledge prior to the sampling experiment.\" Following are the variances of the posterior distribution obtained with these three prior probability distributions: for the Bayes' prior probability (Beta(1,1)), the posterior variance is: for the Jeffreys' prior probability (Beta(1/2,1/2)), the posterior variance is: and for the Haldane prior probability (Beta(0,0)), the posterior variance is: So, as remarked by Silvey, for large \"n\", the variance is small and hence the posterior distribution is highly concentrated, whereas the assumed prior distribution was very diffuse.  This is in accord with what one would hope for, as vague prior knowledge is transformed (through Bayes theorem) into a more precise posterior knowledge by an informative experiment.  For small \"n\" the Haldane Beta(0,0) prior results in the largest posterior variance while the Bayes Beta(1,1) prior results in the more concentrated posterior.  Jeffreys prior Beta(1/2,1/2) results in a posterior variance in between the other two.  As \"n\" increases, the variance rapidly decreases so that the posterior variance for all three priors converges to approximately the same value (approaching zero variance as \"n\" → ∞).  Recalling the previous result that the \"Haldane\" prior probability Beta(0,0) results in a posterior probability density with \"mean\" (the expected value for the probability of success in the \"next\" trial) identical to the ratio s/n of the number of successes to the total number of trials, it follows from the above expression that also the \"Haldane\" prior Beta(0,0) results in a posterior with \"variance\" identical to the variance expressed in terms of the max.  likelihood estimate s/n and sample size (in section titled \"Variance\"): with the mean \"μ\" = \"s\"/\"n\" and the sample size \"ν\" = \"n\". In Bayesian inference, using a prior distribution Beta(\"α\"Prior,\"β\"Prior) prior to a binomial distribution is equivalent to adding (\"α\"Prior − 1) pseudo-observations of \"success\" and (\"β\"Prior − 1) pseudo-observations of \"failure\" to the actual number of successes and failures observed, then estimating the parameter \"p\" of the binomial distribution by the proportion of successes over both real- and pseudo-observations.  A uniform prior Beta(1,1) does not add (or subtract) any pseudo-observations since for Beta(1,1) it follows that (\"α\"Prior − 1) = 0 and (\"β\"Prior − 1) = 0.  The Haldane prior Beta(0,0) subtracts one pseudo observation from each and Jeffreys prior Beta(1/2,1/2) subtracts 1/2 pseudo-observation of success and an equal number of failure.  This subtraction has the effect of smoothing out the posterior distribution.  If the proportion of successes is not 50% (\"s\"/\"n\" ≠ 1/2) values of \"α\"Prior and \"β\"Prior less than 1 (and therefore negative (\"α\"Prior − 1) and (\"β\"Prior − 1)) favor sparsity, i.e. distributions where the parameter \"p\" is closer to either 0 or 1.  In effect, values of \"α\"Prior and \"β\"Prior between 0 and 1, when operating together, function as a concentration parameter. The accompanying plots show the posterior probability density functions for sample sizes \"n\" ∈ {3,10,50}, successes \"s\" ∈ {\"n\"/2,\"n\"/4} and Beta(\"α\"Prior,\"β\"Prior) ∈ {Beta(0,0),Beta(1/2,1/2),Beta(1,1)}.  Also shown are the cases for \"n\" = {4,12,40}, success \"s\" = {\"n\"/4} and Beta(\"α\"Prior,\"β\"Prior) ∈ {Beta(0,0),Beta(1/2,1/2),Beta(1,1)}.  The first plot shows the symmetric cases, for successes \"s\" ∈ {n/2}, with mean = mode = 1/2 and the second plot shows the skewed cases \"s\" ∈ {\"n\"/4}.  The images show that there is little difference between the priors for the posterior with sample size of 50 (characterized by a more pronounced peak near \"p\" = 1/2).  Significant differences appear for very small sample sizes (in particular for the flatter distribution for the degenerate case of sample size = 3).  Therefore, the skewed cases, with successes \"s\" = {\"n\"/4}, show a larger effect from the choice of prior, at small sample size, than the symmetric cases.  For symmetric distributions, the Bayes prior Beta(1,1) results in the most \"peaky\" and highest posterior distributions and the Haldane prior Beta(0,0) results in the flattest and lowest peak distribution.  The Jeffreys prior Beta(1/2,1/2) lies in between them.  For nearly symmetric, not too skewed distributions the effect of the priors is similar.  For very small sample size (in this case for a sample size of 3) and skewed distribution (in this example for \"s\" ∈ {\"n\"/4}) the Haldane prior can result in a reverse-J-shaped distribution with a singularity at the left end.  However, this happens only in degenerate cases (in this example \"n\" = 3 and hence \"s\" = 3/4 < 1, a degenerate value because s should be greater than unity in order for the posterior of the Haldane prior to have a mode located between the ends, and because \"s\" = 3/4 is not an integer number, hence it violates the initial assumption of a binomial distribution for the likelihood) and it is not an issue in generic cases of reasonable sample size (such that the condition 1 < \"s\" < \"n\" − 1, necessary for a mode to exist between both ends, is fulfilled). In Chapter 12 (p. 385) of his book, Jaynes asserts that the \"Haldane prior\" Beta(0,0) describes a \"prior state of knowledge of complete ignorance\", where we are not even sure whether it is physically possible for an experiment to yield either a success or a failure, while the \"Bayes (uniform) prior Beta(1,1) applies if\" one knows that \"both binary outcomes are possible\".  Jaynes states: \"\"interpret the Bayes-Laplace (Beta(1,1)) prior as describing not a state of complete ignorance\", but the state of knowledge in which we have observed one success and one failure...once we have seen at least one success and one failure, then we know that the experiment is a true binary one, in the sense of physical possibility.\"  Jaynes does not specifically discuss Jeffreys prior Beta(1/2,1/2) (Jaynes discussion of \"Jeffreys prior\" on pp. 181, 423 and on chapter 12 of Jaynes book refers instead to the improper, un-normalized, prior \"1/\"p\" \"dp\"\" introduced by Jeffreys in the 1939 edition of his book, seven years before he introduced what is now known as Jeffreys' invariant prior: the square root of the determinant of Fisher's information matrix. \" \"1/p\" is Jeffreys' (1946) invariant prior for the exponential distribution, not for the Bernoulli or binomial distributions\").  However, it follows from the above discussion that Jeffreys Beta(1/2,1/2) prior represents a state of knowledge in between the Haldane Beta(0,0) and Bayes Beta (1,1) prior. Similarly, Karl Pearson in his 1892 book The Grammar of Science (p. 144 of 1900 edition) maintained that the Bayes (Beta(1,1) uniform prior was not a complete ignorance prior, and that it should be used when prior information justified to \"distribute our ignorance equally\"\".  K. Pearson wrote: \"Yet the only supposition that we appear to have made is this: that, knowing nothing of nature, routine and anomy (from the Greek ανομία, namely: a- \"without\", and nomos \"law\") are to be considered as equally likely to occur.  Now we were not really justified in making even this assumption, for it involves a knowledge that we do not possess regarding nature.  We use our \"experience\" of the constitution and action of coins in general to assert that heads and tails are equally probable, but we have no right to assert before experience that, as we know nothing of nature, routine and breach are equally probable.  In our ignorance we ought to consider before experience that nature may consist of all routines, all anomies (normlessness), or a mixture of the two in any proportion whatever, and that all such are equally probable.  Which of these constitutions after experience is the most probable must clearly depend on what that experience has been like.\" If there is sufficient sampling data, \"and the posterior probability mode is not located at one of the extremes of the domain\" (x=0 or x=1), the three priors of Bayes (Beta(1,1)), Jeffreys (Beta(1/2,1/2)) and Haldane (Beta(0,0)) should yield similar \"posterior\" probability densities.  Otherwise, as Gelman et al. (p. 65) point out, \"if so few data are available that the choice of noninformative prior distribution makes a difference, one should put relevant information into the prior distribution\", or as Berger (p. 125) points out \"when different reasonable priors yield substantially different answers, can it be right to state that there \"is\" a single answer?  Would it not be better to admit that there is scientific uncertainty, with the conclusion depending on prior beliefs? .\" In standard logic, propositions are considered to be either true or false.  In contradistinction, subjective logic assumes that humans cannot determine with absolute certainty whether a proposition about the real world is absolutely true or false.  In subjective logic the posteriori probability estimates of binary events can be represented by beta distributions. A wavelet is a wave-like oscillation with an amplitude that starts out at zero, increases, and then decreases back to zero.  It can typically be visualized as a \"brief oscillation\" that promptly decays.  Wavelets can be used to extract information from many different kinds of data, including – but certainly not limited to – audio signals and images.  Thus, wavelets are purposefully crafted to have specific properties that make them useful for signal processing.  Wavelets are localized in both time and frequency whereas the standard Fourier transform is only localized in frequency.  Therefore, standard Fourier Transforms are only applicable to stationary processes, while wavelets are applicable to non-stationary processes.  Continuous wavelets can be constructed based on the beta distribution.  Beta wavelets can be viewed as a soft variety of Haar wavelets whose shape is fine-tuned by two shape parameters α and β. The beta distribution can be used to model events which are constrained to take place within an interval defined by a minimum and maximum value.  For this reason, the beta distribution — along with the triangular distribution — is used extensively in PERT, critical path method (CPM), Joint Cost Schedule Modeling (JCSM) and other project management/control systems to describe the time to completion and the cost of a task.  In project management, shorthand computations are widely used to estimate the mean and standard deviation of the beta distribution: where \"a\" is the minimum, \"c\" is the maximum, and \"b\" is the most likely value (the mode for \"α\" > 1 and \"β\" > 1). The above estimate for the mean formula_382 is known as the PERT three-point estimation and it is exact for either of the following values of \"β\" (for arbitrary α within these ranges): or skewness = formula_386, and excess kurtosis = formula_387 The above estimate for the standard deviation \"σ\"(\"X\") = (\"c\" − \"a\")/6 is exact for either of the following values of \"α\" and \"β\": Otherwise, these can be poor approximations for beta distributions with other values of α and β, exhibiting average errors of 40% in the mean and 549% in the variance. The beta distribution may also be reparameterized in terms of its mean \"μ\" (0 < \"μ\" < 1) and the addition of both shape parameters \"ν\" = \"α\" + \"β\" > 0 ( p. 83).  Denoting by αPosterior and βPosterior the shape parameters of the posterior beta distribution resulting from applying Bayes theorem to a binomial likelihood function and a prior probability, the interpretation of the addition of both shape parameters to be sample size = \"ν\" = \"α\"·Posterior + \"β\"·Posterior is only correct for the Haldane prior probability Beta(0,0).  Specifically, for the Bayes (uniform) prior Beta(1,1) the correct interpretation would be sample size = \"α\"·Posterior + \"β\" Posterior − 2, or \"ν\" = (sample size) + 2.  Of course, for sample size much larger than 2, the difference between these two priors becomes negligible.  (See section Bayesian inference for further details.)  In the rest of this article ν = α + β will be referred to as \"sample size\", but one should remember that it is, strictly speaking, the \"sample size\" of a binomial likelihood function only when using a Haldane Beta(0,0) prior in Bayes theorem. This parametrization may be useful in Bayesian parameter estimation.  For example, one may administer a test to a number of individuals.  If it is assumed that each person's score (0 ≤ \"θ\" ≤ 1) is drawn from a population-level Beta distribution, then an important statistic is the mean of this population-level distribution.  The mean and sample size parameters are related to the shape parameters α and β via Under this parametrization, one may place an uninformative prior probability over the mean, and a vague prior probability (such as an exponential or gamma distribution) over the positive reals for the sample size, if they are independent, and prior data and/or beliefs justify it. The mode and \"concentration\" formula_392 can also be used to calculate the parameters for a beta distribution. The Balding–Nichols model is a two-parameter parametrization of the beta distribution used in population genetics.  It is a statistical description of the allele frequencies in the components of a sub-divided population: where formula_395 and formula_396; here \"F\" is (Wright's) genetic distance between two populations. See the articles Balding–Nichols model, F-statistics, fixation index and coefficient of relationship, for further information. Solving the system of (coupled) equations given in the above sections as the equations for the mean and the variance of the beta distribution in terms of the original parameters \"α\" and \"β\", one can express the \"α\" and \"β\" parameters in terms of the mean (\"μ\") and the variance (var): This parametrization of the beta distribution may lead to a more intuitive understanding than the one based on the original parameters \"α\" and \"β\".  For example, by expressing the mode, skewness, excess kurtosis and differential entropy in terms of the mean and the variance: A beta distribution with the two shape parameters α and β is supported on the range [0,1] or (0,1).  It is possible to alter the location and scale of the distribution by introducing two further parameters representing the minimum, \"a\", and maximum \"c\" (\"c\" > \"a\"), values of the distribution, by a linear transformation substituting the non-dimensional variable \"x\" in terms of the new variable \"y\" (with support [\"a\",\"c\"] or (\"a\",\"c\")) and the parameters \"a\" and \"c\": The probability density function of the four parameter beta distribution is equal to the two parameter distribution, scaled by the range (\"c\"-\"a\"), (so that the total area under the density curve equals a probability of one), and with the \"y\" variable shifted and scaled as follows: That a random variable \"Y\" is Beta-distributed with four parameters α, β, \"a\", and \"c\" will be denoted by: The measures of central location are scaled (by (\"c\"-\"a\")) and shifted (by \"a\"), as follows: The statistical dispersion measures are scaled (they do not need to be shifted because they are already centered on the mean) by the range (c-a), linearly for the mean deviation and nonlinearly for the variance: Since the skewness and excess kurtosis are non-dimensional quantities (as moments centered on the mean and normalized by the standard deviation), they are independent of the parameters \"a\" and \"c\", and therefore equal to the expressions given above in terms of \"X\" (with support [0,1] or (0,1)): The first systematic modern discussion of the beta distribution is probably due to Karl Pearson FRS (27 March 1857 – 27 April 1936), an influential English mathematician who has been credited with establishing the discipline of mathematical statistics.  In Pearson's papers the beta distribution is couched as a solution of a differential equation: Pearson's Type I distribution which it is essentially identical to except for arbitrary shifting and re-scaling (the beta and Pearson Type I distributions can always be equalized by proper choice of parameters).  In fact, in several English books and journal articles in the few decades prior to World War II, it was common to refer to the beta distribution as Pearson's Type I distribution.  William P. Elderton (1877–1962) in his 1906 monograph \"Frequency curves and correlation\" further analyzes the beta distribution as Pearson's Type I distribution, including a full discussion of the method of moments for the four parameter case, and diagrams of (what Elderton describes as) U-shaped, J-shaped, twisted J-shaped, \"cocked-hat\" shapes, horizontal and angled straight-line cases.  Elderton wrote \"I am chiefly indebted to Professor Pearson, but the indebtedness is of a kind for which it is impossible to offer formal thanks.\"  Elderton in his 1906 monograph provides an impressive amount of information on the beta distribution, including equations for the origin of the distribution chosen to be the mode, as well as for other Pearson distributions: types I through VII.  Elderton also included a number of appendixes, including one appendix (\"II\") on the beta and gamma functions.  In later editions, Elderton added equations for the origin of the distribution chosen to be the mean, and analysis of Pearson distributions VIII through XII. As remarked by Bowman and Shenton \"Fisher and Pearson had a difference of opinion in the approach to (parameter) estimation, in particular relating to (Pearson's method of) moments and (Fisher's method of) maximum likelihood in the case of the Beta distribution.\"  Also according to Bowman and Shenton, \"the case of a Type I (beta distribution) model being the center of the controversy was pure serendipity.  A more difficult model of 4 parameters would have been hard to find.\" Ronald Fisher (17 February 1890 – 29 July 1962) was one of the giants of statistics in the first half of the 20th century, and his long running public conflict with Karl Pearson can be followed in a number of articles in prestigious journals.  For example, concerning the estimation of the four parameters for the beta distribution, and Fisher's criticism of Pearson's method of moments as being arbitrary, see Pearson's article \"Method of moments and method of maximum likelihood\" (published three years after his retirement from University College, London, where his position had been divided between Fisher and Pearson's son Egon) in which Pearson writes \"I read (Koshai's paper in the Journal of the Royal Statistical Society, 1933) which as far as I am aware is the only case at present published of the application of Professor Fisher's method.  To my astonishment that method depends on first working out the constants of the frequency curve by the (Pearson) Method of Moments and then superposing on it, by what Fisher terms \"the Method of Maximum Likelihood\" a further approximation to obtain, what he holds, he will thus get, \"more efficient values\" of the curve constants.\" David and Edwards's treatise on the history of statistics cites the first modern treatment of the beta distribution, in 1911, using the beta designation that has become standard, due to Corrado Gini,(May 23, 1884 – March 13, 1965), an Italian statistician, demographer, and sociologist, who developed the Gini coefficient.  N.L.Johnson and S.Kotz, in their comprehensive and very informative monograph on leading historical personalities in statistical sciences credit Corrado Gini as \"an early Bayesian...who dealt with the problem of eliciting the parameters of an initial Beta distribution, by singling out techniques which anticipated the advent of the so called empirical Bayes approach.\"  Bayes, in a posthumous paper published in 1763 by Richard Price, obtained a beta distribution as the density of the probability of success in Bernoulli trials (see the section titled \"Applications, Bayesian inference\" in this article), but the paper does not analyze any of the moments of the beta distribution or discuss any of its properties.\n\nQuantitative genetics Quantitative genetics is a branch of population genetics that deals with phenotypes that vary continuously (in characters such as height or mass)—as opposed to discretely identifiable phenotypes and gene-products (such as eye-colour, or the presence of a particular biochemical). Both branches use the frequencies of different alleles of a gene in breeding populations (gamodemes), and combine them with concepts from simple Mendelian inheritance to analyze inheritance patterns across generations and descendant lines.  While population genetics can focus on particular genes and their subsequent metabolic products, quantitative genetics focuses more on the outward phenotypes, and makes summaries only of the underlying genetics. Due to the continuous distribution of phenotypic values, quantitative genetics must employ many other statistical methods (such as the \"effect size\", the \"mean\" and the \"variance\") to link phenotypes (attributes) to genotypes.  Some phenotypes may be analyzed either as discrete categories or as continuous phenotypes, depending on the definition of cut-off points, or on the \"metric\" used to quantify them.  Mendel himself had to discuss this matter in his famous paper, especially with respect to his peas attribute \"tall/dwarf\", which actually was \"length of stem\".  Analysis of quantitative trait loci, or QTL, is a more recent addition to quantitative genetics, linking it more directly to molecular genetics. In diploid organisms, the average genotypic \"value\" (locus value) may be defined by the allele \"effect\" together with a dominance effect, and also by how genes interact with genes at other loci (epistasis).  The founder of quantitative genetics - Sir Ronald Fisher - perceived much of this when he proposed the first mathematics of this branch of genetics. Being a statistician, he defined the gene effects as deviations from a central value—enabling the use of statistical concepts such as mean and variance, which use this idea.  The central value he chose for the gene was the midpoint between the two opposing homozygotes at the one locus.  The deviation from there to the \"greater\" homozygous genotype can be named \"\"+a\"\" ; and therefore it is \"\"-a\"\" from that same midpoint to the \"lesser\" homozygote genotype.  This is the \"allele\" effect mentioned above.  The heterozygote deviation from the same midpoint can be named \"\"d\"\", this being the \"dominance\" effect referred to above.  The diagram depicts the idea.  However, in reality we measure phenotypes, and the figure also shows how observed phenotypes relate to the gene effects.  Formal definitions of these effects recognize this phenotypic focus.  Epistasis has been approached statistically as interaction (i.e., inconsistencies), but \"epigenetics\" suggests a new approach may be needed. If 0<d<a, the dominance is regarded as \"partial\" or \"incomplete\"—while d=a indicates full or \"classical\" dominance.  Previously, d>a was known as \"over-dominance\". Mendel's pea attribute \"length of stem\" provides us with a good example.  Mendel stated that the tall true-breeding parents ranged from 6–7 feet in stem length (183 – 213 cm), giving a median of 198 cm (= P1).  The short parents ranged from 0.75–1.25 feet in stem length (23 – 46 cm), with a rounded median of 34 cm (= P2).  Their hybrid ranged from 6–7.5 feet in length (183–229 cm), with a median of 206 cm (= F1).  The mean of P1 and P2 is 116 cm, this being the phenotypic value of the homozygotes midpoint (mp).  The allele affect (\"a\") is [P1-mp] = 82 cm = -[P2-mp].  The dominance effect (\"d\") is [F1-mp] = 90 cm.  This historical example illustrates clearly how phenotype values and gene effects are linked. To obtain means, variances and other statistics, both \"quantities\" and their \"occurrences\" are required.  The gene effects (above) provide the framework for \"quantities\": and the \"frequencies\" of the contrasting alleles in the fertilization gamete-pool provide the information on \"occurrences\".  Commonly, the frequency of the allele causing \"more\" in the phenotype (including dominance) is given the symbol p, while the frequency of the contrasting allele is q.  An initial assumption made when establishing the algebra was that the parental population was infinite and random mating, which was made simply to facilitate the derivation.  The subsequent mathematical development also implied that the frequency distribution within the effective gamete-pool was uniform: there were no local perturbations where p and q varied.  Looking at the diagrammatic analysis of sexual reproduction, this is the same as declaring that p = p = p; and similarly for q.  This mating system, dependent upon these assumptions, became known as \"panmixia\". Panmixia rarely actually occurs in nature, as gamete distribution may be limited, for example by dispersal restrictions or by behaviour, or by chance sampling (those local perturbations mentioned above).  It is well-known that there is a huge wastage of gametes in Nature, which is why the diagram depicts a \"potential\" gamete-pool separately to the \"actual\" gamete-pool.  Only the latter sets the definitive frequencies for the zygotes: this is the true \"gamodeme\" (\"gamo\" refers to the gametes, and \"deme\" derives from Greek for \"population\").  But, under Fisher's assumptions, the \"gamodeme\" can be effectively extended back to the \"potential\" gamete-pool, and even back to the parental base-population (the \"source\" population).  The random sampling arising when small \"actual\" gamete-pools are sampled from a large \"potential\" gamete-pool is known as \"genetic drift\", and is considered subsequently. While panmixia may not be widely extant, the \"potential\" for it does occur, although it may be only ephemeral because of those local perturbations.  It has been shown, for example, that the F2 derived from \"random fertilization of F1 individuals\" (an \"allogamous\" F2), following hybridization, is an \"origin\" of a new \"potentially\" panmictic population.  It has also been shown that if panmictic random fertilization occurred continually, it would maintain the same allele and genotype frequencies across each successive panmictic sexual generation—this being the \"Hardy Weinberg\" equilibrium.  However, as soon as genetic drift was initiated by local random sampling of gametes, the equilibrium would cease. Male and female gametes within the actual fertilizing pool are considered usually to have the same frequencies for their corresponding alleles.  (Exceptions have been considered.)  This means that when p male gametes carrying the A allele randomly fertilize p female gametes carrying that same allele, the resulting zygote has genotype AA, and, under random fertilization, the combination occurs with a frequency of p x p (= p).  Similarly, the zygote aa occurs with a frequency of q. Heterozygotes (Aa) can arise in two ways: when p male (A allele) randomly fertilize q female (a allele) gametes, and \"vice versa\".  The resulting frequency for the heterozygous zygotes is thus 2pq.  Notice that such a population is never more than half heterozygous, this maximum occurring when p=q= 0.5. In summary then, under random fertilization, the zygote (genotype) frequencies are the quadratic expansion of the gametic (allelic) frequencies: formula_1.  (The \"=1\" states that the frequencies are in fraction form, not percentages; and that there are no omissions within the framework proposed.) Notice that \"random fertilization\" and \"panmixia\" are \"not\" synonyms. Mendel's pea experiments were constructed by establishing true-breeding parents with \"opposite\" phenotypes for each attribute.  This meant that each opposite parent was homozygous for its respective allele only.  In our example, \"tall \"vs\" dwarf\", the tall parent would be genotype TT with p = 1 (and q = 0); while the dwarf parent would be genotype tt with q = 1 (and p = 0).  After controlled crossing, their hybrid is Tt, with p = q = ½.  However, the frequency of this heterozygote = 1, because this is the F1 of an artificial cross: it has not arisen through random fertilization.  The F2 generation was produced by natural self-pollination of the F1 (with monitoring against insect contamination), resulting in p = q = ½ being maintained.  Such an F2 is said to be \"autogamous\".  However, the genotype frequencies (0.25 TT, 0.5 Tt, 0.25 tt) have arisen through a mating system very different from random fertilization, and therefore the use of the quadratic expansion has been avoided.  The numerical values obtained were the same as those for random fertilization only because this is the special case of having originally crossed homozygous opposite parents.  We can notice that, because of the dominance of T- [frequency (0.25 + 0.5)] over tt [frequency 0.25], the 3:1 ratio is still obtained. A cross such as Mendel's, where true-breeding (largely homozygous) opposite parents are crossed in a controlled way to produce an F1, is a special case of hybrid structure.  The F1 is often regarded as \"entirely heterozygous\" for the gene under consideration.  However, this is an over-simplification and does not apply generally—for example when individual parents are not homozygous, or when \"populations\" inter-hybridise to form \"hybrid swarms\".  The general properties of intra-species hybrids (F1) and F2 (both \"autogamous\" and \"allogamous\") are considered in a later section. Having noticed that the pea is naturally self-pollinated, we cannot continue to use it as an example for illustrating random fertilization properties.  Self-fertilization (\"selfing\") is a major alternative to random fertilization, especially within Plants.  Most of the Earth's cereals are naturally self-pollinated (rice, wheat, barley, for example), as well as the pulses.  Considering the millions of individuals of each of these on Earth at any time, it's obvious that self-fertilization is at least as significant as random fertilization.  Self-fertilization is the most intensive form of \"inbreeding\", which arises whenever there is restricted independence in the genetical origins of gametes.  Such reduction in independence arises if parents are already related, and/or from genetic drift or other spatial restrictions on gamete dispersal.  Path analysis demonstrates that these are tantamount to the same thing.  Arising from this background, the \"inbreeding coefficient\" (often symbolized as F or f) quantifies the effect of inbreeding from whatever cause.  There are several formal definitions of f, and some of these are considered in later sections.  For the present, note that for a long-term self-fertilized species f = 1. Natural self-fertilized populations are not single \" \"pure lines\" \", however, but mixtures of such lines.  This becomes particularly obvious when considering more than one gene at a time.  Therefore, allele frequencies (p and q) other than 1 or 0 are still relevant in these cases (refer back to the Mendel Cross section).  The genotype frequencies take a different form, however. In general, the genotype frequencies become formula_2 for AA and formula_3 for Aa and formula_4 for aa. Notice that the frequency of the heterozygote declines in proportion to f.  When \"f\" = 1, these three frequencies become respectively p, 0 and q Conversely, when f = 0, they reduce to the random-fertilization quadratic expansion shown previously. The population mean shifts the central reference point from the homozygote midpoint (mp) to the mean of a sexually reproduced population.  This is important not only to relocate the focus into the natural world, but also to use a measure of \"central tendency\" used by Statistics/Biometrics.  In particular, the square of this mean is the Correction Factor, which is used to obtain the genotypic variances later. For each genotype in turn, its allele effect is multiplied by its genotype frequency; and the products are accumulated across all genotypes in the model.  Some algebraic simplification usually follows to reach a succinct result. The contribution of AA is formula_5, that of Aa is formula_6, and that of aa is formula_7.  Gathering together the two a terms and accumulating over all, the result is: formula_8.  Simplification is achieved by noting that formula_9, and by recalling that formula_10, thereby reducing the right-hand term to formula_11. The succinct result is therefore formula_12. This defines the population mean as an \"offset\" from the homozygote midpoint (recall a and d are defined as \"deviations\" from that midpoint).  The Figure depicts G across all values of p for several values of d, including one case of slight over-dominance.  Notice that G is often negative, thereby emphasizing that it is itself a \"deviation\" (from mp). Finally, to obtain the \"actual\" Population Mean in \"phenotypic space\", the midpoint value is added to this offset: formula_13. An example arises from data on ear length in maize.  Assuming for now that one gene only is represented, a = 5.45 cm, d = 0.12 cm [virtually \"0\", really], mp = 12.05 cm.  Further assuming that p = 0.6 and q = 0.4 in this example population, then: G = 5.45 (0.6 − 0.4) + (0.48)0.12 = 1.15 cm (rounded); and P = 1.15 + 12.05 = 13.20 cm (rounded). The contribution of AA is formula_14, while that of aa is formula_15.  [See above for the frequencies.]  Gathering these two a terms together leads to an immediately very simple final result: formula_16.  As before, formula_13. Often, \"G\" is abbreviated to \"G\". Mendel's peas can provide us with the allele effects and midpoint (see previously); and a mixed self-pollinated population with p = 0.6 and q = 0.4 provides example frequencies.  Thus: G = 82 (0.6 − .04) = 59.6 cm (rounded); and P = 59.6 + 116 = 175.6 cm (rounded). A general formula incorporates the inbreeding coefficient f, and can then accommodate any situation.  The procedure is exactly the same as before, using the weighted genotype frequencies given earlier.  After translation into our symbols, and further rearrangement: formula_18 Here, G is G, which was given earlier.  (Often, when dealing with inbreeding, \"G\" is preferred to \"G\".) Supposing that the maize example [given earlier] had been constrained on a holme (a narrow riparian meadow), and had partial inbreeding to the extent of f = 0.25, then, using the third version (above) of G: G = 1.15 − 0.25 (0.48) 0.12 = 1.136   cm (rounded), with P = 13.194   cm (rounded). There is hardly any effect from inbreeding in this example, which arises because there was virtually no dominance in this attribute (d → 0).  Examination of all three versions of G reveals that this would lead to trivial change in the Population mean.  Where dominance was notable, however, there would be considerable change. Genetic drift was introduced when discussing the likelihood of panmixia being widely extant as a natural fertilization pattern.  [See section on Allele and Genotype frequencies.]  Here the sampling of gametes from the \"potential\" gamodeme is discussed in more detail.  The sampling involves random fertilization between pairs of random gametes, each of which may contain either an A or an a allele.  The sampling is therefore binomial sampling.  Each sampling \"packet\" involves 2N alleles, and produces N zygotes (a \"progeny\" or a \"line\") as a result.  During the course of the reproductive period, this sampling is repeated over and over, so that the final result is a mixture of sample progenies.  The result is \"dispersed random fertilization\" formula_19 These events, and the overall end-result, are examined here with an illustrative example. The \"base\" allele frequencies of the example are those of the \"potential gamodeme\": the frequency of A is p = 0.75, while the frequency of a is q = 0.25.  [\"White label\" \"1\" in the diagram.]  Five example actual gamodemes are binomially sampled out of this base (s = the number of samples = 5), and each sample is designated with an \"index\" k: with k = 1 ... s sequentially.  (These are the sampling \"packets\" referred to in the previous paragraph.)  The number of gametes involved in fertilization varies from sample to sample, and is given as 2N [at \"white label\" \"2\" in the diagram].  The total (Σ) number of gametes sampled overall is 52 [\"white label\" \"3\" in the diagram].  Because each sample has its own size, \"weights\" are needed to obtain averages (and other statistics) when obtaining the overall results.  These are formula_20, and are given at \"white label\" \"4\" in the diagram. Following completion of these five binomial sampling events, the resultant actual gamodemes each contained different allele frequencies—(p and q).  [These are given at \"white label\" \"5\" in the diagram.]  This outcome is actually the genetic drift itself.  Notice that two samples (k = 1 and 5) happen to have the same frequencies as the \"base\" (\"potential\") gamodeme.  Another (k = 3) happens to have the \"p\" and \"q\" \"reversed\".  Sample (k = 2) happens to be an \"extreme\" case, with p = 0.9 and q = 0.1 ; while the remaining sample (k = 4) is \"middle of the range\" in its allele frequencies.  All of these results have arisen only by \"chance\", through binomial sampling.  Having occurred, however, they set in place all the downstream properties of the progenies. Because sampling involves chance, the \"probabilities\" ( ∫ ) of obtaining each of these samples become of interest.  These binomial probabilities depend on the starting frequencies (p and q) and the sample size (2N).  They are tedious to obtain, but are of considerable interest.  [See \"white label\" \"6\" in the diagram.]  The two samples (k = 1, 5), with the allele frequencies the same as in the \"potential gamodeme\", had higher \"chances\" of occurring than the other samples.  Their binomial probabilities did differ, however, because of their different sample sizes (2N).  The \"reversal\" sample (k = 3) had a very low Probability of occurring, confirming perhaps what might be expected.  The \"extreme\" allele frequency gamodeme (k = 2) was not \"rare\", however; and the \"middle of the range\" sample (k=4) \"was\" rare.  These same Probabilities apply also to the progeny of these fertilizations. Here, some \"summarizing\" can begin.  The \"overall allele frequencies\" in the progenies bulk are supplied by weighted averages of the appropriate frequencies of the individual samples.  That is: formula_21 and formula_22.  (Notice that k is replaced by • for the overall result—a common practice.)  The results for the example are p = 0.631 and q = 0.369 [\"black label\" \"5\" in the diagram].  These values are quite different to the starting ones (p and q) [\"white label\" \"1\"].  The sample allele frequencies also have variance as well as an average.  This has been obtained using the \"sum of squares (SS)\" method [See to the right of \"black label\" \"5\" in the diagram].  [Further discussion on this variance occurs in the section below on Extensive genetic drift.] The \"genotype frequencies\" of the five sample progenies are obtained from the usual quadratic expansion of their respective allele frequencies (\"random fertilization\").  The results are given at the diagram's \"white label\" \"7\" for the homozygotes, and at \"white label\" \"8\" for the heterozygotes.  Re-arrangement in this manner prepares the way for monitoring inbreeding levels.  This can be done either by examining the level of \"total\" homozygosis [(p + q) = (1 − 2pq)] , or by examining the level of heterozygosis (2pq), as they are complementary.  Notice that samples \"k= 1, 3, 5\" all had the same level of heterozygosis, despite one being the \"mirror image\" of the others with respect to allele frequencies.  The \"extreme\" allele-frequency case (k= \"2\") had the most homozygosis (least heterozygosis) of any sample.  The \"middle of the range\" case (k= \"4\") had the least homozygosity (most heterozygosity): they were each equal at 0.50, in fact. The \"overall summary\" can continue by obtaining the \"weighted average\" of the respective genotype frequencies for the progeny bulk.  Thus, for AA, it is formula_23, for Aa , it is formula_24 and for aa, it is formula_25.  The example results are given at \"black label\" \"7\" for the homozygotes, and at \"black label\" \"8\" for the heterozygote.  Note that the heterozygosity mean is \"0.3588\", which the next section uses to examine inbreeding resulting from this genetic drift. The next focus of interest is the dispersion itself, which refers to the \"spreading apart\" of the progenies' \"population means\".  These are obtained as formula_26 [see section on the Population mean], for each sample progeny in turn, using the example gene effects given at \"white label\" \"9\" in the diagram.  Then, each formula_27 is obtained also [at \"white label\" \"10\" in the diagram].  Notice that the \"best\" line (k = 2) had the \"highest\" allele frequency for the \"more\" allele (A) (it also had the highest level of homozygosity).  The \"worst\" progeny (k = 3) had the highest frequency for the \"less\" allele (a), which accounted for its poor performance.  This \"poor\" line was less homozygous than the \"best\" line; and it shared the same level of homozygosity, in fact, as the two \"second-best\" lines (k = 1, 5).  The progeny line with both the \"more\" and the \"less\" alleles present in equal frequency (k = 4) had a mean below the \"overall average\" (see next paragraph), and had the lowest level of homozygosity.  These results reveal the fact that the alleles most prevalent in the \"gene-pool\" (also called the \"germplasm\") determine performance, not the level of homozygosity per se.  Binomial sampling alone effects this dispersion. The \"overall summary\" can now be concluded by obtaining formula_28 and formula_29.  The example result for P is 36.94 (\"black label\" \"10\" in the diagram).  This later is used to quantify \"inbreeding depression\" overall, from the gamete sampling.  [See the next section.]  However, recall that some \"non-depressed\" progeny means have been identified already (k = 1, 2, 5).  This is an enigma of inbreeding—while there may be \"depression\" overall, there are usually superior lines among the gamodeme samplings. Included in the \"overall summary\" were the average allele frequencies in the mixture of progeny lines (p and q).  These can now be used to construct a hypothetical panmictic equivalent.  This can be regarded as a \"reference\" to assess the changes wrought by the gamete sampling.  The example appends such a panmictic to the right of the Diagram.  The frequency of AA is therefore (p) = 0.3979.  This is less than that found in the dispersed bulk (0.4513 at \"black label\" \"7\").  Similarly, for aa, (q) = 0.1303—again less than the equivalent in the progenies bulk (0.1898).  Clearly, \"genetic drift\" has increased the overall level of homozygosis by the amount (0.6411 − 0.5342) = 0.1069.  In a complementary approach, the heterozygosity could be used instead.  The panmictic equivalent for Aa is 2 p q = 0.4658, which is \"higher\" than that in the sampled bulk (0.3588) [\"black label\" \"8\"].  The sampling has caused the heterozygosity to decrease by 0.1070, which differs trivially from the earlier estimate because of rounding errors. The inbreeding coefficient (f) was introduced in the early section on Self Fertilization.  Here, a formal definition of it is considered: f is the probability that two \"same\" alleles (that is A and A, or a and a), which fertilize together are of common ancestral origin—or (more formally) f is the probability that two homologous alleles are autozygous.  Consider any random gamete in the \"potential\" gamodeme that has its syngamy partner restricted by binomial sampling.  The probability that that second gamete is homologous autozygous to the first is 1/(2N), the reciprocal of the gamodeme size.  For the five example progenies, these quantities are 0.1, 0.0833, 0.1, 0.0833 and 0.125 respectively, and their weighted average is 0.0961.  This is the \"inbreeding coefficient\" of the example progenies bulk, provided it is \"unbiased\" with respect to the full binomial distribution.  An example based upon \"s = 5\" is likely to be biased, however, when compared to an appropriate entire binomial distribution based upon the sample number (\"s\") approaching infinity (\"s → ∞\").  Another derived definition of f for the full Distribution is that f also equals the rise in homozygosity, which equals the fall in heterozygosity.  For the example, these frequency changes are \"0.1069\" and \"0.1070\", respectively.  This result is different to the above, indicating that bias with respect to the full underlying distribution is present in the example.  For the example \"itself\", these latter values are the better ones to use, namely f = 0.10695. The \"population mean\" of the equivalent panmictic is found as \"[a (p-q) + 2 pq d] + mp\".  Using the example \"gene effects\" (\"white label\" \"9\" in the diagram), this mean is formula_30 37.87.  The equivalent mean in the dispersed bulk is 36.94 (\"black label\" \"10\"), which is depressed by the amount \"0.93\".  This is the \"inbreeding depression\" from this Genetic Drift.  However, as noted previously, three progenies were \"not\" depressed (k = 1, 2, 5), and had means even greater than that of the panmictic equivalent.  These are the lines a plant breeder looks for in a line selection programme. If the number of binomial samples is large (s → ∞ ), then p → p and q → q.  It might be queried whether panmixia would effectively re-appear under these circumstances.  However, the sampling of allele frequencies has \"still occurred\", with the result that σ ≠ 0.  In fact, as s → ∞, the formula_31, which is the \"variance\" of the \"whole binomial distribution\".  Furthermore, the \"Wahlund equations\" show that the progeny-bulk \"homozygote\" frequencies can be obtained as the sums of their respective average values (p or q) \"plus\" σ.  Likewise, the bulk \"heterozygote\" frequency is (2 p q) \"minus\" twice the σ.  The variance arising from the binomial sampling is conspicuously present.  Thus, even when s → ∞, the progeny-bulk \"genotype\" frequencies still reveal \"increased homozygosis\", and \"decreased heterozygosis\", there is still \"dispersion of progeny means\", and still \"inbreeding\" and \"inbreeding depression\".  That is, panmixia is \"not\" re-attained once lost because of genetic drift (binomial sampling).  However, a new \"potential\" panmixia can be initiated via an allogamous F2 following hybridization. Previous discussion on genetic drift examined just one cycle (generation) of the process.  When the sampling continues over successive generations, conspicuous changes occur in σ and f. Furthermore, another \"index\" is needed to keep track of \"time\": t = 1 ... y where y = the number of \"years\" (generations) considered.  The methodology often is to add the current binomial increment (Δ = \"\"de novo\"\") to what has occurred previously.  The entire Binomial Distribution is examined here.  [There is no further benefit to be had from an abbreviated example.] Earlier this variance (σ ) was seen to be:- formula_32 With the extension over time, this is also the result of the \"first\" cycle, and so is formula_33 (for brevity).  At cycle 2, this variance is generated yet again—this time becoming the \"de novo\" variance (formula_34)—and accumulates to what was present already—the \"carry-over\" variance.  The \"second\" cycle variance (formula_35) is the weighted sum of these two components, the weights being formula_36 for the \"de novo\" and formula_37 = formula_38 for the\"carry-over\". Thus, The extension to generalize to any time \"t\" , after considerable simplification, becomes: - Because it was this variation in allele frequencies that caused the \"spreading apart\" of the progenies' means (\"dispersion\"), the change in σ over the generations indicates the change in the level of the \"dispersion\". The method for examining the inbreeding coefficient is similar to that used for \"σ \".  The same weights as before are used respectively for \"de novo f\" ( Δ f ) [recall this is 1/(2N) ] and \"carry-over f\".  Therefore, formula_39 , which is similar to Equation (1) in the previous sub-section. In general, after rearrangement, formula_40 The graphs to the left show levels of inbreeding over twenty generations arising from genetic drift for various \"actual gamodeme\" sizes (2N). Still further rearrangements of this general equation reveal some interesting relationships. (A) After some simplification, formula_41.  The left-hand side is the difference between the current and previous levels of inbreeding: the \"change in inbreeding\" (δf).  Notice, that this \"change in inbreeding\" (δf) is equal to the \"de novo inbreeding\" (Δf) only for the first cycle—when f is \"zero\". (B) An item of note is the (1-f), which is an \"index of \"non-inbreeding\"\".  It is known as the \"panmictic index\".  formula_42. (C) Further useful relationships emerge involving the \"panmictic index\". formula_43. (D) A key link emerges between \"σ \" and \"f\".  Firstly ... formula_44 Secondly, presuming that f = 0, the right-hand side of this equation reduces to the section within the brackets of Equation (2) at the end of the last sub-section.  That is, if initially there is no inbreeding, formula_45 !  Furthermore, if this then is rearranged, formula_46.  That is, when initial inbreeding is zero, the two principal viewpoints of \"binomial gamete sampling\" (genetic drift) are directly inter-convertible. It is easy to overlook that \"random fertilization\" includes self-fertilization.  Sewall Wright showed that a proportion 1/N of \"random fertilizations\" is actually \"self fertilization\" formula_47, with the remainder (N-1)/N being \"cross fertilization\" formula_48.  Following path analysis and simplification, the new view \"random fertilization inbreeding\" was found to be: formula_49.  Upon further rearrangement, the earlier results from the binomial sampling were confirmed, along with some new arrangements.  Two of these were potentially very useful, namely: (A) formula_50; and (B) formula_51. The recognition that selfing may \"intrinsically be a part of\" random fertilization leads to some issues about the use of the previous \"random fertilization\" 'inbreeding coefficient'.  Clearly, then, it is inappropriate for any species incapable of \"self fertilization\", which includes plants with self-incompatibility mechanisms, dioecious plants, and bisexual animals.  The equation of Wright was modified later to provide a version of random fertilization that involved only \"cross fertilization\" with no \"self fertilization\".  The proportion 1/N formerly due to \"selfing\" now defined the \"carry-over\" gene-drift inbreeding arising from the previous cycle.  The new version is: formula_52. The graphs to the right depict the differences between standard \"random fertilization\" RF, and random fertilization adjusted for \"cross fertilization alone\" CF. As can be seen, the issue is non-trivial for small gamodeme sample sizes. It now is necessary to note that not only is \"panmixia\" \"not\" a synonym for \"random fertilization\", but also that \"random fertilization\" is \"not\" a synonym for \"cross fertilization\". In the sub-section on \"The sample gamodemes – Genetic drift\", a series of gamete samplings was followed, an outcome of which was an increase in homozygosity at the expense of heterozygosity.  From this viewpoint, the rise in homozygosity was due to the gamete samplings.  Levels of homozygosity can be viewed also according to whether homozygotes arose allozygously or autozygously.  Recall that autozygous alleles have the same allelic origin, the likelihood (frequency) of which is the inbreeding coefficient (\"f\") by definition.  The proportion arising \"allozygously\" is therefore (1-f).  For the A-bearing gametes, which are present with a general frequency of p, the overall frequency of those that are autozygous is therefore (f \"p\").  Similarly, for a-bearing gametes, the autozygous frequency is (f \"q\").  These two viewpoints regarding genotype frequencies must be connected to establish consistency. Following firstly the \"auto/allo\" viewpoint, consider the \"allozygous\" component.  This occurs with the frequency of (1-f), and the alleles unite according to the \"random fertilization\" quadratic expansion.  Thus: formula_53 Consider next the \"autozygous\" component.  As these alleles are \"autozygous\", they are effectively selfings, and produce either AA or aa genotypes, but no heterozygotes.  They therefore produce formula_54 \"\"AA\"\" homozygotes plus formula_55 \"\"aa\"\" homozygotes.  Adding these two components together results in: formula_56 for the AA homozygote; formula_57 for the aa homozygote; and formula_58 for the Aa heterozygote.  This is the same equation as that presented earlier in the section on \"Self fertilization – an alternative\".  The reason for the decline in heterozygosity is made clear here.  Heterozygotes can arise only from the allozygous component, and its frequency in the sample bulk is just (1-f): hence this must also be the factor controlling the frequency of the heterozygotes. Secondly, the \"sampling\" viewpoint is re-examined.  Previously, it was noted that the decline in heterozygotes was formula_59.  This decline is distributed equally towards each homozygote; and is added to their basic \"random fertilization\" expectations.  Therefore, the genotype frequencies are: formula_60 for the \"\"AA\"\" homozygote; formula_61 for the \"\"aa\"\" homozygote; and formula_62 for the heterozygote. Thirdly, the \"consistency\" between the two previous viewpoints needs establishing.  It is apparent at once [from the corresponding equations above] that the heterozygote frequency is the same in both viewpoints.  However, such a straightforward result is not immediately apparent for the homozygotes.  Begin by considering the AA homozygote's final equation in the \"auto/allo\" paragraph above:- formula_56.  Expand the brackets, and follow by re-gathering [within the resultant] the two new terms with the common-factor \"f\" in them.  The result is: formula_64.  Next, for the parenthesized \" \"p\" \", a \"(1-q)\" is substituted for a \"p\", the result becoming formula_65.  Following that substitution, it is a straightforward matter of multiplying-out, simplifying and watching signs.  The end result is formula_66, which is exactly the result for AA in the \"sampling\" paragraph.  The two viewpoints are therefore \"consistent\" for the AA homozygote.  In a like manner, the consistency of the aa viewpoints can also be shown.  The two viewpoints are consistent for all classes of genotypes. In previous sections, dispersive random fertilization (\"genetic drift\") has been considered comprehensively, and self-fertilization and hybridizing have been examined to varying degrees.  The diagram to the left depicts the first two of these, along with another \"spatially based\" pattern: \"islands\".  This is a pattern of \"random fertilization\" featuring \"dispersed gamodemes\", with the addition of \"overlaps\" in which \"non-dispersive\" random fertilization occurs.  With the \"islands\" pattern, individual gamodeme sizes (2N) are observable, and overlaps (m) are minimal.  This is one of Sewall Wright's array of possibilities.  In addition to \"spatially\" based patterns of fertilization, there are others based on either \"phenotypic\" or \"relationship\" criteria.  The \"phenotypic\" bases include \"assortative\" fertilization (between similar phenotypes) and \"disassortative\" fertilization (between opposite phenotypes).  The \"relationship\" patterns include \"sib crossing\", \"cousin crossing\" and \"backcrossing\"—and are considered in a separate section.  \"Self fertilization\" may be considered both from a spatial or relationship point of view. The breeding population consists of s small dispersed random fertilization gamodemes of sample size formula_67 ( k = 1 ... \"s\" ) with \" \"overlaps\" \" of proportion formula_68 in which non-dispersive random fertilization occurs.  The \" dispersive proportion \" is thus formula_69.  The bulk population consists of \"weighted averages\" of sample sizes, allele and genotype frequencies and progeny means, as was done for genetic drift in an earlier section.  However, each \"gamete sample size\" is reduced to allow for the \"overlaps\", thus finding a formula_70 effective for formula_69. For brevity, the argument is followed further with the subscripts omitted.  Recall that formula_72 is formula_73 in general.  [Here, and following, the \"2N\" refers to the \"previously defined\" sample size, not to any \"islands adjusted\" version.] After simplification, formula_74 Notice that when \"m = 0\" this reduces to the previous \"Δ f\".  The reciprocal of this furnishes an estimate of the \" formula_70 \"effective for formula_69\" \", mentioned above. This Δf is also substituted into the previous \"inbreeding coefficient\" to obtain formula_77 where \"t\" is the index over generations, as before. The effective \"overlap proportion\" can be obtained also, as formula_78 The graphs to the right show the \"inbreeding\" for a gamodeme size of \"2N = 50\" for \"ordinary dispersed random fertilization \" (RF) \"(m=0)\", and for \"four overlap levels ( m = 0.0625, 0.125, 0.25, 0.5 )\" of islands \"random fertilization\".  There has indeed been reduction in the inbreeding resulting from the \"non-dispersed random fertilization\" in the overlaps.  It is particularly notable as m → 0.50.  Sewall Wright suggested that this value should be the limit for the use of this approach. The \"gene-model\" examines the heredity pathway from the point of view of \"inputs\" (alleles/gametes) and \"outputs\" (genotypes/zygotes), with fertilization being the \"process\" converting one to the other.  An alternative viewpoint concentrates on the \"process\" itself, and considers the zygote genotypes as arising from allele shuffling.  In particular, it regards the results as if one allele had \"substituted\" for the other during the shuffle, together with a residual that deviates from this view.  This formed an integral part of Fisher's method, in addition to his use of frequencies and effects to generate his genetical statistics.  A discursive derivation of the \"allele substitution\" alternative follows. Suppose that the usual random fertilization of gametes in a \"base\" gamodeme—consisting of p gametes (A) and q gametes (a)—is replaced by fertilization with a \"flood\" of gametes all containing a single allele (A or a, but not both).  The zygotic results can be interpreted in terms of the \"flood\" allele having \"substituted for\" the alternative allele in the underlying \"base\" gamodeme.  The diagram assists in following this viewpoint: the upper part pictures an A substitution, while the lower part shows an a substitution.  (The diagram's \"RF allele\" is the allele in the \"base\" gamodeme.) Consider the upper part firstly.  Because \"base\" A is present with a frequency of p, the \"substitute\" A fertilizes it with a frequency of p resulting in a zygote AA with an allele effect of a. Its contribution to the outcome, therefore, is the product formula_79.  Similarly, when the \"substitute\" fertilizes \"base\" a (resulting in Aa with a frequency of q and heterozygote effect of d), the contribution is formula_80.  The overall result of substitution by A is, therefore, formula_81.  This is now oriented towards the population mean [see earlier section] by expressing it as a deviate from that mean : formula_82 After some algebraic simplification, this becomes formula_83 - the \"substitution effect\" of A. A parallel reasoning can be applied to the lower part of the diagram, taking care with the differences in frequencies and gene effects.  The result is the \"substitution effect\" of a, which is formula_84 The common factor inside the brackets is the \"average allele substitution effect\", and is formula_85 It can also be derived in a more direct way, but the result is the same. In subsequent sections, these substitution effects help define the gene-model genotypes as consisting of a partition predicted by these new effects (substitution \"expectations\"), and a residual (substitution deviations) between these expectations and the previous gene-model effects.  The \"expectations\" are also called the breeding values and the deviations are also called dominance deviations. Ultimately, the variance arising from the \"substitution expectations\" becomes the so-called \"Additive genetic variance (σ)\" (also the \"Genic variance\" )— while that arising from the \"substitution deviations\" becomes the so-called \"Dominance variance (σ)\".  It is noticeable that neither of these terms reflects the true meanings of these variances.  The \"genic variance\" is less dubious than the \" additive genetic variance\", and more in line with Fisher's own name for this partition.  A less-misleading name for the \"dominance deviations variance\" is the \"quasi-dominance variance\" [see following sections for further discussion].  These latter terms are preferred herein. The gene-model effects (a, d and -a) are important soon in the derivation of the \"deviations from substitution\", which were first discussed in the previous \"Allele Substitution\" section.  However, they need to be redefined themselves before they become useful in that exercise.  They firstly need to be re-centralized around the population mean (G), and secondly they need to be re-arranged as functions of β, the \"average allele substitution effect\". Consider firstly the re-centralization.  The re-centralized effect for AA is a• = a - G which, after simplification, becomes a• = 2\"q\"(a-\"p\"d).  The similar effect for Aa is d• = d - G = a(\"q\"-\"p\") + d(1-2\"pq\"), after simplification.  Finally, the re-centralized effect for aa is (-a)• = -2\"p\"(a+\"q\"d). Secondly, consider the re-arrangement of these re-centralized effects as functions of β.  Recalling from the \"Allele Substitution\" section that β = [a +(q-p)d], rearrangement gives a = [β -(q-p)d].  After substituting this for a in a• and simplifying, the final version becomes a•• = 2q(β-qd).  Similarly, d• becomes d•• = β(q-p) + 2pqd; and (-a)• becomes (-a)•• = -2p(β+pd). The zygote genotypes are the target of all this preparation.  The homozygous genotype AA is a union of two \"substitution effects of A\", one from each sex.  Its \"substitution expectation\" is therefore β = 2β = 2\"q\"β (see previous sections).  Similarly, the \"substitution expectation\" of Aa is β = β + β = (\"q\"-\"p\")β ; and for aa, β = 2β = -2\"p\"β.  These \"substitution expectations\" of the genotypes are also called \"breeding values\". \"Substitution deviations\" are the differences between these \"expectations\" and the \"gene effects\" after their two-stage redefinition in the previous section.  Therefore, d = a•• - β = -2\"q\"d after simplification.  Similarly, d = d•• - β = 2\"pq\"d after simplification.  Finally, d = (-a)•• - β = -2\"p\"d after simplification.  Notice that all of these \"substitution deviations\" ultimately are functions of the gene-effect \"d\"—which accounts for the use of [\"d\" plus subscript] as their symbols.  However, it is a serious \"non sequitur\" in logic to regard them as accounting for the dominance (heterozygosis) in the entire gene model : they are simply \"functions\" of \"d\" and not an \"audit\" of the \"d\" in the system.  They \"are\" as derived: \"deviations from the substitution expectations\"! The \"substitution expectations\" ultimately give rise to the σ (the so-called \"Additive\" genetic variance); and the \"substitution deviations\" give rise to the σ (the so-called \"Dominance\" genetic variance).  Be aware, however, that the average substitution effect (β) also contains \"d\" [see previous sections], indicating that dominance is also embedded within the \"Additive\" variance [see following sections on the Genotypic Variance for their derivations].  Remember also [see previous paragraph] that the \"substitution deviations\" do not account for the dominance in the system (being nothing more than deviations from the \"substitution expectations\"), but which happen to consist algebraically of functions of \"d\".  More appropriate names for these respective variances might be σ (the \"Breeding expectations\" variance) and σ (the \"Breeding deviations\" variance).  However, as noted previously, \"Genic\" (σ ) and \"Quasi-Dominance\" (σ ), respectively, will be preferred herein. There are two major approaches to defining and partitioning \"genotypic variance\".  One is based on the \"gene-model effects\", while the other is based on the \"genotype substitution effects\" They are algebraically inter-convertible with each other.  In this section, the basic random fertilization derivation is considered, with the effects of inbreeding and dispersion set aside.  This is dealt with later to arrive at a more general solution.  Until this \"mono-genic\" treatment is replaced by a \"multi-genic\" one, and until \"epistasis\" is resolved in the light of the findings of \"epigenetics\", the Genotypic variance has only the components considered here. It is convenient to follow the Biometrical approach, which is based on correcting the \"unadjusted sum of squares (USS)\" by subtracting the \"correction factor (CF)\".  Because all effects have been examined through frequencies, the USS can be obtained as the sum of the products of each genotype's frequency' and the square of its \"gene-effect\".  The CF in this case is the mean squared.  The result is the SS, which, again because of the use of frequencies, is also immediately the \"variance\". The formula_86, and the formula_87.  The formula_88 After partial simplification, formula_89 The last line is in Mather's terminology. Here, σ is the \"homozygote\" or allelic variance, and σ is the \"heterozygote\" or dominance variance.  The \"substitution deviations\" variance (σ) is also present.  The \"(weighted_covariance)\" is abbreviated hereafter to \" cov \". These components are plotted across all values of p in the accompanying figure.  Notice that \"cov\" is negative for \"p > 0.5\". Further gathering of terms [in Mather format] leads to formula_90, where formula_91.  It is useful later in Diallel analysis, which is an experimental design for estimating these genetical statistics. If, following the last-given rearrangements, the first three terms are amalgamated together, rearranged further and simplified, the result is the variance of the Fisherian \"substitution expectation\". That is: formula_92 Notice particularly that σ is not σ.  The first is the \"substitution expectations\" variance, while the second is the \"allelic\" variance.  Notice also that σ (the \"substitution-deviations\" variance) is \"not\" σ (the \"dominance\" variance), and recall that it is an artifact arising from the use of \"G\" for the Correction Factor.  [See the \"blue paragraph\" above.]  It now will be referred to as the \"quasi-dominance\" variance. Also note that σ < σ (\"2pq\" being always a fraction); and note that (1) σ = 2pq σ, and that (2) σ = σ / (2pq).  That is: it is confirmed that σ does not quantify the dominance variance in the model.  It is σ which does that.  However, the dominance variance (σ) can be estimated readily from the σ if \"2pq\" is available. From the Figure, these results can be visualized as accumulating σ, σ and cov to obtain σ, while leaving the σ still separated.  It is clear also in the Figure that σ < σ, as expected from the equations. The overall result (in Fisher's format) is formula_93 The Fisherian components have just been derived, but their derivation via the \"substitution effects\" themselves is given also, in the next section. Reference to the several earlier sections on allele substitution reveals that the two ultimate effects are \"genotype substitution \" expectations and \"genotype substitution deviations\".  Notice that these are each already defined as deviations from the \"random fertilization\" population mean (G).  For each genotype in turn therefore, the product of the frequency and the square of the relevant effect is obtained, and these are accumulated to obtain directly a SS and σ.  Details follow. σ = p\" β + 2\"pq\" β + q\" β, which simplifies to σ = 2\"pq\"β—the Genic variance. σ = p\" d + 2\"pq\" d + q\" d, which simplifies to σ = (2\"pq\") d—the quasi-Dominance variance. Upon accumulating these results, σ = σ + σ .  These components are visualized in the graphs to the right.  The \"average allele substitution\" effect is graphed also, but the symbol is \"α\" (as is common in the citations) rather than \"β\" (as is used herein). Once again, however, refer to the earlier discussions about the true meanings and identities of these components.  Fisher himself did not use these modern terms for his components.  The \"substitution expectations\" variance he named the \"genetic\" variance; and the \"substitution deviations\" variance he regarded simply as the unnamed residual between the \"genotypic\" variance (his name for it) and his \"genetic\" variance.  [The terminology and derivation used in this article are completely in accord with Fisher's own.]  Mather's term for the \"expectations\" variance—\"genic\"—is obviously derived from Fisher's term, and avoids using \"genetic\" (which has become too generalized in usage to be of value in the present context).  The origin is obscure of the modern misleading terms \"additive\" and \"dominance\" variances. Note that this allele-substitution approach defined the components separately, and then totaled them to obtain the final Genotypic variance.  Conversely, the gene-model approach derived the whole situation (components and total) as one exercise.  Bonuses arising from this were (a) the revelations about the real structure of σ, and (b) the real meanings and relative sizes of σ and σ (see previous sub-section).  It is also apparent that a \"Mather\" analysis is more informative, and that a \"Fisher\" analysis can always be constructed from it.  The opposite conversion is not possible, however, because information about cov would be missing. In the section on genetic drift, and in other sections that discuss inbreeding, a major outcome from allele frequency sampling has been the \"dispersion\" of progeny means.  This collection of means has its own average, and also has a variance: the \"amongst-line variance\".  (This is a variance of the attribute itself, not of \"allele frequencies\".)  As dispersion develops further over succeeding generations, this amongst-line variance would be expected to increase.  Conversely, as homozygosity rises, the within-lines variance would be expected to decrease.  The question arises therefore as to whether the total variance is changing—and, if so, in what direction.  To date, these issues have been presented in terms of the \"genic (σ )\" and \"quasi-dominance (σ )\" variances rather than the gene-model components.  This will be done herein as well. The crucial \"overview equation\" comes from Sewall Wright, and is the outline of the inbred genotypic variance based on a \"weighted average of its extremes\", the weights being quadratic with respect to the \"inbreeding coefficient\" formula_94.  This equation is: formula_95 where formula_94 is the inbreeding coefficient, formula_97 is the genotypic variance at \"f=0\", formula_98 is the genotypic variance at \"f=1\", formula_99 is the population mean at \"f=0\", and formula_100 is the population mean at \"f=1\". The formula_101 component [in the equation above] outlines the reduction of variance within progeny lines.  The formula_94 component addresses the increase in variance amongst progeny lines.  Lastly, the formula_103 component is seen (in the next line) to address the \"quasi-dominance\" variance.  These components can be expanded further thereby revealing additional insight.  Thus:- formula_104 Firstly, \"σ\" [in the equation above] has been expanded to show its two sub-components [see section on \"Genotypic variance\"].  Next, the \"σ\" has been converted to \"4pqa \", and is derived in a section following.  The third component's substitution is the difference between the two \"inbreeding extremes\" of the population mean [see section on the \"Population Mean\"]. Summarising: the within-line components are formula_105 and formula_106; and the amongst-line components are formula_107 and formula_108. Rearranging gives the following: formula_109 The version in the last line is discussed further in a subsequent section. Similarly, formula_110 Graphs to the left show these three genic variances, together with the three quasi-dominance variances, across all values of f, for p = 0.5 (at which the quasi-dominance variance is at a maximum).  Graphs to the right show the Genotypic variance partitions (being the sums of the respective \"genic\" and \"quasi-dominance\" partitions) changing over ten generations with an example \"f = 0.10\". Answering, firstly, the questions posed at the beginning about the total variances [the Σ in the graphs] : the \"genic variance\" rises linearly with the \"inbreeding coefficient\", maximizing at twice its starting level.  The \"quasi-dominance variance\" declines at the rate of \"(1 − f )\" until it finishes at zero.  At low levels of \"f\", the decline is very gradual, but it accelerates with higher levels of \"f\". Secondly, notice the other trends.  It is probably intuitive that the within line variances decline to zero with continued inbreeding, and this is seen to be the case (both at the same linear rate \"(1-f)\" ).  The amongst line variances both increase with inbreeding up to \"f = 0.5\", the \"genic variance\" at the rate of \"2f\", and the \"quasi-dominance variance\" at the rate of \"(f − f)\".  At \"f > 0.5\", however, the trends change.  The amongst line \"genic variance\" continues its linear increase until it equals the total \"genic variance\".  But, the amongst line \"quasi-dominance variance\" now declines towards \"zero\", because \"(f − f)\" also declines with \"f > 0.5\". Recall that when \"f=1\", heterozygosity is zero, within-line variance is zero, and all genotypic variance is thus \"amongst-line\" variance and deplete of dominance variance.  In other words, σ is the variance amongst fully inbred line means.  Recall further [from \"The mean after self-fertilization\" section] that such means (G's, in fact) are G = a(p-q).  Substituting \"(1-q)\" for the \"p\", gives G = a (1 − 2q) = a − 2aq.  Therefore, the σ is the σ actually.  Now, in general, the \"variance of a difference (x-y)\" is [ σ + σ − 2 cov ].  Therefore, σ = [ σ + σ − 2 cov ] .  But a (an allele \"effect\") and q (an allele \"frequency\") are \"independent\"—so this covariance is zero.  Furthermore, a is a constant from one line to the next, so σ is also zero.  Further, 2a is another constant (k), so the σ is of the type \"σ\".  In general, the variance \"σ\" is equal to k σ .  Putting all this together reveals that σ = (2a) σ .  Recall [from the section on \"Continued genetic drift\"] that \"σ = pq f \".  With \"f=1\" here within this present derivation, this becomes \"pq 1\" (that is pq), and this is substituted into the previous. The final result is: σ = σ = 4a pq = 2(2pq a) = 2 σ . It follows immediately that f\" σ = \"f\" 2 σ .  [This last \"f\" comes from the \"initial Sewall Wright equation\" : it is not\" the \"f \" just set to \"1\" in the derivation concluded two lines above.] Previous sections found that the within line \"genic variance\" is based upon the \"substitution-derived\" genic variance ( σ )—but the \"amongst line\" \"genic variance\" is based upon the \"gene model\" allelic variance ( σ ).  These two cannot simply be added to get \"total genic variance\".  One approach in avoiding this problem was to re-visit the derivation of the \"average allele substitution effect\", and to construct a version, ( β ), that incorporates the effects of the dispersion.  Crow and Kimura achieved this using the re-centered allele effects (a•, d•, (-a)• ) discussed previously [\"Gene effects re-defined\"].  However, this was found subsequently to under-estimate slightly the \"total Genic variance\", and a new variance-based derivation led to a refined version. The \"refined\" version is: β = { a + [(1−\"f\" ) / (1 + \"f\" )] 2(q − p ) ad + [(1-\"f\" ) / (1 + \"f\" )] (q − p ) d } Consequently, σ = (1 + \"f\" ) 2pq β does now agree with [ (1-f) σ + 2f σ ] exactly. The \"total genic variance\" is of intrinsic interest in its own right.  But, prior to the refinements by Gordon, it had had another important use as well.  There had been no extant estimators for the \"dispersed\" quasi-dominance.  This had been estimated as the difference between Sewall Wright's \"inbred genotypic variance\" and the total \"dispersed\" genic variance [see the previous sub-section].  An anomaly appeared, however, because the \"total quasi-dominance variance\" appeared to increase early in inbreeding despite the decline in heterozygosity. The refinements in the previous sub-section corrected this anomaly.  At the same time, a direct solution for the \"total quasi-dominance variance\" was obtained, thus avoiding the need for the \"subtraction\" method of previous times.  Furthermore, direct solutions for the \"amongst-line\" and \"within-line\" partitions of the \"quasi-dominance variance\" were obtained also, for the first time.  [These have been presented in the section \"Dispersion and the genotypic variance\".] The environmental variance is phenotypic variability, which cannot be ascribed to genetics.  This sounds simple, but the experimental design needed to separate the two needs very careful planning.  Even the \"external\" environment can be divided into spatial and temporal components (\"Sites\" and \"Years\"); or into partitions such as \"litter\" or \"family\", and \"culture\" or \"history\".  These components are very dependent upon the actual experimental model used to do the research.  Such issues are very important when doing the research itself, but in this article on quantitative genetics this overview may suffice. It is an appropriate place, however, for a summary: Phenotypic variance = genotypic variances + environmental variances + genotype-environment interaction + experimental \"error\" variance i.e., σ² = σ² + σ² + σ² + σ² \" or\" σ² = σ² + σ² + σ² + σ² + σ² + σ² after partitioning the genotypic variance (G) into component variances \"genic\" (A), \"quasi-dominance\" (D), and \"epistatic\" (I). The Environmental variance will appear in other sections, such as \"Heritability\" and \"Correlated attributes\". The heritability of a trait is the proportion of the total (phenotypic) variance (σ ) that is attributable to genetic variance, whether it be the full genotypic variance, or some component of it.  It quantifies the degree to which phenotypic variability is due to genetics: but the precise meaning depends upon which genetical variance partition is used in the numerator of the proportion.  Research estimates of heritability have standard errors, just as have all estimated statistics. Where the numerator variance is the whole Genotypic variance ( σ ), the heritability is known as the \"broadsense\" heritability (\"H\").  It quantifies the degree to which variability in an attribute is determined by genetics as a whole.  formula_111 [See section on the Genotypic variance.] If only Genic variance (σ) is used in the numerator, the heritability may be called \"narrow sense\" (h).  It quantifies the extent to which phenotypic variance is determined by Fisher's \"substitution expectations\" variance.  formula_112Fisher proposed that this narrow-sense heritability might be appropriate in considering the results of natural selection, focusing as it does on change-ability, \"that is\" upon \"adaptation\".  He proposed it with regard to quantifying Darwinian evolution. Recalling that the allelic variance (\"σ \") and the dominance variance (\"σ \") are eu-genetic components of the gene-model [see section on the Genotypic variance], and that \"σ \" (the \"substitution deviations\" or \" \"quasi-dominance\" \" variance) and \"cov\" are due to changing from the homozygote midpoint (mp) to the population mean (G), it can be seen that the real meanings of these heritabilities are obscure.  The heritabilities formula_113 and formula_114 have unambiguous meaning. Narrow-sense heritability has been used also for predicting generally the results of artificial selection.  In the latter case, however, the broadsense heritability may be more appropriate, as the whole attribute is being altered: not just adaptive capacity.  Generally, advance from selection is more rapid the higher the heritability.  [See section on \"Selection\".]  In animals, heritability of reproductive traits is typically low, while heritability of disease resistance and production are moderately low to moderate, and heritability of body conformation is high. Repeatability (r) is the proportion of phenotypic variance attributable to differences in repeated measures of the same subject, arising from later records.  It is used particularly for long-lived species.  This value can only be determined for traits that manifest multiple times in the organism's lifetime, such as adult body mass, metabolic rate or litter size.  Individual birth mass, for example, would not have a repeatability value: but it would have a heritability value.  Generally, but not always, repeatability indicates the upper level of the heritability. r = (s² + s²)/s² where s² = phenotype-environment interaction = repeatability. The above concept of repeatability is, however, problematic for traits that necessarily change greatly between measurements.  For example, body mass increases greatly in many organisms between birth and adult-hood.  Nonetheless, within a given age range (or life-cycle stage), repeated measures could be done, and repeatability would be meaningful within that stage. From the heredity perspective, relations are individuals that inherited genes from one or more common ancestors.  Therefore, their \"relationship\" can be \"quantified\" on the basis of the probability that they each have inherited a copy of an allele from the common ancestor.  In earlier sections, the \"Inbreeding coefficient\" has been defined as, \"the probability that two \"same\" alleles ( A and A, or a and a ) have a common origin\"—or, more formally, \"The probability that two homologous alleles are autozygous.\"  Previously, the emphasis was on an individual's likelihood of having two such alleles, and the coefficient was framed accordingly.  It is obvious, however, that this probability of autozygosity for an individual must also be the probability that each of its \"two parents\" had this autozygous allele.  In this re-focused form, the probability is called the \"co-ancestry coefficient\" for the two individuals \"i\" and \"j\" ( \"f\" ).  In this form, it can be used to quantify the relationship between two individuals, and may also be known as the \"coefficient of kinship\" or the \"consanguinity coefficient\". \"Pedigrees\" are diagrams of familial connections between individuals and their ancestors, and possibly between other members of the group that share genetical inheritance with them.  They are relationship maps.  A pedigree can be analyzed, therefore, to reveal coefficients of inbreeding and co-ancestry.  Such pedigrees actually are informal depictions of \"path diagrams\" as used in \"path analysis\", which was invented by Sewall Wright when he formulated his studies on inbreeding.  Using the adjacent diagram, the probability that individuals \"B\" and \"C\" have received autozygous alleles from ancestor \"A\" is \"1/2\" (one out of the two diploid alleles).  This is the \"de novo\" inbreeding (Δf) at this step.  However, the other allele may have had \"carry-over\" autozygosity from previous generations, so the probability of this occurring is (\"de novo complement\" multiplied by the \"inbreeding of ancestor A\" ), that is (1 − Δf ) f = (1/2) f .  Therefore, the total probability of autozygosity in B and C, following the bi-furcation of the pedigree, is the sum of these two components, namely (1/2) + (1/2)f = (1/2) (1+f ) .  This can be viewed as the probability that two random gametes from ancestor A carry autozygous alleles, and in that context is called the \"coefficient of parentage\" ( f ).  It appears often in the following paragraphs. Following the \"B\" path, the probability that any autozygous allele is \"passed on\" to each successive parent is again (1/2) at each step (including the last one to the \"target\" X ).  The overall probability of transfer down the \"B path\" is therefore (1/2) .  The power that (1/2) is raised to can be viewed as \"the number of intermediates in the path between A and X \", n = 3 .  Similarly, for the \"C path\", n = 2 , and the \"transfer probability\" is (1/2) .  The combined probability of autozygous transfer from A to X is therefore [ f (1/2) (1/2) ] .  Recalling that \" f = (1/2) (1+f ) \", f = f = (1/2) (1 + f ) .  In this example, assuming that f = 0, f = 0.0156 (rounded) = f , one measure of the \"relatedness\" between P and Q. In this section, powers of (1/2) were used to represent the \"probability of autozygosity\".  Later, this same method will be used to represent the proportions of ancestral gene-pools which are inherited down a pedigree [the section on \"Relatedness between relatives\"]. In the following sections on sib-crossing and similar topics, a number of \"averaging rules\" are useful.  These derive from path analysis.  The rules show that any co-ancestry coefficient can be obtained as the average of \"cross-over co-ancestries\" between appropriate grand-parental and parental combinations.  Thus, referring to the adjacent diagram, \"Cross-multiplier 1\" is that f = average of ( f , f , f , f ) = (1/4) [f + f + f + f ] = f .  In a similar fashion, \"cross-multiplier 2\" states that f = (1/2) [ f + f ]—while \"cross-multiplier 3\" states that f = (1/2) [ f + f ] .  Returning to the first multiplier, it can now be seen also to be f = (1/2) [ f + f ], which, after substituting multipliers 2 and 3, resumes its original form. In much of the following, the grand-parental generation is referred to as (t-2) , the parent generation as (t-1) , and the \"target\" generation as t. The diagram to the right shows that \"full sib crossing\" is a direct application of \"cross-Multiplier 1\", with the slight modification that \"parents A and B\" repeat (in lieu of \"C and D\") to indicate that individuals \"P1\" and \"P2\" have both of \"their\" parents in common—that is they are \"full siblings\".  Individual Y is the result of the crossing of two full siblings.  Therefore, f = f = (1/4) [ f + 2 f + f ] .  Recall that f and f were defined earlier (in Pedigree analysis) as \"coefficients of parentage\", equal to \" (1/2)[1+f ] \" and \" (1/2)[1+f ] \" respectively, in the present context.  Recognize that, in this guise, the grandparents \"A\" and \"B\" represent \"generation (t-2) \".  Thus, assuming that in any one generation all levels of inbreeding are the same, these two \"coefficients of parentage \" each represent (1/2) [1 + f ] .  Now, examine f .  Recall that this also is \"f \" or \"f \", and so represents \"their \" generation - f .  Putting it all together, f = (1/4) [ 2 f + 2 f ] = (1/4) [ 1 + f + 2 f ] .  That is the \"inbreeding coefficient \" for \"Full-Sib crossing\" .  The graph to the left shows the rate of this inbreeding over twenty repetitive generations.  The \"repetition\" means that the progeny after cycle t become the crossing parents that generate cycle (t+1 ), and so on successively.  The graphs also show the inbreeding for \"random fertilization 2N=20\" for comparison.  Recall that this inbreeding coefficient for progeny \"Y\" is also the \"co-ancestry coefficient\" for its parents, and so is a measure of the \"relatedness of the two Fill siblings\". Derivation of the \"half sib crossing\" takes a slightly different path to that for Full sibs.  In the adjacent diagram, the two half-sibs at generation (t-1) have only one parent in common—parent \"A\" at generation (t-2).  The \"cross-multiplier 1\" is used again, giving f = f = (1/4) [ f + f + f + f ] .  There is just one \"coefficient of parentage\" this time, but three \"co-ancestry coefficients\" at the (t-2) level (one of them—f—being a \"dummy\" and not representing an actual individual in the (t-1) generation).  As before, the \"coefficient of parentage\" is (1/2)[1+f ] , and the three \"co-ancestries\" each represent f .  Recalling that \" f \" represents \" f \", the final gathering and simplifying of terms gives f = f = (1/8) [ 1 + f + 6 f ] .  The graphs at left include this \"half-sib (HS) inbreeding\" over twenty successive generations.  As before, this also quantifies the \"relatedness\" of the two half-sibs at generation (t-1) in its alternative form of f . A pedigree diagram for selfing is on the right.  It is so straightforward it doesn't require any cross-multiplication rules.  It employs just the basic juxtaposition of the \"inbreeding coefficient\" and its alternative the \"co-ancestry coefficient\"; followed by recognizing that, in this case, the latter is also a \"coefficient of parentage\".  Thus, f = f = f = (1/2) [ 1 + f ] .  This is the fastest rate of inbreeding of all types, as can be seen in the graphs above.  The selfing curve is, in fact, a graph of the \"coefficient of parentage\". These are derived with methods similar to those for siblings.  As before, the \"co-ancestry\" viewpoint of the \"inbreeding coefficient\" provides a measure of \"relatedness\" between the parents P1 and P2 in these cousin expressions. The pedigree for \"First Cousins (FC)\" is given to the right.  The prime equation is f = f = f = (1/4) [ f + f + f + f ].  After substitution with corresponding inbreeding coefficients, gathering of terms and simplifying, this becomes f = (1/4) [ 3 f + (1/4) [2 f + f + 1 ]] , which is a version for iteration—useful for observing the general pattern, and for computer programming.  A \"final\" version is f = (1/16) [ 12 f + 2 f + f + 1 ] . The \"Second Cousins (SC)\" pedigree is on the left.  Parents in the pedigree not related to the \"common Ancestor\" are indicated by numerals instead of letters.  Here, the prime equation is f = f = f = (1/4) [ f + f + f + f ].  After working through the appropriate algebra, this becomes f = (1/4) [ 3 f + (1/4) [3 f + (1/4) [2 f + f + 1 ]]] , which is the iteration version.  A \"final\" version is f = (1/64) [ 48 f + 12 f + 2 f + f + 1 ] . To visualize the \"pattern in full cousin\" equations, start the series with the \"full sib\" equation re-written in iteration form: f = (1/4)[2 f + f + 1 ].  Notice that this is the \"essential plan\" of the last term in each of the cousin iterative forms: with the small difference that the generation indices increment by \"1\" at each cousin \"level\".  Now, define the \"cousin level\" as k = 1 (for First cousins), = 2 (for Second cousins), = 3 (for Third cousins), etc., etc.; and = 0 (for Full Sibs, which are \"zero level cousins\").  The \"last term\" can be written now as: (1/4) [ 2 f + f + 1] .  Stacked in front of this \"last term\" are one or more \"iteration increments\" in the form (1/4) [ 3 f + ... , where j is the \"iteration index\" and takes values from 1 ... k over the successive iterations as needed.  Putting all this together provides a general formula for all levels of \"full cousin\" possible, including \"Full Sibs\".  For kth \"level\" full cousins, f{k} = \"Ιter\" { (1/4) [ 3 f + } + (1/4) [ 2 f + f + 1] .  At the commencement of iteration, all f are set at \"0\", and each has its value substituted as it is calculated through the generations.  The graphs to the right show the successive inbreeding for several levels of Full Cousins. For \"first half-cousins (FHC)\", the pedigree is to the left.  Notice there is just one common ancestor (individual A).  Also, as for \"second cousins\", parents not related to the common ancestor are indicated by numerals.  Here, the prime equation is f = f = f = (1/4) [ f + f + f + f ].  After working through the appropriate algebra, this becomes f = (1/4) [ 3 f + (1/8) [6 f + f + 1 ]] , which is the iteration version.  A \"final\" version is f = (1/32) [ 24 f + 6 f + f + 1 ] .  The iteration algorithm is similar to that for \"full cousins\", except that the last term is (1/8) [ 6 f + f + 1 ] .  Notice that this last term is basically similar to the half sib equation, in parallel to the pattern for full cousins and full sibs.  In other words, half sibs are \"zero level\" half cousins. There is a tendency to regard cousin crossing with a human-oriented point of view, possibly because of a wide interest in Genealogy.  The use of pedigrees to derive the inbreeding perhaps reinforces this \"Family History\" view.  However, such kinds of inter-crossing occur also in natural populations—especially those that are sedentary, or have a \"breeding area\" that they re-visit from season to season.  The progeny-group of a harem with a dominant male, for example, may contain elements of sib-crossing, cousin crossing, and backcrossing, as well as genetic drift, especially of the \"island\" type.  In addition to that, the occasional \"outcross\" adds an element of hybridization to the mix.  It is \"not\" panmixia. Following the hybridizing between A and R, the F1 (individual B) is crossed back (BC1) to an original parent (R) to produce the BC1 generation (individual C).  [It is usual to use the same label for the act of \"making\" the back-cross \"and\" for the generation produced by it.  The act of back-crossing is here in \"italics\". ]  Parent R is the \"recurrent\" parent.  Two successive backcrosses are depicted, with individual D being the BC2 generation.  These generations have been given \"t\" indices also, as indicated.  As before, f = f = f = (1/2) [ f + f ] , using \"cross-multiplier 2\" previously given.  The \"f\" just defined is the one that involves generation \"(t-1)\" with \"(t-2)\".  However, there is another such \"f\" contained wholly \"within\" generation \"(t-2)\" as well, and it is \"this\" one that is used now: as the \"co-ancestry\" of the \"parents\" of individual C in generation \"(t-1)\".  As such, it is also the \"inbreeding coefficient\" of C, and hence is f.  The remaining f is the \"coefficient of parentage\" of the \"recurrent parent\", and so is \"(1/2) [1 + f ] \".  Putting all this together : f = (1/2) [ (1/2) [ 1 + f ] + f ] = (1/4) [ 1 + f + 2 f ] .  The graphs at right illustrate Backcross inbreeding over twenty backcrosses for three different levels of (fixed) inbreeding in the Recurrent parent. This routine is commonly used in Animal and Plant Breeding programmes.  Often after making the hybrid (especially if individuals are short-lived), the recurrent parent needs separate \"line breeding\" for its maintenance as a future recurrent parent in the backcrossing.  This maintenance may be through selfing, or through full-sib or half-sib crossing, or through restricted randomly fertilized populations, depending on the species' reproductive possibilities.  Of course, this incremental rise in f carries-over into the f of the backcrossing.  The result is a more gradual curve rising to the asymptotes than shown in the present graphs, because the \"f\" is not at a fixed level from the outset. In the section on \"Pedigree analysis\", formula_115 was used to represent probabilities of autozygous allele descent over n generations down branches of the pedigree.  This formula arose because of the rules imposed by sexual reproduction: (i) two parents contributing virtually equal shares of autosomal genes, and (ii) successive dilution for each generation between the zygote and the \"focus\" level of parentage.  These same rules apply also to any other viewpoint of descent in a two-sex reproductive system.  One such is the proportion of any ancestral gene-pool (also known as ‘germplasm’) which is contained within any zygote’s genotype. Therefore, the proportion of an ancestral genepool in a genotype is: formula_116 where n = number of sexual generations between the zygote and the focus ancestor. For example, each parent defines a genepool contributing formula_117 to its offspring; while each great-grandparent contributes formula_118 to its great-grand-offspring. The zygote's total genepool (Γ) is, of course, the sum of the sexual contributions to its descent. formula_119 Individuals descended from a common ancestral genepool obviously are related.  This is not to say they are identical in their genes (alleles), because, at each level of ancestor, segregation and assortment will have occurred in producing gametes.  But they will have originated from the same pool of alleles available for these meioses and subsequent fertilizations.  [This idea was encountered firstly in the sections on pedigree analysis and relationships.]  The genepool contributions [see section above] of their nearest common ancestral genepool(an \"ancestral node\") can therefore be used to define their relationship.  This leads to an intuitive definition of relationship which conforms well with familiar notions of \"relatedness\" found in family-history; and permits comparisons of the \"degree of relatedness\" for complex patterns of relations arising from such genealogy. The only modifications necessary (for each individual in turn) are in Γ and are due to the shift to \"shared common ancestry\" rather than \"individual total ancestry\".  For this, define Ρ (in lieu of Γ) ; m = number of ancestors-in-common at the node (i.e. m = 1 or 2 only) ; and an \"individual index\" k. Thus: formula_120 where, as before, \"n = number of sexual generations\" between the individual and the ancestral node. An example is provided by two first full-cousins.  Their nearest common ancestral node is their grandparents which gave rise to their two sibling parents, and they have both of these grandparents in common.  [See earlier pedigree.]  For this case, \"m=2\" and \"n=2\", so for each of them formula_121 In this simple case, each cousin has numerically the same Ρ . A second example might be between two full cousins, but one (\"k=1\") has three generations back to the ancestral node (n=3), and the other (\"k=2\") only two (n=2) [i.e. a second and first cousin relationship].  For both, m=2 (they are full cousins). formula_122 and formula_123 Notice each cousin has a different Ρ . In any pairwise relationship estimation, there is one Ρ for each individual: it remains to average them in order to combine them into a single \"Relationship coefficient\".  Because each \"Ρ\" is a fraction of a total genepool, the appropriate average for them is the \"geometric mean\" This average is their Genepool Relationship Coefficient—the \"GRC\". For the first example (two full first-cousins), their GRC = 0.5; for the second case (a full first and second cousin), their GRC = 0.3536. All of these relationships (GRC) are applications of path-analysis.  A summary of some levels of relationship (GRC) follow. These, in like manner to the Genotypic variances, can be derived through either the gene-model (\"Mather\") approach or the allele-substitution (\"Fisher\") approach.  Here, each method is demonstrated for alternate cases. These can be viewed either as the covariance between any offspring and \"any one\" of its parents (PO), or as the covariance between any offspring and the \" \"mid-parent\" \" value of both its parents (MPO). This can be derived as the \"sum of cross-products\" between parent gene-effects and \"one-half\" of the progeny expectations using the allele-substitution approach.  The \"one-half\" of the progeny expectation accounts for the fact that \"only one of the two parents\" is being considered.  The appropriate parental gene-effects are therefore the second-stage redefined gene effects used to define the genotypic variances earlier, that is: a″ = 2q(a − qd) and d″ = (q-p)a + 2pqd and also (-a)″ = -2p(a + pd) [see section \"Gene effects redefined\"].  Similarly, the appropriate progeny effects, for allele-substitution expectations are one-half of the earlier \"breeding values\", the latter being: a = 2qa, and a = (q-p)a and also a = -2pa [see section on \"Genotype substitution – Expectations and Deviations\"]. Because all of these effects are defined already as deviates from the genotypic mean, the cross-product sum using {genotype-frequency * parental gene-effect * half-breeding-value} immediately provides the \"allele-substitution-expectation covariance\" between any one parent and its offspring.  After careful gathering of terms and simplification, this becomes cov(PO) = pqa = ½ s . Unfortunately, the \"allele-substitution-deviations\" are usually overlooked, but they have not \"ceased to exist\" nonetheless!  Recall that these deviations are: d = -2q d, and d = 2pq d and also d = -2p d [see section on \"Genotype substitution – Expectations and Deviations\"].  Consequently, the cross-product sum using {genotype-frequency * parental gene-effect * half-substitution-deviations} also immediately provides the allele-substitution-deviations covariance between any one parent and its offspring.  Once more, after careful gathering of terms and simplification, this becomes cov(PO) = 2pqd = ½ s . It follows therefore that: cov(PO) = cov(PO) + cov(PO) = ½ s + ½ s , when dominance is \"not\" overlooked ! Because there are many combinations of parental genotypes, there are many different mid-parents and offspring means to consider, together with the varying frequencies of obtaining each parental pairing.  The gene-model approach is the most expedient in this case.  Therefore, an \"unadjusted sum of cross-products (USCP)\"—using all products { parent-pair-frequency * mid-parent-gene-effect * offspring-genotype-mean }—is adjusted by subtracting the {overall genotypic mean} as \"correction factor\" (CF).  After multiplying out all the various combinations, carefully gathering terms, simplifying, factoring and cancelling-out where applicable, this becomes: cov(MPO) = pq [a + (q-p)d ] = pq a = ½ s , with no dominance having been overlooked in this case, as it had been used-up in defining the a. The most obvious application is an experiment that contains all parents and their offspring, with or without reciprocal crosses, preferably replicated without bias, enabling estimation of all appropriate means, variances and covariances, together with their standard errors.  These estimated statistics can then be used to estimate the genetic variances.  Twice \"the difference between the estimates of the two forms of (corrected) parent-offspring covariance\" provides an estimate of s; and twice the \"cov(MPO)\" estimates s. With appropriate experimental design and analysis, standard errors can be obtained for these genetical statistics as well.  This is the basic core of an experiment known as \"Diallel analysis\", the Mather, Jinks and Hayman version of which is discussed in another section. A second application involves using \"regression analysis\", which estimates from statistics the ordinate (Y-estimate), derivative (regression coefficient) and constant (Y-intercept) of calculus.  The \"regression coefficient\" estimates the \"rate of change\" of the function predicting Y from X, based on minimizing the residuals between the fitted curve and the observed data (MINRES).  No alternative method of estimating such a function satisfies this basic requirement of MINRES.  In general, the regression coefficient is estimated as \"the ratio of the covariance(XY) to the variance of the determinator (X)\".  In practice, the sample size is usually the same for both X and Y, so this can be written as SCP(XY) / SS(X), where all terms have been defined previously.  In the present context, the parents are viewed as the \"determinative variable\" (X), and the offspring as the \"determined variable\" (Y), and the regression coefficient as the \"functional relationship\" (ß) between the two.  Taking cov(MPO) = ½ s as cov(XY), and s / 2 (the variance of the mean of two parents—the mid-parent) as s, it can be seen that ß = [½ s] / [½ s] = h .  Next, utilizing cov(PO) = [ ½ s + ½ s ] as cov(XY), and s as s, it is seen that 2 ß = [ 2 (½ s + ½ s )] / s = H . Analysis of \"epistasis\" has previously been attempted via an \"interaction variance\" approach of the type \" s \", and \" s\" and also \" s\".  This has been integrated with these present covariances in an effort to provide estimators for the epistasis variances.  However, the findings of epigenetics suggest that this may not be an appropriate way to define epistasis. Covariance between half-sibs (HS) is defined easily using allele-substitution methods; but, once again, the dominance contribution has historically been omitted.  However, as with the mid-parent/offspring covariance, the covariance between full-sibs (FS) requires a \"parent-combination\" approach, thereby necessitating the use of the gene-model corrected-cross-product method; and the dominance contribution has not historically been overlooked.  The superiority of the gene-model derivations is as evident here as it was for the Genotypic variances. The sum of the cross-products { common-parent frequency * half-breeding-value of one half-sib * half-breeding-value of any other half-sib in that same common-parent-group } immediately provides one of the required covariances, because the effects used [\"breeding values\"—representing the allele-substitution expectations] are already defined as deviates from the genotypic mean [see section on \"Allele substitution – Expectations and deviations\"].  After simplification.  this becomes: cov(HS) = ½ pq a = ¼ s .  However, the \"substitution deviations\" also exist, defining the sum of the cross-products { common-parent frequency * half-substitution-deviation of one half-sib * half-substitution-deviation of any other half-sib in that same common-parent-group }, which ultimately leads to: cov(HS) = p q d = ¼ s .  Adding the two components gives: cov(HS) = cov(HS) + cov(HS) = ¼ s + ¼ s . As explained in the introduction, a method similar to that used for mid-parent/progeny covariance is used.  Therefore, an \"unadjusted sum of cross-products\" (USCP) using all products—{ parent-pair-frequency * the square of the offspring-genotype-mean }—is adjusted by subtracting the {overall genotypic mean} as \"correction factor (CF)\".  In this case, multiplying out all combinations, carefully gathering terms, simplifying, factoring, and cancelling-out is very protracted.  It eventually becomes: cov(FS) = pq a + p q d = ½ s + ¼ s , with no dominance having been overlooked. The most useful application here for genetical statistics is the \"correlation between half-sibs\".  Recall that the correlation coefficient (\"r\") is the ratio of the covariance to the variance [see section on \"Associated attributes\" for example].  Therefore, r = cov(HS) / s = [¼ s + ¼ s ] / s = ¼ H .  The correlation between full-sibs is of little utility, being r = cov(FS) / s = [½ s + ¼ s ] / s .  The suggestion that it \"approximates\" (\"½ h\") is poor advice. Of course, the correlations between siblings are of intrinsic interest in their own right, quite apart from any utility they may have for estimating heritabilities or genotypic variances. It may be worth noting that [ cov(FS) − cov(HS)] = ¼ s .  Experiments consisting of FS and HS families could utilize this by using intra-class correlation to equate experiment variance components to these covariances [see section on \"Coefficient of relationship as an intra-class correlation\" for the rationale behind this]. The earlier comments regarding epistasis apply again here [see section on \"Applications (Parent-offspring\"]. Selection operates on the attribute (phenotype), such that individuals that equal or exceed a selection threshold (z) become effective parents for the next generation.  The \"proportion\" they represent of the base population is the \"selection pressure\".  The \"smaller\" the proportion, the \"stronger\" the pressure.  The \"mean of the selected group\" (P) is superior to the \"base-population mean\" (P) by the difference called the \"selection differential (S)\".  All these quantities are phenotypic.  To \"link\" to the underlying genes, a \"heritability\" (h) is used, fulfilling the role of a \"coefficient of determination\" in the biometrical sense.  The \"expected genetical change\"—still expressed in \"phenotypic units of measurement\"—is called the \"genetic advance (ΔG)\", and is obtained by the product of the \"selection differential (S)\" and its \"coefficient of determination\" (h).  The expected \"mean of the progeny\" (P) is found by adding the \"genetic advance (ΔG)\" to the \"base mean (P)\".  The graphs to the right show how the (initial) genetic advance is greater with stronger selection pressure (smaller \"probability\").  They also show how progress from successive cycles of selection (even at the same selection pressure) steadily declines, because the Phenotypic variance and the Heritability are being diminished by the selection itself.  This is discussed further shortly. Thus formula_124. and formula_125. The \"narrow-sense heritability (h)\" is usually used, thereby linking to the \"genic variance (σ) \".  However, if appropriate, use of the \"broad-sense heritability (H)\" would connect to the \"genotypic variance (σ)\" ; and even possibly an \"allelic heritability [ h = (σ) / (σ) ]\" might be contemplated, connecting to (σ ).  [See section on Heritability.] To apply these concepts \"before\" selection actually takes place, and so predict the outcome of alternatives (such as choice of \"selection threshold\", for example), these phenotypic statistics are re-considered against the properties of the Normal Distribution, especially those concerning truncation of the \"superior tail\" of the Distribution.  In such consideration, the \"standardized\" selection differential (i)″ and the \"standardized\" selection threshold (z)″ are used instead of the previous \"phenotypic\" versions.  The phenotypic standard deviate (σ) is also needed.  This is described in a subsequent section. Therefore, ΔG = (i σ) h, where \"(i σ)\" = \"S\" previously. The text above noted that successive ΔG declines because the \"input\" [the phenotypic variance ( σ )] is reduced by the previous selection.  The heritability also is reduced.  The graphs to the left show these declines over ten cycles of repeated selection during which the same selection pressure is asserted.  The accumulated genetic advance (ΣΔG) has virtually reached its asymptote by generation 6 in this example.  This reduction depends partly upon truncation properties of the Normal Distribution, and partly upon the heritability together with \"meiosis determination ( b )\".  The last two items quantify the extent to which the \"truncation\" is \"offset\" by new variation arising from segregation and assortment during meiosis.  This is discussed soon, but here note the simplified result for \"undispersed random fertilization (f = 0)\". Thus : σ = σ [1 − i ( i-z) ½ h], where i ( i-z) = K = truncation coefficient and ½ h = R = reproduction coefficient This can be written also as σ = σ [1 − K R ], which facilitates more detailed analysis of selection problems. Here, i and z have already been defined, ½ is the \"meiosis determination (b) for f=0\", and the remaining symbol is the heritability.  These are discussed further in following sections.  Also notice that, more generally, R = b h.  If the general meiosis determination ( b ) is used, the results of prior inbreeding can be incorporated into the selection.  The phenotypic variance equation then becomes: σ = σ [1 − i ( i-z) b h]. The \"Phenotypic variance\" truncated by the \"selected group\" ( σ ) is simply σ [1 − K], and its contained \"genic variance\" is (h σ ).  Assuming that selection has not altered the \"environmental\" variance, the \"genic variance\" for the progeny can be approximated by σ = ( σ − σ) .  From this, h = ( σ / σ ).  Similar estimates could be made for σ and H , or for σ and h if required. The following rearrangement is useful for considering selection on multiple attributes (characters).  It starts by expanding the heritability into its variance components.  ΔG = i σ ( σ / σ ) .  The \"σ\" and \"σ\" partially cancel, leaving a solo \"σ\".  Next, the \"σ\" inside the heritability can be expanded as (\"σ × σ\"), which leads to : ΔG = i σ ( σ / σ ) = i σ h . Corresponding re-arrangements could be made using the alternative heritabilities, giving ΔG = i σ H or ΔG = i σ h. This traditional view of adaptation in quantitative genetics provides a model for how the selected phenotype changes over time, as a function of the selection differential and heritability.  However it does not provide insight into (nor does it depend upon) any of the genetic details - in particular, the number of loci involved, their allele frequencies and effect sizes, and the frequency changes driven by selection.  This, in contrast, is the focus of work on polygenic adaptation within the field of population genetics.  Recent studies have shown that traits such as height have evolved in humans during the past few thousands of years as a result of small allele frequency shifts at thousands of variants that affect height. The entire \"base population\" is outlined by the normal curve to the right.  Along the Z axis is every value of the attribute from least to greatest, and the height from this axis to the curve itself is the frequency of the value at the axis below.  The equation for finding these frequencies for the \"normal\" curve (the curve of \"common experience\") is given in the ellipse.  Notice it includes the mean (µ) and the variance (σ).  Moving infinitesimally along the z-axis, the frequencies of neighbouring values can be \"stacked\" beside the previous, thereby accumulating an area that represents the probability of obtaining all values within the stack.  [That's integration from calculus.]  Selection focuses on such a probability area, being the shaded-in one from the \"selection threshold (z)\" to the end of the superior tail of the curve.  This is the \"selection pressure\".  The selected group (the effective parents of the next generation) include all phenotype values from z to the \"end\" of the tail.  The mean of the selected group is µ, and the difference between it and the base mean (µ) represents the selection differential (S).  By taking partial integrations over curve-sections of interest, and some rearranging of the algebra, it can be shown that the \"selection differential\" is S = [ y (σ / Prob.)]  , where y is the \"frequency\" of the value at the \"selection threshold\" z (the \"ordinate\" of \"z\").  Rearranging this relationship gives S / σ = y / Prob., the left-hand side of which is, in fact, \"selection differential divided by standard deviation\"—that is the \"standardized selection differential (i)\".  The right-side of the relationship provides an \"estimator\" for i—the ordinate of the \"selection threshold\" divided by the \"selection pressure\".  Tables of the Normal Distribution can be used, but tabulations of i itself are available also.  The latter reference also gives values of i adjusted for small populations (400 and less), where \"quasi-infinity\" cannot be assumed (but \"was\" presumed in the \"Normal Distribution\" outline above).  The \"standardized selection differential (i)\" is known also as the intensity of selection. Finally, a cross-link with the differing terminology in the previous sub-section may be useful: µ (here) = \"P\" (there), µ = \"P\" and σ = \"σ\". The meiosis determination (b) is the \"coefficient of determination\" of meiosis, which is the cell-division whereby parents generate gametes.  Following the principles of \"standardized partial regression\", of which path analysis is a pictorially oriented version, Sewall Wright analyzed the paths of gene-flow during sexual reproduction, and established the \"strengths of contribution\" (\"coefficients of determination\") of various components to the overall result.  Path analysis includes \"partial correlations\" as well as \"partial regression coefficients\" (the latter are the \"path coefficients\").  Lines with a single arrow-head are directional \"determinative paths\", and lines with double arrow-heads are \"correlation connections\".  Tracing various routes according to \"path analysis rules\" emulates the algebra of standardized partial regression. The path diagram to the left represents this analysis of sexual reproduction.  Of its interesting elements, the important one in the selection context is \"meiosis\".  That's where segregation and assortment occur—the processes that partially ameliorate the truncation of the phenotypic variance that arises from selection.  The path coefficients b are the meiosis paths.  Those labeled a are the fertilization paths.  The correlation between gametes from the same parent (g) is the \"meiotic correlation\".  That between parents within the same generation is r.  That between gametes from different parents (f) became known subsequently as the \"inbreeding coefficient\".  The primes ( ' ) indicate generation (t-1), and the \"un\"primed indicate generation t. Here, some important results of the present analysis are given.  Sewall Wright interpreted many in terms of inbreeding coefficients. The meiosis determination (b) is \"½ (1+g)\" and equals ½ (1 + f) , implying that g = f. With non-dispersed random fertilization, f) = 0, giving b = ½, as used in the selection section above.  However, being aware of its background, other fertilization patterns can be used as required.  Another determination also involves inbreeding—the fertilization determination (a) equals 1 / [ 2 ( 1 + f ) ] .  Also another correlation is an inbreeding indicator—r = 2 f / ( 1 + f ), also known as the \"coefficient of relationship\".  [Do not confuse this with the \"coefficient of kinship\"—an alternative name for the \"co-ancestry coefficient\".  See introduction to \"Relationship\" section.]  This r re-occurs in the sub-section on dispersion and selection. These links with inbreeding reveal interesting facets about sexual reproduction that are not immediately apparent.  The graphs to the right plot the \"meiosis\" and \"syngamy (fertilization)\" coefficients of determination against the inbreeding coefficient.  There it is revealed that as inbreeding increases, meiosis becomes more important (the coefficient increases), while syngamy becomes less important.  The overall role of reproduction [the product of the previous two coefficients—r] remains the same.  This \"increase in b\" is particularly relevant for selection because it means that the \"selection truncation of the Phenotypic variance\" is offset to a lesser extent during a sequence of selections when accompanied by inbreeding (which is frequently the case). The previous sections treated \"dispersion\" as an \"assistant\" to \"selection\", and it became apparent that the two work well together.  In quantitative genetics, selection is usually examined in this \"biometrical\" fashion, but the changes in the means (as monitored by ΔG) reflect the changes in allele and genotype frequencies beneath this surface.  Referral to the section on \"Genetic drift\" brings to mind that it also effects changes in allele and genotype frequencies, and associated means; and that this is the companion aspect to the dispersion considered here (\"the other side of the same coin\").  However, these two forces of frequency change are seldom in concert, and may often act contrary to each other.  One (selection) is \"directional\" being driven by selection pressure acting on the phenotype: the other (genetic drift) is driven by \"chance\" at fertilization (binomial probabilities of gamete samples).  If the two tend towards the same allele frequency, their \"coincidence\" is the probability of obtaining that frequencies sample in the genetic drift: the likelihood of their being \"in conflict\", however, is the \"sum of probabilities of all the alternative frequency samples\".  In extreme cases, a single syngamy sampling can undo what selection has achieved, and the probabilities of it happening are available.  It is important to keep this in mind.  However, genetic drift resulting in sample frequencies similar to those of the selection target does not lead to so drastic an outcome—instead slowing progress towards selection goals. Upon jointly observing two (or more) attributes (\"eg\" height and mass), it may be noticed that they vary together as genes or environments alter.  This co-variation is measured by the covariance, which can be represented by \" cov \" or by θ.  It will be positive if they vary together in the same direction; or negative if they vary together but in opposite direction.  If the two attributes vary independently of each other, the covariance will be zero.  The degree of association between the attributes is quantified by the correlation coefficient (symbol r or ρ ).  In general, the correlation coefficient is the ratio of the \"covariance\" to the geometric mean of the two variances of the attributes.  Observations usually occur at the phenotype, but in research they may also occur at the \"effective haplotype\" (effective gene product) [see Figure to the right].  Covariance and correlation could therefore be \"phenotypic\" or \"molecular\", or any other designation which an analysis model permits.  The phenotypic covariance is the \"outermost\" layer, and corresponds to the \"usual\" covariance in Biometrics/Statistics.  However, it can be partitioned by any appropriate research model in the same way as was the phenotypic variance.  For every partition of the covariance, there is a corresponding partition of the correlation.  Some of these partitions are given below.  The first subscript (G, A, etc.) indicates the partition.  The second-level subscripts (X, Y) are \"place-keepers\" for any two attributes. The first example is the \"un-partitioned\" phenotype. The genetical partitions (a) \"genotypic\" (overall genotype),(b) \"genic\" (substitution expectations) and (c) \"allelic\" (homozygote) follow. (a) formula_127 (b) formula_128 (c) formula_129 With an appropriately designed experiment, a \"non-genetical\" (environment) partition could be obtained also. The metabolic pathways from gene to phenotype are complex and varied, but the causes of correlation amongst attributes lie within them.  An outline is shown in the Figure to the right.\n\nCalifornia Trail The California Trail was an emigrant trail of about 3000 mi across the western half of the North American continent from Missouri River towns to what is now the state of California.  After it was established, the first half of the California Trail followed the same corridor of networked river valley trails as the Oregon Trail and the Mormon Trail, namely the valleys of the Platte, North Platte and Sweetwater rivers to Wyoming.  In the present states of Wyoming, Idaho, and Utah, the California and Oregon trails split into several different trails or cutoffs. By 1847, two former fur trading frontier forts marked trailheads for major alternative routes through Utah and Wyoming to Northern California.  The first was Jim Bridger's Fort Bridger (est. 1842) in present-day Wyoming on the Green River, where the Mormon Trail turned southwest over the Wasatch Mountains to the newly established Salt Lake City, Utah.  From Salt Lake the Salt Lake Cutoff (est. 1848) went north and west of the Great Salt Lake and rejoined the California Trail in the City of Rocks in present-day Idaho. The main Oregon and California Trails crossed the Green River on several different ferries and trails (cutoffs) that led to or bypassed Fort Bridger and then crossed over a range of hills to the Great Basin drainage of the Bear River (Great Salt Lake).  Just past present-day Soda Springs, Idaho, both trails initially turned northwest, following the Portneuf River (Idaho) valley to the British Hudson's Bay Company's Fort Hall (est. 1836) on the Snake River in present-day Idaho.  From Fort Hall the Oregon and California trails went about 50 mi southwest along the Snake River Valley to another \"parting of the ways\" trail junction at the junction of the Raft and Snake rivers.  The California Trail from the junction followed the Raft River to the City of Rocks in Idaho near the present Nevada-Idaho-Utah tripoint.  The Salt Lake and Fort Hall routes were about the same length: about 190 mi . From the City of Rocks the trail went into the present state of Utah following the South Fork of the Junction Creek.  From there the trail followed along a series of small streams, such as Thousand Springs Creek in the present state of Nevada until approaching present-day Wells, Nevada, where they met the Humboldt River.  By following the crooked, meandering Humboldt River Valley west across the arid Great Basin, emigrants were able to get the water, grass, and wood they needed for themselves and their teams.  The water turned increasingly alkaline as they progressed down the Humboldt, and there were almost no trees.  \"Firewood\" usually consisted of broken brush, and the grass was sparse and dried out.  Few travelers liked the Humboldt River Valley passage. [The] Humboldt is not good for man nor beast...and there is not timber enough in three hundred miles of its desolate valley to make a snuff-box, or sufficient vegetation along its banks to shade a rabbit, while its waters contain the alkali to make soap for a nation. At the end of the Humboldt River, where it disappeared into the alkaline Humboldt Sink, travelers had to cross the deadly Forty Mile Desert before finding either the Truckee River or Carson River in the Carson Range and Sierra Nevada mountains that were the last major obstacles before entering Northern California. An alternative route across the present states of Utah and Nevada that bypassed both Fort Hall and the Humboldt River trails was developed in 1859.  This route, the Central Overland Route, which was about 280 mi shorter and more than 10 days quicker, went south of the Great Salt Lake and across the middle of present-day Utah and Nevada through a series of springs and small streams.  The route went south from Salt Lake City across the Jordan River to Fairfield, Utah, then west-southwest past Fish Springs National Wildlife Refuge, Callao, Utah, Ibapah, Utah, to Ely, Nevada, then across Nevada to Carson City, Nevada.  (Today's U.S. Route 50 in Nevada roughly follows this route.)  (See: Pony Express Map) In addition to immigrants and migrants from the East, after 1859 the Pony Express, Overland stages and the First Transcontinental Telegraph (1861) all followed this route with minor deviations. Once in western Nevada and eastern California, the pioneers worked out several paths over the rugged Carson Range and Sierra Nevada mountains into the gold fields, settlements and cities of northern California.  The main routes initially (1846–48) were the Truckee Trail to the Sacramento Valley and after about 1849 the Carson Trail route to the American River and the Placerville, California gold digging region. Starting about 1859 the Johnson Cutoff (Placerville Route, est. 1850–51) and the Henness Pass Route (est. 1853) across the Sierras were greatly improved and developed.  These main roads across the Sierras were both toll roads so there were funds to pay for maintenance and upkeep on the roads.  These toll roads were also used to carry cargo west to east from California to Nevada, as thousands of tons of supplies were needed by the gold and silver miners, etc. working on the Comstock Lode (1859–88) near the present Virginia City, Nevada.  The Johnson Cutoff, from Placerville to Carson City along today's U.S. Route 50 in California, was used by the Pony Express (1860–61) year-round and in the summer by the stage lines (1860–69).  It was the only overland route from the East to California that could be kept partially open for at least horse traffic in the winter. The California Trail was heavily used from 1845 until several years after the end of the American Civil War; in 1869 several rugged wagon routes were established across the Carson Range and Sierra Nevada mountains to different parts of northern California.  After about 1848 the most popular route was the Carson Route which, while rugged, was still easier than most others and entered California in the middle of the gold fields.  The trail was heavily used in the summers until the completion of the First Transcontinental Railroad in 1869 by the Union Pacific and Central Pacific Railroads. Trail traffic rapidly fell off as the cross-country trip was much quicker and easier by train—about seven days.  The economy class fare across the western United States of about $69 was affordable by most California-bound travelers. The trail was used by about 2,700 settlers from 1846 up to 1849.  These settlers were instrumental in helping convert California to a U.S. possession.  Volunteer members of John C. Fremont's California Battalion assisted the Pacific Squadron's sailors and marines in 1846 and 1847 in conquering California in the Mexican–American War.  After the discovery of gold in January 1848, word spread about the California Gold Rush.  Starting in late 1848 till 1869, more than 250,000 businessmen, farmers, pioneers and miners passed over the California Trail to California.  The traffic was so heavy that in two years the new settlers added so many people to California that by 1850 it qualified for admission as the 31st state with 120,000 residents.  The Trail travelers were added to those migrants going by wagon from Salt Lake City to Los Angeles, California in winter, the travelers down the Gila River trail in Arizona, and those traveling by sea routes around Cape Horn and the Magellan Strait, or by sea and then across the Isthmus of Panama, Nicaragua or Mexico, and then by sea to California.  Roughly half of California's new settlers came by trail and the other half by sea. The original route had many branches and cutoffs, encompassing about 5500 mi in total.  About 1000 mi of the rutted traces of these trails remain in Kansas, Nebraska, Wyoming, Idaho, Utah, Nevada and California as historical evidence of the great mass migration westward.  Portions of the trail are now preserved by the Bureau of Land Management (BLM), and the National Park Service (NPS) as the California National Historic Trail and marked by BLM, NPS and the many state organizations of the Oregon-California Trails Association (OCTA).  Maps put out by the United States Geological Survey (USGS) show the network of rivers followed to get to California. The beginnings of the California and Oregon Trails were laid out by mountain men and fur traders from about 1811 to 1840 and were only passable initially on foot or by horseback.  South Pass, the easiest pass over the U.S. continental divide of the Pacific Ocean and Atlantic Ocean drainages, was discovered by Robert Stuart and his party of seven in 1812 while he was taking a message from the west to the east back to John Jacob Astor about the need for a new ship to supply Fort Astoria on the Columbia River—their supply ship \"Tonquin\" had blown up.  In 1824, fur traders/trappers Jedediah Smith and Thomas Fitzpatrick rediscovered the South Pass as well as the Sweetwater, North Platte and Platte River valleys connecting to the Missouri River. British fur traders primarily used the Columbia River and Snake Rivers to take their supplies to their trading posts.  After 1824 U.S. fur traders had discovered and developed first pack and then wagon trails along the Platte, North Platte, Sweetwater and Big Sandy River (Wyoming) to the Green River (Colorado River) where they often held their annual Rocky Mountain Rendezvous (1827–40) held by a fur trading company at which U.S. trappers, mountain men and Indians sold and traded their furs and hides and replenished their supplies they had used up in the previous year.  A rendezvous typically only lasted a few weeks and was known to be a lively, joyous place, where nearly all were allowed—free trappers, Native Americans, native trapper wives and children, travelers and later on, even tourists who would venture from even as far as Europe to observe the games and festivities.  Trapper Jim Beckwourth describes: \"\"Mirth, songs, dancing, shouting, trading, running, jumping, singing, racing, target-shooting, yarns, frolic, with all sorts of drinking and gambling extravagances that white men or Indians could invent.\"\"  Initially from about 1825 to 1834 the fur traders used pack trains to carry their supplies in and the traded furs out. Parts of the California Trail route were partially discovered and developed by American fur traders like, Kit Carson, Joseph R. Walker, and Jedediah Smith who often worked with the Rocky Mountain Fur Company and after 1834 by the American Fur Company and explored widely in the west.  British Hudson's Bay Company trappers led by Peter Skene Ogden and others scouted the Humboldt River off and on from about 1830 to 1840—little of their explorations was known.  A few U.S. and British fur trappers and traders had explored what is now called the Humboldt River (named Mary's River by Ogden) that crosses most of the present state of Nevada and provides a natural corridor to western Nevada and eastern California.  The Humboldt River was of little interest to the trappers as it was hard to get to, dead ended in an alkali sink and had few beavers.  The details of the Humboldt River and how to get to it was known to only a few trappers.  When trapping largely ceased in the 1840s due to a change in men's hat style that didn't use the felt from beaver's fur there was a number of out of work fur trappers/traders who were familiar with many of the Indians, trails and rivers in the west. In 1832 Captain Benjamin Bonneville, a United States Military Academy graduate on temporary leave, followed the fur traders paths along the valleys of the Platte, North Platte and Sweetwater Rivers to South Pass (Wyoming) with a fur traders's caravan of 110 men and 20 wagons over and on to the Green River—the first wagons over South Pass.  In the spring of 1833, Captain Benjamin Bonneville sent a party of men under former fur trapper and \"now\" explorer Joseph R. Walker to explore the Great Salt Lake desert and Big Basin and attempt to find an overland route to California.  Eventually the party re-discovered the Humboldt River crossing much of present-day Nevada.  After crossing the hot and dry Forty Mile Desert they passed through the Carson River Canyon across the Carson Range and ascended the Sierra Nevada (U.S.).  They descended from the Sierras via the Stanislaus River drainage to the Central Valley (California) and proceeded on west as far as Monterey, California—the Californio capital.  His return route from California went across the southern Sierra mountains via what's named now Walker Pass—named by U.S. Army topographic engineer, explorer, adventurer and map maker John Charles Fremont. The Humboldt River Valley was key to forming a usable California Trail.  The Humboldt River with its water and grass needed by the livestock (oxen, mules horses and later cattle) and emigrants provided a key link west to northern California.  One of several \"parting of the ways\" that split the Oregon Trail and California trails was eventually established at the Snake River and Raft River junctions in what is now Idaho.  The Raft River, Junction Creek in the future states of Idaho and Utah and Thousand Springs Creek in the future states of Nevada and Utah provided the usable trail link between the Snake and Humboldt rivers. After about 1832 a rough wagon trail had been blazed to the Green River—the chief tributary of the Colorado River.  After 1832 the fur traders often brought wagon loads of supplies to trade with the white and Native American fur trappers at their annual rendezvous usually somewhere on the Green River.  They returned to the Missouri River towns by following their rough trail in reverse.  The future Oregon/California wagon trail had minimal improvements usually limited to partially filling in impassable gullys, etc..  By 1836, when the first Oregon migrant wagon train was organized in Independence, Missouri, a wagon trail had been scouted and roughed out to Fort Hall, Idaho.  In July 1836, Missionary wives Narcissa Whitman and Eliza Spalding were the first white pioneer women to cross South Pass on their way to Oregon Territory via Fort Hall.  They left their wagons at Fort Hall and went the rest of the way by pack train and boats down the Columbia River as recommended by the Hudson's Bay Company trappers at Fort Hall. The first recorded party to use part of the California Trail to get to California was the Bartleson-Bidwell Party in 1841.  They left Missouri with 69 people and reasonably easily reached the future site of Soda Springs, Idaho on the Bear River (Great Salt Lake) by following experienced trapper Thomas \"Broken-hand\" Fitzpatrick on his way to Fort Hall.  Near Soda Springs the Bear River swung southwest towards the Great Salt Lake and the regular Oregon Trail headed northwest out of the Big Basin drainage and into the Portneuf River (Idaho) drainage to Fort Hall on the Snake River.  About half of the party elected to attempt to continue by wagon to California and half elected to go to Oregon on the more established Oregon Trail.  The California-bound travelers (including one woman and one child), knew only that California was west of them and there was reportedly a river across most of the 'Big Basin' that led part of the way to California.  Without guides or maps, they traveled down the Bear River as it looped southwest through Cache Valley, Utah.  When they found the Bear River terminating in the Great Salt Lake, they traveled west across the Big Basin through the rough and sparse semi-desert north of the Great Salt Lake. After crossing most of what would become the state of Utah and passing into the future state of Nevada, they missed the head of the Humboldt River and abandoned their wagons in Nevada at Big Spring at the foot of the Pequop Mountains.  They continued west using their oxen and mules as pack animals eventually finding the Humboldt River and followed it west to its termination in an alkali sink near present-day Lovelock, Nevada.  Crossing the difficult Forty Mile Desert they turned to the south on the east side of the Sierras until they reached the Walker River draining east out of the Sierra Nevada (U.S.) mountains.  They followed the Walker westward as they ascended over the rugged Sierra Nevada mountains roughly in the same region crossed by Jedediah Smith in 1828.  They finished their rugged trip over the Sierras and into the future state of California by killing and eating many of their oxen for food.  All California emigrants survived the journey.  Their rough and rugged route across the future states of Idaho, Utah, Nevada and across the California Sierras was subsequently followed by almost nobody. Joseph B. Chiles, a member of the Bartleson-Bidwell Party, returned east in 1842 and organized the first of his seven California-bound immigrant companies in 1843.  Following the Oregon Trail to Fort Bridger, the Chiles company enlisted mountain man Joseph R. Walker as a guide.  Chiles and Walker split the company into two groups.  Walker led the company with the wagons west toward California by following the Oregon Trail to Fort Hall, Idaho and turning west off the Oregon trail at the Snake River, Raft River junction.  At the head of the Raft River they crossed a divide into the Big Basin drainage and followed a series of streams like Thousand Springs Creek in what is now Nevada to the Humboldt River valley near today's Wells, Nevada.  They blazed a wagon trail down the Humboldt River Valley and across Forty Mile Desert until they hit the Carson River.  Here instead of immediately attempting to cross the Sierras by following the Carson River as it came out of the mountains they turned south, traveling east of the Sierras along what is now roughly the Nevada and California border—about where U.S. Route 395 in California is today.  With scarce provisions, winter approaching and failing draft animals, by the end of 1843 they had traveled almost 300 mi east of the Sierras before they abandoned their wagons near Owens Lake in eastern central California and proceeded by pack train to make a December crossing of the Sierra Nevada (U.S.) mountains over Walker Pass (35°39′47″N 118°1′37″W on California State Route 178) in the southeast Sierras.  An arduous route used by almost no one else. Trying to find a different route, Chiles led the rest of the settlers in a pack train party down the Oregon Trail to where it intersected the Malheur River in eastern Oregon which he then followed across Oregon to California—again a slow arduous path unused by nearly all subsequent travelers. Another mixed party on horse back of U.S. Army topographers, hunters, scouts, etc. of about 50 men in 1843–1844 led by U.S. Army Colonel John C. Frémont of the U.S. Corp of Topographical Engineers and his chief scout Kit Carson took their exploration company down the Humboldt River, crossing Forty Mile Desert and then following what is now called the Carson River across the Carson Range that is east of what is now called Lake Tahoe—seen but not explored by Fremont from a peak near what is now called Carson Pass.  They made a winter crossing of the Carson Range and Sierra Nevadas in February 1843.  From Carson pass they followed the northern Sierra's southern slopes, to minimize snow depth, of what is now called the American River valley down to Sutter's Fort located near what is now Sacramento, California.  Fremont took the data gathered by his topographers and map makers in his 1843–44 and 1846–47 explorations of much of the American west to create and publish (by order of Congress) the first \"decent\" map of California and Oregon in 1848. The first group to cross the Sierras with their wagons was the Stephens-Townsend-Murphy Party of 1844.  They departed from the Oregon Trail along the Snake River by following the Raft River to the City of Rocks in Idaho and then passed over the Big Basin continental divide and used a series of springs and small streams in what is now Nevada to get to the future Humboldt River town of Wells, Nevada.  They followed the Humboldt River across Nevada and the future Truckee Trail Route across the rugged Forty Mile Desert and along the Truckee River to the foot of the Sierras near what is now Donner Lake.  They got over the Sierras at Donner Pass by unloading the wagons and packing the contents to the top using their ox teams as pack animals.  The wagons were then partially dis-assembled and then pulled by multiple teams of oxen up the steep slopes and cliffs.  Some wagons were left at Donner Lake.  Once on top, the remaining wagons were reassembled and reloaded for their trip to Sutter's Fort (Sacramento, California).  They were caught by early winter snows and abandoned their wagons near Emigrant Gap and had to hike out of the Sierras after being rescued by a party from Sutter's Fort on February 24, 1845.  Their abandoned wagons were retrieved in the spring of 1845 and pulled the rest of the way to Sutter's Fort.  California then had only a very limited rudimentary Mission Indian industry and solid wheeled ox-carts—no wagons.  A usable but very rough wagon route had finally been worked out along the Humboldt River and the rugged, hot and dry Forty Mile Desert across Nevada and over the rugged and steep Sierra Nevada (U.S.) by California-bound settlers.  In the following years, several other rugged routes over the Sierras were developed. Pioneered by Lansford Hastings in 1846, the Hastings Cutoff left the California Trail at Fort Bridger in Wyoming.  In 1846 the party guided by Lansford Hastings passed successfully through the rugged, narrow, rock-filled Weber River canyon to get over the Wasatch Mountains.  In a few places the wagons had to be floated down the river in some narrow spots and the wagons had to be pried over large rocks in many places.  Passing the future site of Ogden, Utah and Salt Lake City, Utah Hastings party proceeded south of the Great Salt Lake and then across about 80 miles of water less Bonneville Salt Flats and around the Ruby Mountains in Nevada before getting to the Humboldt River Valley California trail.  The severely water-challenged Hastings Cutoff trail across the Great Salt Lake's salt flats rejoined the California Trail about 7 mi west of modern-day Elko, Nevada.  The party led by Hastings were just two weeks ahead of the Donner Party but did successfully get to California before snow closed the passes and stranded the Donner Party in the Sierras. As recommended by a message from Hastings, after he got through Weber canyon, another branch of the Hastings trail was cut across the Wasatch Mountains by the Donner Party.  Their rough trail required clearing a very rough wagon trail through thick brush down Emigration Canyon to get into the Salt Lake Valley.  To avoid cutting too much brush in some places they used multiple ox teams to pull wagons up steep slopes to get around brush loaded canyon sections.  Cutting this rough trail slowed the Donner Party down by about two weeks—Hastings successfully navigated the rugged Weber Canyon in about four days.  The Mormon Trail over the Wasatch Mountains followed roughly the same path as the Donner Party trail of 1846 but they built a much better trail with many more workers in 1847 to get to the Salt Lake valley with much less hassle—this was their main route to and from their Salt Lake communities.  The Weber Canyon trail was judged too rugged for regular use without a lot of work—later done by Mormon workers on the First Transcontinental Railroad in 1868–69.  All of the Hastings Cutoffs to California were found to be very hard on the wagons, livestock and travelers as well as being longer, harder, and slower to traverse than the regular trail and was largely abandoned after 1846.  It was discovered by some hurrying travelers in 1849 (before the experience of the 1846 travelers was widely known) that during a wet year, wagons could not be pulled across the Great Salt Lake Desert; it was too soft. In 1848, the Salt Lake Cutoff was discovered by returning Mormon Battalion soldiers and others from the City of Rocks (in the future state of Idaho) to the northwest of the Great Salt Lake and on to Salt Lake City.  This cutoff allowed travelers to use the Mormon Trail from Fort Bridger over the Wasatch Mountains to Salt Lake City, Utah and back to the California Trail.  In Salt Lake they could get repairs and fresh supplies and livestock by trade or cash.  The Mormons were trying to establish new Mormon communities in Utah and needed almost everything then.  The trail from Fort Bridger to Salt Lake City and over the Salt Lake Cutoff was about 180 mi before it rejoined the California Trail near the City of Rocks in Idaho.  This cutoff had adequate water and grass, and many thousands of travelers used this cutoff for years.  The \"regular\" California Trail from Fort Bridger via Fort Hall on the Snake River and on to the City of Rocks was within a few miles of being the same distance as going to Salt Lake City and on to the City of Rocks via the Salt Lake Cutoff. In April 1859, an expedition of U.S. Corp of Topographical Engineers led by U.S. Army Captain James H. Simpson left U.S. Army's Camp Floyd (Utah) (now Fairfield, Utah) in central Utah to establish an army western supply route across the Great Basin to California.  Upon his return in early August 1859, Simpson reported that he had surveyed what became the Central Overland Route from Camp Floyd to Genoa, Nevada.  This route went through central Nevada roughly where U.S. Route 50 goes today from Carson City, Nevada to Ely, Nevada.  From Ely the route is approximated today by the roads to Ibapah, Utah, Callao, Utah, Fish Springs National Wildlife Refuge, Fairfield, Utah to Salt Lake City, Utah (See: Pony Express Map and Pony Express auto route) The Central Overland Route was about 280 mi shorter than the 'standard' California Trail Humboldt River route.  This Central Overland Route, with minor modifications was used by settler’s wagon trains, the Pony Express, stagecoach lines and the First Transcontinental Telegraph after 1859. Several accounts of travel along the Central Overland Route have been published.  In July 1859 Horace Greeley made the trip, at a time when Chorpenning was using only the eastern segment (they reconnected with the main California Trail near present-day Beowawe, Nevada).  Greeley published his detailed observations in his 1860 book \"\"An Overland Journey from New York to San Francisco\"\".  In October 1860 the English explorer Richard Burton traveled the entire route at a time when the Pony Express was operating.  He gave detailed descriptions of each of the way stations in his 1861 book \"\"The City of the Saints, Across the Rocky Mountains to California\"\".  In the summer of 1861 Samuel Clemens (Mark Twain) traveled the route with his brother Orion on their way to Nevada's new territorial capital in Carson City, Nevada, but provided only sparse descriptions of the road in his 1872 book \"\"Roughing It\"\". Books, pamphlets and guides were available for trail information after about 1846.  After 1848, information about the trip to California and Oregon and what was needed for the trip was often available in the local newspapers as, after 1849, mail and news (heavily subsidized) etc. got back to U.S. (via Panama) in about 40 days.  By 1848, the newspapers of the day often published articles about California.  After deciding to go, the first thing many did was sell their farm (business, etc.) and start putting together an outfit.  The 1850 U.S. Census of California shows that more than 95% of the people going to California in 1849 were male. The first decision to make was what route to take to California—the California Trail or the various sea routes to California.  Initially about half of the Argonauts going to California went by sea and half overland by trail.  Most of those going by sea, which was quicker but more costly, lived on or near the East Coast of the United States and were familiar with ships and shipping.  Most of those going overland already lived in the mid-west or near the Ohio, Mississippi or Missouri Rivers. Nearly all reached their jumping off place by using a steamboat to get there with their animals and supplies.  Of the about 20% of the Argonauts who returned from California they usually returned by sea across the Isthmus of Panama particularly after 1855 when the paddle steamer shipping lines and the Panama Railroad across Panama cut the return trip to about 40 days versus about 140 days by wagon. About 50–70% of the Argonauts who went by the California Trail were farmers, and many already had many of the supplies, wagons, animals etc. needed.  A pioneer’s typical outfit, for three to six people, usually consisted one or two small, sturdy farm wagons outfitted with bows and a canvas cover (new cost about $75 to $175 each), six to ten head of oxen ($75 to $300) and chains and yokes or harnesses to attach them to the wagons.  For traveling about 2000 mi over rough terrain the wagons used were typically as small and as light as would do the job, approximately half the size of the larger Conestoga wagons used for freight.  The typical California Trail wagon weighed about 1300 lb empty with about 2500 lb of capacity (starting with less than 2000 lb recommended) and about 88 ft3 of storage space in an 11 ft -long, 4 ft -wide, by 2 ft -high box.  These wagons could be easily pulled by 4 to 6 oxen or 4 to 6 mules or horses.  More animals than initially needed were usually recommended since some could (and usually did) stray off, die or be stolen during the trip.  In addition to providing transport, shelter and protection against bad weather during the trip at the end of the trip many wagons were parked and became a temporary home until a more permanent cabin or shelter could be built.  The average number of occupants of a typical wagon was about three pioneers per wagon (Mormon \"church teams\" often had eight-plus pioneers). Accompanying nearly all wagon trains was a herd of horses, cows, oxen or mules.  In many years it is estimated that there were more animals than people using the trail.  A thriving trade consisted of herds of cows and sheep bought in the mid-west, herded over the trail and sold in California, Oregon etc..  The usually much cheaper animals in the mid-west could be herded to California etc. and sold for usually a substantial profit.  Large herds were typically separated from the regular wagon trains because of their different speeds and herding requirements.  These animals were usually the daytime responsibility of one or more herder(s) and the nighttime responsibility of the three or more wagon train guards.  Each adult male, on a rotating schedule, was usually required to spend part of a night on guard duty. The typical wagon with 40 to diameter wheels could easily move over rough ground and rocks without high centering and even over most tree stumps if required.  The wooden wheels were protected with an iron rim (tire) typically about 1.5 in wide.  These iron tires were installed hot so they would shrink tightly onto the wood wheel when they cooled.  Nevertheless, it was often necessary to use wooden wedges to keep the iron rim on or soak the wheel in water.  The dry desert air sometimes dried the tires so much the iron tire was prone to fall off.  Wagon wheels could often be repaired by blacksmiths found along the way or replaced with an abandoned wagon's wheel but otherwise if damaged the wagon usually had to be abandoned.  Some damaged wagons were salvaged by cutting the wagon in half and converting the front or rear half of the wagon into a two-wheeled cart.  Most of the wagons had a large toolbox, mounted on the left side, usually containing an ax, wagon jack, ropes, short handled shovel, wheel chains for securing the wheels for steep descents and extra chain to hook up another team if double teaming was required for steep ascents and other tools often needed or used.  The wagon jack was used for raising each wagon wheel.  Then the large axle nut could be unscrewed and the wheel removed for greasing which was required periodically.  The wheels were greased with a mixture of tar or pine resin and lard contained in a covered wooden bucket or large ox horn often hanging from the rear axle to keep its greasy contents away from other goods.  Starting with at least one gallon of wagon grease was recommended.  On a wagon there was essentially no reverse or brakes and the turning radius was nearly always greater than 125 ft so the teamsters had to think about how to extract the wagon and his team from wherever they went. When mules or horses were chosen to pull the wagons, they typically cost about twice as much money and required more expensive harnesses.  Oxen (used by 60–70%) were found to be cheaper, tougher, stronger, easier to catch, more easily trained, less prone to be stolen and better able to survive on the often sparse feed found along the way.  Their only drawback was they were initially about 10% slower (2–3 miles/hour), but they often passed the mule and horse pulled teams as the trip wore on and the other teams wore out.  Since the most popular draft animal was ox teams (~70%), most walked nearly all the 2,000 or more miles to their destination.  Some of the luckier ones had riding horses or mules and could afford to have someone else drive the wagon team.  Oxen are driven by walking on the left side and yelling \"Gee\" to turn right, \"Haw\" to turn left, \"Git-up\" to go forward and \"Whoa\" to stop—words often emphasized with a snapping whip (and occasional swear words).  Mules were the second choice (used by 20–30%) but trained animals were hard to find, and it took up to two months to train them.  Mules did better than horses on the often poor feed found along the way.  Mule teams were often used with the driver (teamster) riding on the left hand \"wheel\" mule with reins to the rest of the team—this saved weight in the wagon and was often more comfortable than the rough riding wagons were.  Horses were often found to be incapable of the months of daily work and poor feed encountered without using supplemental grain (initially unavailable or too heavy), and thousands were recorded as dying near the end of the trip in the Forty Mile Desert.  Horses and mules had the added disadvantage that they nearly always required herding and guarding day and night to prevent them from wandering off, stampeding, or being stolen.  They were also harder to find and re-capture if they got lost.  Often late in the trip mixed teams that included dairy cows and riding ponies were sometimes hitched up to make a usable team.  Trading posts along the way did a thriving business in buying worn down teams at low prices and selling fresh animals.  After a few weeks care and good feeding, these same teams could often be resold at a substantial profit. One or more horses or mules were often included per wagon for riding, hunting, scouting and keeping herd on the animals.  Saddles, bridles, hobbles, ropes, harnesses etc. were needed if they had a horse or riding mule, and many men did.  Extra harness parts, rope, steel chain and wagon parts were often carried.  Steel shoes for oxen, mules or horses and some spare parts for the wagons were carried by most.  Tar was often carried to help repair an injured ox's hoof.  If the team was properly taken care of, they usually survived the trip in good shape, but if they were pushed too hard for too long they died or became too weak to continue.  Many of the \"49ers\" were in a great rush and often pushed their animals too hard and they had to buy new animals along the way. Food for the trip had to be compact, lightweight, and nonperishable.  The more knowledgeable also brought dried fruit and vegetables to provide some variety (and Vitamin C) and were a known (to many) scurvy prevention.  The method of preparing desiccated vegetables was to squeeze them in a press to remove most of their juice and then bake them for several hours in a low temperature oven.  The vegetables like dried peas kept well if kept dry and a piece of dried vegetables the size of a fist when put in water and cooked could feed four.  The recommended food to take per adult for the four- to six-month trip was 150 lb of flour, 20 lb of corn meal, 50 lb of bacon, 40 lb of sugar, 10 lb of coffee, 15 lb of dried fruit, 5 lb of salt, half a pound (0.25 kg) of saleratus (baking soda), 2 lb of tea, 5 lb of rice, and 15 lb of beans.  Condiments like: mustard, cinnamon, nutmeg, vinegar, pepper and other spices were usually included.  Ex-trappers, ex-army soldiers and Indians often used pemmican made by pounding jerky until it was a coarse meal, putting it into a leather bag and then pouring rendered fat (and sometimes pulverized dried berries) over it—this was very light weight, could keep for months and provided a lot of energy.  Some families took along milk cows and goats for milk and chickens (penned in crates tied to the wagons) for eggs and chicken dinners.  Additional food like pickles, canned butter, cheese or pickled eggs were occasionally carried, but canned goods were expensive and relatively heavy to carry and food preservation was primitive, so few perishable items could be safely kept for the four to six-month duration of the trip.  These provisions were usually kept in water-tight containers and carried inside the covered wagon to minimize getting wet.  At river crossings their food usually had to be removed and carried across on a boat or raft to keep it dry—one of the reasons toll bridges or ferries were popular.  Meat filled barrels (200 lb ) were often bought and then, to reduce weight, the bacon and ham were usually transferred to bran filled sacks and stuck in the bottom of the wagons to stay as cool as possible—the barrel being discarded.  In hot weather bacon and ham was often hauled in large barrels packed in bran so the hot sun would not melt the fat.  Medicinal supplies carried usually consisted of salves and ointments, laudanum (about the only effective pain medicine then and much over used), and a few home remedies. The typical cost of enough food for four people for six months was about $150.  The cost of other supplies, livestock, wagons etc. per person could easily double this cost.  In the 1840s, $150.00 represented about 150 days worth of work or half a year’s typical salary so most of the poor were excluded from travel unless they got a job herding and guarding the livestock or driving a wagon. The amount of food required was lessened if beef cattle, calves or sheep were taken along for a walking food supply.  Prior to the 1870s, vast herds of buffalo in Nebraska provided fresh meat and jerky for the trip.  In general, wild game and fish could not be depended on, but when found, were a welcome change in a monotonous diet.  Travelers could hunt antelope, buffalo, trout, deer and occasionally sage hens, elk, bear, duck, geese, and salmon along the trail.  Many travelers went via Salt Lake City, Utah and the Salt Lake Cutoff to get repairs, fresh or additional supplies, fresh vegetables and fresh livestock. Cooking along the trail was typically done over a campfire dug into the ground and made of wood, dried buffalo chips, willow or sagebrush—whatever was easily available.  After a rain the 'Buffalo chips' were often hard to start on fire.  Flint and steel or matches were used to start fires.  Cooking equipment was typically light and included only simple cooking utensils such as butcher knives, forks, metal plates and cups, spoons, large spoons, spatulas, ladles, Dutch ovens, pots and pans, grills, spits, coffee pots, pot hooks and an iron tripod to suspend the pans and pots over the fire.  Some brought small stoves, but these were often jettisoned along the way as too heavy and unnecessary.  The usual meal for breakfast, lunch and dinner eaten by the mostly male Argonauts (many didn't want to or know how to cook) was bacon/ham, beans, coffee and biscuits/bread/corn bread or flapjacks. If three or more were traveling together a tent was often included; but most slept on the ground—getting in the wagon only in case of bad weather.  Wooden or canvas buckets were brought for carrying water, and most travelers carried canteens or water bags for daily use.  One of the first tasks, after unhooking the animals and letting them water and graze, at almost every stop was getting a new supply of water for drinking, cooking and washing.  The next task was usually rounding up enough fuel to start a fire for cooking and heating up the coffee.  At least one ten gallon water barrel was brought, but it was usually kept nearly empty to minimize weight (some water in it helped prevent it from leaking); it was typically only filled for waterless stretches.  Most casks were discarded near the end of trip as too heavy and no longer needed after Forty Mile Desert.  Some brought a new invention—an India Rubber combination mattress and water carrier. Each man typically took a rifle or shotgun (double barrel recommended) and occasional pistol along with the necessary balls, gunpowder and primers for hunting game and protection against snakes and Indians.  Many took their fishing gear along—at least lines and hooks as a usable pole could usually be cut from a willow or other bush.  Belt knives or folding knives were carried by nearly all men and boys and considered essential.  Farm tools such as a plow, pick, shovel, scythe, rake, hoe; plus carpentry tools—saw, hammer, nails, broad axe, mallet, plane were often carried along.  Farmers typically took seeds for corn, wheat and other crops.  Some even included fruit trees and vines in their loads.  Awls, scissors, pins, needles, thread and leather laces to repair clothes, shoes, harnesses, equipment and occasionally people were constantly in use.  Spare leather used for repairs was often needed and used.  Goggles to keep dust out of eyes were used by some.  Storage boxes for food and supplies were often the same height so they could be arranged to give a flat surface inside the wagon for sleeping during bad weather.  If the cargo weighed too much, and it often did initially, these boxes were typically discarded and nearly everything put into bags. Nearly all brought at least two changes of clothes with extra shirts and jackets (wool usually recommended for its toughness and warmth) hats and multiple pairs of boots—two to three pairs often wore out on a trip.  Moccasins at $0.50 to $1.00 per pair and buffalo robes at $4.00 to $8.00 each were often bought (or traded for equivalent valued items) from Indians encountered on the way.  A thin fold-up mattress, blankets, buffalo robes, pillows, canvas or rubber gutta percha ground covers were used for sleeping (usually on the ground) at night.  About 25 lb of soap was recommended for a party of four for washing, bathing and washing clothes.  A washboard and tub was also usually included to aid in washing clothes.  Wash days typically occurred once or twice a month or less, depending on availability of good grass, water, fuel and time.  Shaving was usually given up for the trip to save on water and bother.  Tobacco was popular, both for personal use and for trading with Indians and other pioneers.  Some alcohol was typically taken for \"medicinal\" purposes—and used up along the way.  Sometimes an unfolded feather bed mattress was brought for cushioning the ride in the wagon if there were pregnant women or young children along.  The wagons had no springs of any kind, and the ride along the trail was very rough—rough enough to churn butter if a cow was brought along.  Despite modern depictions where nearly everybody rides, almost nobody unless a child, pregnant wife or injured traveler actually rode long in the wagons; it was too dusty, too rough and too hard on the livestock.  Most walked nearly all the way. Travelers also brought books, Bibles, trail guides, writing quills, and ink and paper for keeping a diary or writing a letter. Goods, supplies and equipment were often shared by fellow travelers.  Other goods that were forgotten, broke or wore out could often be found discarded by someone else along the way or bought from a fellow traveler, post or fort along the way.  Equipment and wheel repairs and other goods could often be procured from blacksmith shops established at some forts and some ferries along the way—most did a thriving business.  New iron shoes for horses, mules and oxen were often put on by blacksmiths.  Emergency supplies, repairs and livestock were often provided by local residents in Oregon, California and Utah for late travelers on the trail who were hurrying to beat the snow and had run out of supplies, broken down or needed fresh animals. Along the way, non-essential items were often abandoned to lighten the load, or in case of emergency.  Many travelers would salvage discarded items, picking up essentials or trading their lower quality items for better ones found along the road.  In the early years, the Mormons sent scavenging parties back along the trail to salvage as much iron and other supplies as possible and haul it to Salt Lake City where supplies of all kinds were needed.  Blacksmiths there could then recycle the salvaged iron to make almost any iron/steel object needed.  Others would use discarded wagons, wheels and furniture as firewood.  During the 1849 gold rush, Fort Laramie was known as \"Camp Sacrifice\" because of the large amounts merchandise discarded nearby.  Travelers had pushed along the relatively easy path to Fort Laramie with their 'luxury' items but discarded them before the difficult mountain crossing ahead and after discovering that many items could be purchased at the forts or located for free along the way.  Many of the smarter travelers carried their \"excess\" goods to Salt Lake City where they could trade them for new supplies or money. Some professional tools used by surgeons, blacksmiths, carpenters, farmers, etc. were carried by nearly all.  Shovels, crow bars, picks, hoes, mattocks, saws, hammers, axes and hatchets were used to clear or make a road through trees or brush, cut down the banks to cross a wash or steep banked stream, build a raft or bridge, or repair the wagon where necessary.  In general, as little road work as possible was done.  Travel was often along the top of ridges to avoid the brush and washes common in many valleys.  Because the wagons tipped over easily on a side hill they were often dragged straight up a steep hill, with multiple teams if necessary and then skidded straight down the opposite side with chained up wheels if required. See U.S. River maps-USGS for map of rivers followed across the United States. The Oregon, California, Mormon and later the shorter Bozeman (into Montana) Trails (sometimes called the Emigrant Trails) all went west along much of the same network of trails until Wyoming, Utah or Idaho, where they split off to reach their respective destinations.  The exact route of the trail to get to California depended on the starting point of the trip, the final destination in California, the whims of the pioneers, the water and grass available on the trail, the threats of Indian attacks on parts of the trail, and the information they had or acquired along the way and the time of year.  No government agents or bodies controlled the numbers and routing of the emigrants.  The only \"help\" they could depend on was from their fellow travelers, a few blacksmiths and entrepreneurs running trading posts, and the few Army forts scattered along the road in Nebraska and Wyoming.  In emergencies, the early pioneers, with and without Army help, nearly always organized relief parties. To get the two essentials, water and grass for the travelers and their animals, the trails nearly always followed river valleys across the continent.  The other \"essential,\" 'wood' for fires, utilized any easily found burnable fuel—trees, brush, 'buffalo chips', abandoned wagons and supplies, sage brush, etc..  The wagons and their teams were the ultimate \"off road\" equipment in their time and were able to traverse incredibly steep mountain ranges, gullies, large and small streams, forests, brush, and other rough country.  Initially, the almost total lack of improved roads severely constrained travel in some areas, as the pioneers had to detour, find, or make a way through or around difficult terrain.  The trails, when not in flat country, typically went down ridge tops to avoid the trees and gullies normally found in valleys.  When the Army established the shorter Central Overland Route in 1859 from Salt Lake City, Utah to Carson City, Nevada, it used local streams and springs found in the desert along the way.  On the open plains, the wagons typically spread out to minimize traveling in dust.  Later travelers typically used improvements and routes established by previous travelers.  To be able to finish the four- to six-month trip in one season, most trips were started in early April or May, as soon as the grass was growing and the trails were dry enough to support the wagons.  The trips hopefully terminated in early September or October before snow started falling again. Feeder routes or Eastern branches of the named emigrant trails crossed the states of Missouri and Iowa before reaching and crossing the Missouri River.  Initially, steamboat navigable waters on the Missouri River ended just upstream of Independence, Missouri/Kansas City, Kansas.  By 1846, the Great Flood of 1844's damage to up-river traffic was fixed, as primitive dredging had opened up the Missouri River as far as the Platte River confluence near Kanesville, Iowa (later renamed Council Bluffs).  By 1853, Omaha, Nebraska, on the west bank, became the starting point of choice for many, as armed conflicts in \"Bleeding Kansas\" made travel across Kansas more hazardous. Many emigrants from the eastern seaboard traveled from the east coast across the Allegheny Mountains to Brownsville, Pennsylvania (a barge building and outfitting center) or Pittsburgh and thence down the Ohio River on flatboats or steamboats to St. Louis, Missouri.  Many others from Europe traveled by sailing ship to the mouth of the Mississippi River where steam powered tugs towed them up river about 80 mi to New Orleans, Louisiana.  From there, cheap (about $5.00) and fast (about 6 days) steamboats brought them to St. Louis.  Many bought most of their supplies, wagons and teams in St. Louis and then traveled by steamboats up the Missouri River to their departure point. The main branch(es) of the trail started at one of several towns on the Missouri River—Independence/Kansas City, St. Joseph, Missouri, Kanesville and Omaha, plus others.  Those starting in either St. Joseph/Independence, Missouri, or Kansas City, Kansas, typically followed the Santa Fe Trail route until they could be ferried across the Kansas and Wakarusa Rivers.  They then followed either the Little Blue River or Republican River across Kansas and into Nebraska.  If they started above the Kansas and Missouri River junction from the future town sites of Atchison, Kansas or Leavenworth, Kansas, they typically traversed northwest across the plains until they encountered the Big Blue River and its tributary, the Little Blue.  The trail generally followed the Little Blue, which ended near the Platte River.  The only general problem through the rolling hills of Kansas was the need to cross several large creeks or rivers with sharp banks.  These required either doing a lot of work to dig a wagon ford, or using a previously established ford or toll bridge.  In Nebraska and Kansas, Indian tribes ran many of the toll bridges or ferries. If they started in Iowa or Nebraska, after getting across the Missouri River, most followed the northern side of the Platte River from near its junction on the Missouri River ferrying across the Elkhorn River and the wide and muddy Loup River, which intercept the Platte River.  As the 1850s progressed and armed hostilities escalated in \"bleeding\" Kansas, travelers increasingly traveled up the Missouri River to leave from or near Omaha.  After 1847, many ferries and steamboats were active during the emigration season start to facilitate crossing the Missouri to the Nebraska or Kansas side of the river. When the Union Pacific Railroad started west in 1865, Omaha was their eastern terminus.  The eastern end of the trail has been compared to a frayed rope of many strands that joined up at the Platte River near new Fort Kearny (est. 1848) in Nebraska.  Those on the north side of the Platte would have to cross the Platte River to use the mail, repair and supply services available at Fort Kearny. The preferred camping spots for travelers on the trails north and south of the muddy Platte River were along one of the many fresh water streams draining into the Platte or the occasional fresh water spring found along the way.  These preferred camping spots became sources of cholera infections during the third cholera pandemic (1852–1860).  Many thousands of people used the same camping spots whose water supplies became contaminated by human wastes.  Cholera causes vomiting and severe diarrhea, and in places where human wastes contaminate water supplies the causal bacteria, Vibrio cholera, could easily spread among travelers.  Once the water supplies became contaminated, because the cholera bacillus is zoophilic (it can infect birds, various mammals, and live in micro-organisms) it could easily spread and remain a threat along much of the Trail.  Cholera, when untreated, can result in fatality rates between fifty and ninety percent.  Even after the British physician and pioneer of anesthesia, John Snow, had helped demonstrate that cholera was transmitted through water in 1854, it did not become common knowledge until decades later; scientists continued to debate the cause of cholera until the beginning of the twentieth century.  Treatments were almost always ineffective and sometimes hastened death.  It would have been a terrifying companion while crossing the desolate high plains and passes of the Rocky Mountain West. Cholera killed many thousands in New York City, New York, St. Louis, Missouri, New Orleans, Louisiana, and other towns on the Missouri and Mississippi Rivers who inadvertently drank cholera contaminated water.  Cholera is thought to have been brought to these river cities, etc. and the California, Oregon and Mormon Trails by infected immigrants from Europe.  Cholera killed additional thousands in London England, Liverpool, England, and other cities in Europe and around the world.  These widespread infections and thousands of deaths finally gave impetus to building, at great cost, effective citywide water and sewage systems in many European and US cities. Germs that caused cholera and other diseases were still undiscovered as a disease spreading mechanism in this era.  The \"Germ theory of disease\" and the systematic observation of possible disease causing microorganisms were just starting in this era.  The cause of cholera, ingesting \"invisible\" cholera germs from cholera infected fecal contaminated water or food was not known.  Although magnifying lenses had been discovered in 1592 effective microscopes that could see germs well were just being developed and widely used starting in the 1860s.  The prevention or effective treatment for cholera, once patients were infected, were unknown in this era and death rates then sometimes reached 50% of infected people.  Cholera infections spread rampantly in the era before possible sources of cholera were identified, cholera carriers isolated and before effective water and sewage treatment facilities were developed and deployed. Many thousands of emigrants died in Kansas, Nebraska, and Wyoming and were buried along the trail in unmarked graves. The Platte River in the future states of Nebraska and Wyoming typically had many channels and islands and was too shallow, crooked, muddy and unpredictable for even a canoe to travel very far on as it pursued its braided paths to the Missouri River.  The Platte River Valley, however, provided an easily passable wagon corridor sloping easily up as it went almost due west with access to water, grass, buffalo meat and hides and 'buffalo chips' for fire 'wood'.  There were trails on both sides of the muddy, about 1 mi wide and shallow (2 in to 60 in ) Platte River.  In all the trail(s) traveled about 450 mi in the present state of Nebraska in the Platte River Valley.  The Platte's water was silty and bad-tasting but it could be used if no other water was available.  Letting it sit in a bucket for an hour or so allowed most of the silt to settle out. Those traveling south of the Platte crossed the South Platte with its muddy and treacherous crossings using one of about three ferries (in dry years it could sometimes be forded without a ferry) before continuing up the North Platte into present-day Wyoming to Fort Laramie.  After crossing over the South Platte the travelers encountered Ash Hollow with its steep descent down windlass hill.  Several days further on they would encounter huge rock formations sticking out of the prairie called Courthouse Rock and twenty miles (32 km) further on the startling Chimney Rock, then Castle Rock, and finally Scotts Bluff.  Before 1852 those on the North side ferried (or after about 1850 took a toll bridge) across the North Platte to the south side and Fort Laramie. After 1852, they used Child's Cutoff to stay on the north side to about the present day town of Casper, Wyoming, where they crossed over to the south side.  After crossing the Laramie River, the road west of Fort Laramie became much rougher as streams feeding the North Platte cut the terrain into many hills and ravines.  The river was now often in a deep canyon, and the road had to veer away from it.  Sallie Hester, an immigrant of 1850, described the terrain as something clawed by a gigantic bear: \"sixty miles of the worst road in the world.\"  In all from Omaha, Nebraska (1050 ft ) the Platte and North Platte were followed for about 650 mi to Casper (5050 ft ).  Fortunately, swifter flowing waters after Fort Laramie seemed to minimize the chance for cholera germ transmission, and its fatal attacks diminished significantly. Continuing upstream from Casper, the North Platte bends to the southwest headed for the Colorado Rockies.  About 50 mi southwest of Casper the North Platte is joined by the Sweetwater River (Wyoming).  This river junction is deep in a canyon now filled by the Pathfinder Reservoir.  The trail crossed over the North Platte by ferry and later by bridge.  Some of the original immigrant travelers proceeded several miles along the North Platte River to Red Buttes, where a bend in the river formed a natural amphitheater dominated by red cliffs on the hill above.  The cold North Platte was easier to ford here for those who were unwilling or unable to pay to cross at one of the ferries downstream.  This was the last good camp spot before leaving the river and entering the water less stretch between the North Platte and the Sweetwater River.  From here the settlers entered a difficult portion called Rock Avenue which moved from spring to spring across mostly alkaline soil and steep hills until it reached the Sweetwater River.  Later settlers who had crossed to the northern side of the river at Casper would come to favor a route through a small valley called Emigrant Gap which headed directly to Rock Avenue, bypassing Red Buttes. Upon arrival in the Sweetwater valley, the trail encounters one of the most important landmarks on the trail, Independence Rock.  Independence Rock was named by Jedediah Smith and party when they first observed it in 1824 on July 4--Independence Day in the United States.  Jedediah and his fellow trappers rediscovered South Pass and the Sweetwater River in 1824.  Immigrants also tried to reach Independence Rock on July 4 in order to help ensure that they will be at their destinations in California or Oregon before the winter snows came and closed the trails.  Many of the travelers left their names on the rock, either carved or painted on with axle grease.  It is estimated that more than 50,000 signatures were inscribed on Independence Rock.  Other notable landmarks along the Sweetwater valley include Split Rock, Devil's Gate and Martin's Cove, where, in October to November 1856, the Martin Handcart Company was stranded by an early heavy snow and a late start and about 145 died before they were rescued by the rescue parties (about 250 wagons with supplies and help were dispatched from Utah) sent by Brigham Young from Salt Lake City. The immigrant trail continues west along the Sweetwater River eventually crossing the meandering river nine times, including three times within a 2 mi section through a narrow canyon in the Rattlesnake Hills.  Prior to the 6th crossing, the trail crossed an unusual location known as Ice Slough.  A covering of peat like vegetation grew over a small stream.  The stream froze in winter and didn't thaw until early summer due to the insulating layer of vegetation.  The ice was a welcome treat for settlers who were often enduring temperatures over 90 °F in July.  The trail crosses the Sweetwater three more times and encounters a large hill known as Rocky Ridge on the northern side of the river.  This barren and rocky section lasted almost 12 mi , and was considered a major obstacle in the trail.  The same storm in November 1856 that debilitated the Martin Handcart Company also stranded the Willie Handcart Company on the eastern side of the ridge.  Before rescuers could arrive, 56 people died in freezing temperatures out of a company of about 600.  Following Rocky Ridge, the trail descends one more time into the Sweetwater valley to the ninth and final crossing of the Sweetwater at Burnt Ranch. In 1853, a new route named Seminoe cutoff was established on the southern side of the river.  It was named after trapper Basil LaJeunesse who was referred to as Seminoe by the Shoshone Indians.  The Seminoe cutoff split from the main trail at the 6th crossing and rejoined it at Burnt Ranch, bypassing both Rocky Ridge and four of the river crossings, which was an advantage in the early spring and summer during high runoff.  The route was used extensively in the 1850s, especially by the Mormon companies. Immediately after crossing the Sweetwater at Burnt Ranch the trail crosses the continental divide at South Pass, unarguably the most important landmark on the entire trail.  South Pass itself is an unimpressive open saddle between the Wind River Range to the north and the Antelope Hills to the south, but it represented a major milestone in the trip.  In 1848, Congress created the Oregon Territory which included all the territory in Wyoming west of the Continental Divide.  Crossing South Pass meant that the settlers had truly arrived in the Oregon Territory, though their ultimate destination was still a great distance away.  Nearby Pacific Springs offered the first water since the trail had left the Sweetwater River and marked the beginning of a relatively dry stretch of trail until the settlers reached Big Sandy River that joined with the Green River more than 40 mi away. The Sublette-Greenwood Cutoff (established 1844) cut about 50 mi off the main route through Fort Bridger.  It left the main emigrant trail about 20 mi from South Pass at Parting of the Ways junction and then headed almost due west.  About ten miles (16 km) further they encountered Big Sandy River—about ten feet wide and one foot deep.  This was the last water before crossing about 45 mi of desert consisting of soft dry soil that rose in suffocating clouds before reaching the next water at the Green River about 4 mi below the present town of La Barge, Wyoming.  Here, the Green cut a steep 400 ft channel through the Green River Desert, which travelers had to descend by a steep rocky path to reach the life-giving water.  Often, thirsty teams stampeded to the water with terrible results.  The descent was soon scattered with fragments of many wagons and dead animals.  The Sublette cutoff saved about 50 mi but the typical price was numerous dead oxen and the wrecks of many wagons.  After crossing the Green they then had to continue crossing mountain ranges where the trail gets over 8000 ft in several places before finally connecting with the main trail near today's Cokeville, Wyoming in the Bear River valley.  (For map See: Sublette-Greenwood Cutoff Map,) The Green River is a major tributary of the Colorado River and is a large, deep and powerful river.  It ranges from 100 to wide in the upper course where it typically was forded and ranges from 3 to in depth.  After the opening of the Oregon, California and Mormon trails were opened several ferries were set up to cross it at both the main trail and the Sublette Cutoff but during peak travel seasons in July the wait to cross was often several days.  At the Green River on the main trail after crossing the river many took the Slate Creek Cutoff (also called the Kinney Cutoff), which turned north up the Green River for about ten miles (16 km) before turning almost due west to connect to the Sublette Cutoff road.  This cutoff eliminated most of the waterless desert crossing of the Sublette Cutoff. After 1848, those needing repairs, fresh livestock, fresh vegetables, fruit or other supplies could stay on the Mormon Trail for about 120 mi from Fort Bridger to Salt Lake City, Utah and other Utah towns.  Salt Lake City, located at about halfway 1000 mi on the trip, was the only significant settlement along the route.  From Salt Lake City they could easily get back to the California (or Oregon) Trail by following the Salt Lake Cutoff about 180 mi from Salt Lake City northwest around the north end of Great Salt Lake, rejoining the main trail at the City of Rocks near the present Idaho-Utah border.  The trip from Fort Bridger via Salt Lake City to the City of Rocks was about 300 mi —about 20 mi shorter than the trail via Fort Hall. Hundreds of late arriving Forty-niners, and some parties of Mormons, both packers and teamsters, looking to avoid the fate of the Donner Party, in the fall and winter of 1849–1850 used the snow free Southern Route to Southern California.  This route, that ran southwest from Salt Lake City, was pioneered by Jefferson Hunt in 1847–48 and a party of veterans of the Mormon Battalion returning from California in 1848.  From Parowan onward to the southwest, the original route closely followed the route of the Old Spanish Trail diverting from that route between the Virgin River at Halfway Wash to Resting Springs, following the cutoff discovered by John Freemont on his return from California in 1844.  This road only diverted to find places that could be traversed by the wagons of Mormon and Forty-niner parties that pioneered it.  Later immigrants and the Mormon colonists of San Bernardino, in the early 1850s followed it.  At the same time along what became known as the Mormon Road were seeded the Mormon settlements that developed into towns and cities of modern Utah, Arizona, Nevada and Southern California. The Lander Road, located further north than the main trail to Fort Hall, also bypassed Fort Bridger and was about 85 mi shorter to Fort Hall.  It was built under the supervision of Frederick W. Lander by federal contractors in 1858—one of the first federally sponsored roads in the west.  Lander's Road officially was called the \"Fort Kearney, South Pass and Honey Lake Road\" and was a federally funded attempt to improve the Oregon and California trails.  The little used Honey Lake part of the proposed route near the present states of Nevada and California border was improved in 1859 under Lander's direction but did not go much beyond improving some watering holes—work ceased in 1860.  The \"Lander Road\" was the first section of the federally funded road through The future states of Wyoming and Idaho.  Expeditions under the command of Frederick W. Lander surveyed a new route starting at Burnt Ranch following the last crossing of the Sweetwater River before it turned west over South Pass.  The Lander Road followed the Sweetwater River further north, skirting the Wind River Range before turning west and crossing the continental divide north of South Pass.  The road crossed the Green River near the present town of Big Piney, Wyoming and then passing over 8800 ft Thompson Pass in the Wyoming Range near the head of the Grey's River and then crosses another high pass across the Salt River Range before descending into Star Valley (Wyoming).  The trail entered Star Valley about 6 mi south of the present town of Smoot, Wyoming.  From Smoot, the road then continued north about 20 mi down Star Valley west of the Salt River before turning almost due west at Stump Creek near the present town of Auburn, Wyoming and passing into the present state of Idaho and following the Stump Creek valley about ten miles (16 km) northwest over the Caribou Mountains (Idaho) (this section of the trail is now accessible only by US Forest Service path as the main road (Wyoming Highway 34) now goes through Tincup canyon to get across the Caribous.)  After crossing the Caribou Range the road split, turning almost ninety degrees and progressing southwest to Soda Springs, Idaho or alternately heading almost due west and passing south of Grays Lake (now part of the Grays Lake National Wildlife Refuge) to Fort Hall Idaho.  The Lander Road had good grass, fishing, water and wood but was high, rough and steep in many places.  Later, after 1869, it was mostly used by ranchers moving their stock to and from summer grazing or markets.  For maps of the Lander road in Wyoming and Idaho see NPS National Trail Map For more information visit Afton, Wyoming to see its Lander and Pioneer Museum. By crossing the lush Wyoming and Salt River Ranges instead of circling via the deserts to the south, the route provided ample wood, grass and water for the travelers, and cut nearly 7 days off the total travel time for wagon trains going to Fort Hall.  Despite the better conditions for livestock, the mountainous terrain and unpredictable weather made passage sometimes difficult and required continuing federally funded maintenance on the mountainous road—not a sure thing just before, during and after the American Civil War.  Funds were appropriated in 1858 and 115 men (hired in Utah) completed the road in Wyoming and Idaho in 90 days, clearing timber and moving about 62000 cuyd of earth.  The Lander's road or cutoff opened in 1859 when it was extensively used.  Records after 1859 are lacking and its use after that period are assumed to sharply decrease since the Sublette Cutoff, the Central Overland Route and other cutoffs were just about as fast or faster and were much less strenuous.  Today the Lander cutoff road(s) are roughly followed by a series of county and Forest Service roads. An alternative route, the Central Overland Route, across Utah and Nevada that bypassed both Fort Hall and the Humboldt River trails was developed in 1859.  This route was discovered, surveyed and developed by a team of U.S. Army workers led by Captain James H. Simpson of the U.S. Army Corps of Topographical Engineers and went from individual streams and springs across the Great Basin desert in central Utah and Nevada—avoiding the Humboldt River trail and its often combative Indians and Forty Mile Desert.  This route was about 280 mi shorter and over ten days quicker.  The route followed the Mormon Trail from South Pass to the newly settled Salt Lake City, Utah and passed south of the Great Salt Lake across central Utah and Nevada.  The route today is approximated today by the roads from: Salt Lake City, Utah, Fairfield, Utah (then called Camp Floyd), Fish Springs National Wildlife Refuge, Callao, Utah, Ibapah, Utah to Ely, Nevada.  From Ely the route is approximated by the U.S. Route 50 in Nevada from Ely, Nevada to Carson City, Nevada.  (See: Pony Express Map) Many California bound travelers took the about 280 mi and over two weeks shorter Central Overland Route to Salt Lake City and across central Utah and Nevada.  Initially the springs and trail were maintained by the army as a western supply route to Camp Floyd, which was set up after the Utah War of 1856–57.  By 1860 Camp Floyd was abandoned as the army left to fight the U.S. Civil War and the Central Overland Route was their only long term legacy. Starting in March 1860 and continuing till October 1861 the Pony express established many small relay stations along the Central Overland Route for their mail express riders.  From the end of Central Overland route in Carson City, Nevada they followed the Johnson Pass (Placerville route) to California since it was the fastest and only route that was then kept open in winter across the Sierra Nevada (U.S.) mountains.  On March 2, 1861, before the American Civil War had actually begun at Fort Sumter, the United States Government formally revoked the contract of the Butterfield Overland Stagecoach Company in anticipation of the coming conflict.  A more secure route for communication and passengers between the non-Confederate states and the west was needed.  The stock, coaches, etc., on the southern Gila River route Butterfield Stage route through or close to some potential Confederate states were pulled off and moved to a new route between St. Joseph, Missouri and Placerville, California along the existing Oregon, California Trails to Salt Lake City and then through central Utah and Nevada.  It took about three months to make the transfer of stages and stock, and to build a number of new stations, secure hay and grain, and get everything in readiness for operating a six-times-a-week mail line.  On June 30, 1861 the \"Central Overland California Route\" from St. Joseph, Missouri, to Placerville, California, went into effect.  By traveling day and night and using frequent team changes the stages could make the trip in about 28 days.  News paper correspondents reported that they had a preview of hell when they took the trip. These combined stage and Pony Express stations along the Central Route across Utah and Nevada were joined by the first transcontinental telegraph stations (completed October 24, 1861).  This combination wagon-stagecoach-pony express-telegraph line route is labeled the \"Pony Express National Historic Trail\" on the National Trail Map.  From Salt Lake City, the telegraph line followed much of the Mormon-California-Oregon trail(s) to Omaha, Nebraska.  After the first transcontinental railroad was completed in 1869, the telegraph lines along the railroad tracks became the main line, since the required relay stations, lines and telegraph operators were much easier to supply and maintain along the railroad.  The telegraph lines that diverged from the railroad lines or significant population centers were largely abandoned. The main trail after crossing the South Pass encountered a number of small springs and creeks before hitting the Green River.  After ferrying across the Green the main trail went on to Fort Bridger.  Here they could take the Mormon Trail to Salt Lake City or go to Fort Hall.  The main trail going to Fort Hall went almost due north from Fort Bridger to the Little Muddy Creek where it passed over the Bear River Divide to the pleasant Bear River Valley.  The Bear River wanders about 350 mi through three states as it makes a large inverted U around the north end of the Wasatch Range and then turns south and eventually empties into the Great Salt Lake as part of the Great Basin drainage system.  The trail along the Bear usually had good grass, water, good fishing and wood.  Once on the Bear River they followed the Bear's valley mostly north along today's Utah, Idaho, Wyoming border.  In the Thomas Fork area, the trail is forced to go up \"Big Hill\" to by-pass a narrow canyon filled by the Bear River (Today's U.S. Route 30 blasted and bulldozed a wider canyon to follow the river).  Big Hill had a tough ascent often requiring doubling up of teams and a very steep and dangerous descent (wagon trail scars are still visible today).  A few miles further north is present day Montpelier, Idaho (site of an Oregon-California Trail interpretive Center).  They followed the Bear River to present-day Soda Springs, Idaho.  Here, there were many hot springs, mineral deposits, wood, and good grass and water.  Many travelers stopped there for a few days to refresh their animals, themselves, wash clothes etc.  A few miles after Soda Springs, the Bear River turned southwest towards the Great Salt Lake, and the main trail turned northwest near \"Sheep's Rock\" to follow the Portneuf River valley to Fort Hall (Idaho) in the Oregon Country along the Snake River.  The route from Fort Bridger to Fort Hall was about 210 mi taking nine to twelve days. About 5 mi west of Soda Springs, \"Hudspeth's Cutoff\" (est. 1849) took off from the main trail heading almost due west and by-passed Fort Hall.  Hudspeth's Cutoff had five mountain ranges to cross and took about the same amount of time as the main route to Fort Hall but many took it thinking it was shorter.  Its main advantage was that it did spread out the traffic on busy years and made more grass available. The original California Trail pioneers, the Bartleson–Bidwell Party, only knew that California was west of Soda Springs—somewhere.  They lacked guides or information on the best route west to California.  Shortly after Soda Springs the Bear River heads southwest as it rounds the Wasatch Mountains and heads for the Great Salt Lake.  Not knowing what else to do and knowing they needed grass and water, they followed the river.  After following the Bear and building a trail through Cache Valley, Utah and crossing over the Malad Mountains, they got to somewhere near today's Bear River City, Utah.  Then they realized the Bear River was going to terminate in the Great Salt Lake.  Continuing west by going north of the Great Salt Lake across numerous alkali and salt-encrusted flats, they had a very difficult time because of the few springs and poor feed available for their animals.  They finally abandoned their wagons in eastern Nevada when they realized the route they were on was getting ever rougher and they had missed the head of the Humboldt River.  In addition, their animals were getting in ever poorer condition.  After a very hard struggle they finished their trip to California successfully by building pack saddles for their horses, oxen and mules and converting their wagon train into a pack train.  After finally finding the Humbodlt, they continued slogging west and continuing to struggle through most of November 1841 getting over the Sierras—gradually killing and eating up their oxen for food as their food supplies dwindled.  The long and very difficult trail they had blazed was used by virtually none of the succeeding emigrants.  (See: NPS California Trail Map for the \"Bartleson-Bidwell Route\") The very successful Salt Lake Cutoff, developed in 1848, went over much the same territory in Utah but stayed further north of the Great Salt Lake and had much better access to water and grass. West of Fort Hall, the trail traveled about 40 mi on the south side of the Snake River southwest towards present day Lake Walcott (reservoir) on the Snake River.  At the junction of the Raft River and Snake River, the trail diverged from the Oregon Trail at another \"Parting of the Ways\" junction by leaving the Snake River and following the small and short Raft River about 65 mi southwest past present day Almo, Idaho and the City of Rocks.  Hudspeth's Cutoff rejoined the California trail at Cassia Creek on the Raft River about 20 mi northeast of the City of Rocks.  Nearly all were impressed by the City of Rocks—now a national reserve and Idaho State Park.  Near the City of Rocks is where the Salt Lake Cutoff rejoined the California Trail.  (For Oregon-California trail map in Idaho see: Oregon-California Trail in Idaho for trails in Wyoming, Idaho, Utah etc. see NPS National Trail Map.) The Humboldt River is fed by melting snow flowing from the Ruby and other mountains in north central Nevada and runs over 300 mi mostly westward across the Great Basin to the Humboldt Sink in western Nevada where it evaporates.  The Great Basin covers essentially all of Nevada and parts of Utah, Idaho, Oregon and California and has no outlet to the sea.  The Great Basin lies in the rain shadow of the Sierra Nevada Mountains, and what little rainfall occurs there—stays there.  The Humboldt, headed nearly straight west, provided an easily followed pathway with feed and water across the Great Basin desert.  The Humboldt was praised for having water, fishing and feed along its banks and also cursed for its barely adequate grass, meandering and often muddy channel, and hot weather.  Its water quality became progressively worse the further the river went west.  The fire 'wood' needed for cooking and making coffee consisted of occasional junipers and ever present sagebrush and willows. As found by about 1844, the trail at \"parting of the ways\" (Idaho) from the Snake River leads along the Raft River about 60 mi southwest to the head of the Raft River and the City of Rocks (now called: City of Rocks National Reserve).  The Hudspeth Cutoff and the Salt Lake Cutoff all rejoined the California Trail near the City of Rocks (For maps see NPS map California Trail:).  The trail then continued west over 7100 ft Granite Pass, which involved a steep, treacherous descent.  West of Granite Pass, the trail was in the Great Basin drainage.  Rainfall in the Great Basin either flowed to the Humboldt River, sank into the ground or evaporated.  The trail then jogged northwest until it reached Goose Creek where it headed southwest, nicking the far northwest corner of Utah and on into the future state of Nevada.  The trail then headed southwest, down Goose Creek for about 34 mi until it hit Thousand Springs Valley's creeks and springs.  The trail followed Thousand Springs Valley until it intercepted West Brush Creek and Willow Creek, which run into the Humboldt River.  This about 160 mi trail through Idaho and Nevada connecting the Snake River to the Humboldt River passed enough springs and creeks to provide the necessary feed and water for the California bound emigrants.  The trail hit the Humboldt River in northeastern Nevada near present-day Wells, Nevada.  Another branch of the trail went through Bishops Canyon and intercepted the trail about 10 mi west of Wells. () Humboldt Wells had good water and grass.  The distance from City of Rocks to Wells was about 100 mi . The trail followed the north banks of the Humboldt west for about 65 mi until it encountered the narrow 5 mi long Carlin Canyon on the Humboldt.  Here the meandering river passed through a steep section of mountains, and its river valley became very narrow or only the width of the stream bed.  Various trail guides said you would have to ford the Humboldt from four to nine times to get through the canyon.  Carlin Canyon became nearly impassable during periods of high water and a cutoff, the Greenhorn Cutoff, was developed to bypass the canyon when flooded.  West of Carlin Canyon, the trail climbed through Emigrant Pass and then descended again to rejoin the Humboldt at Gravelly Ford (near today’s Beowawe, Nevada).  At Gravelly Ford the often muddy Humboldt had a good gravel bottom and was easily forded.  There was usually plenty of grass and fresh water springs nearby.  Many stayed here a while to rest and recuperate their livestock and themselves.  After the ford, the trail divided into two branches, following the north and south banks of the river.  The trail on the north side of the river was much better, allowing an easy miss of the Reese River sink.  Those who took the south side would have to travel around a big bend in the Humboldt and then cross the usually dry alkali-laden Reese River sink.  The two branches of the Trail rejoined at Humboldt Bar (sink). At the Humboldt Sink (about 100 mi northeast of present-day Reno, Nevada) the Humboldt River disappeared into a marshy alkali laden lake that late in some years was a dry lake bed.  Near the end of the Humboldt, one of the worst sections of the California Trail showed up, the Forty Mile Desert. The Truckee River, which drains the Lake Tahoe basin and Donner Lake, and the Carson River, which drains Hope valley and adjacent mountains, are two major rivers that flow eastward out of the Sierra Nevada into the Great Basin and are only about 40 mi from the end of the Humboldt.  The Truckee River terminates in Pyramid Lake with a salinity approximately 1/6 that of sea water and supports several species of fish.  The Carson River disappears into another alkali-laden marsh called the Carson Sink.  All California Trail emigrants would have to cross the Forty Mile Desert to get to either river.  Before crossing the Forty Mile Desert, the California main trail splits with one branch going towards the Truckee River Route (or \"Truckee Trail\") (est. 1844) going roughly almost due west where Interstate 80 goes today towards the site of modern-day Wadsworth, Nevada.  The Truckee was called the Salmon-Trout River on Fremont’s 1848 map of the area.  The Carson Trail branch (est. 1848) went roughly from today's I-80 and U.S. Highway 95 junction to modern day Fallon, Nevada (near Rag Town) southwest across Forty Mile Desert to the Carson River. The Forty Mile Desert was a barren stretch of waterless alkali wasteland that stretched from Humboldt Bar to both the Carson and Truckee rivers and beyond.  The desert covered an area of over 70 mi by 150 mi , forming a fire box: its loose, white, salt-covered sands and baked alkali clay wastes reflected the sun's heat onto the stumbling travelers and animals.  What few plants there are typically covered with thorns and live low to the ground. The annual rainfall in the Forty mile desert is only 5 in It was one of the most dreaded sections of the California Trail, as emigrants reached it just when they were often weak, tired, and nearly out of food.  They were also often suffering from scurvy, and their animals and equipment were often worn out.  They were about 150 mi from the end of the 2000 mi trail and had been traveling on the trail from four to six months.  For many emigrants, Forty Mile Desert was the end of their trail.  Most emigrants got there in late August through early October—one of the hottest, driest times of the year.  If possible, they traveled the desert by night because of the great heat, but it often took over a day and a night to traverse.  About halfway across the desert on the \"Truckee Trail\", they came to a foul tasting hot springs (now a thermal power plant), but its water was usually too hot for even very thirsty animals to consume.  Many dead animals were concentrated at and in these \"bad\" water springs—often preventing access to them.  Water had to be pooled off and allowed to cool before it could be used by man or beast.  The trail on the last 8 mi the alkali flats gave way to soft alkali laden sand, six to ten inches (15–25 cm) deep and very hard for the animals to pull the wagons through.  The ground was littered with the debris of goods, abandoned wagons and dead and dying animals that had all been discarded in a desperate attempt by the pioneers to make it all the way across.  Often a wagon would be abandoned and the team would be unhooked and taken on alone to get water.  After drinking their fill of fresh water and recuperating on the other side, many would go back and retrieve their wagon—others simply abandoned them there.  Many animals (and people) died on this crossing.  A count made in 1850 showed these appalling statistics for Forty Mile Desert: 1,061 dead mules, about 5,000 dead horses, 3,750 dead cattle and oxen, and 953 emigrant graves. The main route of the California Trail until 1848 is approximated by modern Nevada State Route 233 in eastern Nevada and Interstate 80 in central and western Nevada.  The section of the trail from Wells, Nevada to City of Rocks in Idaho can be approximated by starting at Wells, going north on US Route 93 to Wilkins, Nevada and then turning onto a gravel county road 765 (Wilkins Montello Rd), that goes from Wilkins to the Goose Creek Road that goes through Nevada and back into Idaho—not advised for winter or spring use.  (Use Google Maps: Wilkins NV to Almo ID walking option to get approximate route of trail.) The high, rugged Carson Range and Sierra Nevada mountains on the eastern California border were the final obstacles that had to be overcome before westbound travelers could proceed.  The Sierra Nevada comprise a large block of weather-worn granite tilted towards the west.  They extend about 400 mi from near the Fandango Pass in the north to the Tehachapi Pass in the south.  The western slopes are scarred by glacier and river carved canyons but slope much more gradually west taking about 70 mi to fall from their rugged over 7000 ft crests to the about 25 ft elevation of the Central Valley.  The even more rugged glacier and river scarred eastern slopes are typically much more precipitous, rising to the rugged Sierra crest from their about 4000 ft base in the Great Basin in many places in less than 10 mi . Precipitation in the Sierra Nevada flows to the Pacific Ocean if it falls on the western slope of the range.  If precipitation falls on the eastern side of the Sierra crest it flows into the Great Basin where it evaporates, sinks underground or flows into lakes or sinks (mostly saline).  These sinks are often dry alkali laden flats late in the year.  The eastern side lies in a rain shadow getting much less rain than the western side.  Creeks, streams, or rivers originating east of the Sierra crest find no outlet to either the Gulf of Mexico or the Pacific Ocean.  (The water piped over the Sierras to Los Angeles is the only exception.) A second smaller but yet significant block of weather worn granite formed the Carson Range of mountains located east of today's Lake Tahoe, between the two ranges.  From the Humboldt River Route, first the Carson Range and then the Sierras would have to be passed to get to western California.  Even today there are only about nine roads that go over the Sierra and about half of these may be closed in winter.  See: National Park Service California Trail Map The Truckee Trail (established 1844 by the Stephens-Townsend-Murphy Party) over the Sierra Nevada took about 50 mi to cross Forty Mile Desert but it did have a hot springs in about the middle that could be consumed if given time to cool.  After hitting the Truckee River just as it turned almost due north towards Pyramid Lake near today's Wadsworth, Nevada, the emigrants had crossed the dreaded Forty Mile Desert.  The emigrants blessed the Truckee's cool and sweet tasting water, fresh grass and the cool shade from the first trees (cottonwoods) the emigrants had seen in hundreds of miles.  The travelers often rested themselves and their animals for a few days before proceeding.  Real shade, grass for their animals and no more bitter, soapy-tasting Humboldt River water were much appreciated.  The Truckee Trail followed the Truckee River past present day Reno, Nevada (then called Big Meadows) and went west until they encountered Truckee River Canyon near the present Nevada-California border.  This canyon was one of the paths across the Carson Range of mountains.  This steep, narrow, rock and cold water filled canyon could be traversed by wagons but required about 27 crossings of the cold Truckee River and much prying and shoving to get wagons and teams over the rocks to proceed up the canyon. In 1845, Caleb Greenwood and his three sons developed a new route that by-passed the rough and rugged trail up the Truckee River Canyon by leaving the river near the present town of Verdi, Nevada and following a ravine northwest over a 6200 ft pass across the Carson Range (followed today by the Henness Pass Road) and down to Dog Valley and from there southwest down through the present Stampede and Prosser Creek Reservoirs before rejoining the Truckee trail near today's Truckee, California.  This was about ten mile (16 km) longer route but it avoided most of the continual crossings of the rock filled Truckee River and became the main route for the \"Truckee Trail.\"  Initially, the trail passed to the north of Lake Tahoe and then followed Donner Creek to the north side of Donner Lake before ascending the precipitous climb north of the lake to Donner Pass.  There were several \"Truckee\" routes over the Sierras here over time but nearly all required the wagons to be disassembled and hoisted straight up various cliffs using multiple teams to get the wagon parts and goods to the top.  Some cliffs were ascended by tilting tall fallen trees against the cliffs and using multiple teams to pull the wagons up the improvised steep ramps.  All routes required using multiple teams to get the wagons to the top and differing amounts of wagon dis-assembly.  The trail initially crossed the Sierra crest through 7000 ft Donner Pass. From Donner summit, the trail then proceeded on a rugged cliff and rock-strewn path down the South Fork of the Yuba River—fed by an alpine lake.  The first resting spot after the pass for many was beautiful Summit Valley (now mostly covered by Lake Van Norden reservoir) a few miles from the summit. The trail down the western slope of the Sierras from Donner pass had enormous granite boulders and numerous rocky outcrops and steep slopes before passing through Emigrant Gap (California).  Here a Historical marker on Interstate 80 reads: \"The spring of 1845 saw the first covered wagons surmount the Sierra Nevada.  They left the valley, ascended to the ridge, and turned westward to old Emigrant Gap, where they were lowered their wagons by ropes to the floor of Bear River (Feather River tributary) Valley.  Hundreds of wagons followed before, during, and after the gold rush.  This was a hazardous portion of the overland emigrant trail.\"  After getting down off the ridge most emigrants stayed at Bear Valley to rest themselves and their teams and recover before traveling the approximate 70 mi remaining to Sutter's Fort.  This combination of a very steep and difficult ascent and a sharp difficult descent into Bear Valley on a route that terminated far from the gold strike regions all combined to make the Truckee Trail little used after about 1849 when the Carson Trail was developed.  The main route quickly became variations of the Carson Trail which was rough but not as difficult as the Truckee Trail and terminated in the main gold digging regions around Placerville, California. After being nearly abandoned, several branches of the Truckee Trail were eventually developed in the early 1860s for freight wagons and emigrants going both ways on the California trail.  To be more useful the Truckee Trail needed extensive and expensive work spent on it.  The route of the Truckee trail was chosen as the \"best\" way to get a railroad over the Sierras.  In 1863 the Central Pacific Railroad put about 300 men to work on the trail and spent over $300,000 working on a \"new\" toll road roughly following the original Truckee route with several new upgrades.  In 1864, the CPRR opened the Dutch Flat Donner Lake Toll Wagon Road (DFDLWR) to earn money hauling freight to Nevada while also supplying their railroad workers building the First Transcontinental Railroad from Dutch Flat California over the Donner summit and on to what today is Verdi, Nevada.  The freight going to the gold and silver strikes in Nevada at the Comstock Lode were calculated to pay about $13,000,000 per year in wagon tolls—a fraction of this was well worth pursuing.  One branch of the original Lincoln Highway over Donner summit built in about 1925 climbed the eastern Sierras to Donner Pass with multiple steep switchbacks.  Today, the part of Interstate 80 in California and Nevada from \"40 Mile Desert\", \"Truckee River\", \"Donner Pass\", \"Sacramento\" very roughly approximates the original Truckee Trail route. Starting in about 1846, the Joseph Aram party found an alternate route on the south side of Donner Lake.  Their route ran past the future town of Truckee, California up Coldstream Canyon south of Donner Lake to a 7800 ft saddle between Mt. Judah and Mt. Lincoln, about two miles (3 km) south of Donner's pass.  Here the final climb was up over the somewhat higher but less precipitous Roller Pass.  The oxen were taken to the top where they could pull on more or less level ground and about 400 ft of chain was let down to a wagon and twelve or more yoke of oxen then pulled the wagon up the final steep (about 30 degree) slope.  To minimize friction on the chain it ran over round logs (rollers) put at the top. (Roller Pass Truckee Trail Map) By not requiring dis-assembly and allowing the wagon to stay packed this was a much faster way to the top but was still tortuously slow taking two to three days or longer to get to the top with wagon, people, animals and goods.  In about 1848 or 1849 a large group of pioneers cut a switchback trail over the final steep section of Roller Pass, eliminating the need for rollers and chains to get over Roller Pass.  From the top of the pass all the pioneers could see was a rugged mountain slope headed west that would require almost 80 mi more of strenuous and dangerous effort to get to their goals. Branching off the Truckee Trail was the Nevada City Road (est 1850) to Nevada City.  This 25 mi cutoff is closely followed today by California State Route 20 from Emigrant Gap on Interstate 80 to Nevada City, California. Portions of the Nevada City Trail are evident at the top of Coyote Street, and North Bloomfield Road, just north of Nevada City.  Plaques can be found where these roads meet the top of Harmony Ridge, as this was the ridge used to descend from the high sierra, to the foothills of California. The Auburn Emigrant Road (1852) from the Truckee trail to Auburn was established to bring emigrants to the new gold diggings at Auburn, California.  Its thought to have extended from roughly present day Nevada City, California, roughly the end of the Truckee Trail, to Auburn.  California State Route 49 from Auburn to Nevada City approximates this path.  Later toll roads would be built along the rough pack trail from Auburn to Emigrant Gap (California) where Interstate 80 and the Central Pacific Railroad would later go.  In 1852 Auburn was reachable by wagons from Sacramento. The Henness Pass Road (est. 1850) was an 80 mi trail over the Sierras from today's Verdi, Nevada (Dog Valley) to Camptonville and Marysville, California.  The route was developed as a wagon toll route by Patrick Henness starting in about 1850.  The Henness Pass Road was located about 15 mi north of the Truckee trail.  The route went from The Truckee Trail in Dog Valley (near today's Verdi, Nevada) up the Little Truckee River to Webber Lake to the summit, through 6920 ft Henness Pass, along the ridge dividing the North and Middle Yuba Rivers and into Camptonville and Marysville.  After extensive road work, paid for in part by Marysville, California commercial interests, freight could be shipped by steamboat to Marysville and picked up there for shipment over the Sierras.  After 1860, extensions went southward to Carson City, Nevada and on to the Comstock Lode in Virginia City, Nevada.  Beginning in 1860 and continuing for some nine years, the road underwent major improvements, becoming one of the busiest trans-Sierra trails being favored by teamsters and stage drivers over the Placerville Route (Johnson Cutoff) because of its lower elevations, easier grades, and access to ship cargo.  Many summer camps and relay stations were created along the route at roughly seven to ten-mile (16 km) intervals to accommodate oxen, horse and mule-powered wagons.  In busy times the wagons traveled all day, filling the road, and the six or so stages traveled at night.  The route was given up by most teamsters when the Central Pacific Railroad and Virginia and Truckee Railroad were completed in 1869, and it became cheaper and easier to ship freight by the railroad(s).  People in Virginia City reported a 20–50% lower cost for supplies when the railroads were put in.  Today the Henness wagon road is a mostly gravel U.S. Forest Service road called the Henness Pass Road from Verdi Nevada to Camptonville, California. The Beckwourth Trail (est. 1850 by James Beckwourth) left the \"Truckee River Route\" at Truckee Meadows (now the site of Sparks, Nevada) and proceeded north along roughly the route of U.S. Route 395 before crossing the Sierras on what is now California State Route 70 at 5221 ft Beckwourth Pass.  After crossing the pass, the trail passed west along the ridge tops (avoiding Feather River Canyon) through present day Plumas, Butte and Yuba counties into California's Central Valley, finally terminating at Marysville.  The Oroville-Quincy Highway (California State Route 162) (partially gravel road) and California State Route 70 from Quincy to Highway 395 in general follow the path of the original Beckwourth Trail.  The Feather River Route built down the Feather River canyon between 1906 and 1909 by the Western Pacific Railroad parallels much of the route.  This road was only intermittently used by miners headed for the Northern California mines. The much used Carson Trail (est. 1848) (also called Mormon Emigrant Trail) crossed Forty Mile Desert by leaving the Humboldt Sink and skirting the western edge of the Carson Sink and hit the Carson River near modern-day Fallon, Nevada.  The Carson Trail was named after the Carson River, which was in turn named after Kit Carson, scout for John Charles Fremont who had guided the Fremont party over the Sierra through what was subsequently called Carson Pass in February 1844.  The trail across the Forty Mile Desert had the usual 6–12 inches (15–30 cm) of loose sand that made traversing the desert very hard for the often tired and worn out draft animals.  The Forty Mile Desert had water in about the middle, Salt Creek, but it was poisonous to drink.  The trail through the desert was soon cluttered with discarded supplies, thousands of dead and dying animals, abandoned wagons, and hundreds of emigrant graves.  Some estimated that only about half the wagons that started the trip across Forty Mile Desert got to the other side. The Carson Trail was initially developed by about 45 discharged members of the Mormon Battalion.  They, together with one woman, were driving 17 wagons and about 300 head of horse and cattle east to Salt Lake City in 1848.  The wagons were veterans of the 1846 or 1847 emigration as California had at that time no facilities for building anything besides simple solid wheeled ox carts.  They followed Iron Mountain Ridge southeast of what is now Placerville, California (there were essentially no settlements east of Sutter's Fort in 1848) before hitting Tragedy Spring near Silver Lake.  Here they found three of their scouts murdered.  The unknown culprits were believed to have been Native Americans.  From there the Mormon group ascended to 9400 ft at West Pass and then dropped down to Caples Lake.  From there they went through Carson Pass, elevation 8574 ft .  The only way down to the valley below was very steep ridge requiring many changes in direction with ropes and chains before they reached Red Lake at the head of Hope Valley.  To get across the Carson Range of mountains the trail then followed the Carson River, traveling about six miles (10 km) in a very rough stretch of the Carson River canyon.  The canyon was filled with boulders and rocks that had often fallen over a thousand feet into the canyon carved by the river through the Carson Range.  In some places the canyon had to be widened enough for wagons to pass and impassable boulders removed by the Mormons headed east.  They found that if they started a fire (driftwood was easily available) on boulders or impassably narrow canyon walls the hot rocks became easily breakable when doused with cold water and hit by picks and shovels.  After several applications of fire, water and industrious pick use, the parts of the trail that were formerly impassable were opened up.  In about 1853, the road through the canyon was converted intermittently to a toll road and made much easier to use when even more large boulders were removed and two permanent bridges were constructed. Travelers heading west in 1848 and later, crossed Forty Mile Desert, then followed the trail blazed by the Mormons in 1848 up the Carson River valley from what is now Fallon, Nevada, in 1850 the town was called \"Ragtown\".  Then, to get over the Carson Range, it was a very rough road through Carson River Canyon where the wagons had to be wrestled over the boulders by ropes, pry bars, levers and a few improvised bridges before the wagon trains finally entered beautiful 7100 ft Hope Valley.  Westward travelers from Hope Valley had to climb a steep, rocky and tortuous path over the back wall of a glacier carved cirque to reach Carson Pass.  The section of trail at the end of Hope Valley near Red Lake is called \"The Devil's Ladder\" where the trail has to climb over 700 ft of very steep mountain in the final half mile (1 km).  Today, a hiker's careful eye can still find notches, grooves and rust marks left by iron rimmed wagon wheels.  Nearby, trees scarred by ropes, chains and pulleys used to haul the heavy wagons up the precipitous slope, can be seen.  Travelers could get to the top of the pass in about one day of hard work, an acceptable trade-off for many emigrants.  The trail crossed the Sierra Crest through 8574 ft Carson Pass. At that time, the trail forward was blocked by the Carson Spur, a sharp ridge not passable by wagons.  To proceed, the Carson Trail had to follow the path blazed by the Mormons and make a sharp turn south at what is now Caples Lake (reservoir) and ascend 9400 ft West Pass before finally making it over the Sierra Crest.  The half day path up over West Pass was easy compared to the climb to Carson Pass and was used by thousands of wagons from 1848 to 1863.  The Carson Trail was a straightforward push to Placerville and the heart of the gold country and was a main route for emigrants for many years.  A better route variation was finally blasted out of the face of the cliffs at Carson Spur in 1863 by the Amador and Nevada Wagon Road—a toll road around Carson Spur.  Over time the Carson Trail developed many branches and toll roads for freight wagons, emigrants and miners going both ways over the Sierra. One of the major drawbacks of the Carson Trail was its elevation, with substantial sections of the trail over 8000 ft , where snow often covered it from late fall well into the spring season.  The Placerville route (Johnson Cutoff) became the preferred trail, as it was lower and extensively improved.  It could be used much of the winter season for at least horse travel. The present highway route—California State Route 88 follows much of the original Carson Trail route from the California/Nevada border for 38 miles to Mormon-Emigrant Trail/Iron Mountain Road, which goes to Pollock Pines, California and from there on to Placerville, California.  The current road avoids the highest section over West Pass by crossing the Carson Spur.  Kirkwood Mountain Resort and ski area now occupies some of the higher parts of the original Carson Trail. The Johnson Cutoff (1850–51) road (also called: Placerville Route, Lake Tahoe Route and Day's Route) from Carson City, Nevada to Placerville (then called Hangtown) used part of the Carson Trail to about present day Carson City.  This cutoff was developed by John Calhoun Johnson of Placerville in about 1850–51.  Leaving the future site of Carson City, the cutoff passed over the Carson Range by following Cold Creek (via Kings Canyon Road) and passing over 7150 ft Spooner Summit (now used by U.S. Route 50).  Once near Lake Tahoe it was forced to climb some further steep ridges by rocky spurs jutting into the lake and swampy ground (modern U.S. Highway 50 corrects both these problems).  After getting to the southern end of the lake, the trail veered west near Echo Lake and climbing steeply made it over the Sierras on 7400 ft Echo Summit (Johnson's Pass).  The steep descent from Johnson's Pass brought the trail down to Slippery Ford on the South Fork American River.  From there, Johnson's Cutoff headed westward following the river from Strawberry to today's Kyburz, California, before crossing to its north side and ascending about 1400 ft to Peavine Ridge and following its crest to get around a rocky stretch of the river.  After descending Peavine ridge the trail forded the South Fork of the American River near Pacific House.  From about today's Pollock Pines, California it followed the ridge line on the south side of the river to Placerville.  Johnson's route became a serious competitor as the main route over the Sierras.  This route, with considerable up grades and modifications, eventually became one of the main all-season routes over the Sierras since it could be kept open at least intermittently in the winter. In 1855, the California Legislature passed \"An Act to Construct a Wagon Road over the Sierra Nevada Mountains\" and appropriated $100,005 dollars to do it.  Sherman Day, a part-time California State Senator was appointed to survey the possible routes.  After extensive searches, he recommended the Placerville route (Johnson's Cutoff) as the best prospect and surveyed an improved route.  The California Supreme Court ruled in 1856 that the law was unconstitutional since it violated the state Constitution's allowable $300,000 debt limit without public vote.  Discouraged but not defeated, road proponents got El-Dorado, Sacramento and Yolo counties to kick in $50,000 for road construction.  Contracts were let and they got a new bridge across the South Fork American River ($11,300); a new sidehill road along Peavine ridge that was only 100 ft to 500 ft above the river and avoided the sharp ascents and descents there and extensive work on a new road up to Johnson's Summit (Echo Summit) and another less precipitous road down to Lake Valley.  This was the first route over the Sierraas on which extensive, public financed, improvements were made.  The new route was christened the Day Route.  Winter and its attendant runoffs raised havoc with the road and in spring 1860, when the mobs were trying to get to Virginia City, Nevada and the new Comstock Lode strike, it was reported as a barely passably trail in places (April 1860).  To get supplies to Virginia City, Nevada and the Comstock area after 1860, the road was extensively improved as a toll road to the mines in Virginia City, Nevada.  It is now followed roughly by U.S. Highway 50. In 1860–61, the Pony Express used Daggetts Pass and Johnson's cutoff route to deliver their mail—even in the winter. The Luther Pass Trail (1854) was established to connect the Carson River Canyon road with the Johnson Cutoff (Placerville Road or Lake Tahoe Road).  Luther Pass (present CA SR 89) joined the older emigrant route northeast of Carson Pass through Carson River Canyon rather than following the trails along Lake Tahoe.  Going East after descending from Echo Summit and getting to the south end of Lake Valley, it headed southeast over 7740 ft Luther Pass into Hope Valley where it connected with the main Carson Trail through Carson River canyon to get over the Carson Range. Branching off the Johnson's Cutoff (Placerville Road) was about 10 mi Daggett Pass toll road (Georgetown Pack Trail) (est. abt 1850).  This route was developed as a toll road to get across the Carson Range of mountains.  Going east it leaves The Placerville Route near what is now Stateline, Nevada (near South Lake Tahoe) and progresses up Kingsbury Grade to 7330 ft Daggett Pass and on down the Kingsbury Grade to Carson Valley.  After 1859 and the discovery of gold and silver in the Comstock Lode, this road was extensively improved and used by teamsters going to Virginia City, Nevada as it cut about 15 mi off the usual road through Carson River Canyon.  Today Nevada State Route 207 closely approximates this road. The Grizzly Flat Road (1852) to Grizzly Flat & Placerville was an extension of the Carson trail that went down the middle fork of the Consumes River to what was then a busy gold diggings at Grizzly Flat—located about 35 mi east of Placerville. The Volcano Road (1852) off the Carson Trail was made in 1852 when Amador County and Stockton merchants paid a contractor to construct a road from Corral Flat on what is now the Carson Trail (California State Route 88) to Volcano, California.  Today the cutoff is approximately followed off SR 88 by the Fiddletown Silver Lake Road, Shake Ridge Road And Ram's Horn Grade. Big Tree Road & Ebbetts Pass Road (est. about 1851–1862) from the gold mining towns of Murphys, California & Stockton, California to gold and silver mining towns/mines near Markleeville in eastern California and western Nevada.  It approximates the present California State Route 4 route over 8730 ft Ebbetts Pass.  Descriptions of the pass match those used by Jedediah Smith in the late spring of 1827 when leaving California, as well as the pass used by John Bidwell and the Bartleson-Bidwell Party on their emigration to California in 1841. Originally a free pack trail route when first used in about 1851 by \"Major\" John Ebbetts, it was improved to a wagon road and became a toll road to silver mining towns in eastern California and western Nevada from 1864 through 1910, and then a free county road in 1911.  It was used by very few emigrants to California. The road reverted to a free county road in 1911 and was accepted into the California State Highway system in 1926 as California State Route 4.  It was not until the early 1950s that the road over Monitor Pass to U.S. Route 395 was completed, connecting the eastern terminus of State Route 4 to U.S. Route 395 via California State Route 89 near the community of Topaz, California.  Today's Ebbetts Pass National Scenic Byway is a very scenic drive but one of the least traveled highways across the Sierra Nevada Mountains.  It is anchored at either end by two state parks—Calaveras Big Trees State Park and Grover Hot Springs State Park.  It passes through the Stanislaus and Humboldt-Toiyabe National Forests. Today's Ebbetts Pass road, SR 4, has an extensive section of highway that is less than two lanes wide with no dividing line.  It also has some very steep sections, particularly on the eastern Sierra slopes, with several sharp hairpin corners.  It is not recommended for vehicles towing long trailers or commercial truck traffic.  Watch out for bicyclists and motorcyclists. Both the \"Carson River\" and \"Truckee River\" trails eventually ended up at Sutter's Fort in Sacramento, California.  In 1848 most emigrants developed and used this route.  In 1849 about one-third of all emigrants used the Carson Trail with later years many more using it.  Starting in 1848, many left the main trail to stay in a mining district(s) or town(s) that developed along or off the trail(s). In 1852, the Sonora Road was opened from the Carson Trail to Sonora, California by the Clark-Skidmore Company.  From the Humboldt Sink it crossed Forty Mile Desert to the Carson River and then went almost due south to the Walker River, which it followed to the Sierras before making the very steep (about 26 degrees in parts) and rugged ascent to 9625 ft Sonora Pass. From there the road drops down twisting forested mountain ridges to Sonora.  This was the highest road developed across the Sierras—and still a very scenic drive.  (modern Tioga Pass out of Yosemite National Park is slightly higher) California State Route 108 between Sonora and U.S. Highway 395 roughly approximates the route of the Sonora Road over the Sierras.  This route was little used after about 1854. The Applegate-Lassen Cutoff or Applegate Trail (est. 1846–48) left the California Trail near the modern-day Rye Patch Reservoir in what is now called Lassen's meadow on the Humboldt River in Nevada.  The trail headed northwest until it could pass north of the worst of the California Sierra Nevada mountains.  The trail passed through Rabbithole springs, crossed the Black Rock Desert and High Rock Canyon before finally (after nearly 100 mi of desert travel) arriving at Surprise Valley and climbing steeply to go over 6300 ft Fandango Pass.  From there, travelers faced a descent down a very steep hill to Fandango Valley on the shores of Goose Lake on the Oregon-California border.  Just south of Goose Lake the combined Oregon-California trail split at Davis Creek.  The Applegate Trail branch proceeded northwest into southeastern Oregon along the Lost River before turning almost due north roughly along the route of today's Interstate 5 to go the Willamette Valley in Oregon. The California branch, the Lassen Cutoff (established in 1848 with a help from eager Oregon gold seekers), proceeded southwest through the Devil's Garden along the Pit River and passed east of Lassen Peak until it eventually swung west at present day Lake Almanor (reservoir) and arrived at Lassen's rancho near the Sacramento River.  From there it followed the river south in the Central Valley (California) about 110 mi to Sutter's Fort and the gold fields.  This road was so rough that today in many places it can only be traveled by the occasional forest trail and hiking paths. The Applegate-Lassen Cutoff was almost 150 mi further than other routes and took roughly fifteen to thirty days of additional travel to get to Sutter's Fort, which was unknown to nearly all who initially took it.  It avoided Forty Mile Desert and many of the high passes and difficult climbs of other routes, but it introduced some difficult desert crossings and had very limited grass and water.  For most it was a very bad choice of routes.  Much of the traffic on this alternate route in the early days was due to confusion, as enough travelers turned off on this route to make many of those following think wrongly that it was the main route.  Most had dispensed with hiring guides who actually knew the trail by then and almost none had any written guides about the Applegate-Lassen Trail.  Most did not realize for several days or even weeks they had made a wrong turn.  It is estimated that in 1849 about 7,000 to 8,000 (about one-third of California trail travelers that year) inadvertently took this much longer trail and found that the earlier travelers and their animals had stripped the desert bare and set fires that had burned most available grass.  There was almost no forage left for their animals, and they lost many hundreds of animals and suffered severe hardships and several deaths, as many ran out of supplies before rescue parties sent out from Sutter's Fort could reach them.  By 1853, other faster, easier, and shorter routes had been worked out, and traffic on the Applegate-Lassen cutoff declined to a trickle. In 1851, William Nobles surveyed a shorter variation of the Applegate–Lassen trail.  It was developed to make it easier to get to Shasta, California (which paid him $2,000) in the Central Valley and was first used in 1852.  The route, called Noble's Road, left the main trail near Lasson's meadow (now Rye Patch Reservoir) in Nevada, and bypassed most of the large Applegate-Lassen loop north almost to Goose Lake (Oregon-California) on the Oregon-California border.  This reasonably easy wagon route followed the Applegate-Lassen Trail to the Boiling Spring at Black Rock in Black Rock Desert and then went almost due west from there to Shasta, California, in the Central Valley via Smoke Creek Desert to present-day Honey Lake and present-day Susanville before passing North of Mt. Lassen and on to Shasta (near present-day Redding).  The route today can be approximated by taking Nevada State Route 49 (Jungo Road) from Winnemucca, Nevada, to Gerlach, Nevada, and from there to Susanville via Smoke Creek Road.  From there, California State Route 44 through Lassen Volcanic National Park to Redding approximates the rest of the trail.  It depended upon springs for water, as there were no dependable creeks along most of the route.  East of Mt. Lassen, it used part of Lassen's road in reverse over a distance of about 20 mi .  In that section of trail, a traveler going to Shasta City might travel north passing another traveler going south to Sutter's Fort California. In 1857, Congress appropriated $300,000 for building a wagon road to California and to establish the Fort Kearny, South Pass and Honey Lake Wagon Road.  Exactly why the road was to terminate at Honey Lake near Susanville is a legislative mystery, since very few went that way in 1857 or later.  The road was built in response to pressure from California Congressmen who wanted a good road to California, preferably one that bypassed Forty Mile Desert.  The first part of the route was surveyed by Frederick W. Lander working under William Magraw.  In 1858, Lander guided several hundred workers who built the Landers Cutoff passing the Green River well north of the established ferries, over Thompson Pass into Star Valley Wyoming, and from there up Stump Creek and on to Fort Hall in Idaho.  In 1860, Landers was instructed to find a new route north of the Humboldt.  To help the emigrants leaving the main trail at Lassen's meadow and going to Honey Lake, Lander had two large reservoir tanks built at Rabbit Hole and Antelope Springs.  These reservoirs helped Nobles Road keep its status as an emigrant trail, but only the few emigrants interested in going to Northern California used it. Initially, the trails across the Sierras were improved only enough to make them barely passable.  The main initial attraction for improved toll roads across the Sierras was Virginia City, Nevada and the Comstock Lode strike in the Washoe district of Nevada in 1859.  This strike rapidly developed after about 1860 when they found out how potentially massive the gold and silver deposits there were.  A good, easily traversed road was needed to haul in miners, other workers, supplies, etc., and road improvements and maintenance could be financed by the road tolls.  The Comstock Lode mines would require millions of dollars of investment to buy and ship in thousands of tons of mining supplies, food and firewood to supply the mines.  Almost no cities existed in Nevada then, and Virginia City would be Nevada's first major city .  In addition, until the mills could be built, high grade ore was shipped to California for processing.  The gold and silver ore there required developing a new massive industrial scale mining operation by multiple mines to get it out.  New techniques would have to be developed to get the silver out, the Washoe process.  New techniques were required to support the mines, which were often in weak ground.  The square-set timber process ultimately used millions of board feet of lumber.  Millions of gallons of water per day had to be pumped out of the mines usually by massive steam powered Cornish pumps, which ultimately had over 3000 ft long pumping rods that weighed over 1500000 lbs and used over 33 cords of wood fuel per day, each.  In addition, the mine hoists and up to 75 mills were all run with steam engines, all using copious amounts of wood.  Winter heating consumed more thousands of cords of wood.  All these thousands of cords of firewood had to be freighted in.  The heavy firewood and timber needs of the \"Comstock Lode\"' strike lead to much of the Carson Range and part of the Sierra Nevada being extensively denuded of timber.  As the mines developed they went into progressively hotter regions until they were mining in up to 130 degree Fahrenheit (55 degree Celsius) temperatures.  To survive in these temperatures the miners used up tons of ice (frozen in the winter and hauled in) each day.  The gold and silver found more than paid for the wages, development, lumbering and shipping costs.  In the next twenty years, over $300,000,000 (in 1880 dollars) worth of gold (at about $20/oz.)  and silver (at about $1.00/oz.)  were extracted. Starting in 1860, many emigrant trails over difficult terrain and streams were improved and replaced by toll roads and bridges built and financed by private entrepreneurs and some cities.  Later, other strikes in western Nevada and eastern California would give impetus to new toll roads to a new mining town. Initially, the two main toll roads over the Sierras that were improved and developed were the Henness Pass Route from Nevada City, California to Virginia City, Nevada and the Placerville Route, (also called Johnsons's Cutoff and the Tahoe Wagon Road) from Placerville, California to Lake Tahoe and over the Carson range to Virginia City.  The Placerville route would be the first route that could be kept at least partially open even in winter.  The Henness Pass route was partially built by a $25,000 grant from Marysville and Nevada City.  The Placerville route was somewhat shorter at about 100 mi and had the additional advantage that freight could be shipped to Folsom, California about 23 mi out of Sacramento on the Sacramento Valley Railroad—built in 1856.  This freight could then be transferred onto wagons that had good roads to Placerville and later clear to Virginia City.  In their heyday from about 1861–1866, these roads had major improvements made at many thousands of dollars per road and paid the salaries of a small army of employees that worked on building and maintaining different sections of the road and the service centers located roughly every ten miles.  A typical wage then was from $1.00 to $2.00/day for laborers, teamsters etc., with higher wages when men were scarce.  The miners in Virginia City were paid the very high wage of $4.00/day.  A team could be hired for a few dollars/day.  The storm-induced and spring runoff gullies and ruts in the roads would have to be filled in, culverts installed, streams and canyons bridged, gravel hauled in to fill the soft spots in the road, rough spots evened out and road cuts made in the side hills to get around the hills.  The only tools available to build and maintain roads then were hand tools: picks, shovels, crow bars, hoes, axes, wheelbarrows, hand saws, etc. used with a lot of human sweat.  This was aided by judicious use of black powder to eliminate really bad spots.  The only power available was human, ox or mule pulled plows, wagons and mule powered dump carts.  The railroads would be built with essentially the same tools.  Every spring, extensive repairs costing additional thousands would be needed to repair the ravages of winter and the gully washing spring thaws. During summer daylight hours, the roads were often packed for miles in busy spots with heavily laden wagons headed east and west usually pulled by up to ten mules.  Wagons headed west were mostly empty, but some carried the literally tons of silver mined in the Washoe district (Virginia City) back to San Francisco.  Passing spots were located frequently along the roads to allow two way traffic.  The roughly 200 mi round trip over the Henness Pass road or the Placerville Route could be done by freight wagons in about 16–18 days. Mail and passenger stages usually went at night to avoid most of the slower (~3 mph) wagon traffic.  As counted in 1862, the average number of passengers carried each day on the Placerville Route's Pioneer Stage Company line with 12 coaches and 600 horses averaged about 37 passengers/day.  Horses were changed at roughly every 10–20 mile intervals and the drivers often vied to make the fastest time.  A typical stage trip took approximately 18 hours from Placerville to Virginia City with an 18-hour return.  Holdups, stage wrecks and other accidents were an occasional occurrence on both routes.  In 1864, stage receipts were estimated by newspapers of the time to be about $527,000 at $27.00 per passenger on the Placerville route.  The Henness Pass road's California Stage Company and Nevada Stage Line carried somewhat fewer passengers.  Both stage coach routes together were estimated by newspapers of the day to have gross receipts, including mail subsidies, of over $1,000,000 total in 1864.  The typical freight charges were about $120.00 to $160.00/ton (6–8 cents/pound) with am additional $20.00 to $30.00 toll charges/wagon.  A Central Pacific Railroad agent (J. R. Atkins) estimated, after counting all Placerville toll road traffic in August and September 1862, that the freight charges to Virginia City over the Placerville route would have been about $5,000,000, which delivered roughly 20000000 lb of freight in eight weeks.  Similar amounts presumable were shipped over the Henness pass route.  In a given month during the busy season, over 2,000 wagons (sometimes up to three wagons were pulled by one team) and over 10,000 draft animals (mostly mules) per month were counted on the Placerville road alone in 1862.  The Placerville Route and Henness Pass route even had sprinkler wagons that wetted down the road during daylight hours about every three hours to minimize dust and wear and tear on the road.  There were 93 hotels, stage relay stations and lodging stations located along the Placerville Route with similar stations along the Henness Pass route located at roughly ten mile (16 km) intervals.  The teamsters stayed at these locations at the end of each day's travel.  The Placerville Route tried to stay open in winter to at least horse traffic and was only closed temporarily by winter storms.  The Pony Express used this route in the summer and winter of 1860–61.  The net profit per year from these toll roads was probably over $100,000/yr in 1862 and increasing every year. Competition arrived in July 1864 when the Central Pacific railroad entrepreneurs opened Dutch Flat and Donner Lake Wagon Road (DFDLWR) This route was opened over much of the route the new Central Pacific railroad would use over Donner Summit.  This route followed much of the original Truckee Trail route with the major exception that its large work force could smooth and straighten the route and make major side hill cuts that built around many of the steep grades and over or through major obstacles.  Below Dutch Flat where the original Truckee Trail diverged from modern roads to descend into a steep canyon and use the Bear River ridge to get around impassable terrain the Dutch Flat and Donner Lake Wagon Road (and the Central Pacific track) was cut around many of the sharp ridges that had prevented a wagon road there.  Despite the Dutch Flat and Donner Lake Wagon Road name, the railhead would not actually reach Dutch Flat (about 60 mi east of Sacramento) until July 4, 1866, as it was built over difficult terrain and required very heavy construction to reach Dutch Flat.  Their toll road was built with a reported $200,000 (1864 dollars) investment and involved about 350 men and many teams of animals working for over ten months.  Initially, the road extended from the railhead (then Newcastle, about 30 mi east of Sacramento) over Donner summit to Verdi, Nevada where it joined the road developed by the Henness Pass road to Virginia City, Nevada.  After it was opened, this route was advertised by the California Stage Company to reach Virginia City in three hours less time (about 17 hours) than the Sacramento-Placerville Route and have lower grades and wider roads, (20 ft ), than the other routes.  This new toll road was developed so the new railroad could earn money even as it was being built as well as supplying their own hefty transportation needs.  As the railroad construction progressed over the Sierras, freight could be shipped to near the railhead then transferred to wagons that could use the new toll road to complete their journey.  It slowly took over much of the shipping to Virginia City and the Washoe district as the railroad progressed over Donner Summit (December 1868) and into Truckee and beyond.  Today's Interstate 80 goes over much of the same route and is the main transportation artery over the Sierras in northern California. Tolls existed on nearly all Sierra trail crossings as improvements were made; but most other roads after the two (later three) main toll roads were developed, were relatively lightly used.  A typical toll from Sacramento to Virginia City Nevada was about $25.00 to $30.00 round trip for a freight wagon carrying at least 2000 lb up to 6000 lb of cargo with additional tolls possible for additional animals over six (usually $1.50/animal) and some additional bridge tolls were also needed.  Some teams had up to ten animals pulling up to three wagons trailered behind each other.  Some counties and cities did help build some roads but mainly granted franchises so toll road operators could build and maintain good roads and bridges with assurances of minimum competition and compensation.  Some resented the toll charges, but the users of the road paid for the improvements and maintenance on the roads, and taxpayers of this era in general were very hesitant to pick up the very hefty cost of building and maintaining good \"free\" roads. Nearly all of the heavy wagon freighting and stage use over the Sierras ceased after the completion of the Central Pacific and Virginia Truckee railroads in 1869.  The ongoing massive needs for millions of board feet of Sierra timber and thousands of cords of firewood in the Comstock Lode mines and towns would be the single major exception, although they even built narrow gauge railroads to haul much of this.  Stages and wagons were still needed and used for the many cities not serviced by the railroads and the stage and freight lines continued in business.  The first \"highway\" established by the counties was the Placerville toll route that was bought by the counties and made a \"free\" (taxpayer financed) road in 1886.  The first \"highway\" established by the state government was this same Placerville wagon road over the Sierras after it was bought by the state in 1896.  This road eventually became U.S. Route 50. Due to lack of use after 1869, most of the wagon roads across the Sierras were allowed to deteriorate until, by the early 20th century, many were again next to impassable to wagons.  The railroad served nearly all trans-Sierra passenger and freight needs.  The arrival of the automobile in the early 20th century revived the need for good Sierra roads.  By 1910, only the Placerville route (now a state highway) was maintained well enough for car and truck traffic to get over the Sierras.  The Truckee Trail that was modified and upgraded to the Dutch Flat and Donner Wagon Road over Donner summit had deteriorated so badly that the road had to be extensively rebuilt and relocated to become passable for cars or trucks.  After extensive upgrades and modifications this road would become U.S. Route 40 and later Interstate 80. Others besides emigrants were also using parts of the trail(s) for freighting, extensive livestock herding of cows, sheep and horses, stage lines, and briefly in 1860–61 the Pony Express.  Traffic in the California-Nevada area was often two ways as the fabulously rich mines like the Comstock Lode (found in 1859) in Nevada and other gold and silver discoveries in eastern California, Nevada, Idaho and Montana needed supplies freighted out of California.  The completion of the Panama Railroad in 1855 along with fast steamboats traveling to both the Pacific and Atlantic ports in Panama made shipping people and supplies from Europe and the east coast into California and from there to new gold and silver mining towns reasonably inexpensive.  New ranches and settlements located along the trail(s) also needed supplies freighted in.  Gold and silver discoveries in Colorado often had their supplies shipped in from the east coast and midwest along parts of the various emigrant trails.  Steamboats delivered supplies to Missouri River ports from both sites in the eastern United States but also from Europe as New Orleans, Louisiana and others allowed cheap and reasonably fast ship connections to Europe.  Before the railroads came in, horse, mule or oxen pulled freight wagons from either California or the midwest were the only way new supplies from the east, midwest and Europe could get to several states.  Gold, silver, livestock etc. were shipped back to Europe and the east coast to pay for these supplies. Toll bridges and ferries were active at nearly all previously dangerous river crossings as the trail became not only safer but quicker.  Stage coaches by changing teams at newly established stage stations about every ten to (16 km) twenty miles (32 km) and traveling day and night, could make a transit from the Missouri River to California in 25 to 28 days.  After 1861, telegraph relay stations and their crews joined these stage stations along much of the route.  Forts and army patrols helped protect these various stations from Indian attacks throughout the U.S. Civil War period and later.  Regular wagon trains that only had one team per wagon and stopped at night cut their transit time from about 160 days in 1849 to 120 days in 1860.  The tolls on the various bridges, ferries and toll roads typically averaged about $30.00 per wagon by 1860.  All these toll bridges, roads and ferries shortened the journey west by about 40 days and made it much safer as bad parts of the trail were improved and dangerous river crossings were done now by ferries and toll bridges that cost money but were much safer and faster.  Nearly all improvements were financed by tolls on the various roads, bridges and ferries. The ultimate competitor to the California Trail showed up in 1869 when the First Transcontinental Railroad was completed.  The combined Central Pacific Railroad and Union Pacific Railroad carried traffic from the East into California, and the Virginia and Truckee Railroad carried traffic from Reno to Virginia City.  The trip from Omaha, Nebraska to California became faster, cheaper, and safer, with a typical trip taking only seven days and a $65 (economy) fare.  Even before completion, sections of the railroad were used to haul freight and people around Nebraska, Wyoming, Utah, Nevada and California.  The price of many goods imported from the east dropped by 20–50% as the much cheaper transportation costs were mostly passed on to the consumers.  The California trail was used after 1869 by a few intrepid travelers, but it mostly reverted to local traffic traveling to towns or locations along the trail. The Great Basin and the Sierra Nevada mountains through which the trail passed were first explored by British and American fur trappers.  U.S. trapper, explorer and fur trader Jedediah Smith led two expeditions into California and over the Sierra Nevada mountains and back from 1826 to 1829.  It is believed that on his first trip he used the Mojave River route (later part of the Old Spanish Trail) to get into California and 8730 ft Ebbetts Pass when leaving California in the spring 1827.  On Smith's second trip he entered California the same way and left through Oregon.  Smith was killed in 1831 before he could publish his explorations, which were only known by word of mouth. In 1828–29, Peter Skene Ogden, leading expeditions for the British Hudson's Bay Company, explored much of the Humboldt River area—named by him the Mary's River.  The results of these explorations were held as proprietary secrets for many decades by the Hudson's Bay Company.  In 1834 Benjamin Bonneville, a United States Army officer on leave to pursue an expedition to the west financed by John Jacob Astor, sent Joseph R. Walker and a small horse mounted party westward from the Green River in present-day Wyoming.  They were charged with the mission of finding a route to California.  Walker confirmed that the Humboldt River furnished a natural artery across the Great Basin to the Sierra Nevada mountains.  He eventually got across the Sierra Nevada mountains in southern California over Walker Pass.  Bonneville had the account of his and Walker's explorations in the west written up by Washington Irving in 1838.  (See: \"The Adventures of Captain Bonneville\"). A few hundred mountain men and their families had been filtering into California for several decades prior to 1841 over various paths from Oregon and Santa Fe.  The first known emigrants to use parts of the California Trail was the 1841 Bartleson-Bidwell Party.  They followed the Humboldt River across Nevada and eventually made it into northern California.  Other parts of this party split off and were one of the first sets of emigrants to use the Oregon Trail to get to Oregon.  The California-bound travelers, striking out from the Snake River and passing into Nevada, missed the head of the Humboldt River there.  They abandoned their wagons in eastern Nevada and finished the trip by pack train.  After an arduous transit of the Sierras (its believed over Ebbetts Pass), members of this group later founded Chico, California in the Sacramento Valley.  In 1842 (a year without any known California Trail emigration), Joseph Chiles, a member of the Bartleson-Bidwell Party of 1841, returned with several others back east.  In 1843, Chiles led a party (of seven he eventually would lead) back to California.  At Fort Hall he met Joseph Reddeford Walker who he convinced to lead half the settlers with him traveling in wagons back to California down the Humboldt.  Chiles led the rest in a pack train party down the Malheur River to California.  Walker's party in 1843 also abandoned their wagons and finished getting to California by pack train. In 1844, Caleb Greenwood and the Stephens-Townsend-Murphy Party became the first settlers to take wagons over the Sierra Nevada and into California over what became the Truckee Trail.  They abandoned their wagons in the early snow in the winter of 1844/1845 and finished retrieving their wagons from the mountains in the spring of 1845.  In 1845, John C. Frémont and Lansford Hastings guided parties totaling several hundred settlers along the Humboldt River portion of the California Trail to California.  They were the first to make the entire trip by wagon in one traveling season.  In 1846 it is believed that about 1,500 settlers made their way to California over the Truckee branch of the California Trail—just in time to join the war for independence there.  Many of the 1845 and 1846 emigrants were recruited into the California Battalion to assist the U.S. Navy's Pacific Squadron with its sailors and marines in the fight for California's independence from Mexico. The last immigrant party in 1846 was the Donner Party, who were persuaded by Lansford Hastings, who had only traveled over the route he recommended by pack train, to take what would be called the Hastings Cutoff around the south end of the Great Salt Lake.  At the urging of Hastings, the Donner's were induced to make a new 'cutoff' over the rugged Wasatch mountains where there were no wagon trails.  Hastings recommended this despite the fact that he had successfully led about 80 wagons in the Harlan-Young party who blazed a new trail down the rugged Weber River to the Utah valley—he thought the Weber River route was too rugged for general travel.  The Donner party spent over a week's hard work scratching a barely usable path across the Wasatch mountains, getting ever further behind Hastings's Party.  When the Mormons tried using the Donner blazed trail in 1847 they were forced to abandon most of it and cut (with many more settlers available to clear trees and brush) a new and much easier to use trail (part of the Mormon Trail) to the Salt Lake Valley—taking 10 days of hard work to get through the Wasatch mountains. The Hastings Cutoff went across about 80 mi waterless salt flats south of the Great Salt Lake.  While crossing the Salt Flats the Donner party, even though following the tracks of the Harlan-Young party, lost several wagons and many animals.  After crossing, they spent almost a week at Donner Springs near the base of Pilot Peak (Nevada) in Box Elder County, Utah trying to recover themselves and their animals.  They had to use even more time skirting around the Ruby Mountains in Nevada before hitting the Humboldt River and the regular trail.  Altogether, crossing the Wasatch mountains and salt flats and skirting the Rubys cost them about three weeks more time over what staying on the main trail would have taken.  They and their surviving wagons and teams were in poor shape.  To add insult to injury the Donner-Reed party encountered others that had left Ft. Bridger after them, stayed on the main trail to Ft. Hall, and were now ahead of them.  They were the last emigrants of 1846 to arrive in California—east of the Sierras and just as it started to snow.  They were stranded by early snowfall in the eastern Sierras near what is now called Donner Lake and suffered severely including starvation, death and cannibalism (See: Donner Party). The first \"decent\" map of California and Oregon were drawn by Captain John C. Frémont of the U.S. Army's Corps of Topographical Engineers, and his topographers and cartographers in about 1848.  Fremont and his men, led by his guide and former trapper Kit Carson, made extensive expeditions starting in 1844 over parts of California and Oregon including the important Humboldt River and Old Spanish Trail routes.  They made numerous topographical measurements of longitude, latitude and elevation as well as cartographic sketches of the observable surroundings.  His map, although in error in minor ways, was the best map available in 1848.  John C. Frémont gave the Great Salt Lake, Humboldt River, Pyramid Lake, Carson River, Walker River, Old Spanish Trail etc. their current names.  The Truckee River (called the Salmon-Trout River by Fremont) in California and Nevada was mapped.  Lake Tahoe is shown but left unnamed.  The major rivers in California are shown, presumably given the names used by the trappers and Mexican and foreign settlers there.  The Humboldt was named (after the great explorer Alexander von Humboldt).  Fremont and his topographers/cartographers did not have time (it would take literally decades of work to do this) to make extensive explorations of the entire Sierra Nevada range or Great Basin.  Details of the Sierra Nevada and Great Basin concerning the best passes or possible emigrant routes for wagons would be explored and discovered from about 1846 to 1859 by numerous other explorers. Fremont, together with his wife Jessie Benton Fremont, wrote an extensive account of his explorations and published the first \"accurate\" map of California and Oregon making them much more widely known.  The U.S. Senate had 10,000 copies of Fremont’s map and exploration write-up printed.  How many of these maps were actually in the hands of early immigrants is unknown. The trickle of emigrants before 1848 became a flood after the discovery of gold in California in January 1848, the same year that the U.S. acquired and paid for possession of the New Mexico Territory and California Territory in the Treaty of Guadalupe Hidalgo, which terminated the Mexican–American War.  The gold rush to northern California started in 1848 as settlers in Oregon, southern California, South America and Mexico headed for the gold fields even before the gold discovery was widely known about in the east.  The public announcement of the gold discovery by President Polk in late 1848 and the display of an impressive amount of gold in Washington induced thousands of gold seekers in the east to begin making plans to go to California. By the spring of 1849 tens of thousands of gold seekers headed westward for California.  The California Trail was one of three main ways used as Argonauts went by the California Trail, across the disease ridden Isthmus of Panama and around the storm tossed Cape Horn between South America and Antarctica to get to California.  The 1848 and 1849 gold rushers were just the first of many more as many more sought to seek their fortunes during the California Gold Rush, which continued for several years as miners found about $50,000,000 dollars worth of gold (at $21/troy oz) each year. 1849 was also the first year of large scale cholera epidemics in the United States and the rest of the world, and thousands are thought to have died along the trail on their way to California—most buried in unmarked graves in Kansas and Nebraska.  The 1850 census showed this rush was overwhelmingly male as the ratio of women to men in California over 16 was about 5:95 Combined with the settlers that came by sea, the California settlers that came over the California Trail by 1850 were sufficient (at about 93,000) for California to choose its state boundary, write a Constitution, and apply for and receive statehood, which it did as a free state. The busy times on the trail were from late April to early October with almost no winter traffic (several parts of the trail were impassable in winter).  In busy years the trail was more like a large immigrating village hundreds of miles long, as thousands used the same parts of the trail in the same short traveling season.  Many signed up to wagon trains that traveled the whole route together.  Many large trains broke up into several smaller trains to take better advantage of available camping spots, traveling schedules, conditions of teams, etc..  Others, usually traveling as family groups of various sizes, joined and left various trains as their own schedule, inclinations, altercations and traveling conditions dictated.  Because of the numerous scrabbles often present in a given wagon train, a typical train may have several different leaders elected at various times to lead the train.  Possible Indian troubles was about the only condition that kept large trains together for mutual protection.  The 1849 travelers went in a wet year and found good grass almost the entire way and that most had taken too many supplies.  The 1850 migration was in a dry year and with roughly double the amount of travelers on the trail it suffered seriously from lack of grass and good water.  To make things worse many had cut down on the amount of supplies they carried and began running out of food as they traveled down the Humboldt.  Emergency relief expeditions led by the U.S. Army and others from California managed to save most of these late 1850 travelers. Some of the trail statistics for the early years were recorded by the U.S. Army at Fort Laramie, Wyoming from about 1849 to 1855.  None of these original statistical records have been found, as the army lost them or destroyed them.  Some diary references to these records and some partial written copies of the Army records as recorded in several diaries have survived.  Emigration to California spiked considerably due to the 1849 gold rush.  Following the discovery of gold, California remained the destination of choice for most emigrants on the trail up to 1860, with almost 200,000 people traveling there between 1849 and 1860. Travel after 1860 is even less well known as the U.S. Civil War caused considerable disruptions on the trail.  Many of the people on the trail in 1861–1863 were fleeing the war and its attendant drafts in both the south and the north.  Trail Historian Merrill J. Mattes has estimated the number of emigrants for 1861–1867 given in the total column of the above table. These estimates, however, may be low, since they only amount to an extra 125,000 people, and the 1870 census numbers show an increase of 200,000.  This ignores most of California's population increase from the excellent sea and rail connections across Panama that existed by then. Mormon emigration records after 1860 are reasonably well known, as newspaper and other accounts in Salt Lake City give most of the names of emigrants that arrived each year from 1847 to 1868.  Gold and silver strikes in Colorado, Oregon, Idaho, Nevada and Montana also caused a considerable increase in people using the trail(s) often in directions different from the original trail users. Though the numbers are significant in the context of the times, far more people chose to remain at home in the 31 states.  Between 1840 and 1860, the population of the United States rose by 14 million, yet only about 300,000 decided to make the trip.  Between 1860 and 1870 the U.S. population increased by seven million with about 350,000 of this increase being in the Western states.  Many were discouraged by the cost, effort and danger of the trip.  Western scout Kit Carson reputedly said, \"The cowards never started and the weak died on the way.\"  According to several sources 3–10% of the immigrants are estimated to have perished on the way west. These census numbers show a 363,000 population increase in the western states and territories between 1860 and 1870.  Some of this increase is due to a high birth rate in the western states and territories, but most is due to emigrants moving from the east to the west and new immigration from Europe.  Much of the increase in California and Oregon is due to emigration by ship, as there were fast and reasonable low-cost transportation via east and west coast steam ships and the Panama Railroad after 1855.  The census numbers imply at least 200,000 emigrants (or more) used some variation of the California/Oregon/Mormon/Bozeman trail(s) to get to their new homes in the 1860–1870 decade. The cost of traveling over the California or Oregon trail and its extensions varied from nothing to a few hundred dollars per person.  Women seldom went alone outside of family groups and were a distinct minority in the West for decades.  The cheapest way to travel the trail was to hire on to help drive the wagons or herds, allowing one to make the trip for nearly nothing or even make a small profit.  Those with capital could often buy livestock in the Midwest and drive the stock to California or Oregon and usually make good money doing it.  About 60–80% of the travelers were farmers, and as such already owned a wagon, livestock team and many of the necessary supplies, this lowered the cost of the trip to about $50.00 per person for six months food and other items.  Families often planned for a trip months in advance and made many of the extra clothing and other items needed.  Individuals buying most of the needed items would end up spending between $150 and $300 per person.  Some who traveled in \"grand\" style with several wagons and servants could spend much more. As the trail matured, additional costs for ferries and toll roads were thought to have been about $30.00 per wagon or about $10.00/person. The route West was arduous and filled with many dangers, but the number of deaths on the trail is not known with any precision, and there are only wildly varying estimates.  The estimates are made even harder by the common practice then of burying people in unmarked graves that were intentionally disguised to avoid them being dug up by animals or Indians.  Graves were often put in the middle of a trail and then run over by their livestock to make them difficult to find.  Diseases like cholera were the main killer of trail travelers with up to 3% (or more) of all travelers (6,000 to 12,000+ total) dying of cholera in the cholera years of 1849 to 1855.  Indian attacks were probably the second leading cause of death with about 500 to 1,000 being killed from 1841 to 1870.  Other common causes of death included: freezing to death (300–500), drowning in river crossings (200–500), getting run over by wagons (200–500), and accidental gun deaths (200–500). A significant number of travelers were suffering from scurvy by the end of their trips.  Their typical daily diet of flour, dried corn and salted pork/bacon for months on end had very few anti-scurvy ingredients.  Scurvy is a nutritional deficiency disease that can lead to death if not treated.  Treatment consists of a proper diet.  A known scurvy prevention, as worked out by most navies in the 18th century, was found in a diet that contained dried and fresh fruit or vegetables (as finally discovered in 1932—vitamin C rich foods).  The diet in the mining camps was also initially poor in fresh or dried vegetables and fruit, which indirectly led to early deaths of many Argonauts.  Some believe scurvy deaths from poor nutrition may have rivaled cholera as a killer, with most deaths occurring after they reached California.  In a few years, as the gold strikes continued, nearly any and all foods were grown or imported into California—for sale if you had the gold.  How to prevent and treat scurvy was common knowledge in some circles but far from universally known, taught or appreciated as the hazard it was.  The Chinese Argonauts with their insistence on many more vegetables in their diet fared much better. Accidents with Animals serious enough to cause death include: kicks by animals, (getting hit by a shod hoof could be deadly), falling off the horse or mule and hitting your head, getting hit by a falling horse or mule, stampedes, bear attacks, wounded animal attacks, etc..  Deaths probably numbered from 100 to 200 or more along the trail from 1847 to 1869.  Because of the large number of animals on the trail and their close interaction with people accidents with animals that only resulted in minor injury were many times higher. Miscellaneous deaths included deaths by: homicides, lightning strikes, childbirths, snake bites, flash floods, falling trees, wagon wrecks etc. probably numbered from 200 to 500 deaths or more along the trail. Travelers rarely made the entire trip without one or more in their traveling group dying.  According to an evaluation by Trail Authority John Unruh, a 4% death rate or 16,000 out of 400,000 total pioneers on all trails may have died on the trail while making the trip. One of the main enduring legacies of the Oregon and California Trails is the expansion of the United States territory to the West Coast.  Without the many thousands of United States settlers in Oregon and California with their \"boots on the ground\" and more thousands on their way each year, it is highly unlikely that this would have occurred.  Surprising to some, the Oregon and California Trails were both established as known emigrant routes in 1841 by the same emigrant party.  In 1841 the Bartleson-Bidwell Party group set out for California, but about half the party left the original group at Soda Springs, Idaho and proceeded to the Willamette Valley in Oregon and the other half proceeded on to California.  During pre-American Civil War \"Bleeding Kansas\" skirmishes between Kansas and Missouri raiders, the jumping off points for westward-bound wagon trains shifted northward towards Omaha, Nebraska.  The trail branch John Fremont followed from Westport Landing to the Wakarusa Valley south of Lawrence, Kansas became regionally known as the \"California Road.\" Part of the same general route of the trail across Nevada was used for the Central Pacific portion of the first transcontinental railroad.  In the 20th century, the route was used for modern highways, in particular U.S. Highway 40 and later Interstate 80.  Ruts from the wagon wheels and names of emigrants, written with axle grease on rocks, can still be seen in the City of Rocks National Reserve in southern Idaho. The California U.S. Census of 1850 showed 92,597 residents.  To this should be added residents from San Francisco, (the largest city in the state) Santa Clara, and Contra Costa counties whose censuses were burned up or lost and not included in the totals.  Newspaper accounts in 1850 (\"Alta Californian\") gives the population of San Francisco at 21,000; The special California state Census of 1852 finds 6,158 residents of Santa Clara county and 2,786 residents of Contra Costa County.  The corrected California U.S. 1850 Census is over 120,000.  See: U.S. Seventh Census 1850: California\n\nBicalutamide Bicalutamide, sold under the brand name Casodex among others, is an antiandrogen medication that is primarily used to treat prostate cancer.  It is typically used together with a gonadotropin-releasing hormone (GnRH) analogue or surgical removal of the testicles to treat metastatic prostate cancer.  Bicalutamide may also be used to treat excessive hair growth in women, as a component of hormone therapy for transgender women, to treat early puberty in boys, and to prevent priapism.  It is taken by mouth. Common side effects in men include breast enlargement, breast tenderness, and hot flashes.  Other side effects in men include feminization and sexual dysfunction.  While the medication appears to produce few side effects in women, its use in women is not recommended by the Food and Drug Administration (FDA).  Use during pregnancy may harm the baby.  Bicalutamide causes elevated liver enzymes in around 1% of people.  Rarely, it has been associated with cases of liver damage and lung toxicity.  Although the risk of liver changes and damage is small, monitoring of liver enzymes is recommended during treatment. Bicalutamide is a member of the nonsteroidal antiandrogen (NSAA) group of medications.  It works by blocking the androgen receptor (AR), the biological target of the androgen sex hormones testosterone and dihydrotestosterone (DHT).  It does not lower androgen levels.  The medication can have some estrogen-like effects in men.  Bicalutamide is well-absorbed, and its absorption is not affected by food.  The terminal half-life of the medication is around one week.  It is believed to cross the blood–brain barrier and affect both the body and brain. Bicalutamide was patented in 1982 and approved for medical use in 1995.  It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system.  Bicalutamide is available as a generic medication.  The wholesale cost in the developing world is about to per month.  In the United States it costs about and above per month.  The drug is sold in more than 80 countries, including most developed countries.  It is the most widely used antiandrogen in the treatment of prostate cancer, and has been prescribed to millions of men with the disease. Bicalutamide is mainly used in and is only approved for the following indications: It can also be and is used to a lesser extent for the following off-label (non-approved) indications: It has been suggested for but has uncertain effectiveness in the following indications: Bicalutamide is used primarily in the treatment of early and advanced prostate cancer.  It is approved at a dosage of 50 mg/day as a combination therapy with a or orchiectomy (that is, surgical or medical castration) in the treatment of stage D2 , and as a monotherapy at a dosage of 150 mg/day for the treatment of stage C or D1 .  Although effective in and , bicalutamide is no longer indicated for the treatment of localized prostate cancer (LPC) due to negative findings in the Early Prostate Cancer (EPC) trial.  Prior to the introduction of the newer enzalutamide in 2012, bicalutamide was considered to be the standard-of-care antiandrogen in the treatment of prostate cancer, and still remains widely used for this indication.  Compared to earlier antiandrogens like the steroidal antiandrogen (SAA) cyproterone acetate (CPA) and the flutamide and nilutamide, bicalutamide shows an improved profile of effectiveness, tolerability, and safety, and for this reason has largely replaced them in the treatment of prostate cancer. In the early 1940s, it was discovered that growth of prostate cancer in men regressed with surgical castration or high-dose estrogen treatment, which were associated with very low levels of circulating testosterone, and accelerated with the administration of exogenous testosterone.  It has since been elucidated that androgens like testosterone and function as trophic factors for the prostate gland, stimulating cell division and proliferation and producing tissue growth and glandular enlargement, which, in the context of prostate cancer, results in stimulation of tumors and a considerable acceleration of disease progression.  As a result of these discoveries, androgen deprivation therapy (ADT), via a variety of modalities including surgical castration, high-dose estrogens, , analogues, , and androgen biosynthesis inhibitors (e.g., abiraterone acetate), has become the mainstay of treatment for prostate cancer.  Although can shrink or stabilize prostate tumors and hence significantly slow the course of prostate cancer and prolong life, it is, unfortunately, not generally curative.  While effective in slowing the progression of the disease initially, most advanced prostate cancer patients eventually become resistant to and prostate cancer growth starts to accelerate again, in part due to progressive mutations in the that result in the transformation of drugs like bicalutamide from antagonists to agonists. A few observations form the basis of the reasoning behind combined androgen blockade (CAB), in which castration and an are combined.  It has been found that very low levels of androgens, as in castration, are able to significantly stimulate growth of prostate cancer cells and accelerate disease progression.  Although castration ceases production of androgens by the gonads and reduces circulating testosterone levels by about 95%, low levels of androgens continue to be produced by the adrenal glands, and this accounts for the residual levels of circulating testosterone.  Moreover, it has been found that prostate gland levels of , which is the major androgen in the prostate, remain at 40 to 50% of their initial values following castration.  This has been determined to be due to uptake of circulating weak adrenal androgens like dehydroepiandrosterone (DHEA) and androstenedione (A4) by the prostate and their \"de novo\" transformation into testosterone and .  As such, a considerable amount of androgen signaling continues within the prostate gland even with castration. In the past, surgical adrenalectomy and early androgen biosynthesis inhibitors like ketoconazole and aminoglutethimide were successfully employed in the treatment of castration-resistant prostate cancer.  However, adrenalectomy is an invasive procedure with high morbidity, ketoconazole and aminoglutethimide have relatively high toxicity, and both treatment modalities require supplementation with corticosteroids, making them in many ways unideal.  The development of with like bicalutamide and enzalutamide and with newer and more tolerable androgen biosynthesis inhibitors like abiraterone acetate has since allowed for non-invasive, convenient, and well-tolerated therapies that have replaced the earlier modalities. Subsequent clinical research has found that monotherapy with higher dosages of than those used in is roughly equivalent to castration in extending life in men with prostate cancer.  Moreover, monotherapy is overall better tolerated and associated with greater quality of life than is castration, which is thought to be related to the fact that testosterone levels do not decrease with monotherapy and hence by extension that levels of biologically active and beneficial metabolites of testosterone such as estrogens and neurosteroids are preserved.  For these reasons, monotherapy has become an important alternative to castration and in the treatment of prostate cancer. Androgens like testosterone and play a critical role in the pathogenesis of a number of dermatological conditions including acne, seborrhea, hirsutism (excessive facial/body hair growth in women), and pattern hair loss (androgenic alopecia).  In demonstration of this, women with complete androgen insensitivity syndrome (CAIS) do not produce sebum or develop acne and have little to no body, pubic, or axillary hair.  Moreover, men with congenital 5α-reductase type II deficiency, 5α-reductase being an enzyme that greatly potentiates the androgenic effects of testosterone in the skin, have little to no acne, scanty facial hair, reduced body hair, and reportedly no incidence of male pattern hair loss.  Conversely, hyperandrogenism in women, for instance due to or , is commonly associated with acne and hirsutism as well as virilization (masculinization) in general.  In accordance with the preceding, antiandrogens have been found to be highly effective in the treatment of the aforementioned androgen-dependent skin and hair conditions. Low-dose bicalutamide has been found to be effective in the treatment of hirsutism in women in clinical studies.  In one of the studies, the drug was well-tolerated, all of the patients experienced a visible decrease in hair density, and a highly significant clinical improvement was observed with the Ferriman–Gallwey score decreasing by 41.2% at 3 months and by 61.6% at 6 months.  According to a recent review, \"Low dose bicalutamide (25 mg/day) was shown to be effective in the treatment of hirsutism related to and .  It does not have any significant side effects [or lead] to irregular periods.\" In addition to hirsutism, bicalutamide can also be used in the treatment of acne in women.  Several studies have observed complete clearing of acne with flutamide in women, and similar benefits would be expected with bicalutamide.  Bicalutamide may also treat other androgen-dependent skin and hair conditions like seborrhea and pattern hair loss.  Flutamide has been found to produce a decrease of hirsutism score to normal and an 80% or greater decrease in scores of acne, seborrhea, and androgen-dependent hair loss.  Moreover, in combination with an oral contraceptive, flutamide treatment resulted in an increase in cosmetically acceptable scalp hair density in 6 of 7 women suffering from pattern hair loss. Antiandrogens like flutamide and bicalutamide are male-specific teratogens which can feminize male fetuses due to their antiandrogen effects (see below), and for this reason, are not recommended by the for use in women.  Because of this risk, it is strongly recommended that antiandrogens only be used to treat women who are of reproductive age in conjunction with adequate contraception.  Oral contraceptives, which contain an estrogen and a progestin, are typically used for this purpose.  Moreover, oral contraceptives themselves are functional antiandrogens and are independently effective in the treatment of androgen-dependent skin and hair conditions, and hence can significantly augment the effectiveness of antiandrogens in the treatment of such conditions. Bicalutamide is used as a component of hormone replacement therapy (HRT) for transgender women.  Beneficial or desired effects consist of demasculinization and feminization, and include breast development, reduced male-pattern hair, decreased muscle mass, feminine changes in fat distribution, lowered libido, and loss of spontaneous erections.  It is also noteworthy that, when used as a monotherapy, bicalutamide significantly increases estradiol levels in biological males and hence can have indirect estrogenic effects in transgender women; this is a property that can be considered to be desirable in transgender women, as it can produce or contribute to feminization. Unlike the and spironolactone, as well as analogues, no clinical studies assessing bicalutamide as an antiandrogen in the hormonal therapy of transgender women appear to have been conducted to date.  In any case, bicalutamide is clinically effective as an antiandrogen in women with hirsutism and in boys with precocious puberty, and demasculinization and feminization are well-documented effects of bicalutamide in men treated with it for prostate cancer (see below).  In addition, nilutamide, a closely related antiandrogen that possesses essentially the same mechanism of action as bicalutamide, has been evaluated in transgender women in at least five small clinical studies.  It was given at a high dosage of 300 mg/day, the same dosage in which it has been used as a monotherapy in the treatment of prostate cancer.  The corresponding monotherapy dosage of bicalutamide in the treatment of prostate cancer is 150 mg/day. In the clinical studies of nilutamide for transgender hormone therapy, the drug, without being combined with estrogen, induced observable signs of clinical feminization in young transgender women (age range 19–33 years) within 8 weeks, including breast development, decreased male-pattern hair, decreased spontaneous erections and sex drive, and positive psychological and emotional changes.  Signs of breast development occurred in all subjects within 6 weeks and were associated with increased nipple sensitivity; along with decreased hair growth, these changes were the earliest signs of feminization.  The drug more than doubled luteinizing hormone (LH) and testosterone levels and tripled estradiol levels (see below), and the addition of ethinylestradiol, a potent estrogen, to nilutamide therapy after 8 weeks of treatment abolished the increase in , testosterone, and estradiol levels and dramatically suppressed testosterone levels, into the castrate range.  On the basis of these results, both nilutamide alone, and particularly the combination of nilutamide and an estrogen, were regarded as effective in terms of producing antiandrogen effects and feminization in transgender women. Although nilutamide has been found to be clinically effective for transgender hormone therapy, the use of nilutamide in the treatment of prostate cancer and particularly for other indications that are of a less dire nature is now discouraged due to the unique adverse effects of the drug, most importantly a high incidence of interstitial pneumonitis.  This is an adverse effect that can progress to pulmonary fibrosis and can potentially be fatal.  For this reason, newer, safer like bicalutamide have largely replaced nilutamide and are now used for such indications instead. Dr. Madeline Deutsch of the Center of Excellence for Transgender Health at the University of California, San Francisco, in her \"Guidelines for the Primary and Gender-Affirming Care of Transgender and Gender Nonbinary People\" (2016), has said the following on the topic of bicalutamide in transgender women: In spite of its risk of hepatotoxicity, is the most widely used antiandrogen in Europe and elsewhere in the world for hormone therapy in transgender women.  In addition, it is widely used in cisgender women for the treatment of acne and hirsutism.  In the , one of the only countries where has not been approved for medical use, the relatively weak agent spironolactone is the antiandrogen most commonly used in transgender and cisgender women instead.  Although both and bicalutamide have been associated with hepatotoxicity (while spironolactone is not usually associated with such), bicalutamide has a far lower and only small risk of both elevated liver enzymes and hepatotoxicity (see below); whereas has been associated with over 100 reports of hepatotoxicity, five cases of hepatotoxicity, out of millions of patient exposures, have been associated with bicalutamide to date. Bicalutamide (25–50 mg/day) is useful in combination with the aromatase inhibitor anastrozole as a puberty blocker in the treatment of male precocious puberty.  This is potentially a cost-effective alternative to analogues for the treatment of this condition, as analogues are very expensive.  Moreover, the combination is effective in gonadotropin-independent precocious puberty, namely familial male-limited precocious puberty (also known as testotoxicosis), where analogues notably are not effective.  Bicalutamide has been found to be superior to the spironolactone (which has also been used, in combination with the aromatase inhibitor testolactone) for this indication; it has shown greater effectiveness and possesses fewer side effects in comparison.  For this reason, bicalutamide has replaced spironolactone in the treatment of the condition. Antiandrogens can considerably relieve and prevent priapism (potentially painful penile erections that last more than four hours) via direct blockade of penile .  In accordance, bicalutamide, at low dosages (50 mg every other day or as little as once or twice weekly), has been found in a series of case reports to completely resolve recurrent priapism in men without producing significant side effects, and is used for this indication off-label.  In the reported cases, libido, rigid erections, the potential for sexual intercourse, orgasm, and subjective ejaculatory volume have all remained intact or unchanged, and gynecomastia has not developed when bicalutamide is administered at a total dosage of 25 mg/day or less.  Some gynecomastia and breast tenderness developed in one patient treated with 50 mg/day, but significantly improved upon the dosage being halved.  The observed tolerability profile of bicalutamide in these subjects has been regarded as significantly more favorable than that of analogues and estrogens (which are also used in the treatment of this condition).  However, although successful and well-tolerated, very few cases have been reported. The antigonadotropic antiandrogens , medroxyprogesterone acetate (MPA), and analogues have all been widely used to treat paraphilias (e.g., pedophilia) and hypersexuality in men.  They suppress androgen levels to castrate or near-castrate levels and are highly effective in reducing sexual urges, arousal, and behaviors.  In addition, they are used to treat sex offenders as a means of chemical castration for the purpose of reducing the likelihood of recidivism. Although they have not been studied in the treatment of paraphilias and hypersexuality, like flutamide and bicalutamide have been suggested as potential medications for these indications and may have superior tolerability and safety relative to antigonadotropic antiandrogens.  As an example, because do not reduce estrogen levels, unlike antigonadotropic antiandrogens, they preserve bone mineral density (BMD) and have little or no risk of osteoporosis and associated bone fractures. However, due to unopposed estrogen signaling, a substantial incidence of gynecomastia is associated with .  In addition to potential monotherapy use, have been advocated for temporarily suppressing sex drive during the start of agonist treatment via prevention of the increased androgen signaling associated with the initial testosterone flare. Though treatment of paraphilias and hypersexuality with selective antagonists is a seemingly sound strategy, this may not be true in practice.  Surprisingly, little or no sexual dysfunction, including loss of sex drive and decreased sexual activity, has been observed in clinical studies of monotherapy.  The explanation for this is that do not lower androgen levels, and metabolites of testosterone like estrogens and neurosteroids may be of critical importance for maintenance of sex drive and function in males.  In accordance, testosterone is locally aromatized into estradiol widely throughout the brain and estradiol appears to be the mediator of many of the central actions of testosterone.  For these reasons, unlike antigonadotropic antiandrogens, monotherapy may have limited usefulness in the management of paraphilias and hypersexuality.  The addition of to antigonadotropic antiandrogens like analogues may have some usefulness however, particularly in severe cases.  In any case, insufficient evidence is available at this time, and further research is thus warranted. Bicalutamide is available in 50 mg, 80 mg (in Japan), and 150 mg tablets for oral administration.  No other formulations or routes of administration are available or used.  All formulations of bicalutamide are specifically indicated for the treatment of prostate cancer alone or in combination with surgical or medication castration. A combined formulation of bicalutamide and the  agonist goserelin in which goserelin is provided as a subcutaneous implant for injection and bicalutamide is included as 50 mg tablets for oral ingestion is marketed in Australia and New Zealand under the brand name ZolaCos CP (Zoladex–Cosudex Combination Pack). In individuals with severe, though not mild-to-moderate hepatic impairment, there is evidence that the elimination of bicalutamide is slowed, and hence, caution may be warranted in these patients.  In severe hepatic impairment, the terminal half-life of the active (\"R\")-enantiomer of bicalutamide is increased by about 1.75-fold (76% increase; half-life of 5.9 and 10.4 days for normal and impaired patients, respectively).  The terminal half-life of bicalutamide is unchanged in renal impairment. Bicalutamide is pregnancy category X, or \"contraindicated in pregnancy\", in the , and pregnancy category D, the second most restricted rating, in Australia.  As such, it is contraindicated in women during pregnancy, and women who are sexually active and who can or may become pregnant are strongly recommended to take bicalutamide only in combination with adequate contraception.  It is unknown whether bicalutamide is excreted in breast milk, but many drugs are excreted in breast milk, and for this reason, bicalutamide treatment is similarly not recommended while breastfeeding. Because bicalutamide blocks the , like all antiandrogens, it can interfere with the androgen-mediated sexual differentiation of the genitalia (and brain) during prenatal development.  In pregnant rats given bicalutamide at a dosage of 10 mg/kg/day (resulting in circulating drug levels approximately equivalent to two-thirds of human therapeutic concentrations) and above, feminization of male offspring, such as reduced anogenital distance and hypospadias, as well as impotence, were observed.  No other teratogenic effects were observed in rats or rabbits receiving up to very high dosages of bicalutamide (that corresponded to up to approximately two times human therapeutic levels), and no teratogenic effects of any sort were observed in female rat offspring at any dosage.  As such, bicalutamide is a selective reproductive teratogen in males, and may have the potential to produce undervirilization/sexually ambiguous genitalia in male fetuses. The side effect profile of bicalutamide is highly dependent on sex; that is, on whether the person is male or female.  In men, due to androgen deprivation, a variety of side effects of varying severity may occur during bicalutamide treatment, with breast pain/tenderness and gynecomastia (breast development/enlargement) being the most common.  In addition breast changes, physical feminization and demasculinization in general, including reduced body hair growth, decreased muscle mass and strength, feminine changes in fat mass and distribution, and reduced penile length, may also occur in men.  Other side effects that have been observed in men and that are similarly related to androgen deprivation include hot flashes, sexual dysfunction (e.g., loss of libido, erectile dysfunction), depression, fatigue, weakness, anemia, and decreased semen/ejaculate volume.  In women, on the other hand, due to the little biological importance of androgens in this sex, the side effects of pure antiandrogens or are minimal, and bicalutamide has been found to be very well-tolerated.  General side effects of bicalutamide that may occur in either sex include diarrhea, constipation, abdominal pain, nausea, dry skin, itching, and rash.  The drug is well-tolerated at higher dosages than the 50 mg/day dosage, with rare additional side effects. Bicalutamide monotherapy has been associated with abnormal liver function tests such as elevated liver enzymes in 3.4% of men relative to 1.9% for standard care.  Hepatic changes such as marked increases in liver enzymes or hepatitis that necessitated discontinuation of bicalutamide have occurred in approximately 0.3 to 1% of men in clinical trials.  Monitoring of liver function during treatment is recommended, particularly in the first few months.  In men of advanced age with prostate cancer, bicalutamide monotherapy has been associated with an increase in non-prostate cancer mortality, in part due to an increase in the rate of heart failure.  These mortality-related effects are thought to be a consequence of androgen deprivation, rather than a specific drug-related toxicity of bicalutamide. Five cases of hepatotoxicity or liver failure, two of which resulted in death, have been reported with bicalutamide.  Symptoms that may indicate liver dysfunction include nausea, vomiting, abdominal pain, fatigue, anorexia, \"flu-like\" symptoms, dark urine, and jaundice.  Bicalutamide has also been associated with several case reports of interstitial pneumonitis, which can potentially progress to pulmonary fibrosis.  Symptoms that may indicate lung dysfunction include dyspnea (difficult breathing or shortness of breath), cough, and pharyngitis (inflammation of the pharynx, resulting in sore throat).  Both hepatotoxicity and interstitial pneumonitis are said to be extremely rare events with bicalutamide.  A few cases of photosensitivity have been reported with bicalutamide.  Hypersensitivity reactions (drug allergy) like angioedema and hives have also uncommonly been reported in association with bicalutamide. The most common side effects of bicalutamide monotherapy in men are breast pain/tenderness and gynecomastia.  These side effects may occur in as much as 90% of men treated with bicalutamide monotherapy, but gynecomastia is generally reported to occur in 70–80% of patients.  In the trial, at a median follow-up of 7.4 years, breast pain and gynecomastia respectively occurred in 73.6% and 68.8% of men treated with 150 mg/day bicalutamide monotherapy.  In more than 90% of affected men, bicalutamide-related breast events are mild-to-moderate in severity.  It is only rarely and in severe and extreme cases of gynecomastia that the proportions of the male breasts become so marked that they are comparable to those of women.  In the trial, 16.8% of bicalutamide patients relative to 0.7% of controls withdrew from the study due to breast pain and/or gynecomastia.  The incidence and severity of gynecomastia are higher with estrogens (e.g., diethylstilbestrol) than with like bicalutamide in the treatment of men with prostate cancer. Tamoxifen, a selective estrogen receptor modulator (SERM) with antiestrogenic actions in breast tissue and estrogenic actions in bone, has been found to be highly effective in preventing and reversing bicalutamide-induced gynecomastia in men.  Moreover, in contrast to analogues (which also alleviate bicalutamide-induced gynecomastia), tamoxifen poses minimal risk of accelerated bone loss and osteoporosis.  For reasons that are unclear, anastrozole, an aromatase inhibitor (or an inhibitor of estrogen biosynthesis), has been found to be much less effective in comparison to tamoxifen for treating bicalutamide-induced gynecomastia.  A systematic review of -induced gynecomastia and breast tenderness concluded that tamoxifen (10–20 mg/day) and radiotherapy could effectively manage the side effect without relevant adverse effects, though with tamoxifen showing superior effectiveness.  Surgical breast reduction may also be employed to correct bicalutamide-induced gynecomastia. Bicalutamide may cause sexual dysfunction, including decreased sex drive and erectile dysfunction.  However, the rates of these side effects with bicalutamide monotherapy are very low.  In the trial, at 7.4 years follow-up, the rates of decreased libido and impotence were only 3.6% and 9.3% in the 150 mg/day bicalutamide monotherapy group relative to 1.2% and 6.5% for placebo, respectively.  Most men experience sexual dysfunction only moderately or not at all with bicalutamide monotherapy, and the same is true during monotherapy with other .  In clinical trials, about two-thirds of men with advanced prostate cancer (and of almost invariably advanced age) treated with bicalutamide monotherapy maintained sexual interest, while sexual function was slightly reduced by 18%. Similarly to men, bicalutamide is likely to be associated with minimal or no sexual dysfunction in women.  Studies have not found a significant correlation between libido and circulating testosterone levels in premenopausal women, and treatment of premenopausal women with low doses of testosterone has not been found to improve sexual function in most research.  However, low-dose testosterone therapy has been found to significantly improve multiple domains of sexual function (frequency, libido, orgasm, satisfaction, others) in postmenopausal women, particularly in those who have undergone ovariectomy.  Conversely, women with PCOS, who have higher levels of testosterone, do not show increased sex drive, and treatment with oral contraceptives, which contain estrogen and decrease free testosterone levels, have been found not to decrease sexual function but to increase it in both healthy women and those with PCOS.  In addition, women with CAIS show normal or even increased sexual function in spite of complete loss of AR signaling.  It has been suggested that increased sex drive and sexual function associated with testosterone may be mediated by aromatization into estradiol rather than necessarily via activation of the AR. Bicalutamide reduces the size of the prostate gland and seminal vesicles, though not of the testes.  Significantly reduced penile length is also a recognized adverse effect of .  Reversible hypospermia or aspermia (that is, reduced or absent semen/ejaculate production) may occur.  However, bicalutamide does not appear to adversely affect spermatogenesis, and thus may not necessarily abolish the capacity/potential for fertility in men (see below).  Due to the induction of chronic overproduction of and testosterone, there was concern that long-term bicalutamide monotherapy might induce Leydig cell hyperplasia and tumors (usually benign), but the evidence indicates that Leydig cell hyperplasia does not occur to a significant extent. The incidence of diarrhea with bicalutamide monotherapy in the trial was comparable to placebo (6.3% vs. 6.4%, respectively).  In phase III studies of bicalutamide monotherapy for , the rates of diarrhea for bicalutamide and castration were 6.4% and 12.5%, respectively, the rates of constipation were 13.7% and 14.4%, respectively, and the rates of abdominal pain were 10.5% and 5.6%, respectively. In the trial, at 7.4 years follow-up, the rate of hot flashes was 9.2% for bicalutamide relative to 5.4% for placebo, which was regarded as relatively low.  In the subgroup of the trial, the rate of hot flashes with bicalutamide was 13.1% (relative to 50.0% for castration). At 5.3 years follow-up, the incidence of depression was 5.5% for bicalutamide relative to 3.0% for placebo in the trial, and the incidence of asthenia (weakness or fatigue) was 10.2% for bicalutamide relative to 5.1% for placebo. Androgens are known to stimulate the formation of red blood cells and increase the red blood cell count and circulating hematocrit levels, effects which they mediate by increasing production of erythropoietin in the kidneys.  In accordance, anabolic–androgenic steroids (AAS) such as oxymetholone and nandrolone decanoate are approved and used in the treatment of severe anemia, and can cause polycythemia as an adverse effect in high dosages.  Conversely, whether via castration, monotherapy, or , mild anemia is a common side effect of in men.  The incidence of anemia with bicalutamide as a monotherapy or with castration was about 7.4% in clinical trials.  A decrease of hemoglobin levels of 1–2 g/dL after approximately six months of treatment may be observed. Androgens are involved in regulation of the skin (e.g., sebum production), and antiandrogens are known to be associated with skin changes.  Skin-related side effects, which included dry skin, itching, and rash, were reported at a rate of 12% in both monotherapy and clinical studies of bicalutamide in men. Combination of bicalutamide with medical (i.e., a analogue) or surgical castration modifies the side effect profile of bicalutamide.  Some of its side effects, including breast pain/tenderness and gynecomastia, are far less likely to occur when the drug is combined with a analogue, while certain other side effects, including hot flashes, depression, fatigue, and sexual dysfunction, occur much more frequently in combination with a analogue.  It is thought that this is due to the suppression of estrogen levels (in addition to androgen levels) by analogues, as estrogen may compensate for various negative central effects of androgen deprivation.  If bicalutamide is combined with a analogue or surgical castration, the elevation of androgen and estrogen levels in men caused by bicalutamide will be prevented and the side effects of excessive estrogens, namely gynecomastia, will be reduced.  However, due to the loss of estrogen, bone loss will accelerate and the risk of osteoporosis developing with long-term therapy will increase. In the group of the study, although 150 mg/day bicalutamide monotherapy had reduced mortality due to prostate cancer relative to placebo, there was a trend toward significantly increased overall mortality for bicalutamide relative to placebo at 5.4-year follow-up (25.2% vs. 20.5%).  This was because more bicalutamide than placebo recipients had died due to causes unrelated to prostate cancer in this group (16.8% vs. 9.5% at 5.4-year follow-up; 10.2% vs. 9.2% at 7.4-year follow-up).  At 7.4-year follow-up, there were numerically more deaths from heart failure (1.2% vs. 0.6%; 49 vs. 25 patients) and gastrointestinal cancer (1.3% vs. 0.9%) in the bicalutamide group relative to placebo recipients, although cardiovascular morbidity was similar between the two groups and there was no consistent pattern suggestive of drug-related toxicity for bicalutamide.  In any case, although the reason for the increased overall mortality with 150 mg/day bicalutamide monotherapy has not been fully elucidated, it has been said that the finding that heart failure was twice as frequent in the bicalutamide group warrants further investigation.  In this regard, it is notable that low testosterone levels in men have been associated in epidemiological studies with cardiovascular disease as well as with a variety of other disease states (including hypertension, hypercholesterolemia, diabetes, obesity, Alzheimer's disease, osteoporosis, and frailty). According to Iversen et al. (2006), the increased non-prostate cancer mortality with bicalutamide monotherapy in patients has also been seen with castration (via orchiectomy or analogue monotherapy) and is likely a consequence of androgen deprivation in men rather than a specific drug toxicity of bicalutamide: A meta-analysis of prospective, randomized clinical trials of agonist-based for the treatment of non-metastatic prostate cancer that included over 4,000 patients found no evidence of increased cardiovascular mortality or overall mortality.  Non-prostate cancer mortality was not specifically assessed. Bicalutamide may cause liver changes rarely, such as elevated transaminases and jaundice.  In the study of 4,052 prostate cancer patients who received 150 mg/day bicalutamide as a monotherapy, the incidence of abnormal liver function tests was 3.4% for bicalutamide and 1.9% for standard care (a 1.5% difference potentially attributable to bicalutamide) at 3-year median follow-up.  For comparison, the incidences of abnormal liver function tests are 42–62% for flutamide, 2–3% for nilutamide, and (dose-dependently) between 9.6% and 28.2% for , whereas there appears to be no risk with enzalutamide.  In the trial, bicalutamide-induced liver changes were usually transient and rarely severe.  The drug was discontinued due to liver changes (manifested as hepatitis or marked increases in liver enzymes) in approximately 0.3% to 1% of patients treated with it for prostate cancer in clinical trials. The risk of liver changes with bicalutamide is considered to be small but significant, and monitoring of liver function is recommended.  Elevation of transaminases above twice the normal range or jaundice may be an indication that bicalutamide should be discontinued.  Liver changes with bicalutamide usually occur within the first 3 or 4 months of treatment, and it is recommended that liver function be monitored regularly for the first 4 months of treatment and periodically thereafter.  Symptoms that may indicate liver dysfunction include nausea, vomiting, abdominal pain, fatigue, anorexia, \"flu-like\" symptoms, dark urine, and jaundice. Out of millions of patient exposures, a total of five cases of bicalutamide-associated hepatotoxicity or liver failure, two of which were fatal, have been reported in the medical literature as of 2016.  One of these cases occurred after only two doses of bicalutamide, and has been regarded as much more likely to have been caused by prolonged prior exposure of the patient to flutamide and .  In the five reported cases of bicalutamide-associated hepatotoxicity, the dosages of the drug were 50 mg/day (three), 100 mg/day (one), and 150 mg/day (one).  Relative to flutamide (which has an estimated incidence rate of 3 in every 10,000), hepatotoxicity is far rarer with bicalutamide and nilutamide, and bicalutamide is regarded as having the lowest risk of the three drugs.  For comparison, by 1996, 46 cases of severe cholestatic hepatitis associated with flutamide had been reported, with 20 of the cases resulting in death.  Moreover, a 2002 review reported that there were 18 reports of hepatotoxicity associated with in the medical literature, with 6 of the reported cases resulting in death, and the review also cited a report of an additional 96 instances of hepatotoxicity that were attributed to , 33 of which resulted in death. The clinical studies that have found elevated liver enzymes and the case reports of hepatotoxicity with bicalutamide have all specifically pertained to men of advanced age with prostate cancer.  It is notable that older age, for a variety of reasons, appears to be an important risk factor for drug-induced hepatotoxicity.  As such, the risk of liver changes with bicalutamide may be less in younger patients, for instance young hirsute women and transgender women.  However, it has been reported on the basis of very limited evidence that this may not be the case with flutamide. From a theoretical standpoint (on the basis of structure–activity relationships), it has been suggested that flutamide, bicalutamide, and nilutamide, to varying extents, all have the potential to cause liver toxicity.  However, in contrast to flutamide, hydroxyflutamide, and nilutamide, bicalutamide exhibits much less or no mitochondrial toxicity and inhibition of enzymes in the electron transport chain such as respiratory complex I (), and this may be the reason for its much lower risk of hepatotoxicity in comparison.  The activity difference may be related to the fact that flutamide, hydroxyflutamide, and nilutamide all possess a nitroaromatic group, whereas in bicalutamide, a cyano group is present in place of this nitro group, potentially reducing toxicity. Several case reports of interstitial pneumonitis (which can progress to pulmonary fibrosis) in association with bicalutamide treatment have been published in the medical literature.  Interstitial pneumonitis with bicalutamide is said to be an extremely rare event, and the risk is far less relative to that seen with nilutamide (which has an incidence rate of 0.5–2% of patients).  In a very large cohort of prostate cancer patients, the incidence of interstitial pneumonitis with was 0.77% for nilutamide but only 0.04% for flutamide and 0.01% for bicalutamide.  An assessment done prior to the publication of the aforementioned study estimated the rates of pulmonary toxicity with flutamide, bicalutamide, and nilutamide as 1 case, 5 cases, and 303 cases per million, respectively.  In addition to interstitial pneumonitis, a single case report of eosinophilic lung disease in association with six months of 200 mg/day bicalutamide treatment exists.  Side effects associated with the rare potential pulmonary adverse reactions of bicalutamide may include dyspnea (difficult breathing or shortness of breath), cough, and pharyngitis (inflammation of the pharynx, resulting in sore throat). A few cases of photosensitivity (hypersensitivity to ultraviolet light-induced skin redness and/or lesions) associated with bicalutamide have been reported.  In one of the cases, bicalutamide was continued due to effectiveness in treating prostate cancer in the patient, and in combination with strict photoprotection (in the form of avoidance/prevention of ultraviolet light exposure), the symptoms disappeared and did not recur.  Flutamide is also associated with photosensitivity, but much more frequently in comparison to bicalutamide. Hypersensitivity reactions (i.e., drug allergy), including angioedema and hives, have uncommonly been reported with bicalutamide. A case report of male breast cancer subsequent to bicalutamide-induced gynecomastia has been published.  According to the authors, \"this is the second confirmed case of breast cancer in association with bicalutamide-induced gynaecomastia (correspondence AstraZeneca).\"  It is notable, however, that gynecomastia does not seem to increase the risk of breast cancer in men.  Moreover, the lifetime incidence of breast cancer in men is approximately 0.1%, the average age of diagnosis of prostate cancer and male breast cancer are similar (around 70 years), and millions of men have been treated with bicalutamide for prostate cancer, all of which are potentially in support of the notion of chance co-occurrences.  In accordance, the authors concluded that \"causality cannot be established\" and that it was \"probable that the association is entirely coincidental and sporadic.\" A single oral dose of bicalutamide in humans that results in symptoms of overdose or that is considered to be life-threatening has not been established.  Dosages of up to 600 mg/day have been well-tolerated in clinical trials, and it is notable that there is a saturation of absorption with bicalutamide such that circulating levels of its active (\"R\")-enantiomer do not further increase above a dosage of 300 mg/day.  Overdose is considered to be unlikely to be life-threatening with bicalutamide or other first-generation (i.e., flutamide and nilutamide).  A massive overdose of nilutamide (13 grams, or 43 times the normal maximum 300 mg/day clinical dosage) in a 79-year-old man was uneventful, producing no clinical signs or symptoms or toxicity.  There is no specific antidote for bicalutamide or overdose, and treatment should be based on symptoms. Bicalutamide is almost exclusively metabolized by CYP3A4.  As such, its levels in the body may be altered by inhibitors and inducers of CYP3A4.  (For a list of CYP3A4 inhibitors and inducers, see here.)  However, in spite of the fact bicalutamide is metabolized by CYP3A4, there is no evidence of clinically significant drug interactions when bicalutamide at a dosage of 150 mg/day or less is co-administered with drugs that inhibit or induce cytochrome P450 enzyme activity. Because bicalutamide circulates at relatively high concentrations and is highly protein-bound, it has the potential to displace other highly protein-bound drugs like warfarin, phenytoin, theophylline, and aspirin from plasma binding proteins.  This could, in turn, result in increased free concentrations of such drugs and increased effects and/or side effects, potentially necessitating dosage adjustments.  Bicalutamide has specifically been found to displace coumarin anticoagulants like warfarin from their plasma binding proteins (namely albumin) \"in vitro\", potentially resulting in an increased anticoagulant effect, and for this reason, close monitoring of prothrombin time and dosage adjustment as necessary is recommended when bicalutamide is used in combination with these drugs.  However, in spite of this, no conclusive evidence of an interaction between bicalutamide and other drugs was found in clinical trials of nearly 3,000 patients. Since their introduction, bicalutamide and the other have largely replaced , an older drug and an , in the treatment of prostate cancer.  Bicalutamide was the third to be marketed, with flutamide and nilutamide preceding, and followed by enzalutamide.  Relative to the earlier antiandrogens, bicalutamide has substantially reduced toxicity, and in contrast to them, is said to have an excellent and favorable safety profile.  For these reasons, as well as superior potency, tolerability, and pharmacokinetics, bicalutamide is preferred and has largely replaced flutamide and nilutamide in clinical practice.  In accordance, bicalutamide is the most widely used antiandrogen in the treatment of prostate cancer.  Between January 2007 and December 2009, it accounted in the for about 87.2% of prescriptions.  Prior to the 2012 approval of enzalutamide, a newer and improved with greater potency and efficacy, bicalutamide was regarded as the standard-of-care antiandrogen in the treatment of the prostate cancer. Flutamide and nilutamide are first-generation , similarly to bicalutamide, and all three drugs possess the same core mechanism of action of being selective antagonists.  However, bicalutamide is the most potent of the three, with the highest affinity for the and the longest half-life, and is the safest, least toxic, and best-tolerated.  For these reasons, bicalutamide has largely replaced flutamide and nilutamide in clinical use, and is by far the most widely used first-generation . In terms of binding to the , the active (\"R\")-enantiomer of bicalutamide has 4-fold greater affinity relative to that of hydroxyflutamide, the active metabolite of flutamide (a prodrug), and 5-fold higher affinity relative to that of nilutamide.  In addition, bicalutamide possesses the longest half-life of the three drugs, with half-lives of 6–10 days for bicalutamide, 5–6 hours for flutamide and 8–9 hours for hydroxyflutamide, and 23–87 hours (mean 56 hours) for nilutamide.  Due to the relatively short half-lives of flutamide and hydroxyflutamide, flutamide must be taken three times daily at 8-hour intervals, whereas bicalutamide and nilutamide may be taken once daily.  For this reason, dosing of bicalutamide (and nilutamide) is more convenient than with flutamide.  The greater affinity and longer half-life of bicalutamide allow it to be used at relatively low dosages in comparison to flutamide (750–1500 mg/day) and nilutamide (150–300 mg/day) in the treatment of prostate cancer. While it has not been directly compared to nilutamide, the effectiveness of bicalutamide has been found to be at least equivalent to that of flutamide in the treatment of prostate cancer in a direct head-to-head comparison.  Moreover, indications of superior efficacy, including significantly greater relative decreases and increases in levels of prostate-specific antigen (PSA) and testosterone, respectively, were observed. It has been reported that hydroxyflutamide and nilutamide, in contrast to bicalutamide, have some ability to weakly activate the at high concentrations. The core side effects of such as gynecomastia, sexual dysfunction, and hot flashes occur at similar rates with the different drugs.  Conversely, bicalutamide is associated with a significantly lower rate of diarrhea compared to flutamide.  In fact, the incidence of diarrhea did not differ between the bicalutamide and placebo groups (6.3% vs. 6.4%, respectively) in the trial, whereas diarrhea occurs in up to 20% of patients treated with flutamide.  The rate of nausea and vomiting appears to be lower with bicalutamide and flutamide than with nilutamide (approximately 30% incidence of nausea with nilutamide, usually rated as mild-to-moderate).  In addition, bicalutamide (and flutamide) is not associated with alcohol intolerance, visual disturbances, or a high rate of interstitial pneumonitis.  In terms of toxicity and rare reactions, as described above, bicalutamide appears to have the lowest relative risks of hepatotoxicity and interstitial pneumonitis, with respective incidences far below those of flutamide and nilutamide.  In contrast to flutamide and nilutamide, no specific complications have been linked to bicalutamide. Enzalutamide, along with the in-development apalutamide and darolutamide, are newer, second-generation .  Similarly to bicalutamide and the other first-generation , they possess the same core mechanism of action of selective antagonism, but are considerably more potent and efficacious in comparison. In comparison to bicalutamide, enzalutamide has 5- to 8-fold higher affinity for the , possesses mechanistic differences resulting in improved deactivation, shows increased (though by no means complete) resistance to mutations in prostate cancer cells causing a switch from antagonist to agonist activity, and has an even longer half-life (8–9 days versus ~6 days for bicalutamide).  In accordance, enzalutamide, at a dosage of 160 mg/day, has been found to produce similar increases in testosterone, estradiol, and levels relative to high-dosage bicalutamide (300 mg/day), and an almost two-fold higher increase in testosterone levels relative to 150 mg/day bicalutamide (114% versus 66%).  These findings suggest that enzalutamide is a significantly more potent and effective antiandrogen in comparison.  Moreover, the drug has demonstrated superior clinical effectiveness in the treatment of prostate cancer in a direct head-to-head comparison with bicalutamide. In terms of tolerability, enzalutamide and bicalutamide appear comparable in most regards, with a similar moderate negative effect on sexual function and activity for instance.  However, enzalutamide has a risk of seizures and other central side effects such as anxiety and insomnia related to off-target GABA receptor inhibition that bicalutamide does not appear to have.  On the other hand, unlike with all of the earlier (flutamide, nilutamide, and bicalutamide), there has been no evidence of hepatotoxicity or elevated liver enzymes in association with enzalutamide treatment in clinical trials.  In addition to differences in adverse effects, enzalutamide is a strong inducer of CYP3A4 and a moderate inducer of CYP2C9 and CYP2C19 and poses a high risk of major drug interactions (CYP3A4 alone being involved in the metabolism of approximately 50 to 60% of clinically important drugs), whereas drug interactions are few and minimal with bicalutamide. include , megestrol acetate, chlormadinone acetate, and spironolactone.  These drugs are steroids, and similarly to , act as competitive antagonists of the , reducing androgenic activity in the body.  In contrast to however, they are non-selective, also binding to other steroid hormone receptors, and exhibit a variety of other activities including progestogenic, antigonadotropic, glucocorticoid, and/or antimineralocorticoid.  In addition, they are not silent antagonists of the , but are rather weak partial agonists with the capacity for both antiandrogenic and androgenic actions.  Of the , is the only one that has been widely used in the treatment of prostate cancer.  As antiandrogens, the have largely been replaced by the and are now rarely used in the treatment of prostate cancer, due to the superior selectivity, efficacy, and tolerability profiles of .  However, some of them, namely and spironolactone, are still commonly used in the management of certain androgen-dependent conditions (e.g., acne and hirsutism in women) and as the antiandrogen component of for transgender women. In a large-scale clinical trial that compared 750 mg/day flutamide and 250 mg/day monotherapies in the treatment of men with prostate cancer, the two drugs were found to have equivalent effectiveness on all endpoints.  In addition, contrarily to the case of men, flutamide has been found in various clinical studies to be more effective than (and particularly spironolactone) in the treatment of androgen-dependent conditions such as acne and hirsutism in women.  This difference in effectiveness in men and women may be related to the fact that like flutamide significantly increase androgen levels in men, which counteracts their antiandrogen potency, but do not increase androgen levels in women.  (In contrast to , , due to its progestogenic and hence antigonadotropic activity, does not increase and rather suppresses androgen levels in both sexes.) Bicalutamide has been found to be at least as effective as or more effective than flutamide in the treatment of prostate cancer, and is considered to be the most powerful antiandrogen of the three first-generation .  As such, although bicalutamide has not been compared head-to-head to or spironolactone in the treatment of androgen-dependent conditions, flutamide has been found to be either equivalent or more effective than them in clinical studies, and the same would consequently be expected of bicalutamide.  Accordingly, a study comparing the efficacy of 50 mg/day bicalutamide versus 300 mg/day in preventing the flare at the start of  agonist therapy in men with prostate cancer found that the two regimens were equivalently effective.  There was evidence of a slight advantage in terms of speed of onset and magnitude for the group, but the differences were small and did not reach statistical significance.  The differences may have been related to the antigonadotropic activity of (which would directly counteract the agonist-induced increase in gonadal androgen production) and/or the fact that bicalutamide requires 4 to 12 weeks of administration to reach steady-state (maximal) levels. All medically used are weak partial agonists of the rather than silent antagonists, and for this reason, possess inherent androgenicity in addition to their predominantly antiandrogenic actions.  In accordance, although produces feminization of and ambiguous genitalia in male fetuses when administered to pregnant animals, it has been found to produce masculinization of the genitalia of female fetuses of pregnant animals.  Additionally, all , including and spironolactone, have been found to stimulate and significantly accelerate the growth of androgen-sensitive tumors in the absence of androgens, whereas like flutamide have no effect and can in fact antagonize the stimulation caused by .  Accordingly, unlike , the addition of to castration has never been found in any controlled study to prolong survival in prostate cancer to a greater extent than castration alone.  In fact, a meta-analysis found that the addition of to castration actually \"reduces\" the long-term effectiveness of and causes an increase in mortality (mainly due to cardiovascular complications induced by ).  Also, there are two case reports of spironolactone actually accelerating progression of metastatic prostate cancer in castrated men treated with it for heart failure, and for this reason, spironolactone has been regarded as contraindicated in patients with prostate cancer.  Because of their intrinsic capacity to activate the , are incapable of maximally depriving the body of androgen signaling, and will always maintain at least some degree of activation. Due to its progestogenic (and by extension antigonadotropic) activity, is able to suppress circulating testosterone levels by 70–80% in men at high dosages.  In contrast, increase testosterone levels by up to 2-fold via blockade of the , a difference that is due to their lack of concomitant antigonadotropic action.  However, in spite of the combined antagonism and marked suppression of androgen levels by (and hence a sort of profile of antiandrogen action), monotherapy with an , , or a analogue/castration all have about the same effectiveness in the treatment of prostate cancer, whereas in the form of the addition of bicalutamide (but not of ) to castration has slightly but significantly greater comparative effectiveness in slowing the progression of prostate cancer and extending life.  These differences may be related to the inherent androgenicity of , which likely serves to limit its clinical efficacy as an antiandrogen in prostate cancer. Due to the different hormonal activities of like bicalutamide and like , they possess different profiles of adverse effects.  is regarded as having an unfavorable side effect profile, and the tolerability of bicalutamide is considered to be superior.  Due to its strong antigonadotropic effects and suppression of androgen and estrogen levels, is associated with severe sexual dysfunction (including loss of libido and impotence) similar to that seen with castration, and osteoporosis, whereas such side effects occur little or not at all with like bicalutamide.  In addition, is associated with coagulation changes and thrombosis (5%), fluid retention (4%), cardiovascular side effects (e.g., ischemic cardiomyopathy) (4–40%), and adverse effects on serum lipid profiles, with severe cardiovascular complications (sometimes being fatal) occurring in approximately 10% of patients.  In contrast, bicalutamide and other are not associated with these adverse effects.  Moreover, has a relatively high rate of generally severe and potentially fatal hepatotoxicity (see here), whereas the risk of hepatotoxicity is far smaller and comparatively minimal with bicalutamide (though not necessarily with other , namely flutamide) (see here).  has also been associated with high rates of depression (20–30%) and other mental side effects such as fatigue, irritability, anxiety, and suicidal thoughts in both men and women, side effects which may be related to vitamin B12 deficiency. It has been said that the only advantage of over castration is its relatively low incidence of hot flashes, a benefit that is mediated by its progestogenic activity.  Due to increased estrogen levels, bicalutamide and other are similarly associated with low rates of hot flashes (9.2% for bicalutamide vs. 5.4% for placebo in the trial).  One advantage of over is that, because it suppresses estrogen levels rather than increases them, it is associated with only a low rate of what is generally only slight gynecomastia (4–20%), whereas are associated with rates of gynecomastia of up to 80%.  Although monotherapy has many tolerability advantages in comparison to , a few of these advantages, such as preservation of sexual function and interest and (i.e., no increased incidence of osteoporosis) and low rates of hot flashes, are lost when are combined with castration.  However, the risk and severity of gynecomastia with are also greatly diminished in this context. Unlike spironolactone, bicalutamide has no antimineralocorticoid activity, and for this reason, has no risk of hyperkalemia (which can, rarely/in severe cases, result in hospitalization or death) or other antimineralocorticoid side effects such as urinary frequency, dehydration, hypotension, hyponatremia, metabolic acidosis, or decreased renal function that may occur with spironolactone treatment. In women, unlike and spironolactone, bicalutamide does not produce menstrual irregularity or amenorrhea, nor does it interfere with ovulation. Castration consists of either medical castration with a analogue or surgical castration via orchiectomy.  analogues include agonists like leuprorelin or goserelin and  antagonists like cetrorelix.  They are powerful antigonadotropins and work by abolishing the -induced secretion of gonadotropins, in turn ceasing gonadal production of sex hormones.  Medical and surgical castration achieve essentially the same effect, decreasing circulating testosterone levels by approximately 95%. Bicalutamide monotherapy has overall been found to be equivalent in effectiveness compared to analogues and castration in the treatment of prostate cancer.  A meta-analysis concluded that there is a slight effectiveness advantage for analogues/castration, but the differences trend towards but do not reach statistical significance.  In , the median survival time was found to be only 6 weeks shorter with bicalutamide monotherapy in comparison to analogue monotherapy. Monotherapy with including bicalutamide, flutamide, nilutamide, and enzalutamide shows a significantly lower risk of certain side effects, including hot flashes, depression, fatigue, loss of libido, and decreased sexual activity, relative to treatment with analogues, ( and analogue combination), , or surgical castration in prostate cancer.  For example, 60% of men reported complete loss of libido with bicalutamide relative to 85% for and 69% reported complete loss of erectile function relative to 93% for .  Another large study reported a rate of impotence of only 9.3% with bicalutamide relative to 6.5% for standard care (the controls), a rate of decreased libido of only 3.6% with bicalutamide relative to 1.2% for standard care, and a rate of 9.2% with bicalutamide for hot flashes relative to 5.4% for standard care.  One other study reported decreased libido, impotence, and hot flashes in only 3.8%, 16.9%, and 3.1% of bicalutamide-treated patients, respectively, relative to 1.3%, 7.1%, and 3.6% for placebo.  It has been proposed that due to the lower relative effect of on sexual interest and activity, with two-thirds of advanced patients treated with them retaining sexual interest, these drugs may result in improved quality of life and thus be preferable for those who wish to retain sexual interest and function relative to other antiandrogen therapies in prostate cancer.  Also, bicalutamide differs from analogues (which decrease and significantly increase the risk of bone fractures) in that it has well-documented benefits on , effects that are likely due to increased levels of estrogen. Bicalutamide acts as a highly selective competitive silent antagonist of the ( = 159–243 nM).  It has no capacity to activate the under normal physiological circumstances (see below).  In addition to competitive antagonism of the , bicalutamide has been found to accelerate the degradation of the , and this action may also be involved in its activity as an antiandrogen.  The activity of bicalutamide lies in the (\"R\")-isomer, which binds to the with an affinity that is about 30-fold higher than that of the (\"S\")-isomer.  Levels of the (\"R\")-isomer also notably are 100-fold higher than those of the (\"S\")-isomer at steady-state. Owing to its selectivity for the , unlike such as and megestrol acetate, bicalutamide does not bind to other steroid hormone receptors, and for this reason, has no additional, off-target hormonal activity (estrogenic or antiestrogenic, progestogenic or antiprogestogenic, glucocorticoid or antiglucocorticoid, or mineralocorticoid or antimineralocorticoid); nor does it inhibit 5α-reductase.  However, it significantly increases estrogen levels secondary to blockade of the in males, and for this reason, does have some \"indirect\" estrogenic effects in men.  Also in contrast to , bicalutamide neither inhibits nor suppresses androgen production in the body (i.e., it does not act as an antigonadotropin or steroidogenesis inhibitor), and instead exclusively mediates its antiandrogen effects by blocking androgen binding and subsequent receptor activation at the level of the . Although the affinity of bicalutamide for the is approximately 50 times lower than that of (IC ≈ 3.8 nM), the main endogenous ligand of the receptor in the prostate gland, sufficiently high relative concentrations of bicalutamide (1,000-fold excess) are effective in preventing activation of the by androgens like and testosterone and subsequent upregulation of the transcription of androgen-responsive genes.  At steady-state, relative to the normal adult male range for testosterone levels (300–1,000 ng/dL), circulating concentrations of bicalutamide at 50 mg/day are 600 to 2,500 times higher and at 150 mg/day 1,500 to 8,000 times higher than circulating testosterone levels, while bicalutamide concentrations, relative to the mean testosterone levels present in men who have been surgically castrated (15 ng/dL), are 42,000 times higher than testosterone levels at 50 mg/day. Whereas testosterone is the major circulating androgen, is the major androgen in the prostate gland.  levels in circulation are relatively low and only approximately 10% of those of circulating testosterone levels.  Conversely, local concentrations of in the prostate gland are 5- to 10-fold higher than circulating levels of .  This is due to high expression of 5α-reductase in the prostate gland, which very efficiently catalyzes the formation of from testosterone such that over 90% of intraprostatic testosterone is converted into .  Relative to testosterone, is 2.5- to 10-fold as potent as an agonist in bioassays, and hence, is a much stronger androgen in comparison.  As such, signaling is exceptionally high in the prostate gland, and the effectiveness of bicalutamide monotherapy in the treatment of prostate cancer, which is roughly equivalent to that of analogues, is a reflection of its capacity to strongly and efficaciously antagonize the at clinically used dosages.  On the other hand, analogues achieve only a 50 to 60% reduction in levels of in the prostate gland, and the combination of a analogue and bicalutamide is significantly more effective than either modality alone in the treatment of prostate cancer. In women, total testosterone levels are 20-fold and free testosterone levels 40-fold lower relative to men.  In addition, whereas bicalutamide monotherapy can increase testosterone levels by up to 2-fold in men, the drug does not increase testosterone levels in women (see below).  For these reasons, much lower dosages of bicalutamide (e.g., 25 mg/day in the hirsutism studies) may be used in women with comparable antiandrogen effectiveness. In men, blockade of the by bicalutamide in the pituitary gland and hypothalamus prevents the negative feedback of androgens on the release of , resulting in an elevation in levels.  Follicle-stimulating hormone (FSH) levels, in contrast, remain essentially unchanged.  The increase in levels leads to an increase in androgen and estrogen levels.  At a dosage of 150 mg/day, bicalutamide has been found to increase testosterone levels by about 1.5- to 2-fold (59–97% increase) and estradiol levels by about 1.5- to 2.5-fold (65–146% increase).  Levels of are also increased to a lesser extent (by 25%), and concentrations of sex hormone-binding globulin (SHBG) and prolactin increase as well (by 8% and 40%, respectively) secondary to the increase in estradiol levels.  The estradiol concentrations produced in men by bicalutamide monotherapy are said to approximate the low-normal estradiol levels of a premenopausal woman, while testosterone levels generally remain in the high end of the normal male range and rarely exceed it.  Dosages of bicalutamide of 10 mg, 30 mg, and 50 mg per day have been found to produce a \"moderate\" effect on sex hormone levels in men with prostate cancer (notably providing indication that the drug has clinically-relevant antiandrogen effects in males at a dosage as low as 10 mg/day).  It is important to note that bicalutamide increases androgen and estrogen levels only in men and not in women; this is because androgen levels are comparatively far lower in women and in turn exert little to no basal suppression of the hypothalamic–pituitary–gonadal (HPG) axis. The reason that testosterone levels are elevated but almost always remain in the normal male range with bicalutamide monotherapy is thought to be due to the concomitantly increased levels of estradiol, as estradiol is potently antigonadotropic and limits secretion of .  In fact, estradiol is a much stronger inhibitor of gonadotropin secretion than is testosterone, and even though circulating concentrations of estradiol are far lower than those of testosterone in men, it is said that estradiol is nonetheless likely the major feedback regulator of gonadotropin secretion in this sex.  In accordance, clomifene, a with antiestrogenic activity, has been found to increase testosterone levels to as much as 250% of initial values in men with hypogonadism, and a study of clomifene treatment in normal men observed increases in and levels of 70–360% and 200–700%, respectively, with increases in testosterone levels that were similar to the increases seen with the gonadotropins.  In addition to systemic or circulating estradiol, local aromatization of testosterone into estradiol in the hypothalamus and pituitary gland may contribute to suppression of gonadotropin secretion. Bicalutamide more than blocks the effects of the increased testosterone levels that it induces in men, which is evidenced by the fact that monotherapy with the drug is about as effective as analogue therapy in the treatment of prostate cancer.  However, in contrast, the effects of the elevated estrogen levels remain unopposed by bicalutamide, and this is largely responsible for the feminizing side effects (e.g., gynecomastia) of the drug in men. It has been proposed that the increase in estrogen levels caused by like bicalutamide compensates for androgen blockade in the brain, which may explain differences in the side effect profiles of these drugs relative to analogues/castration, , and (which, in contrast, decrease both androgen and estrogen levels).  In the case of sexual interest and function, this notion is supported by a variety of findings including animal studies showing that estrogen deficiency results in diminished sexual behavior, treatment with tamoxifen resulting in significantly lowered libido in 30% of men receiving it for male breast cancer, and estrogen administration restoring libido and the frequency of sexual intercourse in men with congenital estrogen deficiency, among others. Several metabolites of testosterone and , including estradiol, 3α-androstanediol, and 3β-androstanediol, are estrogens (mainly potent  agonists in the cases of the latter two), and 3α-androstanediol is additionally a potent GABA receptor-potentiating neurosteroid.  Due to the fact that bicalutamide does not lower testosterone levels, the levels of these metabolites would not be expected to be lowered either, unlike with therapies such as analogues.  (Indeed, testosterone, , and estradiol levels are actually raised by bicalutamide treatment, and for this reason, levels of 3α- and 3β-androstanediol might be elevated to some degree similarly.)  These metabolites of testosterone have been found to have -independent positive effects on sexual motivation, and may explain the preservation of sexual interest and function by bicalutamide and other .  They also have antidepressant, anxiolytic, and cognitive-enhancing effects, and may account for the lower incidence of depression with bicalutamide and other relative to other antiandrogen therapies. Though a pure, or silent antagonist of the under normal circumstances, bicalutamide, as well as other earlier antiandrogens like flutamide and nilutamide, have been found to possess weak partial agonist properties in the setting of overexpression and agonist activity in the case of certain mutations in the ligand-binding domain (LBD) of the .  As both of these circumstances can eventually occur in prostate cancer, resistance to bicalutamide usually develops and the drug has the potential to paradoxically stimulate tumor growth when this happens.  This is the mechanism of the phenomenon of antiandrogen withdrawal syndrome, where antiandrogen discontinuation paradoxically slows the rate of tumor growth.  The newer drug enzalutamide has been shown not to have agonistic properties in the context of overexpression of the , though certain mutations in the can still convert it from an antagonist to agonist. In transgender women, breast development is a desired effect of antiandrogen and/or estrogen treatment.  Bicalutamide induces breast development (or gynecomastia) in biologically male individuals by two mechanisms: 1) blocking androgen signaling in breast tissue; and 2) increasing estrogen levels.  Estrogen is responsible for the induction of breast development under normal circumstances, while androgens powerfully suppress estrogen-induced breast growth.  It has been found that very low levels of estrogen can induce breast development in the presence of low or no androgen signaling.  In accordance, bicalutamide not only induces gynecomastia at a high rate when given to men as a monotherapy, it results in a higher incidence of gynecomastia in combination with a analogue relative to analogue treatment alone (in spite of the presence of only castrate levels of estrogen in both cases). A study of men treated with (flutamide or bicalutamide) monotherapy for prostate cancer found that induced full ductal development and moderate lobuloalveolar development of the breasts from a histological standpoint.  The study also found that, in contrast, treatment of transgender women with estrogen and (which is progestogenic in addition to antiandrogenic, unlike ) resulted in full lobuloalevolar development, as well as pregnancy-like breast hyperplasia in two of the subjects.  In addition, it was observed that the lobuloalveolar maturation reversed upon discontinuation of after sex reassignment surgery (that is, surgical castration) in these individuals.  It was concluded that progestogen in addition to antiandrogen/estrogen treatment is required for the induction of full female-like histological breast development (i.e., that includes complete lobuloalveolar maturation), and that continued progestogen treatment is necessary to maintain such maturation.  It should be noted however that although these findings may have important implications in the contexts of lactation and breastfeeding, epithelial tissue accounts for approximately only 10% of breast volume (with the bulk of the breasts (80–90%) being represented by stromal or adipose tissue), and it is uncertain to what extent, if any, that development of lobuloalveolar structures (a form of epithelial tissue) contributes to breast size and/or shape. Spermatogenesis and male fertility are dependent on , , and high levels of testosterone within the testicles.  does not seem to be involved in spermatogenesis outside of its role in inducing production of testosterone by the Leydig cells in the seminiferous tubules (which make up approximately 80% of the bulk of the testes), whereas this is not the case for , which is importantly involved.  In accordance with the fact that the testes are the source of 95% of circulating testosterone in the body, local levels of testosterone inside of the testes are extremely high, ranging from 20- to 200-fold higher than circulating concentrations.  Moreover, high levels of testosterone within the testes are required for spermatogenesis, although only a small fraction (5–10%) of normal levels appears to actually be necessary for spermatogenesis. Unlike with antigonadotropic antiandrogens like and analogues, it has been reported that bicalutamide monotherapy (at 50 mg/day) has very little effect on the ultrastructure of the testes and on sperm maturation in humans even after long-term therapy (>4 years).  This may be explained by the extremely high local levels of testosterone in the testes, in that it is likely that systemic bicalutamide therapy is unable to achieve concentrations of the drug within the testes that are able to considerably block androgen signaling in this part of the body.  This is particularly so considering that bicalutamide increases circulating testosterone levels, and by extension gonadal testosterone production, by up to two-fold in males, and that only a small fraction of normal intratesticular testosterone levels, and by extension androgen action, appears to be necessary to maintain spermatogenesis. In contrast to bicalutamide and other pure antiandrogens or , antigonadotropic antiandrogens suppress gonadotropin secretion, which in turn diminishes testosterone production by the testes as well as the maintenance of the testes by , resulting in atrophy and loss of their function.  As such, bicalutamide and other may uniquely have the potential to preserve testicular function and spermatogenesis and thus male fertility relative to alternative therapies.  In accordance with this notion, a study found that prolonged, high-dose bicalutamide treatment had minimal effects on fertility in male rats.  However, another study found that low-dose bicalutamide administration resulted in testicular atrophy and reduced the germ cell count in the testes of male rats by almost 50%, though the rate of successful fertilization and pregnancy following mating was not assessed. Treatment of men with exogenous testosterone or other results in suppression of gonadotropin secretion and gonadal testosterone production due to their antigonadotropic effects or activation of the in the pituitary gland, resulting in inhibition or abolition of spermatogenesis and fertility: In contrast, pure antagonists would, in theory, result in the opposite (although reduced semen volume and sexual dysfunction may occur): Although bicalutamide alone would appear to have minimal detrimental effect on spermatogenesis and male fertility, other hormonal agents that bicalutamide may be combined with, including analogues and particularly estrogens (as in transgender hormone therapy), can have a considerable detrimental effect on fertility.  This is largely a consequence of their antigonadotropic activity.  Antigonadotropic agents like high-dose , high-dose androgens (e.g., testosterone esters), and antagonists (though notably not agonists) produce hypogonadism and high rates of severe or complete infertility (e.g., severe oligospermia or complete azoospermia) in men.  However, these effects are fully and often rapidly reversible with their discontinuation, even after prolonged treatment.  In contrast, while estrogens at sufficiently high dosages similarly are able to produce hypogonadism and to abolish or severely impair spermatogenesis, this is not necessarily reversible in the case of estrogens and can be long-lasting after prolonged exposure.  The difference is attributed to an apparently unique, direct adverse effect of high concentrations of estrogens on the Leydig cells of the testes. It has been reported that bicalutamide may have the potential to inhibit the enzymes CYP3A4 and, to a lesser extent, CYP2C9, CYP2C19, and CYP2D6, based on \"in vitro\" research.  However, no relevant inhibition of CYP3A4 has been observed \"in vivo\" with bicalutamide at a dose of 150 mg (using midazolam as a specific marker of CYP3A4 activity).  In animals, bicalutamide has been found to be an inducer of certain cytochrome P450 enzymes.  However, dosages of 150 mg/day or less have shown no evidence of this in humans. Bicalutamide has been identified as a strong CYP27A1 (cholesterol 27-hydroxylase) inhibitor \"in vitro\".  CYP27A1 converts cholesterol into 27-hydroxycholesterol, an oxysterol that has multiple biological functions including direct, tissue-specific activation of the estrogen receptor (ER) (it has been characterized as a selective estrogen receptor modulator) and the liver X receptor.  27-Hydroxycholesterol has been found to increase -positive breast cancer cell growth via its estrogenic action, and hence, it has been proposed that bicalutamide and other CYP27A1 inhibitors may be effective as adjuvant therapies to aromatase inhibitors in the treatment of -positive breast cancer.  In addition to CYP27A1, bicalutamide has been found to bind to and inhibit CYP46A1 (cholesterol 24-hydroxylase) \"in vitro\", but this has yet to be assessed and confirmed \"in vivo\". Bicalutamide, as well as enzalutamide, have been found to act as inhibitors of P-glycoprotein efflux and ATPase activity.  This action may reverse docetaxel resistance in prostate cancer cells by reducing transport of the drug out of these cells. All of the approved for the treatment of prostate cancer have been found to possess an off-target action of acting as weak non-competitive inhibitors of human GABA receptor currents \"in vitro\" to varying extents.  The IC values are 44 μM for flutamide (as hydroxyflutamide), 21 μM for nilutamide, 5.2 μM for bicalutamide, and 3.6 μM for enzalutamide.  In addition, flutamide, nilutamide, and enzalutamide have been found to cause convulsions and/or death in mice at sufficiently high doses.  Bicalutamide was notably not found to do this, but this was likely simply due to the limited central nervous system penetration of bicalutamide in this species.  In any case, enzalutamide is the only approved that has been found to be associated with a significantly increased incidence of seizures and other associated side effects clinically, so the relevance of the aforementioned findings with regard to bicalutamide and the other is unclear. Bicalutamide is extensively and well-absorbed following oral administration, and its absorption is not affected by food.  The absolute bioavailability of bicalutamide in humans is unknown due to its very low water solubility and hence lack of an assessable intravenous formulation.  However, the absolute bioavailability of bicalutamide has been found to be high in animals at low doses (72% in rats at 1 mg/kg; 100% in dogs at 0.1 mg/kg), but diminishes with increasing doses such that the bioavailability of bicalutamide is low at high doses (10% in rats at 250 mg/kg; 31% in dogs at 100 mg/kg).  In accordance, absorption of (\"R\")-bicalutamide in humans is slow and extensive but saturable, with steady-state levels increasing linearly at a dosage of up to 50 mg/day and non-linearly at higher dosages. At higher dosages of 100 to 200 mg/day, absorption of bicalutamide is approximately linear, with a small but increasing departure from linearity above 150 mg/day.  In terms of geometric mean steady-state concentrations of (\"R\")-bicalutamide, the departures from linearity were 4%, 13%, 17%, and 32% with dosages of 100, 150, 200, and 300 mg/day, respectively.  There is a plateau in steady-state levels of (\"R\")-bicalutamide with bicalutamide dosages above 300 mg/day, and, accordingly, dosages of bicalutamide of 300 to 600 mg/day result in similar circulating concentrations of (\"R\")-bicalutamide and similar degrees clinically of efficacy, tolerability, and toxicity.  Relative to 150 mg/day bicalutamide, levels of (\"R\")-bicalutamide are about 15% higher at a dosage of 200 mg/day and about 50% higher at a dosage of 300 mg/day.  In contrast to (\"R\")-bicalutamide, the inactive enantiomer (\"S\")-bicalutamide is much more rapidly absorbed (as well as cleared from circulation). Steady-state concentrations of the drug are reached after 4 to 12 weeks of administration independently of dosage, with an approximate 10- to 20-fold progressive accumulation of circulating levels of (\"R\")-bicalutamide.  In spite of the relatively long time to reach steady-state (which is a product of its long terminal half-life), there is evidence that the achieved blockade of bicalutamide is equivalent to that of flutamide by the end of the first day of treatment.  With single 50 mg and 150 mg doses of bicalutamide, mean peak concentrations (C) of (\"R\")-bicalutamide are 0.77 μg/mL (1.8 μmol/L) (at 31 hours) and 1.4 μg/mL (3.3 μmol/L) (at 39 hours), respectively.  At steady-state, mean circulating concentrations (C) of (\"R\")-bicalutamide with 50 mg/day and 150 mg/day bicalutamide are 8.9 μg/mL (21 μmol/L) and 22 μg/mL (51 μmol/L), respectively.  In another 150 mg/day bicalutamide study, mean circulating concentrations of (\"R\")-bicalutamide were 19.4 μg/mL (45.1 μmol/L) and 28.5 μg/mL (66.3 μmol/L) on days 28 and 84 (weeks 4 and 12) of treatment, respectively. The tissue distribution of bicalutamide is not well-characterized.  However, it has been reported that distribution studies with bicalutamide have shown that preferential (i.e., tissue-selective) accumulation in anabolic (e.g., muscle) tissues does not occur.  There are no available data on hepatic bicalutamide concentrations in humans, but a rat study found that oral bicalutamide treatment resulted in 4-fold higher concentrations of the drug in the liver relative to plasma (a common finding with orally administered drugs, due to transfer through the hepatic portal system prior to reaching circulation).  In men receiving 150 mg/day bicalutamide, concentrations of (\"R\")-bicalutamide in semen were 4.9 μg/mL (11 μmol/L), and the amount of the drug that could potentially be delivered to a female partner during sexual intercourse is regarded as low (estimated at 0.3 μg/kg) and below the amount that is required to induce changes in the offspring of laboratory animals.  Bicalutamide is highly protein-bound (96.1% for racemic bicalutamide, 99.6% for (\"R\")-bicalutamide), mainly to albumin.  It has negligible affinity for and no affinity for corticosteroid-binding globulin. Based on animal research, it was initially thought that bicalutamide was unable to cross the blood–brain barrier into the central nervous system and hence would be a peripherally-selective antiandrogen in humans.  This conclusion was drawn from the finding that bicalutamide does not increase or testosterone levels in multiple tested animal species (including rats and dogs), as antiandrogens like flutamide normally do this by blocking in the pituitary gland and hypothalamus in the brain and thereby disinhibiting the axis.  In humans however, bicalutamide has been found to increase and testosterone levels, and to a comparable extent relative to flutamide and nilutamide.  As such, it appears that there are species differences in the central penetration of bicalutamide and that the drug does indeed cross the blood–brain barrier and affect central function in humans, as supported by potential side effects, in spite of increased testosterone levels, like hot flashes and diminished sexual interest in men.  A newer , darolutamide, has been found to negligibly cross the blood–brain barrier in both animals \"and\" humans, and in accordance, unlike bicalutamide, does not increase or testosterone levels in humans. The metabolism of bicalutamide is hepatic and stereoselective.  The inactive (\"S\")-enantiomer is metabolized mainly by glucuronidation and is rapidly cleared from circulation, while the active (\"R\")-isomer is slowly hydroxylated and then glucuronidated.  In accordance, the active (\"R\")-enantiomer has a far longer half-life than the (\"S\")-isomer, and circulating levels of (\"R\")-bicalutamide are 10- to 20-fold and 100-fold higher than those of (\"S\")-bicalutamide after a single dose and at steady-state, respectively.  (\"R\")-Bicalutamide is almost exclusively metabolized via hydroxylation into (\"R\")-hydroxybicalutamide by the cytochrome P450 enzyme CYP3A4.  Bicalutamide is also glucuronidated by UGT1A9, a UDP-glucuronyltransferase, into bicalutamide glucuronide, and (\"R\")-hydroxybicalutamide glucuronide is formed from the metabolism of (\"R\")-hydroxybicalutamide by UGT1A9.  Similar to the inactive (\"S\")-enantiomer of bicalutamide, (\"R\")-hydroxybicalutamide is glucuronidated and rapidly cleared from circulation.  None of the metabolites of bicalutamide are known to be active.  Following administration of bicalutamide, only low concentrations of the metabolites are detectable in blood plasma, while unchanged bicalutamide predominates.  (\"R\")-Bicalutamide has a long terminal half-life of 5.8 days with a single dose, and a terminal half-life of 7–10 days with repeated administration, which allows for convenient once-daily dosing of bicalutamide. Bicalutamide is eliminated in feces (43%) and urine (34%), whereas its metabolites are eliminated in approximately equal proportions in urine and bile.  It is excreted to a substantial extent in its unmetabolized form, with both bicalutamide and its metabolites excreted mainly as glucuronide conjugates. The pharmacokinetics of bicalutamide are unaffected by food, age, body weight, renal impairment, and mild-to-moderate hepatic impairment.  However, it has been observed that steady-state concentrations of bicalutamide are higher in Japanese individuals than in Caucasians, indicating that ethnicity may be associated with differences in the pharmacokinetics of bicalutamide in some instances. Bicalutamide is a racemic mixture consisting of equal proportions of enantiomers (\"R\")-bicalutamide and (\"S\")-bicalutamide.  Its systematic name () is (\"RS\")-\"N\"-[4-cyano-3-(trifluoromethyl)phenyl]-3-[(4-fluorophenyl)sulfonyl]-2-hydroxy-2-methylpropanamide.  It has a chemical formula of CHFNOS, a molecular weight of 430.37 g/mol, and is a fine white to off-white powder.  The pKa' of bicalutamide is approximately 12.  It is a highly lipophilic compound (log P = 2.92).  At 37 °C (98.6 °F), or normal human body temperature, bicalutamide is practically insoluble in water (4.6 mg/L), acid (4.6 mg/L at pH 1), and alkali (3.7 mg/L at pH 8).  In organic solvents, it is slightly soluble in chloroform and absolute ethanol, sparingly soluble in methanol, and freely soluble in acetone and tetrahydrofuran. First-generation including bicalutamide, flutamide, and nilutamide are synthetic, nonsteroidal anilide (N-phenyl amide) derivatives and structural analogues of each other.  Bicalutamide is a diaryl propionamide, while flutamide is a monoarylpropionamide and nilutamide is a hydantoin.  Bicalutamide and flutamide, though not nilutamide, can also be classified as toluidides. All three of the compounds share a common 3-trifluoromethyl aniline moiety.  Bicalutamide is a modification of flutamide in which a 4-fluoro phenyl sulfonyl moiety has been added and the nitro group on the original phenyl ring has been replaced with a cyano group.  Topilutamide, also known as fluridil, is another that is closely related structurally to the first-generation , but, in contrast to them, is not used in the treatment of prostate cancer and is instead used exclusively as a topical antiandrogen in the treatment of androgenic alopecia. The second-generation enzalutamide and apalutamide were derived from and are analogues of the first-generation , while another second-generation , darolutamide, is said to be structurally distinct and chemically unrelated to the other .  Enzalutamide is a modification of bicalutamide in which the inter-ring linking chain has been altered and cyclized into a 5,5-dimethyl-4-oxo-2-thioxo imidazolidine moiety.  In apalutamide, the 5,5-dimethyl groups of the imidazolidine ring of enzalutamide are cyclized to form an accessory cyclobutane ring and one of its phenyl rings is replaced with a pyridine ring. In 1998, researchers discovered the first nonsteroidal androgens (the arylpropionamides) via structural modification of bicalutamide.  Unlike bicalutamide (which is purely antiandrogenic), these compounds show tissue-selective androgenic effects and were classified as selective androgen receptor modulators (SARMs).  Lead of this series included acetothiolutamide, enobosarm (ostarine; S-22), and andarine (acetamidoxolutamide or androxolutamide; S-4).  They are very close to bicalutamide structurally, with the key differences being that the linker sulfone of bicalutamide has been replaced with an ether or thioether group to confer agonism of the and the 4-fluoro atom of the pertinent phenyl ring has been substituted with an acetamido or cyano group to eliminate reactivity at the position. A few radiolabeled derivatives of bicalutamide have been developed for potential use as radiotracers in medical imaging.  They include [F]bicalutamide, 4-[Br]bromobicalutamide, and [Br]bromo-thiobicalutamide.  The latter two were found to have substantially increased affinity for the relative to that of bicautamide.  However, none of these agents have been evaluated in humans. A number of chemical syntheses of bicalutamide have been published in the literature. The first published chemical synthesis of bicalutamide (Tucker et al., 1988) proceeds as follows: All of the currently marketed , including bicalutamide, were derived from flutamide, which was originally synthesized as a bacteriostatic agent in 1967 at Schering Plough Corporation and was subsequently, and serendipitously, found to possess antiandrogen activity.  Bicalutamide was discovered by Tucker and colleagues at Imperial Chemical Industries in the 1980s and was selected for development from a group of over 1,000 synthesized compounds.  It was patented in 1982, first reported in the scientific literature in June 1987, first studied in a phase I clinical trial in 1987, and the results of the first phase II clinical trial in prostate cancer were published in 1990.  The pharmaceutical division of ICI was split out into an independent company called Zeneca in 1993, and in April and May 1995, Zeneca (now AstraZeneca, after merging with Astra AB in 1999) began pre-approval marketing of bicalutamide for the treatment of prostate cancer in the .  It was first launched in the in May 1995, and was subsequently approved by the on 4 October 1995, for the treatment of prostate cancer at a dosage of 50 mg/day in combination with a analogue. Subsequent to its introduction for use in combination with a analogue, bicalutamide was developed as a monotherapy at a dosage of 150 mg/day for the treatment of prostate cancer, and was approved for this indication in Europe, Canada, and a number of other countries in the early 2000s.  This application of bicalutamide was also under review by the in the in 2002, but ultimately was not approved in this country.  In Japan, bicalutamide is licensed at a dosage of 80 mg/day alone or in combination with a analogue for prostate cancer.  The unique 80 mg dosage of bicalutamide used in Japan was selected for development in this country on the basis of observed pharmacokinetic differences with bicalutamide in Japanese men. Following negative findings of bicalutamide monotherapy for in the trial, approval of bicalutamide for use specifically in the treatment of was withdrawn in a number of countries including the  (in October or November 2003) and several other European countries and Canada (in August 2003).  In addition, the and Canada explicitly recommended against the use of 150 mg/day bicalutamide for this indication.  The drug is effective for, remains approved for, and continues to be used in the treatment of and , on the other hand. The patent protection of bicalutamide expired in the in March 2009 and the drug has subsequently been available as a generic, at greatly reduced cost. Bicalutamide was the fourth antiandrogen (and the third ) to be introduced for the treatment of prostate cancer, following the in 1973 and the flutamide in 1983 (1989 in the ) and nilutamide in 1989 (1996 in the ).  It has been followed by abiraterone acetate in 2011 and enzalutamide in 2012, and may also be followed by in-development drugs such as apalutamide, darolutamide, galeterone, and seviteronel. \"Bicalutamide\" is the generic name of the drug in English and French and its  ,  ,  ,  ,  ,  , and  .  It is also referred to as \"bicalutamidum\" in Latin, \"bicalutamida\" in Spanish and Portuguese, \"bicalutamid\" in German, and \"bikalutamid\" in Russian and other Slavic languages. Bicalutamide is also known by its former developmental code name -176,334. Bicalutamide is marketed by AstraZeneca in oral tablet form under the brand names Casodex, Cosudex, Calutide, Calumid, and Kalumid in many countries.  It is also marketed under the brand names Bicadex, Bical, Bicalox, Bicamide, Bicatlon, Bicusan, Binabic, Bypro, Calutol, and Ormandyl among others in various countries.  The drug is sold under a large number of generic trade names such as Apo-Bicalutamide, Bicalutamide Accord, Bicalutamide Actavis, Bicalutamide Bluefish, Bicalutamide Kabi, Bicalutamide Sandoz, and Bicalutamide Teva as well.  A combination formulation of bicalutamide and goserelin is marketed by AstraZeneca in Australia and New Zealand under the brand name ZolaCos-CP. Bicalutamide is available for the treatment of prostate cancer in most developed countries, and is available in over 80 countries worldwide.  For an extensive list of countries in which bicalutamide is marketed along with its corresponding brand names, see here.  The drug is registered for use as a 150 mg/day monotherapy for the treatment of in at least 55 countries, with the being a notable exception (where it is registered only for use at a dosage of 50 mg/day in combination with castration). Bicalutamide is a prescription drug.  It is not specifically a controlled substance in any country and therefore is not an illegal drug.  However, the manufacture, sale, distribution, and possession of prescription drugs are all still subject to legal regulation throughout much of the world. Bicalutamide is off-patent and available as a generic, and its cost is very low in comparison to a number of other similar medications (from to for a 30-day supply of once-daily 50 mg tablets).  Brand name Casodex costs for a 30-day supply of once-daily 50 mg tablets as of 2017 .  Unlike bicalutamide, the newer enzalutamide is still on-patent, and for this reason, is far more expensive in comparison ( for a 30-day supply as of 2015 ). The patent protection of all three of the first-generation has expired and flutamide and bicalutamide are both available as relatively inexpensive generics.  Nilutamide, on the other hand, has always been a poor third competitor to flutamide and bicalutamide and, in relation to this fact, has not been developed as a generic and is only available as brand name Nilandron, at least in the Bicalutamide is far less expensive than analogues, which, in spite of some having been off-patent many years, have been reported (in 2013) to typically cost –$15,000 per year (or about per month) of treatment. Sales of bicalutamide (as Casodex) worldwide peaked at US$1.3 billion in 2007, and it has been described as a \"billion-dollar-a-year\" drug prior to losing its patent protection starting in 2007.  In 2014, despite the introduction of abiraterone acetate in 2011 and enzalutamide in 2012, bicalutamide was still the most commonly prescribed drug in the treatment of metastatic castration-resistant prostate cancer (mCRPC).  Moreover, in spite of being off-patent, bicalutamide was said to still generate a few hundred million dollars in sales per year for AstraZeneca.  Total worldwide sales of brand name Casodex were approximately US$13.0 billion as of the end of 2016. Between January 2007 and December 2009 (a period of three years), 1,232,143 prescriptions of bicalutamide were dispensed in the , or about 400,000 prescriptions per year.  During that time, bicalutamide accounted for about 87.2% of the market, while flutamide accounted for 10.5% of it and nilutamide for 2.3% of it.  Approximately 96% of bicalutamide prescriptions were written for diagnosis codes that clearly indicated neoplasm.  About 1,200, or 0.1% of bicalutamide prescriptions were dispensed to pediatric patients (age 0–16). A phase II clinical trial of bicalutamide with everolimus in has been conducted. Bicalutamide has been studied in combination with the 5α-reductase inhibitors finasteride and dutasteride in prostate cancer. Bicalutamide has been studied in the treatment of benign prostatic hyperplasia (BPH) in a 24-week trial of 15 patients at a dosage of 50 mg/day.  Prostate volume decreased by 26% in patients taking bicalutamide and urinary irritative symptom scores significantly decreased.  Conversely, peak urine flow rates and urine pressure flow examinations were not significantly different between bicalutamide and placebo.  The decrease in prostate volume achieved with bicalutamide was comparable to that observed with the 5α-reductase inhibitor finasteride, which is approved for the treatment of BPH.  Breast tenderness (93%), gynecomastia (54%), and sexual dysfunction (60%) were all reported as side effects of bicalutamide at the dosage used in the study, although no treatment discontinuations due to adverse effects occurred and sexual functioning was maintained in 75% of patients. Bicalutamide has been tested for the treatment of -positive /-negative locally advanced and metastatic breast cancer in a phase II study for this indication.  Enzalutamide is also being investigated for this type of cancer. Bicalutamide has been studied in a phase II clinical trial for ovarian cancer. Bicalutamide may be used to treat hyperandrogenism and associated benign prostatic hyperplasia secondary to hyperadrenocorticism (caused by excessive adrenal androgens) in male ferrets. However, it has not been formally assessed in controlled studies for this purpose.\n\nHistory of radar The history of radar started with experiments by Heinrich Hertz in the late 19th century that showed that radio waves were reflected by metallic objects.  This possibility was suggested in James Clerk Maxwell's seminal work on electromagnetism.  However, it was not until the early 20th century that systems able to use these principles were becoming widely available, and it was German inventor Christian Hülsmeyer who first used them to build a simple ship detection device intended to help avoid collisions in fog (Reichspatent Nr.  165546).  Numerous similar systems, which provided directional information to objects over short ranges, were developed over the next two decades. The development of systems able to produce short pulses of radio energy was the key advance that allowed modern radar systems to come into existence.  By timing the pulses on an oscilloscope, the range could be determined and the direction of the antenna revealed the angular location of the targets.  The two, combined, produced a \"fix\", locating the target relative to the antenna.  In the 1934–1939 period, eight nations developed independently, and in great secrecy, systems of this type: the United Kingdom, Germany, the United States, the USSR, Japan, the Netherlands, France, and Italy.  In addition, Britain shared their information with the United States and four Commonwealth countries: Australia, Canada, New Zealand, and South Africa, and these countries also developed their own radar systems.  During the war, Hungary was added to this list.  The term \"RADAR\" was coined in 1939 by the United States Signal Corps as it worked on these systems for the Navy. Progress during the war was rapid and of great importance, probably one of the decisive factors for the victory of the Allies.  A key development was the magnetron in the UK, which allowed the creation of relatively small systems with sub-meter resolution.  By the end of hostilities, Britain, Germany, the United States, the USSR, and Japan had a wide diversity of land- and sea-based radars as well as small airborne systems.  After the war, radar use was widened to numerous fields including: civil aviation, marine navigation, radar guns for police, meteorology and even medicine.  Key developments in the post-war period include the travelling wave tube as a way to produce large quantities of coherent microwaves, the development of signal delay systems that led to phased array radars, and ever-increasing frequencies that allow higher resolutions.  Increases in signal processing capability due to the introduction of solid state computers has also had a large impact on radar use. The place of radar in the larger story of science and technology is argued differently by different authors.  On the one hand, radar contributed very little to theory, which was largely known since the days of Maxwell and Hertz.  Therefore, radar did not advance science, but was simply a matter of technology and engineering.  Maurice Ponte, one of the developers of radar in France, states: The fundamental principle of the radar belongs to the common patrimony of the physicists; after all, what is left to the real credit of the technicians is measured by the effective realisation of operational materials. But others point out the immense practical consequences of the development of radar.  Far more than the atomic bomb, radar contributed to the Allied victory in World War II.  Robert Buderi states that it was also the precursor of much modern technology.  From a review of his book: ... radar has been the root of a wide range of achievements since the war, producing a veritable family tree of modern technologies.  Because of radar, astronomers can map the contours of far-off planets, physicians can see images of internal organs, meteorologists can measure rain falling in distant places, air travel is hundreds of times safer than travel by road, long-distance telephone calls are cheaper than postage, computers have become ubiquitous and ordinary people can cook their daily dinners in the time between sitcoms, with what used to be called a \"radar range.\" In 1886–1888 the German physicist Heinrich Hertz conducted his series of experiments that proved the existence of electromagnetic waves (including radio waves), predicted in equations developed in 1862–4 by the Scottish physicist James Clerk Maxwell.  In Hertz's 1887 experiment he found that these waves would transmit through different types of materials and also would reflect off metal surfaces in his lab as well as conductors and dielectrics.  The nature of these waves being similar to visible light in their ability to be reflected, refracted, and polarized would be shown by Hertz and subsequent experiments by other physicists. Radio pioneer Guglielmo Marconi noticed radio waves were being reflected back to the transmitter by objects in radio beacon experiments he conducted on March 3, 1899 on Salisbury Plain.  In 1916 he and a British engineer called Charles Samuel Franklin used short-waves in their experiments, critical to the practical development of radar.  He would relate his findings 6 years later in a 1922 paper delivered before the Institution of Electrical Engineers in London: I also described tests carried out in transmitting a beam of reflected waves across country ... and pointed out the possibility of the utility of such a system if applied to lighthouses and lightships, so as to enable vessels in foggy weather to locate dangerous points around the coasts ... It [now] seems to me that it should be possible to design [an] apparatus by means of which a ship could radiate or project a divergent beam of these rays in any desired direction, which rays, if coming across a metallic object, such as another steamer or ship, would be reflected back to a receiver screened from the local transmitter on the sending ship, and thereby immediately reveal the presence and bearing of the other ship in fog or thick weather. In 1904, Christian Hülsmeyer gave public demonstrations in Germany and the Netherlands of the use of radio echoes to detect ships so that collisions could be avoided.  His device consisted of a simple spark gap used to generate a signal that was aimed using a dipole antenna with a cylindrical parabolic reflector.  When a signal reflected from a ship was picked up by a similar antenna attached to the separate coherer receiver, a bell sounded.  During bad weather or fog, the device would be periodically spun to check for nearby ships.  The apparatus detected the presence of ships up to 3 km , and Hülsmeyer planned to extend its capability to 10 km .  It did not provide range (distance) information, only warning of a nearby object.  He patented the device, called the \"telemobiloscope\", but due to lack of interest by the naval authorities the invention was not put into production. Hülsmeyer also received a patent amendment for estimating the range to the ship.  Using a vertical scan of the horizon with the \"telemobiloscope\" mounted on a tower, the operator would find the angle at which the return was the most intense and deduce, by simple triangulation, the approximate distance.  This is in contrast to the later development of pulsed radar, which determines distance via two-way transit time of the pulse. In 1915, Robert Watson Watt joined the Meteorological Office as a meteorologist, working at an outstation at Aldershot in Hampshire.  Over the next 20 years, he studied atmospheric phenomena and developed the use of radio signals generated by lightning strikes to map out the position of thunderstorms.  The difficulty in pinpointing the direction of these fleeting signals using rotatable directional antennas led, in 1923, to the use of oscilloscopes in order to display the signals.  The operation eventually moved to the outskirts of Slough in Berkshire, and in 1927 formed the Radio Research Station (RRS), Slough, an entity under the Department of Scientific and Industrial Research (DSIR).  Watson Watt was appointed the RRS Superintendent. As war clouds gathered over Britain, the likelihood of air raids and the threat of invasion by air and sea drove a major effort in applying science and technology to defence.  In November 1934, the Air Ministry established the Committee for the Scientific Survey of Air Defence (CSSAD) with the official function of considering \"how far recent advances in scientific and technical knowledge can be used to strengthen the present methods of defence against hostile aircraft\".  Commonly called the \"Tizard Committee\" after its Chairman, Sir Henry Tizard, this group had a profound influence on technical developments in Britain. H. E. Wimperis, Director of Scientific Research at the Air Ministry and a member of the Tizard Committee, had read about a German newspaper article claiming that the Germans had built a death ray using radio signals, accompanied by an image of a very large radio antenna.  Both concerned and potentially excited by this possibility, but highly skeptical at the same time, Wimperis looked for an expert in the field of radio propagation who might be able to pass judgement on the concept.  Watt, Superintendent of the RRS, was now well established as an authority in the field of radio, and in January 1935, Wimperis contacted him asking if radio might be used for such a device.  After discussing this with his scientific assistant, Arnold F. 'Skip' Wilkins, Wilkins quickly produced a back-of-the-envelope calculation that showed the energy required would be enormous.  Watt wrote back that this was unlikely, but added the following comment: \"Attention is being turned to the still difficult, but less unpromising, problem of radio detection and numerical considerations on the method of detection by reflected radio waves will be submitted when required\". Over the following several weeks, Wilkins considered the radio detection problem.  He outlined an approach and backed it with detailed calculations of necessary transmitter power, reflection characteristics of an aircraft, and needed receiver sensitivity.  He proposed using a directional receiver based on Watt's lightning detection concept, listening for powerful signals from a separate transmitter.  Timing, and thus distance measurements, would be accomplished by triggering the oscilloscope's trace with a muted signal from the transmitter, and then simply measuring the returns against a scale.  Watson Watt sent this information to the Air Ministry on February 12, 1935, in a secret report titled \"The Detection of Aircraft by Radio Methods\". Reflection of radio signals was critical to the proposed technique, and the Air Ministry asked if this could be proven.  To test this, Wilkins set up receiving equipment in a field near Upper Stowe, Northamptonshire.  On February 26, 1935, a Handley Page Heyford bomber flew along a path between the receiving station and the transmitting towers of a BBC shortwave station in nearby Daventry.  The aircraft reflected the 6 MHz (49 m) BBC signal, and this was readily detected by Arnold \"Skip\" Wilkins using Doppler-beat interference at ranges up to 8 mi .  This convincing test, known as the \"Daventry Experiment\", was witnessed by a representative from the Air Ministry, and led to the immediate authorization to build a full demonstration system.  This experiment was later reproduced by Wilkins for the 1977 BBC television series \"The Secret War\" episode \"To See a Hundred Miles\". Based on pulsed transmission as used for probing the ionosphere, a preliminary system was designed and built at the RRS by the team.  Their existing transmitter had a peak power of about 1 kW, and Wilkins had estimated that 100 kW would be needed.  Edward George Bowen was added to the team to design and build such a transmitter.  Bowens’ transmitter operated at 6 MHz (50 m), had a pulse-repetition rate of 25 Hz, a pulse width of 25 μs, and approached the desired power. Orfordness, a narrow 19 mi peninsula in Suffolk along the coast of the North Sea, was selected as the test site.  Here the equipment would be openly operated in the guise of an ionospheric monitoring station.  In mid-May 1935, the equipment was moved to Orfordness.  Six wooden towers were erected, two for stringing the transmitting antenna, and four for corners of crossed receiving antennas.  In June, general testing of the equipment began. On June 17, the first target was detected—a Supermarine Scapa flying boat at 17 mi range.  It is historically correct that, on June 17, 1935, radio-based detection and ranging was first demonstrated in Britain.  Watson Watt, Wilkins, and Bowen are generally credited with initiating what would later be called radar in this nation. In December 1935, the British Treasury appropriated £60,000 for a five-station system called Chain Home (CH), covering approaches to the Thames Estuary.  The secretary of the Tizard Committee, Albert Percival Rowe, coined the acronym RDF as a cover for the work, meaning Range and Direction Finding but suggesting the already well-known Radio Direction Finding. Late in 1935, responding to Lindemann's recognition of the need for night detection and interception gear, and realizing existing transmitters were too heavy for aircraft, Bowen proposed fitting only receivers, what would later be called bistatic radar.  Frederick Lindemann's proposals for infrared sensors and aerial mines would prove impractical.  It would take Bowen's efforts, at the urging of Tizard, who became increasingly concerned about the need, to see Air to Surface Vessel (ASV), and through it Airborne Interception (AI), radar to fruition. In 1937, Bowen's team set their crude ASV radar, the world's first airborne set, to detect the Home Fleet in dismal weather.  Only in spring 1939, \"as a matter of great urgency\" after the failure of the searchlight system Silhouette, did attention turn to using ASV for air-to-air interception (AI).  Demonstrated in June 1939, AI got a warm reception from Air Chief Marshal Hugh Dowding, and even more so from Churchill.  This proved problematic.  Its accuracy, dependent on the height of the aircraft, meant that CH, capable of only 4 sm , was not accurate enough to place an aircraft within its detection range, and an additional system was required.  Its wooden chassis had a disturbing tendency to catch fire (even with attention from expert technicians), so much so that Dowding, when told that Watson-Watt could provide hundreds of sets, demanded \"ten that work\".  The Cossor and MetroVick sets were overweight for aircraft use and the RAF lacked night fighter pilots, observers, and suitable aircraft. In 1940, John Randall and Harry Boot developed the cavity magnetron, which made ten-centimetre ( wavelength ) radar a reality.  This device, the size of a small dinner plate, could be carried easily on aircraft and the short wavelength meant the antenna would also be small and hence suitable for mounting on aircraft.  The short wavelength and high power made it very effective at spotting submarines from the air. To aid Chain Home in making height calculations, at Dowding's request, the Electrical Calculator Type Q (commonly called the \"Fruit Machine\") was introduced in 1940. The solution to night intercepts would be provided by Dr. W. B. \"Ben\" Lewis, who proposed a new, more accurate ground control display, the Plan Position Indicator (PPI), a new Ground-Controlled Interception (GCI) radar, and reliable AI radar.  The AI sets would ultimately be built by EMI.  GCI was unquestionably delayed by Watson-Watt's opposition to it and his belief that CH was sufficient, as well as by Bowen's preference for using ASV for navigation, despite Bomber Command disclaiming a need for it, and by Tizard's reliance on the faulty Silhouette system. In March 1936, the work at Orfordness was moved to Bawdsey Manor, nearby on the mainland.  Until this time, the work had officially still been under the DSIR, but was now transferred to the Air Ministry.  At the new Bawdsey Research Station, the Chain Home (CH) equipment was assembled as a prototype.  There were equipment problems when the Royal Air Force (RAF) first exercised the prototype station in September 1936.  These were cleared by the next April, and the Air Ministry started plans for a larger network of stations. Initial hardware at CH stations was as follows: The transmitter operated on four pre-selected frequencies between 20 and 55 MHz, adjustable within 15 seconds, and delivered a peak power of 200 kW.  The pulse duration was adjustable between 5 and 25 μs, with a repetition rate selectable as either 25 or 50 Hz.  For synchronization of all CH transmitters, the pulse generator was locked to the 50 Hz of the British power grid.  Four 360 ft steel towers supported transmitting antennas, and four 240 ft wooden towers supported cross-dipole arrays at three different levels.  A goniometer was used to improve the directional accuracy from the multiple receiving antennas. By the summer of 1937, 20 initial CH stations were in check-out operation.  A major RAF exercise was performed before the end of the year, and was such a success that £10,000,000 was appropriated by the Treasury for an eventual full chain of coastal stations.  At the start of 1938, the RAF took over control of all CH stations, and the network began regular operations. In May 1938, Rowe replaced Watson Watt as Superintendent at Bawdsey.  In addition to the work on CH and successor systems, there was now major work in airborne RDF equipment.  This was led by E. G. Bowen and centered on 200-MHz (1.5 m) sets.  The higher frequency allowed smaller antennas, appropriate for aircraft installation. From the initiation of RDF work at Orfordness, the Air Ministry had kept the British Army and the Royal Navy generally informed; this led to both of these forces having their own RDF developments. In 1931, at the Woolwich Research Station of the Army’s Signals Experimental Establishment (SEE), W. A. S. Butement and P. E. Pollard had examined pulsed 600 MHz (50-cm) signals for detection of ships.  Although they prepared a memorandum on this subject and performed preliminary experiments, for undefined reasons the War Office did not give it consideration. As the Air Ministry’s work on RDF progressed, Colonel Peter Worlledge of the Royal Engineer and Signals Board met with Watson Watt and was briefed on the RDF equipment and techniques being developed at Orfordness.  His report, “The Proposed Method of Aeroplane Detection and Its Prospects”, led the SEE to set up an “Army Cell” at Bawdsey in October 1936.  This was under E. Talbot Paris and the staff included Butement and Pollard.  The Cell’s work emphasize two general types of RDF equipment: gun-laying (GL) systems for assisting anti-aircraft guns and searchlights, and coastal- defense (CD) systems for directing coastal artillery and defense of Army bases overseas. Pollard led the first project, a gun-laying RDF code-named Mobile Radio Unit (MRU).  This truck-mounted system was designed as a small version of a CH station.  It operated at 23 MHz (13 m) with a power of 300 kW.  A single 105 ft tower supported a transmitting antenna, as well as two receiving antennas set orthogonally for estimating the signal bearing.  In February 1937, a developmental unit detected an aircraft at a range of 60 miles (96 km).  The Air Ministry also adopted this system as a mobile auxiliary to the CH system. In early 1938, Butement started the development of a CD system based on Bowen’s evolving 200-MHz (1.5-m) airborne sets.  The transmitter had a 400 Hz pulse rate, a 2-μs pulse width, and 50 kW power (later increased to 150 kW).  Although many of Bowen’s transmitter and receiver components were used, the system would not be airborne so there were no limitations on antenna size. Primary credit for introducing beamed RDF systems in Britain must be given to Butement.  For the CD, he developed a large dipole array, 10 ft high and 24 ft wide, giving much narrower beams and higher gain.  This could be rotated at a speed up to 1.5 revolutions per minute.  For greater directional accuracy, lobe switching on the receiving antennas was adopted.  As a part of this development, he formulated the first – at least in Britain – mathematical relationship that would later become well known as the “radar range equation”. By May 1939, the CD RDF could detect aircraft flying as low as 500 ft and at a range of 25 mi .  With an antenna 60 ft above sea level, it could determine the range of a 2,000-ton ship at 24 mi and with an angular accuracy of as little as a quarter of a degree. Although the Royal Navy maintained close contact with the Air Ministry work at Bawdsey, they chose to establish their own RDF development at the Experimental Department of His Majesty’s Signal School (HMSS) in Portsmouth, Hampshire, on the south coast. HMSS started RDF work in September 1935.  Initial efforts, under R. F. Yeo, were in frequencies between 75 MHz (4 m) and 1.2 GHz (25 cm).  All of the work was under the utmost secrecy; it could not even be discussed with other scientists and engineers at Portsmouth.  A 75 MHz range-only set was eventually developed and designated Type 79X.  Basic tests were done using a training ship, but the operation was unsatisfactory. In August 1937, the RDF development at HMSS changed, with many of their best researchers brought into the activity.  John D. S. Rawlinson was made responsible for improving the Type 79X.  To increase the efficiency, he decreased the frequency to 43 MHz ( 7 metre wavelength ).  Designated Type 79Y, it had separate, stationary transmitting and receiving antennas. Prototypes of the Type 79Y air-warning system were successfully tested at sea in early 1938.  The detection range on aircraft was between 30 and 50 miles (48 and 80 km), depending on height.  The systems were then placed into service in August on the cruiser HMS \"Sheffield\" and in October on the battleship HMS \"Rodney\".  These were the first vessels in the Royal Navy with RDF systems. A radio-based device for remotely indicating the presence of ships was built in Germany by Christian Hülsmeyer in 1904.  Often referred to as the first radar system, this did not directly measure the range (distance) to the target, and thus did not meet the criteria to be given this name. Over the following three decades in Germany, a number of radio-based detection systems were developed but none were true radars.  This situation changed before World War II.  Developments in three leading industries are described. In the early 1930s, physicist Rudolf Kühnhold, Scientific Director at the \"Kriegsmarine\" (German navy) \"Nachrichtenmittel-Versuchsanstalt\" (NVA—Experimental Institute of Communication Systems) in Kiel, was attempting to improve the acoustical methods of underwater detection of ships.  He concluded that the desired accuracy in measuring distance to targets could be attained only by using pulsed electromagnetic waves. During 1933, Kühnhold first attempted to test this concept with a transmitting and receiving set that operated in the microwave region at 13.5 cm (2.22 GHz).  The transmitter used a Barkhausen-Kurz tube (the first microwave generator) that produced only 0.1 watt.  Unsuccessful with this, he asked for assistance from Paul-Günther Erbslöh and Hans-Karl Freiherr von Willisen, amateur radio operators who were developing a VHF system for communications.  They enthusiastically agreed, and in January 1934, formed a company, \"Gesellschaft für Elektroakustische und Mechanische Apparate\" (GEMA), for the effort.  From the start, the firm was always called simply GEMA. Work on a \"Funkmessgerät für Untersuchung\" (radio measuring device for research) began in earnest at GEMA.  Hans Hollmann and Theodor Schultes, both affiliated with the prestigious Heinrich Hertz Institute in Berlin, were added as consultants.  The first apparatus used a split-anode magnetron purchased from Philips in the Netherlands.  This provided about 70 W at 50 cm (600 MHz), but suffered from frequency instability.  Hollmann built a regenerative receiver and Schultes developed Yagi antennas for transmitting and receiving.  In June 1934, large vessels passing through the Kiel Harbor were detected by Doppler-beat interference at a distance of about 2 km .  In October, strong reflections were observed from an aircraft that happened to fly through the beam; this opened consideration of targets other than ships. Kühnhold then shifted the GEMA work to a pulse-modulated system.  A new 50 cm (600 MHz) Philips magnetron with better frequency stability was used.  It was modulated with 2- μs pulses at a PRF of 2000 Hz.  The transmitting antenna was an array of 10 pairs of dipoles with a reflecting mesh.  The wide-band regenerative receiver used Acorn tubes from RCA, and the receiving antenna had three pairs of dipoles and incorporated lobe switching.  A blocking device (a duplexer), shut the receiver input when the transmitter pulsed.  A Braun tube (a CRT) was used for displaying the range. The equipment was first tested at a NVA site at the Lübecker Bay near Pelzerhaken.  During May 1935, it detected returns from woods across the bay at a range of 15 km .  It had limited success, however, in detecting a research ship, \"Welle\", only a short distance away.  The receiver was then rebuilt, becoming a super-regenerative set with two intermediate-frequency stages.  With this improved receiver, the system readily tracked vessels at up to 8 km range. In September 1935, a demonstration was given to the Commander-in-Chief of the \"Kriegsmarine\".  The system performance was excellent; the range was read off the Braun tube with a tolerance of 50 meters (less than 1 percent variance), and the lobe switching allowed a directional accuracy of 0.1 degree.  Historically, this marked the first naval vessel equipped with radar.  Although this apparatus was not put into production, GEMA was funded to develop similar systems operating around 50 cm (500 MHz).  These became the Seetakt for the \"Kriegsmarine\" and the Freya for the \"Luftwaffe\" (German Air Force). Kühnhold remained with the NVA, but also consulted with GEMA.  He is considered by many in Germany as the Father of Radar.  During 1933–6, Hollmann wrote the first comprehensive treatise on microwaves, \"Physik und Technik der ultrakurzen Wellen\" (Physics and Technique of Ultrashort Waves), Springer 1938. In 1933, when Kühnhold at the NVA was first experimenting with microwaves, he had sought information from Telefunken on microwave tubes.  (Telefunken was the largest supplier of radio products in Germany) There, Wilhelm Tolmé Runge had told him that no vacuum tubes were available for these frequencies.  In fact, Runge was already experimenting with high-frequency transmitters and had Telefunken’s tube department working on cm-wavelength devices. In the summer of 1935, Runge, now Director of Telefunken’s Radio Research Laboratory, initiated an internally funded project in radio-based detection.  Using Barkhausen-Kurz tubes, a 50 cm (600 MHz) receiver and 0.5-W transmitter were built.  With the antennas placed flat on the ground some distance apart, Runge arranged for an aircraft to fly overhead and found that the receiver gave a strong Doppler-beat interference signal. Runge, now with Hans Hollmann as a consultant, continued in developing a 1.8 m (170 MHz) system using pulse-modulation.  Wilhelm Stepp developed a transmit-receive device (a duplexer) for allowing a common antenna.  Stepp also code-named the system \"Darmstadt\" after his home town, starting the practice in Telefunken of giving the systems names of cities.  The system, with only a few watts transmitter power, was first tested in February 1936, detecting an aircraft at about 5 km distance.  This led the \"Luftwaffe\" to fund the development of a 50 cm (600 MHz) gun-laying system, the \"Würzburg\". Since before the First World War, Standard Elektrik Lorenz had been the main supplier of communication equipment for the German military and was the main rival of Telefunken.  In late 1935, when Lorenz found that Runge at Telefunken was doing research in radio-based detection equipment, they started a similar activity under Gottfried Müller.  A pulse-modulated set called \"Einheit für Abfragung\" (DFA – Device for Detection) was built.  It used a type DS-310 tube (similar to the Acorn) operating at 70 cm (430 MHz) and about 1 kW power, it had identical transmitting and receiving antennas made with rows of half-wavelength dipoles backed by a reflecting screen. In early 1936, initial experiments gave reflections from large buildings at up to about 7 km .  The power was doubled by using two tubes, and in mid-1936, the equipment was set up on cliffs near Kiel, and good detections of ships at 7 km and aircraft at 4 km were attained. The success of this experimental set was reported to the \"Kriegsmarine\", but they showed no interest; they were already fully engaged with GEMA for similar equipment.  Also, because of extensive agreements between Lorenz and many foreign countries, the naval authorities had reservations concerning the company handling classified work.  The DFA was then demonstrated to the \"Heer\" (German Army), and they contracted with Lorenz for developing \"Kurfürst\" (Elector), a system for supporting \"Flugzeugabwehrkanone\" (Flak, anti-aircraft guns). In the United States, both the Navy and Army needed means of remotely locating enemy ships and aircraft.  In 1930, both services initiated the development of radio equipment that could meet this need.  There was little coordination of these efforts; thus, they will be described separately. In the autumn of 1922, Albert H. Taylor and Leo C. Young at the U.S. Naval Aircraft Radio Laboratory were conducting communication experiments when they noticed that a wooden ship in the Potomac River was interfering with their signals.  They prepared a memorandum suggesting that this might be used for ship detection in a harbor defense, but their suggestion was not taken up.  In 1930, Lawrence A. Hyland working with Taylor and Young, now at the U.S. Naval Research Laboratory (NRL) in Washington, D.C., used a similar arrangement of radio equipment to detect a passing aircraft.  This led to a proposal and patent for using this technique for detecting ships and aircraft. A simple wave-interference apparatus can detect the presence of an object, but it cannot determine its location or velocity.  That had to await the invention of pulsed radar, and later, additional encoding techniques to extract this information from a CW signal.  When Taylor's group at the NRL were unsuccessful in getting interference radio accepted as a detection means, Young suggested trying pulsing techniques.  This would also allow the direct determination of range to the target.  In 1924, Hyland and Young had built such a transmitter for Gregory Breit and Merle A. Tuve at the Carnegie Institution of Washington for successfully measuring the height of the ionosphere. Robert Morris Page was assigned by Taylor to implement Young's suggestion.  Page designed a transmitter operating at 60 MHz and pulsed 10 μs in duration and 90 μs between pulses.  In December 1934, the apparatus was used to detect a plane at a distance of one mile (1.6 km) flying up and down the Potomac.  Although the detection range was small and the indications on the oscilloscope monitor were almost indistinct, it demonstrated the basic concept of a pulsed radar system.  Based on this, Page, Taylor, and Young are usually credited with building and demonstrating the world’s first true radar. An important subsequent development by Page was the duplexer, a device that allowed the transmitter and receiver to use the same antenna without overwhelming or destroying the sensitive receiver circuitry.  This also solved the problem associated with synchronization of separate transmitter and receiver antennas which is critical to accurate position determination of long-range targets. The experiments with pulsed radar were continued, primarily in improving the receiver for handling the short pulses.  In June 1936, the NRL's first prototype radar system, now operating at 28.6 MHz, was demonstrated to government officials, successfully tracking an aircraft at distances up to 25 mi .  Their radar was based on low frequency signals, at least by today's standards, and thus required large antennas, making it impractical for ship or aircraft mounting. Antenna size is inversely proportional to the operating frequency; therefore, the operating frequency of the system was increased to 200 MHz, allowing much smaller antennas.  The frequency of 200 MHz was the highest possible with existing transmitter tubes and other components.  The new system was successfully tested at the NRL in April 1937, That same month, the first sea-borne testing was conducted.  The equipment was temporarily installed on the USS \"Leary\", with a Yagi antenna mounted on a gun barrel for sweeping the field of view. Based on success of the sea trials, the NRL further improved the system.  Page developed the ring oscillator, allowing multiple output tubes and increasing the pulse-power to 15 kW in 5-µs pulses.  A 20-by-23 ft (6 x 7 m), stacked-dipole “bedspring” antenna was used.  In laboratory test during 1938, the system, now designated XAF, detected planes at ranges up to 100 mi .  It was installed on the battleship USS \"New York\" for sea trials starting in January 1939, and became the first operational radio detection and ranging set in the U.S. fleet. In May 1939, a contract was awarded to RCA for production.  Designated CXAM, deliveries started in May 1940.  The acronym RADAR was coined from \"Radio Detection And Ranging\".  One of the first CXAM systems was placed aboard the USS \"California\", a battleship that was sunk in the Japanese attack on Pearl Harbor on December 7, 1941. As the Great Depression started, economic conditions led the U.S. Army Signal Corps to consolidate its widespread laboratory operations to Fort Monmouth, New Jersey.  On June 30, 1930, these were designated the Signal Corps Laboratories (SCL) and Lt. Colonel (Dr.) William R. Blair was appointed the SCL Director. Among other activities, the SCL was made responsible for research in the detection of aircraft by acoustical and infrared radiation means.  Blair had performed his doctoral research in the interaction of electromagnet waves with solid materials, and naturally gave attention to this type of detection.  Initially, attempts were made to detect infrared radiation, either from the heat of aircraft engines or as reflected from large searchlights with infrared filters, as well as from radio signals generated by the engine ignition. Some success was made in the infrared detection, but little was accomplished using radio.  In 1932, progress at the Naval Research Laboratory (NRL) on radio interference for aircraft detection was passed on to the Army.  While it does not appear that any of this information was used by Blair, the SCL did undertake a systematic survey of what was then known throughout the world about the methods of generating, modulating, and detecting radio signals in the microwave region. The SCL's first definitive efforts in radio-based target detection started in 1934 when the Chief of the Army Signal Corps, after seeing a microwave demonstration by RCA, suggested that radio-echo techniques be investigated.  The SCL called this technique radio position-finding (RPF).  Based on the previous investigations, the SCL first tried microwaves.  During 1934 and 1935, tests of microwave RPF equipment resulted in Doppler-shifted signals being obtained, initially at only a few hundred feet distance and later greater than a mile.  These tests involved a bi-static arrangement, with the transmitter at one end of the signal path and the receiver at the other, and the reflecting target passing through or near the path. Blair was evidently not aware of the success of a pulsed system at the NRL in December 1934.  In an internal 1935 note, Blair had commented: Consideration is now being given to the scheme of projecting an interrupted sequence of trains of oscillations against the target and attempting to detect the echoes during the interstices between the projections. In 1936, W. Delmar Hershberger, SCL’s Chief Engineer at that time, started a modest project in pulsed microwave transmission.  Lacking success with microwaves, Hershberger visited the NRL (where he had earlier worked) and saw a demonstration of their pulsed set.  Back at the SCL, he and Robert H. Noyes built an experimental apparatus using a 75 watt, 110 MHz (2.73 m) transmitter with pulse modulation and a receiver patterned on the one at the NRL.  A request for project funding was turned down by the War Department, but $75,000 for support was diverted from a previous appropriation for a communication project. In October 1936, Paul E. Watson became the SCL Chief Engineer and led the project.  A field setup near the coast was made with the transmitter and receiver separated by a mile.  On December 14, 1936, the experimental set detected at up to 7 mi range aircraft flying in and out of New York City. Work then began on a prototype system.  Ralph I. Cole headed receiver work and William S. Marks lead transmitter improvements.  Separate receivers and antennas were used for azimuth and elevation detection.  Both receiving and the transmitting antennas used large arrays of dipole wires on wooden frames.  The system output was intended to aim a searchlight.  The first demonstration of the full set was made on the night of May 26, 1937.  A bomber was detected and then illuminated by the searchlight.  The observers included the Secretary of War, Henry A. Woodring; he was so impressed that the next day orders were given for the full development of the system.  Congress gave an appropriation of $250,000. The frequency was increased to 200 MHz (1.5 m).  The transmitter used 16 tubes in a ring oscillator circuit (developed at the NRL), producing about 75 kW peak power.  Major James C. Moore was assigned to head the complex electrical and mechanical design of lobe switching antennas.  Engineers from Western Electric and Westinghouse were brought in to assist in the overall development.  Designated SCR-268, a prototype was successfully demonstrated in late 1938 at Fort Monroe, Virginia.  The production of SCR-268 sets was started by Western Electric in 1939, and it entered service in early 1941. Even before the SCR-268 entered service, it had been greatly improved.  In a project led by Major (Dr.) Harold A. Zahl, two new configurations evolved – the SCR-270 (mobile) and the SCR-271 (fixed-site).  Operation at 106 MHz (2.83 m) was selected, and a single water-cooled tube provided 8 kW (100 kW pulsed) output power.  Westinghouse received a production contract, and started deliveries near the end of 1940. The Army deployed five of the first SCR-270 sets around the island of Oahu in Hawaii.  At 7:02 on the morning of December 7, 1941, one of these radars detected a flight of aircraft at a range of 136 mi due north.  The observation was passed on to an aircraft warning center where it was misidentified as a flight of U.S. bombers known to be approaching from the mainland.  The alarm went unheeded, and at 7:48, the Japanese aircraft first struck at Pearl Harbor. In 1895, Alexander Stepanovich Popov, a physics instructor at the Imperial Russian Navy school in Kronstadt, developed an apparatus using a coherer tube for detecting distant lightning strikes.  The next year, he added a spark-gap transmitter and demonstrated the first radio communication set in Russia.  During 1897, while testing this in communicating between two ships in the Baltic Sea, he took note of an interference beat caused by the passage of a third vessel.  In his report, Popov wrote that this phenomenon might be used for detecting objects, but he did nothing more with this observation. In a few years following the 1917 Russian Revolution and the establishment the Union of Soviet Socialist Republics (USSR or Soviet Union) in 1924, Germany’s \"Luftwaffe\" had aircraft capable of penetrating deep into Soviet territory.  Thus, the detection of aircraft at night or above clouds was of great interest to the Soviet Air Defense Forces (PVO). The PVO depended on optical devices for locating targets, and had physicist Pavel K. Oshchepkov conducting research in possible improvement of these devices.  In June 1933, Oshchepkov changed his research from optics to radio techniques and started the development of a \"razvedyvlatl’naya elektromagnitnaya stantsiya\" (reconnaissance electromagnetic station).  In a short time, Oshchepkov was made responsible for a technical expertise sector of PVO devoted to \"radiolokatory\" (radio-location) techniques as well as heading a Special Design Bureau (SKB, spetsialnoe konstruktorskoe byuro) in Leningrad. The \"Glavnoe Artilleriyskoe Upravlenie\" (GAU, Main Artillery Administration) was considered the “brains” of the Red Army.  It not only had competent engineers and physicists on its central staff, but also had a number of scientific research institutes.  Thus, the GAU was also assigned the aircraft detection problem, and Lt. Gen. M. M. Lobanov was placed in charge. After examining existing optical and acoustical equipment, Lobanov also turned to radio-location techniques.  For this he approached the \"Tsentral’naya Radiolaboratoriya\" (TsRL, Central Radio Laboratory) in Leningrad.  Here, Yu.  K. Korovin was conducting research on VHF communications, and had built a 50 cm (600 MHz), 0.2 W transmitter using a Barkhausen-Kurz tube.  For testing the concept, Korovin arranged the transmitting and receiving antennas along the flight path of an aircraft.  On January 3, 1934, a Doppler signal was received by reflections from the aircraft at some 600 m range and 100–150 m altitude. For further research in detection methods, a major conference on this subject was arranged for the PVO by the Russian Academy of Sciences (RAN).  The conference was held in Leningrad in mid-January 1934, and chaired by Abram Fedorovich Ioffe, Director of the Leningrad Physical-Technical Institute (LPTI).  Ioffe was generally considered the top Russian physicist of his time.  All types of detection techniques were discussed, but radio-location received the greatest attention. To distribute the conference findings to a wider audience, the proceedings were published the following month in a journal.  This included all of the then-existing information on radio-location in the USSR, available (in Russian language) to researchers in this field throughout the world. Recognizing the potential value of radio-location to the military, the GAU made a separate agreement with the Leningrad Electro-Physics Institute (LEPI), for a radio-location system.  This technical effort was led by B. K. Shembel.  The LEPI had built a transmitter and receiver to study the radio-reflection characteristics of various materials and targets.  Shemlbel readily made this into an experimental bi-static radio-location system called \"Bistro\" (Rapid). The \"Bistro\" transmitter, operating at 4.7 m (64 MHz), produced near 200 W and was frequency-modulated by a 1 kHz tone.  A fixed transmitting antenna gave a broad coverage of what was called a \"radioekran\" (radio screen).  A regenerative receiver, located some distance from the transmitter, had a dipole antenna mounted on a hand-driven reciprocating mechanism.  An aircraft passing into the screened zone would reflect the radiation, and the receiver would detect the Doppler-interference beat between the transmitted and reflected signals. \"Bistro\" was first tested during the summer of 1934.  With the receiver up to 11 km away from the transmitter, the set could only detect an aircraft entering a screen at about 3 km range and under 1,000 m. With improvements, it was believed to have a potential range of 75 km, and five sets were ordered in October for field trials.  \"Bistro\" is often cited as the USSR’s first radar system; however, it was incapable of directly measuring range and thus could not be so classified. LEPI and TsRL were both made a part of \"Nauchno-issledovatelsky institut-9\" (NII-9, Scientific Research Institute #9), a new GAU organization opened in Leningrad in 1935.  Mikhail A. Bonch-Bruyevich, a renowned radio physicist previously with TsRL and the University of Leningrad, was named the NII-9 Scientific Director. Research on magnetrons began at Kharkov University in Ukraine during the mid-1920s.  Before the end of the decade this had resulted in publications with worldwide distribution, such as the German journal \"Annalen der Physik\" (\"Annals of Physics\").  Based on this work, Ioffe recommended that a portion of the LEPI be transferred to the city of Kharkov, resulting in the Ukrainian Institute of Physics and Technology (LIPT) being formed in 1930.  Within the LIPT, the Laboratory of Electromagnetic Oscillations (LEMO), headed by Abram A. Slutskin, continued with magnetron development.  Led by Aleksandr S. Usikov, a number of advanced segmented-anode magnetrons evolved.  (It is noted that these and other early magnetrons developed in the USSR suffered from frequency instability, a problem in their use in Soviet radar systems.) In 1936, one of Usikov’s magnetrons producing about 7 W at 18 cm (1.7 GHz) was used by Shembel at the NII-9 as a transmitter in a \"radioiskatel\" (radio-seeker) called \"Burya\" (Storm).  Operating similarly to \"Bistro\", the range of detection was about 10 km, and provided azimuth and elevation coordinates estimated to within 4 degrees.  No attempts were made to make this into a pulsed system, thus, it could not provide range and was not qualified to be classified as a radar.  It was, however, the first microwave radio-detection system. While work by Shembel and Bonch-Bruyevich on continuous-wave systems was taking place at NII-9, Oshehepkov at the SKB and V. V. Tsimbalin of Ioffe’s LPTI were pursuing a pulsed system.  In 1936, they built a radio-location set operating at 4 m (75 MHz) with a peak-power of about 500 W and a 10-μs pulse duration.  Before the end of the year, tests using separated transmitting and receiving sites resulted in an aircraft being detected at 7 km.  In April 1937, with the peak-pulse power increased to 1 kW and the antenna separation also increased, test showed a detection range of near 17 km at a height of 1.5 km.  Although a pulsed system, it was not capable of directly providing range – the technique of using pulses for determining range had not yet been developed. In June 1937, all of the work in Leningrad on radio-location suddenly stopped.  The infamous Great Purge of dictator Joseph Stalin swept over the military high commands and its supporting scientific community.  The PVO chief was executed.  Oshchepkov, charged with “high crime”, was sentenced to 10 years at a Gulag penal labor camp.  NII-9 as an organization was saved, but Shenbel was dismissed and Bonch-Bruyevich was named the new director. The \"Nauchnoissledovatel'skii ispytalel'nyi institut svyazi RKKA\" (NIIIS-KA, Scientific Research Institute of Signals of the Red Army), had initially opposed research in radio-location, favoring instead acoustical techniques.  However, this portion of the Red Army gained power as a result of the Great Purge, and did an about face, pressing hard for speedy development of radio-location systems.  They took over Oshchepkov’s laboratory and were made responsible for all existing and future agreements for research and factory production.  Writing later about the Purge and subsequent effects, General Lobanov commented that it led to the development being placed under a single organization, and the rapid reorganization of the work. At Oshchepkov’s former laboratory, work with the 4 m (75 MHz) pulsed-transmission system was continued by A. I. Shestako.  Through pulsing, the transmitter produced a peak power of 1 kW, the highest level thus far generated.  In July 1938, a fixed-position, bi-static experimental system detected an aircraft at about 30 km range at heights of 500 m, and at 95 km range, for high-flying targets at 7.5 km altitude.  The system was still incapable of directly determining the range.  The project was then taken up by Ioffe’s LPTI, resulting in the development of a mobile system designated \"Redut\" (Redoubt).  An arrangement of new transmitter tubes was used, giving near 50 kW peak-power with a 10 μs pulse-duration.  Yagi antennas were adopted for both transmitting and receiving. The \"Redut\" was first field tested in October 1939, at a site near Sevastopol, a port in Ukraine on the coast of the Black Sea.  This testing was in part to show the NKKF (Soviet Navy) the value of early-warning radio-location for protecting strategic ports.  With the equipment on a cliff about 160 meters above sea level, a flying boat was detected at ranges up to 150 km.  The Yagi antennas were spaced about 1,000 meters; thus, close coordination was required to aim them in synchronization.  An improved version of the Redut, the Redut-K, was developed by Aksel Berg in 1940 and placed aboard the light cruiser \"Molotov\" in April 1941.  \"Molotov\" became the first Soviet warship equipped with radar. At the NII-9 under Bonch-Bruyevich, scientists developed two types of very advanced microwave generators.  In 1938, a linear-beam, velocity-modulated vacuum tube (a klystron) was developed by Nikolay Devyatkov, based on designs from Kharkiv.  This device produced about 25 W at 15–18 cm (2.0–1.7 GHz) and was later used in experimental systems.  Devyatkov followed this with a simpler, single-resonator device (a reflex klystron).  At this same time, D. E. Malyarov and N. F. Alekseyev were building a series of magnetrons, also based on designs from Kharkov; the best of these produced 300 W at 9 cm (3 GHz). Also at NII-9, D. S. Stogov was placed in charge of the improvements to the \"Bistro\" system.  Redesignated as \"Reven\" (Rhubarb), it was tested in August 1938, but was only marginally better than the predecessor.  With additional minor operational improvements, it was made into a mobile system called \"Radio Ulavlivatel Samoletov\" (RUS, Radio Catcher of Aircraft), soon designated as \"RUS-1\".  This continuous-wave, bi-static system had a truck-mounted transmitter operating at 4.7 m (64 MHz) and two truck-mounted receivers. Although the \"RUS-1\" transmitter was in a cabin on the rear of a truck, the antenna had to be strung between external poles anchored to the ground.  A second truck carrying the electrical generator and other equipment was backed against the transmitter truck.  Two receivers were used, each in a truck-mounted cabin with a dipole antenna on a rotatable pole extended overhead.  In use, the receiver trucks were placed about 40 km apart; thus, with two positions, it would be possible to make a rough estimate of the range by triangulation on a map. The \"RUS-1\" system was tested and put into production in 1939, then entered service in 1940, becoming the first deployed radio-location system in the Red Army.  About 45 \"RUS-1\" systems were built at the Svetlana Factory in Leningrad before the end of 1941, and deployed along the western USSR borders and in the Far East.  Without direct ranging capability, however, the military found the \"RUS-1\" to be of little value. Even before the demise of efforts in Leningrad, the NIIIS-KA had contracted with the UIPT in Kharkov to investigate a pulsed radio-location system for anti-aircraft applications.  This led the LEMO, in March 1937, to start an internally funded project with the code name \"Zenit\" (a popular football team at the time).  The transmitter development was led by Usikov, supplier of the magnetron used earlier in the \"Burya\".  For the \"Zenit\", Usikov used a 60 cm (500 MHz) magnetron pulsed at 10–20 μs duration and providing 3 kW pulsed power, later increased to near 10 kW.  Semion Braude led the development of a superheterodyne receiver using a tunable magnetron as the local oscillator.  The system had separate transmitting and receiving antennas set about 65 m apart, built with dipoles backed by 3-meter parabolic reflectors. \"Zenit\" was first tested in October 1938.  In this, a medium-sized bomber was detected at a range of 3 km.  The testing was observed by the NIIIS-KA and found to be sufficient for starting a contracted effort.  An agreement was made in May 1939, specifying the required performance and calling for the system to be ready for production by 1941.  The transmitter was increased in power, the antennas had selsens added to allow them to track, and the receiver sensitivity was improved by using an RCA 955 acorn triode as the local oscillator. A demonstration of the improved \"Zenit\" was given in September 1940.  In this, it was shown that the range, altitude, and azimuth of an aircraft flying at heights between 4,000 and 7,000 meters could be determined at up to 25 km distance.  The time required for these measurements, however, was about 38 seconds, far too long for use by anti-aircraft batteries.  Also, with the antennas aimed at a low angle, there was a dead zone of some distance caused by interference from ground-level reflections.  While this performance was not satisfactory for immediate gun-laying applications, it was the first full three-coordinate radio-location system in the Soviet Union and showed the way for future systems. Work at the LEMO continued on \"Zenit\", particularly in converting it into a single-antenna system designated \"Rubin\".  This effort, however, was disrupted by the invasion of the USSR by Germany in June 1941.  In a short while, the development activities at Kharkov were ordered to be evacuated to the Far East.  The research efforts in Leningrad were similarly dispersed. After eight years of effort by highly qualified physicists and engineers, the USSR entered World War II without a fully developed and fielded radar system. As a seafaring nation, Japan had an early interest in wireless (radio) communications.  The first known use of wireless telegraphy in warfare at sea was by the Imperial Japanese Navy, in defeating the Russian Imperial Fleet in 1904 at the Battle of Port Arthur.  There was an early interest in equipment for radio direction-finding, for use in both navigation and military surveillance.  The Imperial Navy developed an excellent receiver for this purpose in 1921, and soon most of the Japanese warships had this equipment. In the two decades between the two World Wars, radio technology in Japan made advancements on a par with that in the western nations.  There were often impediments, however, in transferring these advancements into the military.  For a long time, the Japanese had believed that they had the best fighting capability of any military force in the world.  The military leaders, who were then also in control of the government, sincerely felt that the weapons, aircraft, and ships that they had built were fully sufficient and, with these as they were, the Japanese Army and Navy were invincible.  In 1936, Japan joined Nazi Germany and Fascist Italy in a Tripartite Pact. Radio engineering was strong in Japan’s higher education institutions, especially the Imperial (government-financed) universities.  This included undergraduate and graduate study, as well as academic research in this field.  Special relationships were established with foreign universities and institutes, particularly in Germany, with Japanese teachers and researchers often going overseas for advanced study. The academic research tended toward the improvement of basic technologies, rather than their specific applications.  There was considerable research in high-frequency and high-power oscillators, such as the magnetron, but the application of these devices was generally left to industrial and military researchers. One of Japan’s best-known radio researchers in the 1920s–1930s era was Professor Hidetsugu Yagi.  After graduate study in Germany, England, and America, Yagi joined Tohoku University, where his research centered on antennas and oscillators for high-frequency communications.  A summary of the radio research work at Tohoku University was contained in a 1928 seminal paper by Yagi. Jointly with Shintaro Uda, one of Yagi’s first doctoral students, a radically new antenna emerged.  It had a number of parasitic elements (directors and reflectors) and would come to be known as the Yagi-Uda or Yagi antenna.  A U.S. patent, issued in May 1932, was assigned to RCA.  To this day, this is the most widely used directional antenna worldwide. The cavity magnetron was also of interest to Yagi.  This HF (~10-MHz) device had been invented in 1921 by Albert W. Hull at General Electric, and Yagi was convinced that it could function in the VHF or even the UHF region.  In 1927, Kinjiro Okabe, another of Yagi’s early doctoral students, developed a split-anode device that ultimately generated oscillations at wavelengths down to about 12 cm (2.5 GHz). Researchers at other Japanese universities and institutions also started projects in magnetron development, leading to improvements in the split-anode device.  These included Kiyoshi Morita at the Tokyo Institute of Technology, and Tsuneo Ito at Tokoku University. Shigeru Nakajima at Japan Radio Company (JRC) saw a commercial potential of these devices and began the further development and subsequent very profitable production of magnetrons for the medical dielectric heating (diathermy) market.  The only military interest in magnetrons was shown by Yoji Ito at the Naval Technical Research Institute (NTRI). The NTRI was formed in 1922, and became fully operational in 1930.  Located at Meguro, Tokyo, near the Tokyo Institute of Technology, first-rate scientists, engineers, and technicians were engaged in activities ranging from designing giant submarines to building new radio tubes.  Included were all of the precursors of radar, but this did not mean that the heads of the Imperial Navy accepted these accomplishments. In 1936, Tsuneo Ito (no relationship to Yoji Ito) developed an 8-split-anode magnetron that produced about 10 W at 10 cm (3 GHz).  Based on its appearance, it was named \"Tachibana\" (or Mandarin, an orange citrus fruit).  Tsuneo Ito also joined the NTRI and continued his research on magnetrons in association with Yoji Ito.  In 1937, they developed the technique of coupling adjacent segments (called push-pull), resulting in frequency stability, an extremely important magnetron breakthrough. By early 1939, NTRI/JRC had jointly developed a 10-cm (3-GHz), stable-frequency Mandarin-type magnetron (No.  M3) that, with water cooling, could produce 500-W power.  In the same time period, magnetrons were built with 10 and 12 cavities operating as low as 0.7 cm (40 GHz).  The configuration of the M3 magnetron was essentially the same as that used later in the magnetron developed by Boot and Randall at Birmingham University in early 1940, including the improvement of strapped cavities.  Unlike the high-power magnetron in Britain, however, the initial device from the NTRI generated only a few hundred watts. In general, there was no lack of scientific and engineering capabilities in Japan; their warships and aircraft clearly showed high levels of technical competency.  They were ahead of Britain in the development of magnetrons, and their Yagi antenna was the world standard for VHF systems.  It was simply that the top military leaders failed to recognize how the application of radio in detection and ranging – what was often called the Radio Range Finder (RRF) – could be of value, particularly in any defensive role; offense not defense, totally dominated their thinking. In 1938, engineers from the Research Office of Nippon Electric Company (NEC) were making coverage tests on high-frequency transmitters when rapid fading of the signal was observed.  This occurred whenever an aircraft passed over the line between the transmitter and receiving meter.  Masatsugu Kobayashi, the Manager of NEC’s Tube Department, recognized that this was due to the beat-frequency interference of the direct signal and the Doppler-shifted signal reflected from the aircraft. Kobayashi suggested to the Army Science Research Institute that this phenomenon might be used as an aircraft warning method.  Although the Army had rejected earlier proposals for using radio-detection techniques, this one had appeal because it was based on an easily understandable method and would require little developmental cost and risk to prove its military value.  NEC assigned Kinji Satake of their Research Institute to develop a system called the Bi-static Doppler Interference Detector (BDID). For testing the prototype system, it was set up on an area recently occupied by Japan along the coast of China.  The system operated between 4.0–7.5 MHz (75–40 m) and involved a number of widely spaced stations; this formed a radio screen that could detect the presence (but nothing more) of an aircraft at distances up to 500 km .  The BDID was the Imperial Army’s first deployed radio-based detection system, placed into operation in early 1941. A similar system was developed by Satake for the Japanese homeland.  Information centers received oral warnings from the operators at BDID stations, usually spaced between 65 and 240 km (40 and 150 mi).  To reduce homing vulnerability – a great fear of the military – the transmitters operated with only a few watts power.  Although originally intended to be temporary until better systems were available, they remained in operation throughout the war.  It was not until after the start of war that the Imperial Army had equipment that could be called radar. In the mid-1930s, some of the technical specialists in the Imperial Navy became interested in the possibility of using radio to detect aircraft.  For consultation, they turned to Professor Yagi who was the Director of the Radio Research Laboratory at Osaka Imperial University.  Yagi suggested that this might be done by examining the Doppler frequency-shift in a reflected signal. Funding was provided to the Osaka Laboratory for experimental investigation of this technique.  Kinjiro Okabe, the inventor of the split-anode magnetron and who had followed Yagi to Osaka, led the effort.  Theoretical analyses indicated that the reflections would be greater if the wavelength was approximately the same as the size of aircraft structures.  Thus, a VHF transmitter and receiver with Yagi antennas separated some distance were used for the experiment. In 1936, Okabe successfully detected a passing aircraft by the Doppler-interference method; this was the first recorded demonstration in Japan of aircraft detection by radio.  With this success, Okabe’s research interest switched from magnetrons to VHF equipment for target detection.  This, however, did not lead to any significant funding.  The top levels of the Imperial Navy believed that any advantage of using radio for this purpose were greatly outweighed by enemy intercept and disclosure of the sender’s presence. Historically, warships in formation used lights and horns to avoid collision at night or when in fog.  Newer techniques of VHF radio communications and direction-finding might also be used, but all of these methods were highly vulnerable to enemy interception.  At the NTRI, Yoji Ito proposed that the UHF signal from a magnetron might be used to generate a very narrow beam that would have a greatly reduced chance of enemy detection. Development of microwave system for collision avoidance started in 1939, when funding was provided by the Imperial Navy to JRC for preliminary experiments.  In a cooperative effort involving Yoji Ito of the NTRI and Shigeru Nakajima of JRC, an apparatus using a 3-cm (10-GHz) magnetron with frequency modulation was designed and built.  The equipment was used in an attempt to detect reflections from tall structures a few kilometers away.  This experiment gave poor results, attributed to the very low power from the magnetron. The initial magnetron was replaced by one operating at 16 cm (1.9 GHz) and with considerably higher power.  The results were then much better, and in October 1940, the equipment obtained clear echoes from a ship in Tokyo Bay at a distance of about 10 km .  There was still no commitment by top Japanese naval officials for using this technology aboard warships.  Nothing more was done at this time, but late in 1941, the system was adopted for limited use. In late 1940, Japan arranged for two technical missions to visit Germany and exchange information about their developments in military technology.  Commander Yoji Ito represented the Navy’s interest in radio applications, and Lieutenant Colonel Kinji Satake did the same for the Army.  During a visit of several months, they exchanged significant general information, as well as limited secret materials in some technologies, but little directly concerning radio-detection techniques.  Neither side even mentioned magnetrons, but the Germans did apparently disclose their use of pulsed techniques. After receiving the reports from the technical exchange in Germany, as well as intelligence reports concerning the success of Britain with firing using RDF, the Naval General Staff reversed itself and tentatively accepted pulse-transmission technology.  On August 2, 1941, even before Yoji Ito returned to Japan, funds were allocated for the initial development of pulse-modulated radars.  Commander Chuji Hashimoto of the NTRI was responsible for initiating this activity. A prototype set operating at 4.2 m (71 MHz) and producing about 5 kW was completed on a crash basis.  With the NTRI in the lead, the firm NEC and the Research Laboratory of Japan Broadcasting Corporation (NHK) made major contributions to the effort.  Kenjiro Takayanagi, Chief Engineer of NHK’s experimental television station and called “the father of Japanese television”, was especially helpful in rapidly developing the pulse-forming and timing circuits, as well as the receiver display.  In early September 1941, the prototype set was first tested; it detected a single bomber at 97 km and a flight of aircraft at 145 km . The system, Japan’s first full Radio Range Finder (RRF – radar), was designated Mark 1 Model 1.  Contracts were given to three firms for serial production; NEC built the transmitters and pulse modulators, Japan Victor the receivers and associated displays, and Fuji Electrical the antennas and their servo drives.  The system operated at 3.0 m (100 MHz) with a peak-power of 40 kW.  Dipole arrays with matte+-type reflectors were used in separate antennas for transmitting and receiving. In November 1941, the first manufactured RRF was placed into service as a land-based early-warning system at Katsuura, Chiba, a town on the Pacific coast about 100 km from Tokyo.  A large system, it weighed close to 8,700 kg (19,000 lb).  The detection range was about 130 km for single aircraft and 250 km for groups. Early radio-based detection in the Netherlands was along two independent lines: one a microwave system at the firm Philips and the other a VHF system at a laboratory of the Armed Forces. The Philips Company in Eindhoven, Netherlands, operated \"Natuurkundig Laboratorium\" (NatLab) for fundamental research related to its products.  NatLab researcher developed a magnetron split into four elements. In developing a communication system using this magnetron, C.H.J.A. Staal was testing the transmission by using parabolic transmitting and receiving antennas set side-by-side, both aimed at a large plate some distance away.  To overcome frequency instability of the magnetron, pulse modulation was used.  It was found that the plate reflected a strong signal. Recognizing the potential importance of this as a detection device, NatLab arranged a demonstration for the \"Koninklijke Marine\" (Royal Netherlands Navy).  This was conducted in 1937 across the entrance to the main naval port at Marsdiep.  Reflections from sea waves obscured the return from the target ship, but the Navy was sufficiently impressed to initiate sponsorship of the research.  In 1939, an improved set was demonstrated at Wijk aan Zee, detecting a vessel at a distance of 3.2 km . A prototype system was built by Philips, and plans were started by the firm Nederlandse Seintoestellen Fabriek (a Philips subsidiary) for building a chain of warning stations to protect the primary ports.  Some field testing of the prototype was conducted, but the project was discontinued when Germany invaded the Netherlands on May 10, 1940.  Within the NatLab, however, the work was continued in great secrecy until 1942. During the early 1930s, there were widespread rumours of a “death ray” being developed.  The Dutch Parliament set up a Committee for the Applications of Physics in Weaponry under G.J. Elias to examine this potential, but the Committee quickly discounted death rays.  The Committee did, however, establish the \"Laboratorium voor Fysieke Ontwikkeling\" (LFO, Laboratory for Physical Development), dedicated to supporting the Netherlands Armed Forces. Operating in great secrecy, the LFO opened a facility called the \"Meetgebouw\" (Measurements Building) located on the Plain of Waalsdorp.  In 1934, J.L.W.C. von Weiler joined the LFO and, with S.G. Gratama, began research on a 1.25-m (240-MHz) communication system to be used in artillery spotting. In 1937, while tests were being conducted on this system, a passing flock of birds disturbed the signal.  Realizing that this might be a potential method for detecting aircraft, the Minister of War ordered continuation of the experiments.  Weiler and Gratama set about developing a system for directing searchlights and aiming anti-aircraft guns. The experimental “electrical listening device” operated at 70 cm (430 MHz) and used pulsed transmission at an RPF of 10 kHz.  A transmit-receive blocking circuit was developed to allow a common antenna.  The received signal was displayed on a CR tube with a circular time base.  This set was demonstrated to the Army in April 1938 and detected an aircraft at a range of 18 km .  The set was rejected, however, because it could not withstand the harsh environment of Army combat conditions. The Navy was more receptive.  Funding was provided for final development, and Max Staal was added to the team.  To maintain secrecy, they divided the development into parts.  The transmitter was built at the Delft Technical College and the receiver at the University of Leiden.  Ten sets would be assembled under the personal supervision of J.J.A. Schagen van Leeuwen, head of the firm Hazemeijer Fabriek van Signaalapparaten. The prototype had a peak-power of 1 kW, and used a pulse length of 2 to 3 μs with a 10- to 20 kHz PRF.  The receiver was a super-heterodyne type using Acorn tubes and a 6 MHz IF stage.  The antenna consisted of 4 rows of 16 half-wave dipoles backed by a 3- by 3-meter mesh screen.  The operator used a bicycle-type drive to rotate the antenna, and the elevation could be changed using a hand crank. Several sets were completed, and one was put into operation on the Malieveld in The Hague just before the Netherlands fell to Germany in May 1940.  The set worked well, spotting enemy aircraft during the first days of fighting.  To prevent capture, operating units and plans for the system were destroyed.  Von Weiler and Max Staal fled to England aboard one of the last ships able to leave, carrying two disassembled sets with them.  Later, Gratama and van Leeuwen also escaped to England. In 1927, French physicists Camille Gutton and Emile Pierret experimented with magnetrons and other devices generating wavelengths going down to 16 cm.  Camille's son, Henri Gutton, was with the \"Compagnie Générale de Télégraphie Sans Fil\" (CSF) where he and Robert Warneck improved his father's magnetrons. In 1934, following systematic studies on the magnetron, the research branch of the CSF, headed by Maurice Ponte, submitted a patent application for a device designed to detect obstacles using continuous radiation of ultra-short wavelengths produced by a magnetron.  These were still CW systems and depended on Doppler interference for detection.  However, as most modern radars, antennas were collocated.  The device was measuring distance and azimuth but not directly as in the later \"radar\" on a screen (1939).  Still, this was the first patent of an operational radio-detection apparatus using centimetric wavelengths. The system was tested in late 1934 aboard the cargo ship \"Oregon\", with two transmitters working at 80 cm and 16 cm wavelengths.  Coastlines and boats were detected from a range of 10–12 nautical miles.  The shortest wavelength was chosen for the final design, which equipped the liner \"SS Normandie\" as early as mid-1935 for operational use. In late 1937, Maurice Elie at SFR developed a means of pulse-modulating transmitter tubes.  This led to a new 16-cm system with a peak power near 500 W and a pulse width of 6 μs.  French and U.S. patents were filed in December 1939.  The system was planned to be sea-tested aboard the \"Normandie\", but this was cancelled at the outbreak of war. At the same time, Pierre David at the \"Laboratoire National de Radioélectricité\" (National Laboratory of Radioelectricity, LNR) experimented with reflected radio signals at about a meter wavelength.  Starting in 1931, he observed that aircraft caused interference to the signals.  The LNR then initiated research on a detection technique called \"barrage électromagnétique\" (electromagnetic curtain).  While this could indicate the general location of penetration, precise determination of direction and speed was not possible. In 1936, the \"Défense Aérienne du Territoire\" (Defence of Air Territory), ran tests on David’s electromagnetic curtain.  In the tests, the system detected most of the entering aircraft, but too many were missed.  As the war grew closer, the need for an aircraft detection was critical.  David realized the advantages of a pulsed system, and in October 1938 he designed a 50 MHz, pulse-modulated system with a peak-pulse power of 12 kW.  This was built by the firm SADIR. France declared war on Germany on September 1, 1939, and there was a great need for an early-warning detection system.  The SADIR system was taken to near Toulon, and detected and measured the range of invading aircraft as far as 55 km .  The SFR pulsed system was set up near Paris where it detected aircraft at ranges up to 130 km .  However, the German advance was overwhelming and emergency measures had to be taken; it was too late for France to develop radars alone and it was decided that her breakthroughs would be shared with her allies. In mid-1940, Maurice Ponte, from the laboratories of CSF in Paris, presented a cavity magnetron designed by Henri Gutton at SFR (see above) to the GEC laboratories at Wembley, Britain.  This magnetron was designed for pulsed operation at a wavelength of 16 cm.  Unlike other magnetron designs to that day, such as the Boots and Randall magnetron (see British contributions above), this tube used an oxide-coated cathode with a peak power output of 1 kW, demonstrating that oxide cathodes were the solution for producing high-power pulses at short wavelengths, a problem which had eluded British and American researchers for years.  The significance this event was underlined by Eric Megaw, in a 1946 review of early radar developments: \"This was the starting point of the use of the oxide cathode in practically all our subsequent pulsed transmitting waves and as such was a significant contribution to British radar.  The date was the 8th May 1940\".  A tweaked version of this magnetron reached a peak output of 10 kW by August 1940.  It was that model which, in turn, was handed to the Americans as a token of good faith during the negotiations made by the Tizard delegation in 1940 to obtain from the U.S. the resources necessary for Britain to exploit the full military potential of her research and development work. Guglielmo Marconi initiated the research in Italy on radio-based detection technology.  In 1933, while participating with his Italian firm in experiments with a 600 MHz communications link across Rome, he noted transmission disturbances caused by moving objects adjacent to its path.  This led to the development at his laboratory at Cornegliano of a 330-MHz (0.91-m) CW Doppler detection system that he called \"radioecometro\".  Barkhausen-Kurz tubes were used in both the transmitter and receiver. In May 1935, Marconi demonstrated his system to the Fascist dictator Benito Mussolini and members of the military General Staff; however the output power was insufficient for military use.  While Marconi’s demonstration raised considerable interest, little more was done with his apparatus. Mussolini directed that radio-based detection technology be further developed, and it was assigned to the \"Regio Instituto Electrotecnico e delle Comunicazioni\" (RIEC, Royal Institute for Electro-technics and Communications).  The RIEC had been established in 1916 on the campus of the Italian Naval Academy in Livorno.  Lieutenant Ugo Tiberio, a physics and radio-technology instructor at the Academy, was assigned to head the project on a part-time basis. Tiberio prepared a report on developing an experimental apparatus that he called \"telemetro radiofonico del rivelatore\" (RDT, Radio-Detector Telemetry).  The report, submitted in mid-1936, included what was later known as the radar range equation.  When the work got underway, Nello Carrara, a civilian physics instructor who had been doing research at the RIEC in microwaves, was added to be responsible for developing the RDT transmitter. Before the end of 1936, Tiberio and Carrara had demonstrated the EC-1, the first Italian RDT system.  This had an FM transmitter operating at 200 MHz (1.5 m) with a single parabolic cylinder antenna.  It detected by mixing the transmitted and the Doppler-shifted reflected signals, resulting in an audible tone. The EC-1 did not provide a range measurement; to add this capability, development of a pulsed system was initiated in 1937.  Captain Alfeo Brandimarte joined the group and primarily designed the first pulsed system, the EC-2.  This operated at 175 MHz (1.7 m) and used a single antenna made with a number of equi-phased dipoles.  The detected signal was intended to be displayed on an oscilloscope.  There were many problems, and the system never reached the testing stage. Work then turned to developing higher power and operating frequencies.  Carrara, in cooperation with the firm FIVRE, developed a magnetron-like device.  This was composed of a pair of triodes connected to a resonate cavity and produced 10 kW at 425 MHz (70 cm).  It was used in designing two versions of the EC-3, one for shipboard and the other for coastal defense. Italy, joining Germany, entered WWII in June 1940 without an operational RDT.  A breadboard of the EC-3 was built and tested from atop a building at the Academy, but most RDT work was stopped as direct support of the war took priority. In early 1939, the British Government invited representatives from the most technically advanced Commonwealth Nations to visit England for briefings and demonstrations on the highly secret RDF (radar) technology.  Based on this, RDF developments were started in Australia, Canada, New Zealand, and South Africa by September 1939.  In addition, this technology was independently developed in Hungary early in the war period. In Australia, the Radiophysics Laboratory was established at Sydney University under the Council for Scientific and Industrial Research; John H. Piddington was responsible for RDF development.  The first project was a 200-MHz (1.5-m) shore-defense system for the Australian Army.  Designated ShD, this was first tested in September 1941, and eventually installed at 17 ports.  Following the Japanese attack on Pearl Harbor, the Royal Australian Air Force urgently needed an air-warning system, and Piddington’s team, using the ShD as a basis, put the AW Mark I together in five days.  It was being installed in Darwin, Northern Territory, when Australia received the first Japanese attack on February 19, 1942.  A short time later, it was converted to a light-weight transportable version, the LW-AW Mark II; this was used by the Australian forces, as well as the U.S. Army, in early island landings in the South Pacific. The early RDF developments in Canada were at the Radio Section of the National Research Council of Canada.  Using commercial components and with essentially no further assistance from Britain, John Tasker Henderson led a team in developing the Night Watchman, a surface-warning system for the Royal Canadian Navy to protect the entrance to the Halifax Harbour.  Successfully tested in July 1940, this set operated at 200 MHz (1.5 m), had a 1 kW output with a pulse length of 0.5 μs, and used a relatively small, fixed antenna.  This was followed by a ship-borne set designated Surface Warning 1st Canadian (SW1C) with the antenna hand-rotated through the use of a Chevrolet steering wheel in the operator's compartment.  The SW1C was first tested at sea in mid-May 1941, but the performance was so poor compared to the Royal Navy's Model 271 ship-borne radar that the Royal Canadian Navy eventually adopted the British 271 in place of the SW1C. For coastal defense by the Canadian Army, a 200 MHz set with a transmitter similar to the Night Watchman was developed.  Designated CD, it used a large, rotating antenna atop a 70 ft wooden tower.  The CD was put into operation in January 1942. Ernest Marsden represented New Zealand at the briefings in England, and then established two facilities for RDF development – one in Wellington at the Radio Section of the Central NZ Post Office, and another at Canterbury University College in Christchurch.  Charles N. Watson-Munro led the development of land-based and airborne sets at Wellington, while Frederick W. G. White led the development of shipboard sets at Christchurch. Before the end of 1939, the Wellington group had converted an existing 180-MHz (1.6-m), 1 kW transmitter to produce 2-μs pulses and tested it to detect large vessels at up to 30 km; this was designated CW (Coastal Watching).  A similar set, designated CD (Coast Defense) used a CRT for display and had lobe-switching on the receiving antenna; this was deployed in Wellington in late 1940.  A partially completed ASV 200 MHz set was brought from Britain by Marsden, and another group at Wellington built this into an aircraft set for the Royal New Zealand Air Force; this was first flown in early 1940.  At Christchurch, there was a smaller staff and work went slower, but by July 1940, a 430-MHz (70-cm), 5 kW set was tested.  Two types, designated SW (Ship Warning) and SWG (Ship Warning, Gunnery), were placed into service by the Royal New Zealand Navy starting in August 1941.  In all some 44 types were developed in New Zealand during WW1. South Africa did not have a representative at the 1939 meetings in England, but in mid-September, as Ernest Marsden was returning by ship to New Zealand, Basil F. J. Schonland came aboard and received three days of briefings.  Schonland, a world authority on lightning and Director of the Bernard Price Institute of Geophysics at Witwatersrand University, immediately started an RDF development using amateur radio components and Institute’s lightning-monitoring equipment.  Designated JB (for Johannesburg), the 90-MHz (3.3-m), 500-W mobile system was tested in November 1939, just two months after its start.  The prototype was operated in Durban before the end of 1939, detecting ships and aircraft at distances up to 80 km, and by the next March a system was fielded by anti-aircraft brigades of the South African Defence Force. In Hungary, Zoltán Lajos Bay was a Professor of Physics at the Technical University of Budapest as well as the Research Director of Egyesült Izzolampa (IZZO), a radio and electrical manufacturing firm.  In late 1942, IZZO was directed by the Minister of Defense to develop a radio-location (\"rádiólokáció\", radar) system.  Using journal papers on ionospheric measurements for information on pulsed transmission, Bay developed a system called \"Sas\" (Eagle) around existing communications hardware. The \"Sas\" operated at 120 MHz (2.5 m) and was in a cabin with separate transmitting and receiving dipole arrays attached; the assembly was all on a rotatable platform.  According to published records, the system was tested in 1944 atop Mount János and had a range of “better than 500 km”.  A second \"Sas\" was installed at another location.  There is no indication that either \"Sas\" installation was ever in regular service.  After the war, Bay used a modified \"Sas\" to successfully bounce a signal off the moon. At the start of World War II in September 1939, both the United Kingdom and Germany knew of each other's ongoing efforts in radio navigation and its countermeasures – the \"Battle of the beams\".  Also, both nations were generally aware of, and intensely interested in, the other's developments in radio-based detection and tracking, and engaged in an active campaign of espionage and false leaks about their respective equipment.  By the time of the Battle of Britain, both sides were deploying range and direction-finding units (radars) and control stations as part of integrated air defense capability.  However, the German \"Funkmessgerät\" (radio measuring device) systems could not assist in an offensive role and was thus not supported by Adolf Hitler.  Also, the \"Luftwaffe\" did not sufficiently appreciate the importance of British Range and Direction Finding (RDF) stations as part of RAF's air defense capability, contributing to their failure. While the United Kingdom and Germany led in pre-war advances in the use of radio for detection and tracking of aircraft, there were also developments in the United States, the Soviet Union, and Japan.  Wartime systems in all of these nations will be summarized.  The acronym RADAR (for RAdio Detection And Ranging) was coined by the U.S. Navy in 1940, and the subsequent name \"radar\" was soon widely used. When France had just fallen to the Nazis and Britain had no money to develop the magnetron on a massive scale, Churchill agreed that Sir Henry Tizard should offer the magnetron to the Americans in exchange for their financial and industrial help (the Tizard Mission).  An early 6 kW version, built in England by the General Electric Company Research Laboratories, Wembley, London (not to be confused with the similarly named American company General Electric), was given to the US government in September 1940.  The British magnetron was a thousand times more powerful than the best American transmitter at the time and produced accurate pulses.  At the time the most powerful equivalent microwave producer available in the US (a klystron) had a power of only ten watts.  The cavity magnetron was widely used during World War II in microwave radar equipment and is often credited with giving Allied radar a considerable performance advantage over German and Japanese radars, thus directly influencing the outcome of the war.  It was later described by noted Historian James Phinney Baxter III as \"The most valuable cargo ever brought to our shores\". The Bell Telephone Laboratories made a producible version from the magnetron delivered to America by the Tizard Mission, and before the end of 1940, the Radiation Laboratory had been set up on the campus of the Massachusetts Institute of Technology to develop various types of radar using the magnetron.  By early 1941, portable centimetric airborne radars were being tested in American and British aircraft.  In late 1941, the Telecommunications Research Establishment in Great Britain used the magnetron to develop a revolutionary airborne, ground-mapping radar codenamed H2S.  The H2S radar was in part developed by Alan Blumlein and Bernard Lovell.  The magnetron radars used by the US and Britain could spot the periscope of a U boat World War II, which gave impetus to the great surge in radar development, ended between the Allies and Germany in May 1945, followed by Japan in August.  With this, radar activities in Germany and Japan ceased for a number of years.  In other countries, particularly the United States, Britain, and the USSR, the politically unstable post-war years saw continued radar improvements for military applications.  In fact, these three nations all made significant efforts in bringing scientists and engineers from Germany to work in their weapon programs; in the U.S., this was under Operation Paperclip. Even before the end of the war, various project directed toward non-military applications of radar and closely related technologies were initiated.  The US Army Air Forces and the British RAF had made wartime advances in using radar for handling aircraft landing, and this was rapidly expanded into the civil sector.  The field of radio astronomy was one of the related technologies; although discovered before the war, it immediately flourished in the late 1940s with many scientists around the world establishing new careers based on their radar experience. Four techniques, highly important in post-war radars, were matured in the late 1940s-early 1950s: pulse Doppler, monopulse, phased array, and synthetic aperture; the first three were known and even used during wartime developments, but were matured later. One of the early applications of digital computers was in switching the signal phase in elements of large phased-array antennas.  As smaller computers came into being, these were quickly applied to digital signal processing using algorithms for improving radar performance. Other advances in radar systems and applications in the decades following WWII are far too many to be included herein.  The following sections are intended to provide representative samples. In the United States, the Rad Lab at MIT officially closed at the end of 1945.  The Naval Research Laboratory (NRL) and the Army’s Evans Signal Laboratory continued with new activities in centimeter radar development.  The United States Air Force (USAF) – separated from the Army in 1946 – concentrated radar research at their Cambridge Research Center (CRC) at Hanscom Field, Massachusetts.  In 1951, MIT opened the Lincoln Laboratory for joint developments with the CRC.  While the Bell Telephone Laboratories embarked on major communications upgrades, they continued with the Army in radar for their ongoing Nike air-defense program In Britain, the RAF’s Telecommunications Research Establishment (TRE) and the Army’s Radar Research and Development Establishment (RRDE) both continued at reduced levels at Malvern, Worcestershire, then in 1953 were combined to form the Radar Research Establishment.  In 1948, all of the Royal Navy’s radio and radar R&D activities were combined to form the Admiralty Signal and Radar Establishment, located near Portsmouth, Hampshire.  The USSR, although devastated by the war, immediately embarked on the development of new weapons, including radars. During the Cold War period following WWII, the primary \"axis\" of combat shifted to lie between the United States and the Soviet Union.  By 1949, both sides had nuclear weapons carried by bombers.  To provide early warning of an attack, both deployed huge radar networks of increasing sophistication at ever-more remote locations.  In the West, the first such system was the Pinetree Line, deployed across Canada in the early 1950s, backed up with radar pickets on ships and oil platforms off the east and west coasts. The Pinetree Line initially used vintage pulsed radars and was soon supplemented with the Mid-Canada Line (MCL).  Soviet technology improvements made these Lines inadequate and, in a construction project involving 25,000 persons, the Distant Early Warning Line (DEW Line) was completed in 1957.  Stretching from Alaska to Baffin Island and covering over 6,000 mi , the DEW Line consisted of 63 stations with AN/FPS-19 high-power, pulsed, L-Band radars, most augmented by AN/FPS-23 pulse-Doppler systems.  The Soviet Unit tested its first Intercontinental Ballistic Missile (ICBM) in August 1957, and in a few years the early-warning role was passed almost entirely to the more capable DEW Line. Both the U.S. and the Soviet Union then had ICBMs with nuclear warheads, and each began the development of a major anti-ballistic missile (ABM) system.  In the USSR, this was the Fakel V-1000, and for this they developed powerful radar systems.  This was eventually deployed around Moscow as the A-35 anti-ballistic missile system, supported by radars designated by NATO as the Cat House, Dog House, and Hen House. In 1957, the U.S. Army initiated an ABM system first called Nike-X; this passed through several names, eventually becoming the Safeguard Program.  For this, there was a long-range Perimeter Acquisition Radar (PAR) and a shorter-range, more precise Missile Site Radar (MSR). The PAR was housed in a 128 ft -high nuclear-hardened building with one face sloping 25 degrees facing north.  This contained 6,888 antenna elements separated in transmitting and receiving phased arrays.  The L-Band transmitter used 128 long-life traveling-wave tubes (TWTs), having a combined power in the megawatt range The PAR could detect incoming missiles outside the atmosphere at distances up to 1,800 mi . The MSR had an 80 ft , truncated pyramid structure, with each face holding a phased-array antenna 13 ft in diameter and containing 5,001 array elements used for both transmitting and receiving.  Operating in the S-Band, the transmitter used two klystrons functioning in parallel, each with megawatt-level power.  The MSR could search for targets from all directions, acquiring them at up to 300 mi range. One Safeguard site, intended to defend Minuteman ICBM missile silos near the Grand Forks AFB in North Dakota, was finally completed in October 1975, but the U.S. Congress withdrew all funding after it was operational but a single day.  During the following decades, the U.S. Army and the U.S. Air Force developed a variety of large radar systems, but the long-serving BTL gave up military development work in the 1970s. A modern radar developed by of the U.S. Navy that should be noted is the AN/SPY-1.  First fielded in 1973, this S-Band, 6 MW system has gone through a number of variants and is a major component of the Aegis Combat System.  An automatic detect-and-track system, it is computer controlled using four complementary three-dimensional passive electronically scanned array antennas to provide hemispherical coverage. Radar signals, traveling with line-of-sight propagation, normally have a range to ground targets limited by the visible horizon, or less than about 10 mi .  Airborne targets can be detected by ground-level radars at greater ranges, but, at best, several hundred miles.  Since the beginning of radio, it had been known that signals of appropriate frequencies (3 to 30 MHz) could be “bounced” from the ionosphere and received at considerable distances.  As long-range bombers and missiles came into being, there was a need to have radars give early warnings at great ranges.  In the early 1950s, a team at the Naval Research Laboratory came up with the Over-the-Horizon (OTH) radar for this purpose. To distinguish targets from other reflections, it was necessary to use a phase-Doppler system.  Very sensitive receivers with low-noise amplifiers had to be developed.  Since the signal going to the target and returning had a propagation loss proportional to the range raised to the fourth power, a powerful transmitter and large antennas were required.  A digital computer with considerable capability (new at that time) was necessary for analyzing the data.  In 1950, their first experimental system was able to detect rocket launches 600 mi away at Cape Canaveral, and the cloud from a nuclear explosion in Nevada 1,700 mi distant. In the early 1970s, a joint American-British project, code named Cobra Mist, used a 10-MW OTH radar at Orfordness (the birthplace of British radar), England, in an attempt to detect aircraft and missile launchings over the Western USSR.  Because of US-USSR ABM agreements, this was abandoned within two years.  In the same time period, the Soviets were developing a similar system; this successfully detected a missile launch at 2,500 km .  By 1976, this had matured into an operational system named \"Duga\" (“Arc” in English), but known to western intelligence as Steel Yard and called Woodpecker by radio amateurs and others who suffered from its interference – the transmitter was estimated to have a power of 10 MW.  Australia, Canada, and France also developed OTH radar systems. With the advent of satellites with early-warning capabilities, the military lost most of its interest in OTH radars.  However, in recent years, this technology has been reactivated for detecting and tracking ocean shipping in applications such as maritime reconnaissance and drug enforcement. Systems using an alternate technology have also been developed for over-the-horizon detection.  Due to diffraction, electromagnetic surface waves are scattered to the rear of objects, and these signals can be detected in a direction opposite from high-powered transmissions.  Called OTH-SW (SW for Surface Wave), Russia is using such a system to monitor the Sea of Japan, and Canada has a system for coastal surveillance. The post-war years saw the beginning of a revolutionary development in Air Traffic Control (ATC) – the introduction of radar.  In 1946, the Civil Aeronautics Administration (CAA) unveiled an experimental radar-equipped tower for control of civil flights.  By 1952, the CAA had begun its first routine use of radar for approach and departure control.  Four years later, it placed a large order for long-range radars for use in \"en route\" ATC; these had the capability, at higher altitudes, to see aircraft within 200 nautical miles (370 km).  In 1960, it became required for aircraft flying in certain areas to carry a radar transponder that identified the aircraft and helped improve radar performance.  Since 1966, the responsible agency has been called the Federal Aviation Administration (FAA). A Terminal Radar Approach Control (TRACON) is an ATC facility usually located within the vicinity of a large airport.  In the US Air Force it is known as RAPCON (Radar Approach Control), and in the US Navy as a RATCF (Radar Air Traffic Control Facility).  Typically, the TRACON controls aircraft within a 30 to 50 nautical mile (56 to 93 km) radius of the airport at an altitude between 10,000 and 15,000 feet (3,000 to 4,600 m).  This uses one or more Airport Surveillance Radars (ASR-7, 8, & 9), sweeping the sky once every few seconds. The Digital Airport Surveillance Radar (DASR) is a newer TRACON radar system, replacing the old analog systems with digital technology.  The civilian nomenclature for this radar is the ASR-11, and AN/GPN-30 is used by the military.  Two radar systems are included.  The primary is an S-Band (~2.8 GHz) system with 25 kW pulse power.  It provides 3-D tracking of target aircraft and also measures rainfall intensity.  The secondary is a P-Band (~1.05 GHz) system with a peak-power of about 25 kW.  It uses a transponder set to interrogate aircraft and receive operational data.  The antennas for both systems rotate atop a tall tower. During World War II, military radar operators noticed noise in returned echoes due to weather elements like rain, snow, and sleet.  Just after the war, military scientists returned to civilian life or continued in the Armed Forces and pursued their work in developing a use for those echoes.  In the United States, David Atlas, for the Air Force group at first, and later for MIT, developed the first operational weather radars.  In Canada, J.S. Marshall and R.H. Douglas formed the \"Stormy Weather Group \" in Montreal.  Marshall and his doctoral student Walter Palmer are well known for their work on the drop size distribution in mid-latitude rain that led to understanding of the Z-R relation, which correlates a given radar reflectivity with the rate at which water is falling on the ground.  In the United Kingdom, research continued to study the radar echo patterns and weather elements such as stratiform rain and convective clouds, and experiments were done to evaluate the potential of different wavelengths from 1 to 10 centimetres. Between 1950 and 1980, reflectivity radars, which measure position and intensity of precipitation, were built by weather services around the world.  In United States, the U.S. Weather Bureau, established in 1870 with the specific mission of to provide meteorological observations and giving notice of approaching storms, developed the WSR-1 (Weather Surveillance Radar-1), one of the first weather radars.  This was a modified version of the AN/APS-2F radar, which the Weather Bureau acquired from the Navy.  The WSR-1A, WSR-3, and WSR-4 were also variants of this radar.  This was followed by the WSR-57 (Weather Surveillance Radar – 1957) was the first weather radar designed specifically for a national warning network.  Using WWII technology based on vacuum tubes, it gave only coarse reflectivity data and no velocity information.  Operating at 2.89 GHz (S-Band), it had a peak-power of 410 kW and a maximum range of about 580 mi .  AN/FPS-41 was the military designation for the WSR-57. The early meteorologists had to watch a cathode ray tube.  During the 1970s, radars began to be standardized and organized into larger networks.  The next significant change in the United States was the WSR-74 series, beginning operations in 1974.  There were two types: the WSR-74S, for replacements and filling gaps in the WSR-57 national network, and the WSR-74C, primarily for local use.  Both were transistor-based, and their primary technical difference was indicated by the letter, S band (better suited for long range) and C band, respectively.  Until the 1990s, there were 128 of the WSR-57 and WSR-74 model radars were spread across that country. The first devices to capture radar images were developed during the same period.  The number of scanned angles was increased to get a three-dimensional view of the precipitation, so that horizontal cross-sections (CAPPI) and vertical ones could be performed.  Studies of the organization of thunderstorms were then possible for the Alberta Hail Project in Canada and National Severe Storms Laboratory (NSSL) in the US in particular.  The NSSL, created in 1964, began experimentation on dual polarization signals and on Doppler effect uses.  In May 1973, a tornado devastated Union City, Oklahoma, just west of Oklahoma City.  For the first time, a Dopplerized 10-cm wavelength radar from NSSL documented the entire life cycle of the tornado.  The researchers discovered a mesoscale rotation in the cloud aloft before the tornado touched the ground : the tornadic vortex signature.  NSSL's research helped convince the National Weather Service that Doppler radar was a crucial forecasting tool. Between 1980 and 2000, weather radar networks became the norm in North America, Europe, Japan and other developed countries.  Conventional radars were replaced by Doppler radars, which in addition to position and intensity of could track the relative velocity of the particles in the air.  In the United States, the construction of a network consisting of 10 cm wavelength radars, called NEXRAD or WSR-88D (Weather Service Radar 1988 Doppler), was started in 1988 following NSSL's research.  In Canada, Environment Canada constructed the King City station, with a five centimeter research Doppler radar, by 1985; McGill University dopplerized its radar (J. S. Marshall Radar Observatory) in 1993.  This led to a complete Canadian Doppler network between 1998 and 2004.  France and other European countries switched to Doppler network by the end of the 1990s to early 2000s.  Meanwhile, rapid advances in computer technology led to algorithms to detect signs of severe weather and a plethora of \"products\" for media outlets and researchers. After 2000, research on dual polarization technology moved into operational use, increasing the amount of information available on precipitation type (e.g. rain vs. snow).  \"Dual polarization\" means that microwave radiation which is polarized both horizontally and vertically (with respect to the ground) is emitted.  Wide-scale deployment is expected by the end of the decade in some countries such as the United States, France, and Canada. Since 2003, the U.S. National Oceanic and Atmospheric Administration has been experimenting with phased-array radar as a replacement for conventional parabolic antenna to provide more time resolution in atmospheric sounding.  This would be very important in severe thunderstorms as their evolution can be better evaluated with more timely data. Also in 2003, the National Science Foundation established the Engineering Research Center for Collaborative Adaptive Sensing of the Atmosphere, \"CASA\", a multidisciplinary, multi-university collaboration of engineers, computer scientists, meteorologists, and sociologists to conduct fundamental research, develop enabling technology, and deploy prototype engineering systems designed to augment existing radar systems by sampling the generally undersampled lower troposphere with inexpensive, fast scanning, dual polarization, mechanically scanned and phased array radars. The plan position indicator, dating from the early days of radar and still the most common type of display, provides a map of the targets surrounding the radar location.  If the radar antenna on an aircraft is aimed downward, a map of the terrain is generated, and the larger the antenna, the greater the image resolution.  After centimeter radar came into being, downward-looking radars – the H2S ( L-Band) and H2X (C-Band) – provided real-time maps used by the U.S. and Britain in bombing runs over Europe at night and through dense clouds. In 1951, Carl Wiley led a team at Goodyear Aircraft Corporation (later Goodyear Aerospace) in developing a technique for greatly expanding and improving the resolution of radar-generated images.  Called synthetic aperture radar (SAR), an ordinary-sized antenna fixed to the side of an aircraft is used with highly complex signal processing to give an image that would otherwise require a much larger, scanning antenna; thus, the name synthetic aperture.  As each pulse is emitted, it is radiated over a lateral band onto the terrain.  The return is spread in time, due to reflections from features at different distances.  Motion of the vehicle along the flight path gives the horizontal increments.  The amplitude and phase of returns are combined by the signal processor using Fourier transform techniques in forming the image.  The overall technique is closely akin to optical holography. Through the years, many variations of the SAR have been made with diversified applications resulting.  In initial systems, the signal processing was too complex for on-board operation; the signals were recorded and processed later.  Processors using optical techniques were then tried for generating real-time images, but advances in high-speed electronics now allow on-board processes for most applications.  Early systems gave a resolution in tens of meters, but more recent airborne systems provide resolutions to about 10 cm.  Current ultra-wideband systems have resolutions of a few millimeters. There are many other post-war radar systems and applications.  Only a few will be noted. The most widespread radar device today is undoubtedly the radar gun.  This is a small, usually hand-held, Doppler radar that is used to detect the speed of objects, especially trucks and automobiles in regulating traffic, as well as pitched baseballs, runners, or other moving objects in sports.  This device can also be used to measure the surface speed of water and continuously manufactured materials.  A radar gun does not return information regarding the object's position; it uses the Doppler effect to measure the speed of a target.  First developed in 1954, most radar guns operate with very low power in the X or Ku Bands.  Some use infrared radiation or laser light; these are usually called LIDAR.  A related technology for velocity measurements in flowing liquids or gasses is called laser Doppler velocimetry; this technology dates from the mid-1960s. As pulsed radars were initially being developed, the use of very narrow pulses was examined.  The pulse length governs the accuracy of distance measurement by radar – the shorter the pulse, the greater the precision.  Also, for a given pulse repetition frequency (PRF), a shorter pulse results in a higher peak power.  Harmonic analysis shows that the narrower the pulse, the wider the band of frequencies that contain the energy, leading to such systems also being called wide-band radars.  In the early days, the electronics for generating and receiving these pulses was not available; thus, essentially no applications of this were initially made. By the 1970s, advances in electronics led to renewed interest in what was often called short-pulse radar.  With further advances, it became practical to generate pulses having a width on the same order as the period of the RF carrier (T = 1/f).  This is now generally called impulse radar. The first significant application of this technology was in ground-penetrating radar (GPR).  Developed in the 1970s, GPR is now used for structural foundation analysis, archeological mapping, treasure hunting, unexploded ordnance identification, and other shallow investigations.  This is possible because impulse radar can concisely locate the boundaries between the general media (the soil) and the desired target.  The results, however, are non-unique and are highly dependent upon the skill of the operator and the subsequent interpretation of the data. In dry or otherwise favorable soil and rock, penetration up to 300 ft is often possible.  For distance measurements at these short ranges, the transmitted pulse is usually only one radio-frequency cycle in duration; With a 100 MHz carrier and a PRF of 10 kHz (typical parameters), the pulse duration is only 10 ns (nanosecond).  leading to the \"impulse\" designation.  A variety of GPR systems are commercially available in back-pack and wheeled-cart versions with pulse-power up to a kilowatt. With continued development of electronics, systems with pulse durations measured in picoseconds became possible.  Applications are as varied as security and motion sensors, building stud-finders, collision-warning devices, and cardiac-dynamics monitors.  Some of these devices are matchbox sized, including a long-life power source. As radar was being developed, astronomers considered its application in making observations of the Moon and other near-by extraterrestrial objects.  In 1944, Zoltán Lajos Bay had this as a major objective as he developed a radar in Hungary.  His radar telescope was taken away by the conquering Soviet army and had to be rebuilt, thus delaying the experiment.  Under Project Diana conducted by the Army’s Evans Signal Laboratory in New Jersey, a modified SCR-271 radar (the fixed-position version of the SCR-270) operating at 110 MHz with 3 kW peak-power, was used in receiving echoes from the Moon on January 10, 1946.  Zoltán Bay accomplished this on the following February 6. Radio astronomy also had its start following WWII, and many scientists involved in radar development then entered this field.  A number of radio observatories were constructed during the following years; however, because of the additional cost and complexity of involving transmitters and associated receiving equipment, very few were dedicated to radar astronomy.  In fact, essentially all major radar astronomy activities have been conducted as adjuncts to radio astronomy observatories. The radio telescope at the Arecibo Observatory, opened in 1963, is the largest in the world.  Owned by the U.S. National Science Foundation and contractor operated, it is used primarily for radio astronomy, but equipment is available for radar astronomy.  This includes transmitters operating at 47 MHz, 439 MHz, and 2.38 GHz, all with very-high pulse power.  It has a 305-m (1,000-ft) primary reflector fixed in position; the secondary reflector is on tracks to allow precise pointing to different parts of the sky.  Many significant scientific discoveries have been made using the Arecibo radar telescope, including mapping of surface roughness of Mars and observations of Saturns and its largest moon, Titan.  In 1989, the observatory radar-imaged an asteroid for the first time in history. Several spacecraft orbiting the Moon, Mercury, Venus, Mars, and Saturn have carried radars for surface mapping; a ground-penetration radar was carried on the Mars Express mission.  Radar systems on a number of aircraft and orbiting spacecraft have mapped the entire Earth for various purposes; on the Shuttle Radar Topography Mission, the entire planet was mapped at a 30-m resolution. The Jodrell Bank Observatory, an operation of the University of Manchester in Britain, was originally started by Bernard Lovell to be a radar astronomy facility.  It initially used a war-surplus GL-II radar system operating at 71 MHz (4.2 m).  The first observations were of ionized trails in the Geminids meteor shower during December 1945.  While the facility soon evolved to become the third largest radio observatory in the world, some radar astronomy continued.  The largest (250-ft or 76-m in diameter) of their three fully steerable radio telescopes became operational just in time to radar track \"Sputnik 1\", the first artificial satellite, in October 1957. , \"by Wolfgang Holpp\".\n\nHistory of the New York City Subway The New York City Subway is a rapid transit system that serves four of the five boroughs of New York City, New York: the Bronx, Brooklyn, Manhattan, and Queens.  Its operator is the New York City Transit Authority, which is itself controlled by the Metropolitan Transportation Authority of New York.  In 2016, an average of 5.66 million passengers used the system daily, making it the busiest rapid transit system in the United States and the seventh busiest in the world. The first underground line opened on October 27, 1904, almost 35 years after the opening of the first elevated line in New York City, which became the IRT Ninth Avenue Line.  By the time the first subway opened, the lines had been consolidated into two privately owned systems, the Brooklyn Rapid Transit Company (BRT, later Brooklyn–Manhattan Transit Corporation, BMT) and the Interborough Rapid Transit Company (IRT).  Many present lines were built under the Dual Contracts, and after 1913, all lines built for the IRT and most other lines built or improved for the BRT were built by the city and leased to the companies.  The first line of the city-owned and operated Independent Subway System (IND) opened in 1932; this system was intended to compete with the private systems and allow some of the elevated railways to be torn down, but kept within the core of the City due to the low amount of startup capital provided to the municipal Board of Transportation, the later MTA, by the state.  This required it to be run \"at cost\", necessitating fares up to double the five-cent fare popular at the time. In 1940, the two private systems were bought by the city and some elevated lines closed immediately while others closed soon after.  Integration was slow, but several connections were built between the IND and BMT, and now operate as one division called the B Division.  Since the IRT tunnel segments are too small and stations too narrow to accommodate B Division cars, and contain curves too sharp for B Division cars, the IRT remains its own division, the A Division. The New York City Transit Authority (NYCTA), a public authority presided by New York City, was created in 1953 to take over subway, bus, and streetcar operations from the city, and placed under control of the state-level Metropolitan Transportation Authority (MTA) in 1968.  Soon after the MTA took control of the subway, New York City entered a fiscal crisis.  It closed many elevated subway lines that became too expensive to maintain.  Graffiti and crime became common, and equipment and stations fell into decrepit condition.  The New York City Subway tried to stay solvent, so it had to make many service cutbacks and defer necessary maintenance projects.  In the 1980s an $18 billion financing program for the rehabilitation of the subway began. The September 11 attacks resulted in service disruptions on lines running through Lower Manhattan, particularly the IRT Broadway–Seventh Avenue Line, which ran directly underneath the World Trade Center between the Chambers Street and Rector Street stations.  Sections of the tunnel, as well as the Cortlandt Street station, which was directly underneath the Twin Towers, were severely damaged by the collapse and had to be rebuilt, requiring suspension of service on that line south of Chambers Street.  Ten other nearby stations were closed while dust and debris were cleaned up.  By March 2002, seven of those stations had reopened.  The rest (except for Cortlandt Street on the IRT Broadway–Seventh Avenue Line) reopened on September 15, 2002 along with service south of Chambers Street.  Since the 2000s, construction has been undertaken in order to expand the subway system.  Expansions include the 7 Subway Extension that opened in September 2015, and the Second Avenue Subway, the first phase of which opened on January 1, 2017. Even though there was an earlier, underground railroad called the Atlantic Avenue Tunnel since 1844, it had no underground subway stops.  Construction of this tunnel, which was built mainly to create a grade-separated right of way for the Brooklyn and Jamaica Railroad (now the Long Island Rail Road's Atlantic Branch), began in May 1844 and the tunnel was open by December 1844.  This led to South Ferry at the foot of Atlantic Avenue, where passengers could catch ferries to Manhattan.  This extension, running under Cobble Hill, was closed by 1861.  The tunnel was reopened for tourism in 1982, and closed again in 2010. The beginnings of the actual Subway came from various excursion railroads to Coney Island and elevated railroads in Manhattan and Brooklyn.  At that time, New York County (Manhattan Island and part of the Bronx), Kings County (including the Cities of Brooklyn and Williamsburg), and Queens County were separate municipal entities.  In New York, competing steam-powered elevated railroads were built over major avenues.  The first elevated line was constructed in 1867-1870 by Charles Harvey and his West Side and Yonkers Patent Railway company along Greenwich Street and Ninth Avenue (although cable cars were the initial mode of transportation on that railway).  Later more lines were built on Second, Third and Sixth Avenues.  None of these structures remain today, but these lines later shared trackage with subway trains as part of the IRT system. In Kings County, elevated railroads were also built by several companies, over Lexington, Myrtle, Third and Fifth Avenues, Fulton Street and Broadway.  These also later shared trackage with subway trains, and even operated into the subway, as part of the BRT and BMT.  Most of these structures have been dismantled, but some remain in original form, mostly rebuilt and upgraded.  These lines were linked to Manhattan by various ferries and later the tracks along the Brooklyn Bridge (which originally had their own line, and were later integrated into the BRT/BMT).  Also in Kings County, six steam excursion railroads were built to various beaches in the southern part of the county; all but one (the Manhattan Beach Line) eventually fell under BMT control. The Beach Pneumatic Transit was the first attempt to build an underground public transit system in New York City.  In 1869, Alfred Ely Beach and his Beach Pneumatic Transit Company of New York began constructing a pneumatic subway line beneath Broadway.  Funneled through a company he set up, Beach put up $350,000 of his own money to bankroll the project.  Built in only 58 days, its single tunnel, 312 ft long, 8 ft in diameter, was completed in 1870 and ran under Broadway from Warren Street to Murray Street. It remained little more than a curiosity, running only a single car on its one-block-long track to a dead-end at its terminus.  Passengers would simply ride out and back, to see what the proposed subway might be like.  During its first two weeks of operation, the Beach Pneumatic Transit sold over 11,000 rides with 400,000 rides provided during its first year of operation.  Although the public showed initial approval, Beach was delayed in getting permission to expand it due to official obstruction for various reasons.  By the time he finally gained permission in 1873, public and financial support had waned, and the subway was closed down. The final blow to the project was a stock market crash which caused investors to withdraw support.  It is unclear that such a system could have been practical for a large-scale subway network.  After the project was shut down, the tunnel entrance was sealed and the station, built in part of the basement of the Rogers Peet Building, was reclaimed for other uses.  The entire building was lost to fire in 1898.  In 1912, workers excavating for the present-day BMT Broadway Line dug into the old Beach tunnel; today, no part of this line remains as the tunnel was completely within the limits of the present day City Hall Station under Broadway. In 1898, New York, Kings and Richmond Counties, and parts of Queens and Westchester Counties and their constituent cities, towns, villages and hamlets were consolidated into the City of Greater New York.  During this era the expanded City of New York resolved that it wanted the core of future rapid transit to be underground subways, but realized that no private company was willing to put up the enormous capital required to build beneath the streets. Planning for the system that was built began with the Rapid Transit Act, signed into law on May 22, 1894, which created the Board of Rapid Transit Railroad Commissioners.  The act provided that the commission would lay out routes with the consent of property owners and local authorities, either build the system or sell a franchise for its construction, and lease it to a private operating company.  A line through Lafayette Street (then Elm Street) to Union Square was considered, but at first a more costly route under lower Broadway was adopted.  A legal battle with property owners along the route led to the courts denying permission to build through Broadway in 1896.  The Elm Street route was chosen later that year, cutting west to Broadway via 42nd Street.  This new plan, formally adopted on January 14, 1897, consisted of a line from City Hall north to Kingsbridge and a branch under Lenox Avenue and to Bronx Park, to have four tracks from City Hall to the junction at 103rd Street.  The \"awkward alignment...along Forty-Second Street\", as the commission put it, was necessitated by objections to using Broadway south of 34th Street.  Legal challenges were finally taken care of near the end of 1899. The City decided to issue rapid transit bonds outside of its regular bonded debt limit and build the subways itself, and contracted with the IRT (which by that time ran the elevated lines in Manhattan) to equip and operate the subways, sharing the profits with the City and guaranteeing a fixed five-cent fare later confirmed in the Dual Contracts. Starting in 1899, the Brooklyn Rapid Transit Company (BRT; 1896–1923) and Brooklyn–Manhattan Transit Corporation (BMT; 1923–1940) operated rapid transit lines in New York City — at first only elevated railways and later also subways. The BRT was incorporated January 18, 1896, and took over the bankrupt Long Island Traction Company in early February acquiring the Brooklyn Heights Railroad and the lessee of the Brooklyn City Rail Road.  It then acquired the Brooklyn, Queens County and Suburban Railroad leased on July 1, 1898.  The BRT took over the property of a number of surface railroads, the earliest of which, the Brooklyn, Bath and Coney Island Railroad or West End Line, opened for passenger service on October 9, 1863 between Fifth Avenue at 36th Street at the then border of Brooklyn City and Bath Beach in the Town of Gravesend, New York.  A short piece of surface route of this railroad, the BMT West End Line (today's trains ) on the west side of the Coney Island Complex north of the Coney Island Creek, is the oldest existing piece of rapid transit right-of-way in New York City, and in the U.S., having opened on June 8, 1864. On January 30, 1899, the Brooklyn Union Elevated Railroad was incorporated; it acquired the property of the bankrupt Brooklyn Elevated Railroad on February 17.  The BRT gained control a month later, on March 25, and leased the elevated company to the Brooklyn Heights Railroad, which was until then solely a street railway company, on April 1.  The other elevated company in Brooklyn, the Kings County Elevated Railway, was sold under foreclosure to the BRT on July 6, 1899.  Initially the surface and elevated railroad lines ran on steam power, but between 1893 and 1900 the lines were converted to run on electricity.  An exception was the service on the Brooklyn Bridge.  Trains were operated by cables from 1883 to 1896, when they were converted to electric power. By 1900, it had acquired virtually all of the rapid transit and streetcar operations in its target area.  Only the Coney Island and Brooklyn Railroad and the short Van Brunt Street and Erie Basin Railroad remained independent; the former was acquired in 1913 or 1914.  The incorporated lines were: The BRT became bankrupt by 1918.  The New York Consolidated Railroad and New York Municipal Railway were merged in June 1923, the same month that the Brooklyn Rapid Transit Company was reorganized as the Brooklyn–Manhattan Transit Corporation, to form the New York Rapid Transit Corporation. A contract, later known as Contract 1, was executed on February 21, 1900, between the commission and the Rapid Transit Construction Company, organized by John B. McDonald and funded by August Belmont, for the construction of the subway and a 50-year operating lease from the opening of the line.  Ground was broken at City Hall on March 24.  A plan for an extension from City Hall to the Long Island Rail Road's Flatbush Avenue terminal station (now known as Atlantic Terminal) in Brooklyn was adopted on January 24, 1901, and Contract 2, giving a lease of only 35 years, was executed between the commission and the Rapid Transit Construction Company on September 11, with construction beginning at State Street in Manhattan on November 8, 1902.  Belmont incorporated the Interborough Rapid Transit Company (IRT) in April 1902 as the operating company for both contracts; the IRT leased the Manhattan Railway, operator of the four elevated railway lines in Manhattan and the Bronx, on April 1, 1903.  Operation of the subway began on October 27, 1904, with the opening of all stations from City Hall to 145th Street on the West Side Branch. Service was extended to 157th Street on November 12, 1904.  The West Side Branch was extended northward to a temporary terminus of 221st Street and Broadway on March 12, 1906.  This extension was served by shuttle trains operating between 157th Street and 221st Street.  The original system as included in Contract 1 was completed on January 14, 1907, when trains started running across the Harlem Ship Canal on the Broadway Bridge to 225th Street, meaning that 221st Street could be closed.  Once the line was extended to 225th Street, the structure of the 221st Street was dismantled and was moved to 230th Street for a new temporary terminus.  Service was extended to the temporary terminus at 230th Street on January 27, 1907.  An extension of Contract 1 north to 242nd Street at Van Cortlandt Park was approved in 1906 and opened on August 1, 1908.  (The original plan had been to turn east on 230th Street to just west of Bailey Avenue, at the New York Central Railroad's Kings Bridge station.)  When the line was extended to 242nd Street the temporary platforms at 230th Street were dismantled, and were rumored to be brought to 242 Street to serve as the station's side platforms.  There were two stations on the line that opened later; 191st Street and 207th Street.  191st Street was not open until January 14, 1911 because the elevators and other work had not yet been completed.  207th Street was completed in 1906, but since it was located in a sparsely occupied area, the station was opened in 1907. The initial segment of the IRT White Plains Road Line opened on November 26, 1904 between East 180th Street and Jackson Avenue.  Initially, trains on the line were served by elevated trains from the IRT Second Avenue Line and the IRT Third Avenue Line, with a connection running from the Third Avenue local tracks at Third Avenue and 149th Street to Westchester Avenue and Eagle Avenue.  Once the connection to the IRT Lenox Avenue Line opened on July 10, 1905, trains from the newly opened IRT subway ran via the line.  Elevated service via this connection was resumed on October 1, 1907 when Second Avenue locals were extended to Freeman Street during rush hours. The line was then extended to Fulton Street on January 16, 1905, to Wall Street on June 12, 1905, and to Bowling Green and South Ferry on July 10, 1905.  In order to complete Contract 2, the subway had to be extended under the East River to reach Brooklyn.  The tunnel was named the Joralemon Street Tunnel, which was the first underwater subway tunnel connecting Manhattan and Brooklyn, and it opened on January 9, 1908, extending the subway from Bowling Green to Borough Hall.  On May 1, 1908, the construction of Contract 2 was completed when the line was extended from Borough Hall to Atlantic Avenue near the Flatbush Avenue LIRR station.  With the opening of the IRT to Brooklyn, ridership fell off on the BRT's elevated and trolley lines over the Brooklyn Bridge with Brooklyn riders choosing to use the new subway. The subway system began during the War of Currents when Thomas Edison and his opponent, George Westinghouse, struggled over acceptance of direct current or alternating current as the standard way to deliver electricity.  Alternating current became the standard for non-railroad purposes, but New York City Subway adopted direct current as more suitable for urban railroad purposes.  To this day, the New York City Transit Authority converts alternating current to 600V DC third rail to power the trains, as do most earlier and later local transit railways around the world.  (The A Division uses 625V DC third rail.) After the statutory debt ceiling for the now united city of New York had been raised, there were more plans for subway construction until 1908.  The Triborough Plan comprised three new lines: The BRT lines were built to wider profiles because the BRT did not want to use IRT trackage, which was narrower by comparison and carried far fewer passengers per hour.  The design was inspired by the cars built for the Cambridge Subway (Red Line) which were designed based on the results of studies done on how best to design and operate a subway car that could carry the most passengers the most efficiently.  The rolling stock, however, had to be the same track gauge so the trains could interoperate under the Dual Contracts.  The Fourth Avenue and Sea Beach Lines were opened on June 19, 1915, after years of delays for building of these lines and the Nassau Street Line.  The first BRT section, however, had opened on September 16, 1908, from Essex Street across the Williamsburg Bridge, but using narrow-width cars. Until the completion of the Fourth Avenue Line, there was a tram across the Manhattan Bridge, which did not connect to any other trackage in the New York City Subway.  The track was called \"Manhattan Bridge Three Cent Line\" due to their fare of three cents.  Along with the Brooklyn and North River Railroad, the two streetcar companies, began operations on those tracks until the BRT (later BMT), which also had two tracks each over the Brooklyn and Williamsburg Bridges.  When trackage was connected to the bridge in 1915, the trolleys were moved to the upper level roadways until 1929, when service was discontinued. The BRT, which just barely entered Manhattan via the Brooklyn Bridge, wanted the opportunity to compete with the IRT, and the IRT wanted to extend its Brooklyn line to compete with the BRT.  This led to the City's agreeing to contract for future subways with both the BRT and IRT. The expansion of rapid transit was greatly facilitated by the signing of the Dual Contracts on March 19, 1913.  \"Contract 3\" was signed between the IRT and the City; the contract between the BRT and the City was \"Contract 4\".  The majority of the present-day subway system was either built or improved under these contracts, which not only built new lines but added tracks and connections to existing lines of both companies.  The Astoria Line and Flushing Lines were built at this time, and were for some time operated by both companies.  Under the terms of Contracts 3 and 4, the city would build new subway and elevated lines, rehabilitate and expand certain existing elevated lines, and lease them to the private companies for operation.  The cost would be borne more-or-less equally by the City and the companies.  The City's contribution was in cash raised by bond offerings, while the companies' contributions were variously by supplying cash, facilities and equipment to run the lines. As part of the Contracts, the two companies were to share lines in Queens: a short line to Astoria called the Astoria Line; and a longer line reaching initially to Corona, and eventually to Flushing, called the Corona Line.  The lines would operate jointly and would start from a huge station called Queensboro Plaza.  The IRT would access the station both from the 1907 Steinway Tunnel and an extension of the Second Avenue Elevated from Manhattan over the Queensboro Bridge.  The BRT would feed the Queens lines from a new tunnel from the 60th Street Tunnel to Manhattan.  Technically the line was under IRT 'ownership', but the BRT/BMT was granted trackage rights in perpetuity, essentially making it theirs also. However, both lines were built to IRT specifications.  This meant that IRT passengers had a one-seat ride to Manhattan destinations, whereas BRT passengers had to make a change at Queensborough Plaza.  This came to be important when service was extended for the 1939 World's Fair, as the IRT was able to offer direct express trains from Manhattan, and the BRT was not.  This practice lasted well into the municipal ownership of the lines, and was not ended until 1949.  Several provisions were imposed on the companies: the fare was limited to five cents, and this led to financial troubles for the two companies after post-World War I inflation; the City had the right to \"recapture\" any of the lines it built, and run them as its own; and the City was to share in the profits.  This eventually led to their downfall and consolidation into City ownership in 1948. As part of the Dual Contracts, the operations of the original IRT system would change drastically.  Instead of having trains go via Broadway, before turning onto 42nd Street, before finally turning onto Park Avenue, there would be two trunk lines connected by the 42nd Street Shuttle.  The system would be changed from a \"Z\" system to an \"H\" system.  The first trunk line, the Lexington Avenue Line would assume the portion of the original IRT system south of Grand Central.  The line would be extended northward with a new station at Grand Central, and would turn onto Lexington Avenue, where the line would remain as four tracks.  The line would have connections to the new IRT Pelham Line, and IRT Jerome Avenue Line in the Bronx, in addition to a new connection to the IRT White Plains Road Line.  The second trunk, the Broadway–Seventh Avenue Line would assume the portion of the original IRT system north of Times Square, and it would be extended southward with a new station at Times Square, and it would run down Seventh Avenue, Varick Street and West Broadway.  It was predicted that the subway extension would lead to the growth of the Lower West Side, and to neighborhoods such as Chelsea and Greenwich Village.  South of Chambers Street, there were to be two branches constructed.  The first branch would run to the Battery via Greenwich Street, while the second branch would turn eastward under Park Place and Beeckman Street and down William Street running under the East River through a tunnel before running under Clark Street and Fulton Street until it reaches a junction at Borough Hall with the existing Contract 2 IRT Brooklyn Line. On June 3, 1917, the first portion of the Broadway–Seventh Avenue Line south of Times Square–42nd Street, a shuttle to 34th Street–Penn Station, opened; a separate shuttle service, running between 42nd and 34th Streets, was created.  This short extension was opened even though the rest of the route was not yet completed in order to handle the mass of traffic to and from Pennsylvania Station.  Only the northern part of the station was opened at this time, and piles of plaster, rails, and debris could be seen on the rest of the platforms.  This shuttle was extended south to South Ferry, with a shorter shuttle on the Brooklyn branch between Chambers Street and Wall Street, on July 1, 1918.  The new portion of the Lexington Avenue Line from Grand Central to 125th Street opened on July 17, 1918.  However, until the evening of August 1, 1918, it ran as a shuttle on the local tracks only, terminating at 42nd Street and at 167th Street on the IRT Jerome Avenue Line (where the connection from the elevated IRT Ninth Avenue Line merged).  Finally, the new \"H\" system was implemented on August 1, 1918, joining the two halves of the Broadway–Seventh Avenue Line, and the two halves of the Lexington Avenue Line.  An immediate result of the switch was the need to transfer using the 42nd Street Shuttle.  The completion of the \"H\" system doubled the capacity of the IRT system.  The local tracks ran to South Ferry, while the express tracks used the Brooklyn Branch to Wall Street, extended into Brooklyn to Atlantic Avenue via the Clark Street Tunnel on April 15, 1919. The Dual Contracts resulted in the expansion of New York City; people moved to the newly built homes along the newly built subway lines.  These homes were affordable, about the same cost as the houses in Brooklyn and Manhattan.  The Dual Contracts were the key to dispersion of the city’s congested areas.  They helped lower high population areas and probably helped saves lives as people were no longer living in heavily diseased areas.  The population in Manhattan below 59th Street decreased between the years of 1910 and 1920.  People were allowed to move to better parts the same cost and could have a better and more comfortable life in the suburbs.  They could still commute to work every day as most of the better off city workers who moved to the outer boroughs did. Mayor John F. Hylan was an advocate of public operation of the subway, and wanted this goal to be set with a vengeance.  He was fired from the BRT after working as a motorman for some time, and he wanted to avoid having to spend more money to recapture the IRT and BRT, so he tried to push the two operators out of business.  To that end, Hylan had denied allocating money for the BRT by refusing to build new lines, refusing to raise fares (thereby putting the BRT in more debt), denied building permits so that some major building work lasted longer than planned, and even refusing to build a new subway yard for the BRT.  The Malbone Street Wreck in 1918 contributed to the losses incurred by the two companies, which led to the bankruptcy of the BRT in 1918.  The BRT, however, was reorganized into the Brooklyn-Manhattan Transit Corporation (BMT).  The IRT was almost bankrupt, but managed to complete the line to Flushing by 1928.  So, Hylan drew up plans for a third subway network, which should be built and operated in contrast to the existing subway lines, which were privately operated. On the other hand, the city of New York had grown to over five and a half million inhabitants, and urgently needed new subway lines.  The dual system could not keep pace with this ever-increasing ridership.  So, a compromise solution was finally found that would allow Hylan's plans as well as the interests of private operators to be considered.  However, the city's and Hylan's long-term goal was the unification and consolidation of the existing subway, with the city operating a unified subway system (see below).  The city, bolstered by political claims that the private companies were reaping profits at taxpayer expense, determined that it would build, equip and operate a new system itself, with private investment and without sharing the profits with private entities.  This led to the building of the \"Independent City-Owned Subway\" (ICOS), sometimes called the \"Independent Subway System\" (ISS), the \"Independent City-Owned Rapid Transit Railroad\", or simply \"The Eighth Avenue Subway\" after the location of its premier Manhattan mainline.  After the City acquired the BMT and IRT in 1940, the Independent lines were dubbed the \"IND\" to follow the three-letter initialisms of the other systems. The original IND system, consisting of the Eighth Avenue mainline and the Sixth Avenue, Concourse, Culver, and Queens Boulevard branch lines, was entirely underground in the four boroughs that it served, with the exception of the Smith–Ninth Streets and Fourth Avenue stations on the Culver Viaduct over the Gowanus Canal in Gowanus, Brooklyn. As the first line neared completion, New York City offered it for private operation as a formality, knowing that no operator would meet its terms.  Thus the city declared that it would operate it itself, formalizing a foregone conclusion.  The first line opened without a formal ceremony.  The trains began operating their regular schedules ahead of time, and all stations of the Eighth Avenue Line, from 207th Street in Inwood to Hudson Terminal (now World Trade Center), opened simultaneously at one minute after midnight on September 10, 1932. On January 1, 1936, a second trunk line—the Sixth Avenue Line—opened from West Fourth Street (where it splits from the Eighth Avenue Line) to East Broadway.  During construction, streetcar service along Sixth Avenue was terminated.  The city could either restore it upon the completion of construction or abandon it immediately; as the city wanted to tear down the IRT Sixth Avenue Line right away and save on the costs of shoring it up while construction proceeded underneath it, the IRT Sixth Avenue Line was purchased for $12.5 million and terminated by the city on December 5, 1938, with the steel from the el sold to Japan.  To help compensate for the loss in service, service on the Ninth Avenue Elevated was increased. The first section of the Queens Boulevard Line, west from Roosevelt Avenue to 50th Street, opened on August 19, 1933.  E trains ran local to Hudson Terminal (today's World Trade Center) in Manhattan, while the GG (predecessor to current G service) ran as a shuttle service between Queens Plaza and Nassau Avenue on the IND Crosstown Line.  An extension east to Union Turnpike opened on December 31, 1936.  The line was extended to Hillside Avenue and 178th Street, with a terminal station at 169th Street on April 24, 1937.  That day, express service began on the Queens Boulevard Line during rush hours, with E trains running express west of 71st–Continental Avenues, and GG trains taking over the local during rush hours.  The initial headway for express service was between three and five minutes.  23rd Street–Ely Avenue station opened as an in-fill station on August 28, 1939.  Upon its extension into Jamaica, the line drew Manhattan-bound passengers away from the nearby BMT Jamaica Line subway and the Long Island Rail Road. On July 1, 1937, a third trunk line, the Crosstown Line, opened from Nassau Avenue to Bergen Street.  Two years later, on December 15, 1940, local service was begun along the entire IND Sixth Avenue line, including its core part through Midtown Manhattan. Meanwhile, on the East Side, the need for the IND Second Avenue Line had been evident since 1919, when the New York Public Service Commission launched a study at the behest of engineer Daniel L. Turner to determine what improvements were needed in the city's public transport system.  Due to the Great Depression, the soaring costs of the expansion became unmanageable, so it was not built along with the other three IND trunk lines.  Construction on the first phase of the IND was already behind schedule, and the city and state were no longer able to provide funding.  A scaled-down proposal including a turnoff at 34th Street and a connection crosstown was postponed in 1931.  Further revision of the plan and more studies followed.  By 1939, construction had been postponed indefinitely, and Second Avenue was relegated to \"proposed\" status.  The 1939 plan for subway expansion took the line not only into the Bronx (by now as a single line to Throggs Neck) but also south into Brooklyn, connecting to the stub of the IND Fulton Street Line at Court Street.  Construction of the line resumed in 1972, but was ended in 1975 due to the city's fiscal crisis, and work was again restarted in 2007. Since the opening of the original New York City Subway line in 1904, various official and planning agencies have proposed numerous extensions to the subway system.  One of the better known proposals was the \"Second System,\" which was part of a plan by the Independent Subway to construct new subway lines in addition and take over existing subway lines and railroad right-of-ways.  Though most of the routes proposed over the decades have never seen construction, discussion remains strong to develop some of these lines, to alleviate existing subway capacity constraints and overcrowding, the most notable being the Second Avenue Subway.  Plans for new lines date back to the early 1910s. On August 28, 1922, Mayor John Francis Hylan revealed his own plans for the subway system, which was relatively small at the time.  His plan included taking over nearly 100 miles of existing lines and building over 100 miles of new lines.  Construction of all these new lines would be completed by December 31, 1925, and passengers would be able to ride between the ends of New York City on one fare.  The lines were designed to compete with the IRT and BMT. In 1926, a loop subway service was planned to be built to New Jersey. The most grandiose plan, conceived in 1929, was to be part of the city-operated Independent Subway System (IND).  By 1939, with unification planned, all three systems were included.  As this grandiose expansion was not built, the subway system is only 70% of what it was planned to be.  Magnificently engineered, almost entirely underground, with 670 ft platforms and flying junctions throughout, the IND system tripled the City's rapid transit debt, ironically contributing to the demise of plans for an ambitious expansion proposed before the first line of the first system was even opened. Due to this debt, after the IND Sixth Avenue Line was completed, only 28 new stations were built.  Five stations were on the abandoned NYW&B-operated IRT Dyre Avenue Line, fourteen stations were on the abandoned LIRR Rockaway Beach Branch (now the IND Rockaway Line), six were on the Archer Avenue Lines and 63rd Street Lines (built as part of a 1968 plan), two stations (57th Street and Grand Street) were part of the Chrystie Street Connection, and the Harlem–148th Street terminal.  Four MTA Capital Construction-funded stations (the 34th Street station on the 7 Subway Extension and the three stations on the Second Avenue Subway) have been constructed with up to 14 more planned.  However, the four MTA Capital Construction stations cost US$ , reflecting the scale of the debt that the IND brought the city into. In June 1940, the transportation assets of the former BMT and IRT systems were taken over by the City of New York for operation by the City's Board of Transportation, which already operated the IND system.  In June 1953 the New York City Transit Authority, a state agency incorporated for the benefit of the city, now known to the public as \"MTA New York City Transit\", succeeded the BoT.  A combination of factors had this takeover coincide with the end of the major rapid transit building eras in New York City.  The City immediately began to eliminate what it considered redundancy in the system, closing several elevated lines including the IRT Ninth Avenue Line and most of the IRT Second Avenue Line in Manhattan, and the BMT Fifth and Third Avenue Lines and most of the BMT Fulton Street Line in Brooklyn. Despite the unification, a distinction between the three systems survives in the service labels: IRT lines (now referred to as A Division) have numbers and BMT/IND (now collectively B Division) lines use letters.  There is also a physical and less widely noticed difference, as A Division cars are narrower than those of B Division by 18 in and shorter by 9 ft to 24 ft .  Because the A Division lines are of lower capacity for a given capital investment, all new extensions and lines built between World War II and 2007 have been for the B Division.  A Division cars can travel on B Division lines when necessary, but are not used for passenger service on those lines due to the dangerously wide gap between the car and the station platform.  This stems from the IRT and BRT's disagreement during the early days of the subway, where the BRT (part of the current B Division) built cars that would be purposefully too wide for the IRT (today's A Division). The original IRT subway lines (i.e. those built before the Dual Contracts) were built to modified elevated line dimensions.  Whereas the IRT els were originally equipped with cars that were 47 ft long, the cars designed for the IRT subway measure 51.3 ft long.  Both sets of lines did not permit cars wider than 9 ft .  The clearances and curves on these lines are too narrow and too sharp for any IND or BMT equipment.  The later extensions of the IRT, constituting the bulk of the IRT system, were built to wider dimensions, and so are of a profile that could support the use of IND/BMT sized equipment.  In other words, B Division equipment could operate on much of A Division if station platforms were trimmed and trackside furniture moved, thus letting A Division service carry more passengers.  However, there is virtually no chance of this happening because the older, narrower portions of A Division are centrally situated, such that it would be impossible to put together coherent through services.  The most that can be reasonably hoped for is that some branch lines of Division A might be resized and attached to B Division lines.  This was done with the BMT Astoria Line in Queens (which had formerly been dual-operated with normal IRT trains and special narrow BMT shuttles), and has been proposed for a connection of the Second Avenue Subway to the IRT Pelham Line in the East Bronx. The city of New York now hoped that the profits from the remaining formerly privately operated routes would support the expensive and deficit-ridden IND lines and simultaneously be able to repay the systems' debts, without having to increase the original fare of five cents.  But during World War II, which gave a reprieve to the closure of most rail transit in the US, some closures continued, including the remainder of the IRT Second Avenue Line in Manhattan (1942) and the surviving BMT elevated services over the Brooklyn Bridge (1944).  The Second World War also caused renewed inflation, which finally forced an increase to ten cents in 1947 and six years later finally to 15 cents.  Because the consolidation dragged in the first years after unification, some improvements in operational processes were rather slow, soon the question of organization was raised.  The outsourcing of subway operations to the Port Authority of New York and New Jersey were favored at one point.  On June 15, 1953, the New York City Transit Authority (NYCTA) was founded with the aim of ensuring a cost-covering and efficient operation in the subways. There was a need to overhaul rolling stock and infrastructure of the once-private routes, especially for the IRT, where nearly all of the infrastructure was aged.  The oldest cars came there from the time the subway opened in 1904, and the oldest subway cars of BMT in 1953 dated from the system's first years, in 1913.  Therefore, a total of 2,860 cars for the A Division (IRT) – contract numbers R12 to R36 World's Fair (known as \"SMEE\" type) were delivered between 1948 and 1965, which constituted the replacement of almost the entire prewar IRT fleet.  On the B Division (IND/BMT), 2,760 \"SMEE\" type cars in R10 to R42 series were ordered.  Platforms were doubled in length systemwide.  At some stations, gap fillers were installed because the station extensions were curved.  Also in this period, the BMT replaced their signals.  The Main Line R36 cars were the first equipment to be equipped with two-way radio as delivered standard equipment in 1965, with the first use of radio in the subway system on the IRT Lexington Avenue Line in May that year, and the first successful air conditioned train (R38) was placed into service July 19, 1967. Only two new lines were opened in this era, the IRT Dyre Avenue Line in 1941 and the IND Rockaway Line in 1956, with an extension of the latter to Far Rockaway–Mott Avenue in 1958.  Both of these lines were rehabilitations of existing railroad rights-of-way rather than new construction.  The former line was the City portion of the New York, Westchester and Boston Railway, an electrified commuter line closed in 1937, and the latter a line obtained from the Long Island Rail Road.  While the latter is a long and substantial line, it consists mostly of a long right-of-way crossing Jamaica Bay with a single station on Broad Channel island and two branches on a peninsula that is only several city blocks wide.  For a time, the IND Rockaway Line was considered its own subway division. The 169th Street station on the IND Queens Boulevard Line provided an unsatisfactory terminal setup for a four track line, and this required the turning of F trains at Parsons Boulevard, and no storage facilities were provided at the station.  Therefore, the line was going to be extended to 184th Place with a station at 179th Street with two island platforms, sufficient entrances and exits, and storage for four ten-car trains.  The facilities would allow for the operation of express and local service to the station.  Construction on the extension started in 1946, and was projected to be completed in 1949.  The extension was completed later than expected and opened on December 11, 1950.  This extension was delayed due to the Great Depression and World War II.  Both E and F trains were extended to the new station. The originally planned IND system was built to the completion of its original plans after World War II ended, but the system then entered an era of deferred maintenance in which infrastructure was allowed to deteriorate.  In 1951 a half-billion dollar bond issue was passed to build the Second Avenue Subway, but money from this issue was used for other priorities and the building of short connector lines, namely a ramp extending the IND Culver Line over the ex-BMT Culver Line at Ditmas and McDonald Avenues in Brooklyn (1954), allowing IND subway service to operate to Coney Island for the first time, the 60th Street Tunnel Connection (1955), linking the BMT Broadway Line to the IND Queens Boulevard Line, and the Chrystie Street Connection (1967), linking the BMT line via the Manhattan Bridge to the IND Sixth Avenue Line. By January 1955, the Port Authority of New York and New Jersey and the Triborough Bridge and Tunnel Authority could theoretically raise $1.25 billion effective immediately ( ).  In his 1974 book \"The Power Broker\", Robert A. Caro estimated that this amount of money could modernize both the Long Island Rail Road for $700 million and the Hudson & Manhattan Railroad for $500 million, with money left over to build the Second Avenue Subway as well as proposed extensions of subway lines in Queens and Brooklyn.  However, Robert Moses, the city's chief urban planner at the time, did not allow funding for most mass transit expansions in the New York City area, instead building highways and parkways without any provisions for mass transit lines in the future.  Caro noted that the lack of attention to mass transit expansions and routine maintenance contributed to the decline of the subway: \"When Robert Moses came to power in New York in 1934, the city’s mass transportation system was probably the best in the world.  When he left power in 1968 it was quite possibly the worst.\" Soon after, the city entered a fiscal crisis.  Closures of elevated lines continued.  These closures included the entire IRT Third Avenue Line in Manhattan (1955) and the Bronx (1973), as well as the BMT Lexington Avenue Line (1950), much of the remainder of the BMT Fulton Street Line (1956), the downtown Brooklyn part of the BMT Myrtle Avenue Line (1969), and the BMT Culver Shuttle (1975), all in Brooklyn, and the BMT Jamaica Line in Queens starting in 1977.  The BMT Archer Avenue Line was supposed to replace the BMT Jamaica Line's eastern end, but it was never completed to its full extent, and opened in 1988 as a stub-end line, terminating at Jamaica Center. In addition, construction and maintenance of existing lines was deferred, and graffiti and crime were very common.  Trains frequently broke down, were poorly maintained, and were often late, while ridership declined by the millions each year.  As in all of the city, crime was rampant in the subway in the 1970s.  Thefts, robberies, shootings and killings became more frequent.  The rolling stock was very often graffiti-painted or vandalism-damaged both inside and outside.  As the New York City Police Department was completely overwhelmed, the public reacted with unease, and the subway was deliberately avoided.  Around 1980, the reliability of the vehicles was a tenth of their reliability in the 1960s, and 40 percent of the network required speed restrictions.  Because there had been no further studies of the subway since 1975, one third of the fleet was out of use during rush hours due to serious technical defects.  In addition, signs were fitted incorrectly, and spare parts were missing or were bought in too large quantities, could not be found, or could not be installed due to lack of repairmen. The New York City Subway tried to keep its budget balanced between spending and revenue, so deferred maintenance became more common, which drew a slow but steady decline of the system and rolling stock.  Furthermore, the workers were consolidated into the Transport Workers Union in 1968.  A pension was set up, and workers were allowed to retire after 20 years of service without any transitional period.  About a third of the most highly experienced staff immediately retired, resulting in a large shortage of skilled workers. Only in the 1980s did an $18 billion financing program for the rehabilitation of the subway start.  Between 1985 and 1991 over 3,000 subway cars were overhauled and fitted with air conditioning.  In this way, comfort, reliability and durability would be increased in order to postpone new purchases.  The TA only replaced the oldest cars each division, so that despite the fact that the fleet was overaged, the TA bought only 1,350 new vehicles.  Increased patrols and fences around the train yards offered better protection against graffiti and vandalism.  At the same time, the TA began an extensive renovation of the routes.  Within ten years the tracks were thereby renewed almost systemwide.  The Williamsburg Bridge and the Manhattan Bridge, which had strong corrosion damage, were refurbished over the years.  The renovation of the stations was initially limited to security measures, fresh paint, new lighting and signs, but the TA also tried to improve the service that had been neglected.  This ranged from new uniforms and training for the staff to correct destination signs on the rolling stock.  Some subway services were also adapted to the changing needs of customers.  Another stated goal was to reduce crime or at least an improvement in the subjective sense of security.  At night, the railway police and members of the citizens' initiative Guardian Angels, formed in 1979, patrolled in the subway trains.  It was not until the 1990s that the crime in the city and its subway declined significantly.  Nevertheless, the reputation as a slow, dilapidated, dirty and unsafe means of transportation remains associated with the subway. In 1956, the NYCTA chairman, Charles Patterson, proposed removing the seats from the trains on the 42nd Street Shuttle to increase the passenger load. Under a $100,000,000 rebuilding program, increased and lengthened service was implemented during peak hours on the 1 train.  Switching at a junction north of 96th Street, delayed service as trains from the Lenox Avenue Line which ran local switched from the express to the local, while trains from the Broadway Branch that ran express switched from the local to the express.  This bottleneck was removed on February 6, 1959.  All Broadway trains were locals, and all Lenox Avenue trains were expresses, eliminating the need to switch tracks.  All 3 trains began to run express south of 96th Street on that date running to Brooklyn.  1 trains began to run between 242nd Street and South Ferry all times.  Trains began to be branded as Hi-Speed Locals, being as fast as the old express service was with 8-car trains consisting of new R21s and R22s on the line. On November 15, 1962, the express platforms at Lexington Avenue–59th Street opened to reduce transfer congestion at Grand Central–42nd Street, and to allow transfers between the express trains and BMT trains to Queens.  Even before the express platforms were added, this station was the busiest on the line.  Construction on the express station had begun on August 10, 1959. In the mid-1960s, US$ was made available to the Metropolitan Transportation Authority (MTA) of New York City for a large subway expansion proposed by then-Mayor John Lindsay.  About $1,230,000,000 was spent to create three tunnels and a half-dozen holes as part of construction on the Second Avenue and 63rd Street Lines.  Construction for the lines stopped in 1975 because the city almost went bankrupt, yet none of the lines were done when federal payments were suspended in 1985.  The two-phase \"Program for Action\" was funded as follows: A summary of the new subway lines and new subway related expenditures proposed in phase I of the 1968 \"Program for Action\" follows: Phase II of the 1968 \"Program for Action\" contained the following plans: Also as part of the Program for Action, existing elevated structures were to be replaced with new subways.  The eastern end of the BMT Jamaica Line was to be replaced with the BMT Archer Avenue Line, while the IRT Third Avenue Line was being torn down in favor of a new subway line running parallel to the Metro-North tracks at Park Avenue. Because the early subway systems competed with each other, they tended to cover the same areas of the city, leading to much overlapping service.  The amount of service has actually decreased since the 1940s as many elevated railways were torn down, and finding funding for underground replacements has proven difficult.  The subway's decline began in the 1960s and continued through the late 1980s. In 1973, the city's graffiti epidemic surged to levels never seen before; nearly every subway car was tagged with graffiti by the end of the year.  It was aided by the budgetary restraints on New York City, which limited its ability to remove graffiti and perform transit maintenance.  Mayor John Lindsay declared the first war on graffiti in 1972, but it would be a while before the city was able and willing to dedicate enough resources to that problem to start impacting the growing subculture.  The MTA tried rubbing the graffiti off with an acid solution, but maintaining the cars to keep them relatively graffiti-free was costing them around $1.3 million annually.  In winter 1973, the car-washing program was stopped.  In September 1974, exterior washing with an acid solution started, but the solution was found to have caused more harm than good. As graffiti became associated with crime, many demanded that the government take a more serious stance toward it, particularly after the popularization of the Fixing Broken Windows philosophy in 1982.  By the 1980s, increased police surveillance and implementation of increased security measures (razor wire, guard dogs) combined with continuous efforts to clean it up led to the weakening of the New York's graffiti subculture.  As a result of subways being harder to paint, more writers went into the streets, which is now, along with commuter trains and box cars, the most prevalent form of writing.  But the streets became more dangerous due to the burgeoning crack epidemic, legislation was underway to make penalties for graffiti artists more severe, and restrictions on paint sale and display made obtaining materials difficult. An extensive car-washing program in the late 1980s ensured the elimination of graffiti throughout the system's rolling stock.  In 1984 the NYCTA began a five-year program to eradicate graffiti.  The years between 1985 and 1989 became known as the \"die hard\" era.  A last shot for the graffiti artists of this time was in the form of subway cars destined for the scrap yard.  With the increased security, the culture had taken a step back.  The previous elaborate \"burners\" on the outside of cars were now marred with simplistic marker tags which often soaked through the paint.  By mid-1986 the NYCTA were winning their \"war on graffiti.\"  On May 12, 1989, the rolling stock was made 100% graffiti-free, with the washing of the last train in the subway system that still had graffiti.  As the population of artists lowered so did the violence associated with graffiti crews and \"bombing.\" By June 1975, ridership had fallen to 1918 levels, and ridership was decreasing at an average of 25 million passengers a year.  In January 1977, to both save money and increase safety, subway trains were shortened during off hours.  By October 1977, a planned Metropolitan Transportation Center at Third Avenue and 48th Street was dropped.  LIRR trains using the 63rd Street tunnel would run to Grand Central, whenever that line would be built.  $63 million had been spent on Second Avenue Subway construction through December 1978, even though subway construction only consisted of three short segments of tunnel when it was halted in April 1975.  Ridership kept dropping rapidly; it dropped by 25 million passengers between June 30, 1976 and June 30, 1977, and within a span of eight years, 327 million passengers stopped using the subway.  Some estimated that if this rate of decline were to continue, there would be no passengers on the system by 2002. As a result of declining ridership, the number of subway cars used during the morning rush hours dropped from 5,557 in 1974 to about 4,900 in 1978.  Headways were increased, too, so people were waiting longer periods of time for shorter trains that were intensely crowded.  Headways on the A, D, N and RR services were 5 minutes during rush hours (or 12 trains per hour) in 1978; they were 4 minutes (or 15 trains per hour) in 1974. On May 27, 1975, the NYCTA announced that in September of that year 94 daily IRT trips would be discontinued, accounting for 4 percent of then-existing service on the IRT.  The trips were to be discontinued to cut operating deficits.  Express service on the 7 was to be discontinued between the hours of 9:30 a.m. and 3:30 p.m. and was to be replaced by more frequent local service.  Also, during the same month, the NYCTA was considering making the A train a local at all times except rush hours, when it would remain an express. On December 17, 1975, the MTA announced that a 4.4 percent cutback of rush hour train service would take place on January 18, 1976.  The cutbacks, the third of the year, trimmed 279 train runs from the previous 6,900.  Service was most drastically reduced on the Lexington Avenue Line, with seven fewer express trains during the morning rush hour heading southbound.  The cuts were the first of a three-phase program that was put in effect between January and July 1976.  The cuts permitted a savings of $12.6 million a year for the NYCTA, which had an increasing deficit.  Other subway services were changed or discontinued as part of the plan.  On January 19, F trains were planned to stop running express in Brooklyn, and the GG was to be cut back to Smith–Ninth Streets.  In April it was planned that all rush hour 1 trains would begin running to 242nd Street; these runs had previously terminated at 137th Street.  During midday hours, trains on the 1 were to be shortened to five cars.  In July, it was planned that the EE would be discontinued; N trains were to have been extended to Continental Avenue via the Queens Boulevard Line to replace it.  Manhattan-bound N trains were to continue running express, while in the opposite direction they would run local.  N trains would alternate between terminating at Whitehall Street or Coney Island during rush hours.  CC trains, in July, were planned to be extended from Hudson Terminal to Rockaway Park replacing the E, which was to have been cut back to Hudson Terminal.  The K was planned to be discontinued in July. The changes that were supposed to take place in July instead took effect on August 30.  215 more runs were eliminated on that date.  In 1967 there were 8,200 daily trips, and on August 30, 1976 there were 6,337 daily trips. On December 14, 1976, the NYCTA proposed another package of service cuts.  The cuts, planned to take effect in January 1977, would have eliminated service on the Bowling Green–South Ferry Shuttle, the Franklin Avenue Shuttle, and AA service, which was to be replaced by the A during late nights.  GG service would be cut back to Queens Plaza during late evenings and late nights.  B and N service would have been cut back to shuttles, running between 36th Street and Coney Island on their respective lines.  It was also proposed that during off-peak hours 10-car trains would be cut to eight, six or four car trains. The subway had been gradually neglected since the 1940s, and its situation had been exacerbated by the low fare.  On May 20, 1970, two people died at the Jackson Heights–Roosevelt Avenue station in the worst subway collision since the 1928 Times Square derailment.  Following the 1970 accident, \"New York Magazine\" highlighted the state of the subway system in a lengthy expose.  Even though each of the approximately 7,200 subway cars were checked once every six weeks or 7,500 mi of service, four or five dead motors were allowable in a peak-hour 10-car train, according to some transit workers' accounts.  About 85.8% of trains were on schedule in 1970, with 1,142 equipment-related delays in April 1970.  However, issues such as broken lights, fans, and signs; defective doors, wheels, and brakes; and subway cars that often became uncoupled or \"pulled apart\", were still prevalent.  One out of three IRT stations did not have running water in case of emergency.  In addition, the system's staff were leaving in massive numbers, with 5,655 workers having retired or quit from early 1969 to mid-1970. In the late 1970s, hundreds of slow speed orders were found throughout the system due to the risk of derailments.  Graffiti covered every single subway car in the system, and the number of available cars for rush hour services continued to drop, from 5,557 in 1976, then to 5,025 in 1977, and finally to 4,900 in May 1978.  Mean Distance Between Failures (MDBF) rates were at all time lows, as the MDBF rate system-wide was 6,000 miles by 1980.  Maintenance on rolling stock was so bad that by 1979, two hundred retired R16 cars were put back into service to replace the newest rolling stock in the system, the R46.  Most R46s had cracked trucks, and were only allowed to operate during rush hours as they were sent for rehabilitation. At the height of the transit crisis in 1983, on-time performance dropped below 50%.  Hundreds of trains never made it to their destination and in 1981, 325 train runs were abandoned on a typical day.  Additionally, cars caught fire 2,500 times every year. In December 1978 a New York Daily News article highlighted the worst part of the subway.  The worst subway station overall, in terms of crime and its condition, was Grand Central–42nd Street, while the worst elevated station was Metropolitan Avenue in Queens.  The subway cars in the worst condition were the R10s.  The subway line with the worst signals was the IRT Broadway–Seventh Avenue Line, so the signals were upgraded in the 1980s.  The track in the worst shape was that of the BMT Sea Beach Line, which had more or less the same infrastructure as when it opened in 1915. Even with the addition of $800 million of state funds promised in 1978, less than half of the $600 million authorized in the 1967 bond issue for major construction had been spent.  The MTA made improvements to tunnels, tracks, switches and signals.  It had to do this with a smaller amount of funding than available in the past due to the fiscal crisis, and keep the subway operating 24 hours a day.  However, it had a major public relations problem.  As people didn't see any improvements, they assumed that crime was out of control, and for a while it was, but this assumption was maintained even during periods of reduced crime.  In an attempt to alleviate the crime situation and extend the service life of rolling stock, half-length trains began running during off-peak hours.  Infrastructure was in such poor condition that even the 63rd Street and Archer Avenue subway projects were threatened by 1980.  The 63rd Street Line was flooded with water, while the Archer Avenue Line could barely be built past Parsons Boulevard. Due to deferred maintenance, the condition of the subway system reached dangerous conditions in the early 1980s; new construction was, by then, considered ludicrous.  Even as the only new construction was going on in the 63rd Street Line, Manhattan residents at the vicinity of York Avenue and East 63rd Street protested over the MTA's intention to build a ventilation shaft for the line.  During the early 1980s, work on the 63rd Street and Archer Avenue lines continued, although the MTA considered stopping work on these projects in October 1980, and spending the money instead on maintaining the existing system. Structural defects were found in elevated structures systemwide and on the Manhattan and Williamsburg Bridges, causing frequent closures or delays on many subway lines during the 1980s.  Reroutes from both bridges were necessitated; while the Manhattan Bridge, between 1986 and 2004, had two of its four tracks closed at a time for construction, the Williamsburg Bridge needed a shutdown from April to June 1988 for emergency structural repairs to be made.  Federal funding for the repair of the BMT Jamaica Line was deferred throughout the 1980s due to the extremely bad state of the Williamsburg Bridge.  On the bridge, pigeon droppings corroded the bridge's steel, broken cable strands suspending the bridge numbered over 200, and concrete in the bridge began to come off and leave large holes. Due to low ridership and the increasing shabbiness of the subway, parts or most of several lines—the BMT Canarsie Line; the IND Crosstown Line; either the IND Concourse Line or the IRT Jerome Avenue Line north of 161st Street–Yankee Stadium; and the BMT Jamaica Line east of either Broadway Junction or Cypress Hills—were supposedly proposed for closure and abandonment by the late 1970s and early 1980s.  In fact, the eastern part of the BMT Jamaica Line was actually demolished, mainly as part of the Program for Action, but also due to low ridership. In 1981, operation on the New York City Subway was so bad that: In the 1960s, mayor Robert Wagner ordered an increase in the Transit Police force from 1,219 to 3,100 officers.  During the hours at which crimes most frequently occurred (between 8:00 p.m. and 4:00 a.m.), the officers went on patrol in all stations and trains.  In response, crime rates decreased, as extensively reported by the press.  Due to another crime increase in the subway, the rear cars of subway consists were shut at night beginning in July 1974. However, during the subway's main era of decline following the city's 1976 fiscal crisis, crime was being announced on the subway every day, with an additional 11 \"crimes against the infrastructure\" in open cut areas of the subway in 1977, wherein TA staff were injured, some seriously.  There were other rampant crimes as well, so that two hundred were arrested for possible subway crimes in the first two weeks of December 1977, under an operation dubbed \"Subway Sweep\".  Passengers were afraid of the subway because of its crime, angry over long waits for trains that were shortened to save money, and upset over the general malfunctioning of the system.  The subway also had many dark subway cars.  Further compounding the issue, on July 13, 1977, a blackout cut off electricity to most of the city and to Westchester.  Due to a sudden increase of violent crimes on the subway in the last week of 1978, police statistics about crime in the subway were being questioned.  In 1979, six murders on the subway occurred in the first two months of the year, compared to nine during the entire previous year.  The IRT Lexington Avenue Line was known to frequent muggers, so in February 1979, a group headed by Curtis Sliwa, began unarmed patrols of the 4 train during the night time, in an effort to discourage crime.  They were known as the Guardian Angels, and would eventually expand their operations into other parts of the five boroughs.  By February 1980, the Guardian Angels' ranks numbered 220. To attract passengers, in September 1978 the TA introduced the “Train to the Plane”, a premium-fare service that provided limited stops along Sixth Avenue in Manhattan from 57th Street to Howard Beach, where passengers could transfer to a shuttle bus to JFK Airport.  The service was staffed by a transit police officer 24/7, and the additional fare was paid on board.  This was discontinued in 1990 due to low ridership and the high cost of its operation. In March 1979, Mayor Ed Koch asked the city's top law enforcement officials to devise a plan to counteract rising subway violence and to stop insisting that the subways were safer than the streets.  Two weeks after Koch's request, top TA cops were publicly requesting Transit Police Chief Sanford Garelik's resignation because they claimed that he lost control of the fight against subway crime.  Finally, on September 11, 1979, Garelik was fired, and replaced with Deputy Chief of Personnel James B. Meehan, reporting directly to City Police Commissioner Robert McGuire.  Garelik continued in his role of chief of security for the MTA.  By September 1979, around 250 felonies per week (or about 13,000 that year) were being recorded on the subway, making the crime rate the most of any other mass transit network anywhere in the world.  Some police officers supposedly could not act upon quality of life crimes, and that they should only look for violent crimes.  Among other problems included: Meehan had claimed to be able to, along with 2.3 thousand police officers, \"provide sufficient protection to straphangers\", but Sliwa had brought a group together to act upon crime, so that between March 1979 and March 1980, felonies per day dropped from 261 to 154.  However, overall crime grew by 70% between 1979 and 1980. On the IRT Pelham Line in 1980, a sharp rise in window-smashing on subway cars caused $2 million in damages; it spread to other lines during the course of the year.  When the broken windows were discovered in trains that were still in service, they needed to be taken out of service, causing additional delays; in August 1980 alone, 775 vandalism-related delays were reported.  Vandalism of subway cars, including windows, continued through the mid-1980s; between January 27 and February 2, 1985, 1,129 pieces of glass were replaced on subway cars on the 1 , 6 , CC , E , and K trains.  Often, bus transfers, sold on the street for 50 cents, were also sold illegally, mainly at subway-to-bus transfer hubs.  Mayor Koch even proposed to put a subway court in the Times Square subway station to speed up arraignments, as there were so many subway-related crimes by then.  Meanwhile, high-ranking senior City Hall and transit officials considered raising the fare from 60 to 65 cents to fund additional transit police officers, who began to ride the subway during late nights (between 8 p.m. and 4 a.m.) owing to a sharp increase in crime in 1982.  Operation High Visibility, commenced in June 1985, had this program extended to 6 a.m., and a police officer was to be present on every train in the system during that time. On January 20, 1982, MTA Chairman Richard Ravitch told the business group Association for a Better New York, that he would not let his teenage sons ride the subway at night, and that even he, as the subway chairman, was nervous riding the trains.  The MTA began to discuss how the ridership issue could be fixed, but by October 1982, mostly due to fears about transit crime, poor subway performance and some economic factors, ridership on the subway was at extremely low levels matching 1917 ridership.  Within less than ten years, the MTA had lost around 300 million passengers, mainly because of fears of crime.  In July 1985, the Citizens Crime Commission of New York City published a study showing this trend, fearing the frequent robberies and generally bad circumstances.  As a result, the \"Fixing Broken Windows\" policy, which proposed to stop large-profile crimes by prosecuting quality of life crimes, was implemented.  Along this line of thinking, the MTA began a five-year program to eradicate graffiti from subway trains in 1984, and hired one of the original theorists of Broken Windows policing, George L. Kelling, as a consultant for the program in 1985. In the early afternoon of December 22, 1984, Bernhard Goetz shot four young African American men from the Bronx on a New York City Subway train; the incident got nationwide media coverage.  That day, the men—Barry Allen, Troy Canty, Darrell Cabey (all 19), and James Ramseur (18)—boarded a downtown 2 train (Broadway – Seventh Avenue Line express) carrying screwdrivers, apparently on a mission to steal money from video arcade machines in Manhattan.  When the train arrived at the 14th Street station in Manhattan, 15 to 20 other passengers remained with them in R22 subway car 7657, the seventh car of the ten-car train.  At the 14th Street station, Goetz entered the car through the rearmost door, crossed the aisle, and took a seat on the long bench across from the door.  After Canty asked Goetz how he was, Goetz replied affirmatively, at which point the four boys supposedly moved over to the left of Goetz, blocking Goetz off from the other passengers in the car.  They then asked Goetz for money.  He fired five shots, seriously wounding all four men.  Nine days later he surrendered to police and was eventually charged with attempted murder, assault, reckless endangerment, and several firearms offenses.  A jury found him not guilty of all charges except for one count of carrying an unlicensed firearm, for which he served eight months of a one-year sentence.  The incident sparked a nationwide debate on race and crime in major cities, the legal limits of self-defense, and the extent to which the citizenry could rely on the police to secure their safety.  Although Goetz, dubbed the \"Subway Vigilante\" by New York City's press, came to symbolize New Yorkers' frustrations with the high crime rates of the 1980s, he was both praised and vilified in the media and public opinion.  The incident has also been cited as a contributing factor to the groundswell movement against urban crime and disorder, and the successful National Rifle Association campaigns to loosen restrictions on the concealed carrying of firearms. In 1989, the Metropolitan Transportation Authority asked the transit police (then located within the NYCTA) to focus on minor offenses such as fare evasion.  In the early nineties, the NYCTA adopted similar policing methods for Penn Station and Grand Central Terminal.  When in 1993, Mayor Rudy Giuliani and Police Commissioner Howard Safir were elected to official positions, the Broken Windows strategy was more widely deployed in New York City under the rubrics of \"zero tolerance\" and \"quality of life\".  Crime rates in the subway and city dropped, prompting \"New York Magazine\" to declare \"The End of Crime as We Know It\" on the cover of its edition of August 14, 1995.  Giuliani's campaign credited the success to the zero tolerance policy.  The extent to which his policies deserve the credit is disputed.  Incoming New York City Police Department Commissioner William J. Bratton and author of \"Fixing Broken Windows\", George L. Kelling, however, stated the police played an \"important, even central, role\" in the declining crime rates. On April 2, 1995, the New York City Police Department and the Transit Police Department merged. Ironically, the Program for Action forced the closure of a large number of subway lines.  The Bronx remnant of the IRT Third Avenue Line closed in 1973, to be provisionally replaced by a new subway under the Metro-North Railroad tracks on Park Avenue, one block to the west.  The E train stopped using the lower level of the 42nd Street–Port Authority Bus Terminal station on February 28, 1975.  The Culver Shuttle between Ditmas Avenue and Ninth Avenue, having been reduced to a single track for many years and in deteriorating condition, closed permanently on May 11, 1975.  On August 27, 1976, GG service was cut back from Church Avenue to Smith–Ninth Streets.  The K and EE routes were eliminated.  On December 15, 1976, GG service at the other terminal was cut back to Queens Plaza.  The BMT Jamaica Line was cut back from 168th Street to 121st Street between September 11, 1977 and the early 1980s, replaced by the BMT Archer Avenue Line in 1988. As elevated structures were torn down as part of the Program for Action, existing elevated structures became more dangerous by the day.  One individual walking under the BMT Astoria Line displayed, for the \"New York Post\", a large collection of debris that rained from the line as trains passed by.  In January 1979, another individual was almost killed by falling debris under the IRT Pelham Line between Zerega and Castle Hill Avenues.  In September 1979, multiple claims of \"stuff falling from the (West End) El\" along New Utrecht Avenue led attorneys for a Bensonhurst anti-noise group and state senator Martin Solomon to file suit against the MTA to fix the structure. On October 27, 1976, loose chunks of concrete were falling from a retaining wall on the southbound side of the BMT Brighton Line just south of the Beverley Road station.  After many temporary reroutes and usage of the Manhattan-bound express track in both directions, the southbound track alignment had to be changed; all trains had to use a single track and a shuttle had to be created between Beverley Road and Prospect Park.  Starting November 1, 1976 and continuing until February 25, 1977, hundreds of trains were cancelled and/or rerouted due to the construction. Fare evasion seemed a small problem compared to the graffiti and crime; however, fare evasion was causing the NYCTA to lose revenue.  NYCTA’s strategy for restoring riders’ confidence took a two-pronged approach.  In 1981, MTA’s first capital program started system’s physical restoration to a State-of-Good-Repair.  Improving TA’s image in riders’ minds is as important as overcoming deferred maintenance.  Prompt removal of graffiti and prevention of blatant fare evasion would become central pillars of the strategy to assure customers that the subway is “fast, clean, and safe”: Similarly, fare evasion was taken seriously.  The NYCTA began formally measuring evasion in November 1988.  When TA’s Fare Abuse Task Force (FATF) was convened in January 1989, evasion was 3.9%.  After a 15-cent fare increase to $1.15 in August 1990, a record 231,937 people per day, or 6.9%, didn’t pay.  The pandemonium continued through 1991.  To combat the mounting problem, FATF designated 305 “target stations” with most evaders for intensive enforcement and monitoring.  Teams of uniformed and undercover police officers randomly conducted “mini-sweeps”, swarming and arresting groups of evaders.  Special “mobile booking centers” in converted city buses allowed fast-track offender processing.  Fare abuse agents covered turnstiles in shifts and issued citations.  Plainclothes surveyors collected data for five hours per week at target locations, predominantly during morning peak hours.  Finally, in 1992, evasion began to show a steady and remarkable decline, dropping to about 2.7% in 1994. The dramatic decrease in evasion during this period coincided with a reinvigorated Transit Police, a 25% expansion of City police, and a general drop in crime in U.S. cities.  In the city, crime rate decline begun in 1991 under Mayor David Dinkins and continued through next two decades under Mayors Rudolph Giuliani and Michael Bloomberg.  Some observers credited the “broken windows” approach of law enforcement where minor crimes like evasion are routinely prosecuted, and statistical crimefighting tools, whereas others have indicated different reasons for crime reduction.  Regardless of causality, evasion checks resulted in many arrests for outstanding warrants or weapons charges, likely contributing somewhat to public safety improvements.  Arrests weren’t the only way to combat evasions, and by the early 1990s NYCTA was examining methods to improve fare control passenger throughputs, reduce fare collection costs, and maintain control over evasions and general grime.  The AFC system was being designed, and evasion-preventing capability was a key consideration. TA’s queuing studies concluded that purchasing tokens from clerks was not efficient.  Preventing ‘slug’ use required sophisticated measures like tokens with metal alloy centers and electronic token verification devices.  To provide better access control, the NYCTA experimented with floor-to-ceiling gates and “high wheel” turnstiles.  Prototypes installed at 110th Street/Lexington Avenue station during a “target hardening” trial reduced evasions compared to nearby “control” stations.  However, controls consisting entirely of “high-wheels” created draconian, prison-like environments, with detrimental effects on station aesthetics.  Compromises with more secure low-turnstile designs were difficult, as AFC did not prevent fare evasion. Production Automated Fare Collection (AFC) implementation began in 1994.  New turnstiles, including unstaffed high wheels, and floor-to-ceiling service gates, featured lessons learned from trials.  As AFC equipment was rolled out, evasion plummeted.  Fare abuse agents, together with independent monitoring, were eliminated. Ridership during 1979, which increased 4% over 1978 levels, was attributed to an improving economy. A multi-part Daily News series on the subway in 1979 advertised \"a whole new subway\"; numerous improvements and rehabilitations were performed as a result: Over six years, $1.7 billion would be provided by the state and the Port Authority under the Urban Mass Transit Act. Phyllis Cerf Wagner, the aesthetics chairman at the time, additionally announced \"Operation Facelift\", a program calling for new paint, better lighting, increased platform seating, and faster window and door replacements.  She planned the removal underground concession stands, persuaded the MTA to cease painting rolling stock silver with a blue stripe, and envisioned an underground mall modeled after Underground Atlanta, that would stretch from Herald Square to 47th–50th Streets–Rockefeller Center stations (connecting with the existing Rockefeller Center concourse) and Grand Central–42nd Street.  The underground mall was not completed, but the majority of the aforementioned improvements were eventually carried out. During the mid-1980s, reconstruction began.  Stations were refurbished and rolling stock was repaired and replaced.  Poor maintenance trends began to reverse by 1986.  There were three in-service derailments in 1985, compared with 15 in 1984 and 21 in 1983.  The number of \"red tag\" areas — areas of track necessitating immediate repairs, meaning trains needed to slow to 10 mph — dropped from over 500 to two in 1986.  As an example, the BMT Brighton Line's tracks between Sheepshead Bay and Prospect Park were replaced in 1986 — the first time this had been done in 20 years.  However, when the project was completed, trains were either too high or too far away from many of the platforms.  Some areas necessitated the ripping up of tracks, removal of the ballast trackbed, and finally replacement of the track.  The 325 R62 cars, in service for a year on the 4 , were proving reliable, at an average of 50,000 miles between failures (compared to 9,000 for the other subway car models), with the newly overhauled World's Fair R36 equipment on the Flushing Line averaging over 30,000 miles in 1986-87.  The Mean Distance between Failures of many rolling stock classes was improving; it had been as low as 6,000 miles in 1980, and increased to 10,000 miles by September 1986.  At that time, 670 new cars were accepted, 850 overhauled cars were in service, and 3,000 cars were made graffiti-free.  Speedometers were also installed on existing and new rolling stock. On January 1, 1982, the MTA implemented the first of its five-year Capital Improvement programs to repair the existing system.  Scheduled Maintenance Services were formed to replace components before they failed.  Subway cars in classes R26 to R46 went through general overhaul programs to fix and rehabilitate rolling stock.  Older equipment (any car classes with contract numbers below R32s on the B Division and R26s on the A Division) were retrofitted with air conditioning.  The red tag areas were incrementally repaired and welded rail could be seen on many lines by the end of the 1980s.  At the end of the century, the MDBF rates for the entire system were at record highs and steadily increasing.  The Franklin Avenue Shuttle, however, was worse in 1989 than it was in 1980, and necessitated a complete renovation by 1998, because the MTA planned to abandon the line by the end of the century. Starting in the early 1970s, there were plans for improving the subway system. Halfway through Phase I of the 1968 Program for Action, a status report was issued.  It stated: \"Almost all of the projects are well ahead of the goal recommended five years ago.  Despite technical setbacks, legal roadblocks, administrative frustrations and limited funding, progress has been substantial.\"  Other improvements, not stated in the report, were also being done.  As of 1973, progress had been made on several projects, the first six of which were Program for Action projects: In April 1974, the City approved emergency appropriations to fix the following problems: A $3.5 billion bond issue, declined on November 6, 1973, would have financed the following subway improvements: Even through the late 1970s, there were improvement plans, many of which were implemented. A grant consisting of $27 million in federal funds in October 1978 enabled the following improvements: Transit improvements planned for 1979 included: In May 1979, $19.9 million in federal funds was requested for the following transit improvements: In 1976, the MTA, as a cost saving measure, looked to discontinue the Franklin Avenue Shuttle, but neighborhood support for it saved it from being closed.  The possibility of the discontinuation was revisited again in 1998, but again, fierce community opposition to it forced the MTA to rehabilitate the line. In 1977, the Linden Shops opened in Brooklyn, enabling the MTA to build track panels indoors throughout the year, among other objects. On January 16, 1978, the MTA opened three transfer stations: In April 1981, the following projects were considered by the MTA: In 1981, the MTA began installing welded rails on a few underground portions of the system. In June 1983, the following projects were considered by the MTA: On March 25, 1986, the Regional Plan Association (RPA) proposed several changes.  A major part of the plan was eliminating parts of the system and expanding the system to reflect population shifts.  The plan called for eliminating 26 miles of elevated lines and building 17 miles of new subway lines, and 20 miles of new surface lines.  Proposals included: The RPA also recommended the following prioritized expenditures looking forward to the year 2000: In April 1986, the New York City Transit Authority began to study the possibility of eliminating sections of 11 subway lines because of low ridership.  The segments are primarily located in low-income neighborhoods of the Bronx, Brooklyn and Queens, with a total of 79 stations, and 45 miles of track, for a total of 6.5 percent of the system.  The lines were first identified in the first part of a three-year project, the Strategic Plan Initiative, which started in April 1985, by the MTA to evaluate the region's bus, subway, and commuter rail systems.  The eleven segments all had low ridership, needed expensive rebuilding, and duplicated service on parallel lines.  The lines being studied included the following lines: By August 1989, the MTA was considering these projects: In December 1988, three transfers were opened between existing stations, and three brand-new stations were opened.  They were: The new stations were Sutphin Boulevard – Archer Avenue – JFK Airport, Jamaica Center – Parsons/Archer, and Jamaica – Van Wyck.  Other service changes were implemented that day.  Skip-stop service on the J/Z trains was also started on December 11, 1988.  Additionally, IND Fulton Street Line express service was extended from weekdays only to all times except late nights.  Discontinuous services on the B , D , and Q trains over the Manhattan Bridge were replaced by continuous services. On May 12, 1989, the last train with graffiti was taken out of service; the subway has been mostly graffiti-free since this point. On October 29, 1989, the IND 63rd Street Line was opened.  It was nicknamed the \"tunnel to nowhere\" due to its stub end at 21st Street–Queensbridge, and also due to the fact that the three-station extension lay dormant for over a decade after completion.  The 3.2 mile line included three new stations and cost a total of $868 million.  The line was viewed as an enormous waste of money. In the 1980s, the MTA considered buying 208 63 ft subway cars to replace 260 51 ft IRT cars, even though these longer cars were never tried anywhere on the IRT.  The cars would be purchased using $190 million from the Transportation Bond Act voters approved in November 1979.  Advantages of the car were the same as in the R44 and R46 orders—fewer subway cars of longer length can make up a 510-foot train and reducing operating and maintenance costs; however, drawbacks identified for these cars included not lining up with the movable platforms at 14th Street–Union Square, and not fitting tight curves, such as at South Ferry.  After paying consultant Louis T. Klauder and Associates $894,312 to evaluate the merits of the 63-foot car, the plans were dropped. By March 1982, the MTA closed a deal to purchase 325 new IRT subway cars from Kawasaki Heavy Industries of Japan.  It would be the first purchase of foreign-made subway cars that ever ran on the New York City Subway system.  Other candidates for this order included Bombardier and the Budd Company.  The first Capital Program allocated funding for the purchase of 1,150 subway cars, and Kawasaki was not interested in building another 825 IRT cars.  Bombardier ended up winning the contract for the R62As. In October 1982, a consortium of French engineering companies was selected by the MTA to build 225 subway cars, which became known as the R68s.  The consortium was chosen over bids from the Budd Company and the Sumitomo Group.  The first regular R68 train went into revenue service in Brighton Beach on June 20, 1986, after passing a successful 30-day test.  Two hundred option-1 cars were later delivered for a total of 425 cars.  The option for 200 additional R68s was given to Kawasaki and the car class became known as the R68A.  The first R68A cars were delivered to New York City on April 12, 1988 and transferred to the MTA the following day.  The first train of R68As began a 30-day acceptance test on May 18, 1988 on the IND Concourse Line. The R10, R14, R16, R17, R21, and R22 car classes all were retired with the deliveries of the R62/As and R68/As. The September 11 attacks resulted in service disruptions on lines running through Lower Manhattan.  Tracks and stations under the World Trade Center were shut down within minutes of the first plane crash.  All remaining New York City Subway service was suspended from 10:20am to 12:48pm.  Immediately after the attacks and more so after the collapses of the Twin Towers, many trains running in Lower Manhattan lost power and had to be evacuated through the tunnels.  Some trains had power, but the signals did not, requiring special operating procedures to ensure safety. The IRT Broadway–Seventh Avenue Line, which ran below the World Trade Center between Chambers Street and Rector Street was the most crippled.  Sections of the tunnel as well as Cortlandt Street were badly damaged and had to be rebuilt.  Service was immediately suspended south of Chambers Street and then cut back to 14th Street.  There was also subsequent flooding on the line south of 34th Street–Penn Station.  After the flood was cleaned up, express service was able to resume on September 17 with 1 trains running between Van Cortlandt Park–242nd Street and 14th Street, making local stops north of and express stops south of 96th Street, while 2 and 3 trains made all stops in Manhattan (but bypassed all stations between Canal Street and Fulton Street until October 1).  1/9 skip-stop service was suspended. After a few switching delays at 96th Street, service was changed on September 19.  The 1 train resumed local service in Manhattan, but was extended to New Lots Avenue in Brooklyn (switching onto the express tracks at Chambers Street) to replace the 3, which now terminated at 14th Street as an express.  The train continued to make local stops in Manhattan and service between Chambers Street and South Ferry as well as skip-stop service remained suspended.  Normal service on all four trains was restored September 15, 2002, but Cortlandt Street will remain closed while the World Trade Center site is redeveloped. Service on the BMT Broadway Line was also disrupted because the tracks from the Montague Street Tunnel run adjacent to the World Trade Center and there were concerns that train movements could cause unsafe settling of the debris pile.  Cortlandt Street station, which sits under Church Street, sustained significant damage in the collapse of the towers.  It was closed until September 15, 2002 for removal of debris, structural repairs, and restoration of the track beds, which had suffered flood damage in the aftermath of the collapse.  Starting September 17, 2001, N and R service was suspended and respectively replaced by the M (which was extended to Coney Island–Stillwell Avenue via the BMT Montague Street Tunnel, BMT Fourth Avenue Line, and BMT Sea Beach Line) and the J (also extended via Fourth Avenue to Bay Ridge–95th Street).  In Queens, the Q replaced the R while the W replaced the N .  All service on the BMT Broadway Line ran local north of Canal Street except for the Q, which ran normally from 57th Street to Brighton Beach via Broadway and Brighton Express.  J/Z skip-stop service was suspended at this time.  Normal service on all seven trains resumed on October 28. The only subway line running between Midtown and Lower Manhattan was the IRT Lexington Avenue Line, which was overcrowded before the attacks and at crush density until the BMT Broadway Line reopened.  Wall Street was closed until September 21. The IND Eighth Avenue Line, which has a stub terminal serving the E train under Five World Trade Center was not damaged, but covered in soot.  E trains were extended to Euclid Avenue, Brooklyn, replacing the then suspended C train (the A and D trains replaced it as the local north of 59th Street–Columbus Circle on nights and weekends, respectively.  The B train, which ran normally from 145th Street or Bedford Park Boulevard to 34th Street–Herald Square via Central Park West Local, also replaced C trains on weekdays).  Service was cut back to Canal Street when C service resumed on September 21, but Chambers Street and Broadway–Nassau Street remained closed until October 1.  World Trade Center remained closed until January 2002. Generally, ridership kept rising as the subway system improved in its maintenance, cleanliness, frequency, and on-time ratio; ridership started to increase as graffiti and crime rates dropped heavily after 1989.  From 1995 to 2005, ridership on city buses and subways grew by 36%, compared with a population gain in the city of 7%.  With dramatic increases in fuel prices in 2008, as well as increased tourism and residential growth, ridership on buses and subways grew 3.1% up to about 2.37 billion trips a year compared to 2007.  This is the highest ridership since 1965. By 2013, ridership had reached 1.7 billion riders per year (despite closures related to Hurricane Sandy), a level not seen since 1949.  In April 2013, \"New York\" magazine reported that the system was more crowded than it had been in the previous 66 years.  The subway reached a daily ridership of 6 million for 29 days in 2014, and was expected to record a similar ridership level for 55 days in 2015; by comparison, in 2013, daily ridership never reached 6 million. The IND 63rd Street Line's connection to the IND Queens Boulevard Line opened on December 16, 2001.  To serve the new connection, the F train was rerouted via the 63rd Street Line, and to replace the F along 53rd Street, a new V train was created–running between Forest Hills–71st Avenue and Second Avenue via the Queens Boulevard and Sixth Avenue local tracks.  The G, to allow for room to operate the V, was cut back to Court Square.  Two out-of-system transfers were put into place; the first was to allow F passengers to continue to have a free transfer to the Lexington Avenue Line, which was lost when the line was rerouted–the transfer connects the Lexington Avenue/59th Street station and the Lexington Avenue–63rd Street stations.  The second one connected the Court Square station with the 45th Road–Court House Square station. In 2003, money was allocated for the construction of a new station at South Ferry, and in 2005, construction commenced on the new station.  Initially, the station's construction had been opposed because of the high cost and low perceived time savings.  The South Ferry loop station only accommodates the first five cars of a train, so that the rear five cars of a 10-car 1 train cannot load or unload.  Because of the curve at the station gap fillers are required, and as a result the new station was built as a two-track, full (10-car)-length island platform on a less severe curve, permitting the operation of a typical terminal station.  The MTA claimed that the new station saved four to six minutes of a passenger's trip time and increased the peak capacity of the 1 service to 24 trains per hour, as opposed to 16 to 17 trains per hour with the loop station. In the late 1990s and early 2000s, talk began to circulate about taking up the construction of the Second Avenue Subway.  Most New Yorkers regarded these plans with cynicism, since citizens were promised the line since well before the Third Avenue elevated was torn down in 1955.  Funds have been set aside and environmental impact reports have been completed.  A tunnelling contract was awarded to the consortium of Schiavone/Shea/Skanska (S3) by the MTA on March 20, 2007.  This followed preliminary engineering and a final tunnel design completed by a joint venture between AECOM and Arup.  A ceremonial groundbreaking for the subway was held on April 12, 2007 and contractor work to prepare the project's initial construction site at 96th Street and Second Avenue began on April 23, 2007. In October 2007, the 7 Subway Extension construction contract was awarded, extending the IRT Flushing Line to 34th Street.  Groundbreaking began in June 2008 and the tunnels were completed by 2010.  The project, which was the first one funded by the city in over sixty years, was intended to aid redevelopment of Hell's Kitchen around the West Side Yard of the Long Island Rail Road. The MTA faced a budget deficit of US$1.2 billion in 2009.  This resulted in fare increases (three times from 2008 to 2010) and service reductions (including the elimination of two part-time subway services, the V and W ).  Several other routes were modified as a result of the deficit.  The N was made a full-time local in Manhattan (in contrast to being a weekend local/weekday express before 2010), while the Q was extended nine stations north to Astoria–Ditmars Boulevard on weekdays, both to cover the discontinued W.  The M was combined with the V , routing it over the Chrystie Street Connection, IND Sixth Avenue Line and IND Queens Boulevard Line to Forest Hills–71st Avenue on weekdays instead of via the BMT Fourth Avenue Line and BMT West End Line to Bay Parkway.  The G was truncated to Court Square full-time.  Construction headways on eleven routes were lengthened, and off-peak service on seven routes were lengthened. On October 29, 2012, a full closure of the subway was ordered before the arrival of Hurricane Sandy.  All services on the subway, the Long Island Rail Road and Metro-North were gradually shut down that day.  The storm caused serious damage to the system, especially the IND Rockaway Line, which had many sections between Howard Beach–JFK Airport and Hammels Wye on the Rockaway Peninsula heavily damaged, leaving it essentially isolated from the rest of the system.  This required the NYCTA to truck in 20 R32 subway cars to the line to provide some interim service, which was temporarily designated the H .  The H ran between Beach 90th Street and Far Rockaway–Mott Avenue, where passengers could transfer to a free shuttle bus.  The line reopened on May 30, 2013, with a new retaining wall along the line to prevent against future storm surges. Several of the system's tunnels under the East River were flooded by the storm surge.  The South Ferry suffered serious water damage, and on April 4, 2013, the older loop-configured station reopened to provide temporary service.  The first tunnel to be repaired, the Greenpoint Tube under Newtown Creek, was fixed during a series of weekend closures in 2013 and a full closure during summer 2014.  The second tunnel, the Montague Street Tunnel, was closed completely from August 2013 to September 2014. Hurricane Sandy also damaged the Clark Street Tubes, necessitating a full closure on weekends between June 2017 and June 2018, thus affecting 2, 3, 4, and 5 service.  The new South Ferry station reopened on June 27, 2017, in time to accommodate the Clark Street closures.  A week after South Ferry reopened, the MTA closed the BMT Myrtle Avenue Line for ten months to rebuild two of the line's viaducts, the 310 ft approaches to the line's junction with the BMT Jamaica Line and Fresh Pond Bridge over the Montauk Branch in Queens.  This was in preparation for a reconstruction of the BMT Canarsie Line tunnels under the East River. From early 2019 to mid-2020, the BMT Canarsie Line shutdown would close the line west of Bedford Avenue for 18 months.  In April 2017, the shutdown was shortened to 15 months. Originally budgeted at $400 million, the new South Ferry station opened in 2009 at a total of $530 million, with most of the money being a grant from the Federal Transit Administration earmarked for World Trade Center reconstruction.  In January 2009, the opening was delayed because the tracks were too far from the edge of the platform.  Other delays were attributed to leaks in the station.  The problem was corrected and the station opened on March 16, 2009.  With the opening of the new station, a transfer was available to the Whitehall Street station with a new connecting passageway. On December 20, 2013, Mayor Michael Bloomberg took a ceremonial ride on a train to the new 34th Street terminal of the IRT Flushing Line, celebrating a part of his legacy as Mayor, during a press tour of the uncompleted station.  The project was 90% complete in 2013, and was expected to open in 2014, but did not open until September 13, 2015.  According to \"The New York Times\", the delay in the extension's opening was due to the installation of custom-made incline elevators that kept malfunctioning.  A December 2014 \"New York Post\" article also attributed the delay to Hudson Yards' developer, The Related Companies', need to dig caissons for the foundations, just above the subway station; the foundation work needed to be complete before the MTA could proceed with opening the station. The Fulton Center building opened to the public on November 10, 2014, completing a decade-long refurbishment of the Fulton Street station in lower Manhattan. As part of the Second Avenue Subway's opening, W service, which had not run since 2010, was restored on November 7, 2016.  On January 1, 2017, the Second Avenue Subway was opened, with stations crowded with passengers seeking to check out the new line.  About 48,200 passengers entered the new stations on that day, excluding passengers who toured the line by entering at a station in the rest of the system. In June 2017, Governor Andrew Cuomo signed an executive order declaring a state of emergency for the New York City Subway after a series of derailments, track fires, and overcrowding incidents.  On June 27, 2017, thirty-nine people were injured when an A train derailed at 125th Street, damaging tracks and signals then catching on fire.  On July 21, 2017, the second set of wheels on a southbound Q train jumped the track near Brighton Beach, with nine people suffering injuries due to improper maintenance of the car in question.  To solve the system's problems, the MTA officially announced the Genius Transit Challenge on June 28, where contestants could submit ideas to improve signals, communications infrastructure, or rolling stock. On July 25, 2017, Chairman Joe Lhota announced a two-phase, $9 billion New York City Subway Action Plan to stabilize the subway system and to prevent the continuing decline of the system.  The first phase, costing $836 million, consisted of five categories of improvements in Signal and Track Maintenance, Car Reliability, System Safety and Cleanliness, Customer Communication, and Critical Management Group.  The $8 billion second phase would implement the winning proposals from the Genius Transit Challenge and fix more widespread problems. There are several lines being planned.  This includes a subway line under Utica Avenue in Brooklyn; an outer-borough circumferential line, the Triboro RX; a reuse of the northern part of the Rockaway Beach Branch; and a line to LaGuardia Airport. In November 2016, the MTA requested that the Second Avenue Subway's Phase 2 project be entered into the Project Development phase under the Federal Transit Administration's New Starts program.  The FTA granted this request in late December 2016.  Under the approved plan, the MTA would complete an environmental reevaluation by 2018, receive funding by 2020, and open Phase 2 between 2027 and 2029.  The line will be built in four phases and will eventually run as far north as 125th Street in East Harlem during Phase 2, and south to Hanover Square in Lower Manhattan in Phases 3 and 4. When the IRT subway debuted in 1904, the typical tunnel construction method was cut-and-cover.  The street was torn up to dig the tunnel below before being rebuilt from above.  Traffic on the street above would be interrupted due to the digging up of the street.  Temporary steel and wooden bridges carried surface traffic above the construction. Contractors in this type of construction faced many obstacles, both natural and man-made.  They had to deal with rock formations, and ground water, which required pumps.  Twelve miles of sewers, as well as water and gas mains, electric conduits, and steam pipes had to be rerouted.  Street railways had to be torn up to allow the work.  The foundations of tall buildings often ran near the subway construction, and in some cases needed underpinning to ensure stability. This method worked well for digging soft dirt and gravel near the street surface.  However, tunnelling shields were required for deeper sections, such as the Harlem and East River tunnels, which used cast-iron tubes.  Segments between 33rd and 42nd streets under Park Avenue, 116th Street and 120th Street under Broadway, and 145th Street and Dyckman Street (Fort George) under Broadway and Saint Nicholas Avenue as well as the tunnel from 96th Street to Central Park North–110th Street & Lenox Avenue, used either rock or concrete-lined tunnels. About 40% of the subway system runs on surface or elevated tracks, including steel or cast iron elevated structures, concrete viaducts, embankments, open cuts and surface routes.  All of these construction methods are completely grade-separated from road and pedestrian crossings, and most crossings of two subway tracks are grade-separated with flying junctions.  The sole exceptions of at-grade junctions of two lines in regular service are the 142nd Street junction and the Myrtle Avenue junction, whose tracks both intersect at the same level. The 7,700 workers who built the original subway lines were mostly immigrants living in Manhattan. More recent projects use tunnel boring machines, which minimize disruption at street level and avoid already existing utilities, but increase cost.  Examples of such projects include the extension of the IRT Flushing Line and the IND Second Avenue Line. Many train accidents have been recorded since 1918, when a train bound for South Ferry smashed into two trains halted near Jackson Avenue on the IRT White Plains Road Line in the Bronx. Only accidents that caused injuries, deaths, or significant damage are listed. Additionally, in an accident recorded before 1918, a derailment happened on the Ninth Avenue Elevated in Manhattan on September 11, 1905, resulting in 13 deaths and 48 serious injuries. Other accidents in the history of the subway do not involve trains; several people have been fatally electrocuted by the subway's third rails, and yet others have been fatally pushed onto the tracks. In 1960, someone nicknamed the Sunday Bomber set off a series of bombs in the New York City Subway and ferries during Sundays and holidays, killing one woman and injuring 51 other commuters.\n\nPlug-in hybrid A plug-in hybrid electric vehicle (PHEV) is a hybrid electric vehicle that uses rechargeable batteries, or another energy storage device, that can be recharged by plugging it in to an external source of electric power.  A PHEV shares the characteristics both of a conventional hybrid electric vehicle, having an electric motor and an internal combustion engine (ICE), and of an all-electric vehicle, having a plug to connect to the electrical grid.  Most PHEVs are passenger cars but there are also PHEV versions of commercial vehicles and vans, utility trucks, buses, trains, motorcycles, scooters, and military vehicles. The cost of electricity to power plug-in hybrids for all-electric operation has been estimated at less than one quarter of the cost of gasoline in California.  Compared to conventional vehicles, PHEVs produce less air pollution locally and require less petroleum.  PHEVs may produce less in the way of greenhouse gases, which contribute to global warming, than conventional vehicles do.  PHEVs also eliminate the problem of range anxiety associated with all-electric vehicles, because the combustion engine works as a backup when the batteries are depleted, giving PHEVs driving-range comparable to that of other vehicles that have gasoline and diesel tanks.  Plug-in hybrids use no fossil fuel at the point of use during their all-electric range. Plug-in hybrids' greenhouse-gas emissions, during operation in their all-electric range mode, depend on the type of power plant used to feed the electrical grid when the battery is charged.  (See \"Greenhouse gas emissions\", below.)  If the batteries are charged directly from renewable sources off the electrical grid, then the tailpipe greenhouse gas emissions are zero when running only on battery power.  Other benefits include improved national energy security, less frequent fill-ups at the filling station, the convenience of home recharging, opportunities to provide emergency backup power in the home, and vehicle-to-grid (V2G) applications.  Several countries, including the United States, China, and several European countries, have enacted laws to ease the introduction of PHEVs through grants and tax credits, emissions mandates, and financing research and development in advanced batteries and related technology. Chinese battery manufacturer and automaker BYD Auto released the F3DM to the Chinese fleet market in December 2008 and began sales to the public in Shenzhen in March 2010.  General Motors began delivering the Chevrolet Volt in the United States in December 2010; it was the first electric car with a range extender for retail sale in the American market.  s of 2016 , there are over 30 models of series-production highway legal plug-in hybrids for retail sales, including some limited-production luxury sport cars. Plug-in hybrid cars are available mainly in the United States, Canada, Western Europe, Japan, and China.  s of 2016 , the Chevrolet Volt family, including its siblings Opel/Vauxhall Ampera, is the world's best-selling plug-in hybrid in history with combined sales of about 134,500 units.  The Mitsubishi Outlander P-HEV is the world's second top-selling plug-in hybrid ever, with global sales of about 119,500 units, followed by the Toyota Prius PHV, with almost 78,000 units delivered globally, both, through December 2016. s of 2016 , the global stock of plug-in hybrid cars totaled almost 800,000 units, out of over 2 million light-duty plug-in electric cars on the world roads at the end of 2016.  s of 2015 , the United States ranked as the world's largest plug-in hybrid car market with a stock of 193,770 units, followed by China with 86,580 vehicles, the Netherlands with 78,160, Japan with 55,470 units, and the UK with 28,250. Flexibility in power demand, diverse usage patterns and storage capability of PHEVs grow the elasticity of residential electricity demand remarkably.  This elasticity can be used to form the daily aggregated demand profile and/or alter instantaneous demand of a system wherein a large number of residential PHEVs share one electricity retailer. A plug-in hybrid's all-electric range is designated by PHEV-\"[miles]\" or PHEV\"[kilometers]\"km in which the number represents the distance the vehicle can travel on battery power alone.  For example, a PHEV-20 can travel twenty miles (32 km) without using its combustion engine, so it may also be designated as a PHEV32km. The Energy Independence and Security Act of 2007 defines a plug-in electric drive vehicle as a vehicle that: This distinguishes PHEVs from regular hybrid cars mass marketed today, which do not use any electricity from the grid. The Institute of Electrical and Electronics Engineers (IEEE) defines PHEVs similarly, but also requires that the hybrid electric vehicle be able to drive at least ten miles (16 km) in all-electric mode (PHEV-10; PHEV16km), while consuming no gasoline or diesel fuel. The California Air Resources Board uses the term \"off-vehicle charge capable\" (OVCC) to mean having the capability to charge a battery from an off-vehicle electric energy source that cannot be connected or coupled to the vehicle in any manner while the vehicle is being driven. Other popular terms sometimes used for plug-in hybrids are \"grid-connected hybrids\", \"Gas-Optional Hybrid Electric Vehicle\" (GO-HEV) or simply \"gas-optional hybrids\".  General Motors calls its Chevrolet Volt series plug-in hybrid an \"Extended-Range Electric Vehicle\". The Lohner-Porsche Mixte Hybrid, produced as early as 1899, was the first hybrid electric car.  Early hybrids could be charged from an external source before operation.  However, the term \"plug-in hybrid\" has come to mean a hybrid vehicle that can be charged from a standard electrical wall socket.  The term \"plug-in hybrid electric vehicle\" was coined by UC Davis Professor Andrew Frank, who has been called the \"father of the modern plug-in hybrid.\"  The July 1969 issue of \"Popular Science\" featured an article on the General Motors XP-883 plug-in hybrid.  The concept commuter vehicle housed six 12-volt lead–acid batteries in the trunk area and a transverse-mounted DC electric motor turning a front-wheel drive.  The car could be plugged into a standard North American 120 volt AC outlet for recharging. In 2003, Renault began selling the Elect'road, a plug-in series hybrid version of their popular Kangoo, in Europe.  It was sold alongside Renault's \"Electri'cité\" electric-drive Kangoo battery electric van.  The Elect'road had a 150 km range using a nickel-cadmium battery pack and a 500 cc , 16 kilowatt liquid-cooled gasoline \"range-extender\" engine.  It powered two high voltage/high output/low volume alternators, each of which supplied up to 5.5 kW at 132 volts at 5000 rpm .  The operating speed of the internal combustion engine—and therefore the output delivered by the generators—varied according to demand.  The fuel tank had a capacity of 10 L and was housed within the right rear wheel arch.  The range extender function was activated by a switch on the dashboard.  The on-board 3.5 kilowatt charger could charge a depleted battery pack to 95% charge in about four hours from a 240 volts supply.  Passenger compartment heating was powered by the battery pack as well as an auxiliary coolant circuit that was supplied by the range extender engine.  After selling about 500 vehicles, primarily in France, Norway and the UK, at a price of about €25,000, the Elect'road was redesigned in 2007. In September 2004, CalCars converted a 2004 Toyota Prius into a prototype of what it called the PRIUS+.  With the addition of 130 kg (300 lb ) of lead–acid batteries, the PRIUS+ achieved roughly double the fuel economy of a standard Prius and could make trips of up to 15 km using only electric power.  The vehicle, which is owned by CalCars technical lead Ron Gremban, is used in daily driving, as well as a test bed for various improvements to the system. On July 18, 2006, Toyota announced that it \"plans to develop a hybrid vehicle that will run locally on batteries charged by a household electrical outlet before switching over to a gasoline engine for longer hauls.\"  In April 2007 Toyota said it planned to migrate to lithium-ion batteries in future hybrid models, but not in the 2009 model year Prius.  Lithium-ion batteries are expected to significantly improve fuel economy, and have a higher energy-to-weight ratio, but cost more to produce, and raise safety concerns due to high operating temperatures. On November 29, 2006, GM announced plans to introduce a production plug-in hybrid version of Saturn's Greenline Vue SUV with an all-electric range of 10 mi .  GM announced in January 2007 that contracts had been awarded to two companies to design and test lithium-ion batteries for the vehicle but the Saturn line was discontinued before the hybrid Vue could be released.  GM has said that they plan on introducing plug-in and other hybrids \"for the next several years\". In January 2007, GM unveiled the prototype of the Chevrolet Volt, which was expected to feature a plug-in capable, battery-dominant series hybrid architecture called E-Flex.  Future E-Flex plug-in hybrid vehicles may use gasoline, diesel, or hydrogen fuel cell power to supplement the vehicle's battery.  General Motors envisions an eventual progression of E-Flex vehicles from plug-in hybrids to pure electric vehicles, as battery technology improves. On July 25, Japan's Ministry of Land, Infrastructure and Transport certified Toyota's plug-in hybrid for use on public roads, making it the first automobile to attain such approval.  Toyota plans to conduct road tests to verify its all-electric range.  The Prius Plug-in Hybrid was said to have an all-electric range of 13 km . On August 9, 2007, General Motors vice-president Robert Lutz announced that GM is on track for Chevrolet Volt road testing in 2008 and production to begin by 2010.  The Volt was designed with all-electric range of 40 mi .  On September 5, Quantum Technologies and Fisker Coachbuild, LLC announced the launch of a joint venture in Fisker Automotive.  Fisker intended to build a US$80,000 luxury PHEV-50, the Fisker Karma, initially scheduled for late 2009. In September 2007, Aptera Motors announced their Typ-1 two-seater.  They planned to produce both an electric 2e and a plug-in series hybrid 2h with a common three-wheeled, composite body design.  As of 2009, over two thousand hybrid pre-orders had been accepted and production of the hybrid configuration was expected to begin in 2010.  However, the company folded in December 2011. On October 9, 2007, Chinese manufacturer BYD Automobile Company (which is owned by China's largest mobile phone battery maker) announced that it would be introducing a production PHEV-60 sedan in China in the second half of 2008.  BYD exhibited it January 2008 at the North American International Auto Show in Detroit.  Based on BYD's midsize F6 sedan, it uses lithium iron phosphate (LiFeP0)-based batteries instead of lithium-ion, and can be recharged to 70% of capacity in just 10 minutes. In December 2007 Ford delivered the first Ford Escape Plug-in Hybrid of a fleet of 20 demonstration PHEVs to Southern California Edison.  As part of this demonstration program Ford also developed the first ever flexible-fuel plug-in hybrid SUV, which was delivered in June 2008.  This demonstration fleet of plug-ins has been in field testing with utility company fleets in the U.S. and Canada, and during the first two years since the program began, the fleet has logged more than 75,000 miles.  In August 2009 Ford delivered the first Escape Plug-in equipped with intelligent vehicle-to-grid (V2G) communications and control system technology, and Ford plans to equip all 21 plug-in hybrid Escapes with the vehicle-to-grid communications technology.  Sales of the Escape PHEV are scheduled for 2012. On January 14, 2008, Toyota announced they would start sales of lithium-ion battery PHEVs by 2010, but later in the year Toyota indicated they would be offered to commercial fleets in 2009. On March 27, the California Air Resources Board modified their regulations, requiring automobile manufacturers to produce 58,000 plug-in hybrids during 2012 through 2014.  This requirement is an asked-for alternative to an earlier mandate to produce 25,000 pure zero-emissions vehicles, reducing that requirement to 5,000.  On June 26, Volkswagen announced that they would be introducing production plug-ins based on the Golf compact.  Volkswagen uses the term 'TwinDrive' to denote a PHEV.  In September, Mazda was reported to be planning PHEVs.  On September 23, Chrysler announced that they had prototyped a plug-in Jeep Wrangler and a Chrysler Town and Country mini-van, both PHEV-40s with series powertrains, and an all-electric Dodge sports car, and said that one of the three vehicles would go into production. On October 3, the U.S. enacted the Energy Improvement and Extension Act of 2008.  The legislation provided tax credits for the purchase of plug-in electric vehicles of battery capacity over 4 kilowatt-hours.  The federal tax credits were extended and modified by the American Clean Energy and Security Act of 2009, but now the battery capacity must be over 5 kWh and the credit phases out after the automaker has sold at least 200,000 vehicles in the U.S. On December 15, 2008 BYD Auto began selling its F3DM in China, becoming the first production plug-in hybrid sold in the world, though initially was available only for corporate and government customers.  Sales to the general public began in Shenzhen in March 2010, but because the F3DM nearly doubles the price of cars that run on conventional fuel, BYD expects subsidies from the local government to make the plug-in affordable to personal buyers. A global demonstration program involving 600 Toyota Prius Plug-in pre-production test cars began in late 2009 in Japan and by mid-2010 field testing had begun in France, Germany, the United Kingdom, Canada, and the United States. Volvo Cars, in a joint venture with Vattenfall, a Swedish energy company, began a demonstration project with two Volvo V70 Plug-in Hybrids in Göteborg, Sweden since December 2009.  As reported by the test drivers, the V70 Plug-in Hybrid demonstrators have an all-electric range between 20 km to 30 km .  The test plug-in hybrids were built with a button to allow test drivers to manually choose between electricity or diesel engine power at any time.  Volvo announced series production of plug-in diesel-electric hybrids as early as 2012.  Volvo claimed that its plug-in hybrid could achieve 125 mpgus , based on the European test cycle. In October 2010 Lotus Engineering unveiled the Lotus CityCar at the 2010 Paris Motor Show, a plug-in series hybrid concept car designed for flex-fuel operation on ethanol, or methanol as well as regular gasoline.  The lithium battery pack provides an all-electric range of 60 km , and the 1.2-liter flex-fuel engine kicks in to allow to extend the range to more than 500 km . GM officially launched the Chevrolet Volt in the U.S. on November 30, 2010, and retail deliveries began in December 2010.  Its sibling the Opel/Vauxhall Ampera was launched in Europe between late 2011 and early 2012.  The first deliveries of the Fisker Karma took place in July 2011, and deliveries to retail customers began in November 2011.  The Toyota Prius Plug-in Hybrid was released in Japan in January 2012, followed by the United States in February 2012.  Deliveries of the Prius PHV in Europe began in late June 2012.  The Ford C-Max Energi was released in the U.S. in October 2012, the Volvo V60 Plug-in Hybrid in Sweden by late 2012. The Honda Accord Plug-in Hybrid was released in selected U.S. markets in January 2013, and the Mitsubishi Outlander P-HEV in Japan in January 2013, becoming the first SUV plug-in hybrid in the market.  Deliveries of the Ford Fusion Energi began in February 2013.  BYD Auto stopped production of its BYD F3DM due to low sales, and its successor, the BYD Qin, began sales in Costa Rica in November 2013, with sales in other countries in Latin America scheduled to begin in 2014.  Qin deliveries began in China in mid December 2013. Deliveries to retail customers of the limited edition McLaren P1 supercar began in the UK in October 2013, and the Porsche Panamera S E-Hybrid began deliveries in the U.S. in November 2013.  The first retail deliveries of the Cadillac ELR took place in the U.S. in December 2013.  The BMW i8 and the limited edition Volkswagen XL1 were released to retail customers in Germany in June 2014.  The Porsche 918 Spyder was also released in Europe and the U.S. in 2014.  The first units of the Audi A3 Sportback e-tron and Volkswagen Golf GTE were registered in Germany in August 2014. In December 2014 BMW announced the group is planning to offer plug-in hybrid versions of all its core-brand models using eDrive technology developed for its BMW i brand plug-in vehicles (BMW i3 and BMW i8).  The goal of the company is to use plug-in technology to continue offering high performance vehicles while reducing CO emissions below 100g/km.  At the time of the announcement the carmaker was already testing a BMW 3 Series plug-in hybrid prototype.  The first model available for retail sales will be the 2016 BMW X5 eDrive, with the production version unveiled at the 2015 Shanghai Motor Show.  The second generation Chevrolet Volt was unveiled at the January 2015 North American International Auto Show, and retail deliveries began in the U.S. and Canada in October 2015. In March 2015 Audi announced plans to have a plug-in hybrid version in every model series in the coming years.  The carmaker expects plug-in hybrids, together with natural gas vehicles and battery-electric drive systems, to have a key contribution in achieving the company's CO targets.  The Audi Q7 e-tron will follow the A3 e-tron already in the market.  Also in March 2015, Mercedes-Benz announced that the company's main emphasis regarding alternative drives in the next years will be on plug-in hybrids.  The carmaker plans to introduce 10 new plug-in hybrid models by 2017, and its next release was the Mercedes-Benz C 350 e, Mercedes’ second plug-in hybrid after the S 500 Plug-In Hybrid.  Other plug-in hybrid released in 2015 are the BYD Tang, Volkswagen Passat GTE, Volvo XC90 T8, and the Hyundai Sonata PHEV. Global combined Volt/Ampera family sales passed the 100,000 unit milestone in October 2015.  By the end of 2015, over 517,000 highway legal plug-in hybrid electric cars have been sold worldwide since December 2008 out of total global sales of more than 1.25 million light-duty plug-in electric cars. In February 2016, BMW announced the introduction of the \"iPerformance\" model designation, which will be given to all BMW plug-in hybrid vehicles from July 2016.  The aim is to provide a visible indicator of the transfer of technology from BMW i to the BMW core brand.  The new designation will be used first on the plug-in hybrid variants of the new BMW 7 Series, the BMW 740e iPerformance, and the 3 Series, the BMW 330e iPerformance. Hyundai Motor Company made the official debut of its three model Hyundai Ioniq line-up at the 2016 Geneva Motor Show.  The Ioniq family of electric drive vehicles includes the Ioniq Plug-in, which is expected to achieve a fuel economy of 125 mpge in all-electric mode.  The Ioniq Plug-in is scheduled to be released in the U.S. in the fourth quarter of 2017. The second generation Prius plug-in hybrid, called Prius Prime in the U.S. and Prius PHV in Japan, was unveiled at the 2016 New York International Auto Show.  Retail deliveries of the Prius Prime began in the U.S. in November 2016, and is scheduled to be released Japan by the end of 2016.  The Prime has an EPA-rated all-electric range of 25 mi , over twice the range of the first generation model, and an EPA rated fuel economy of 133 mpge in all-electric mode (EV mode), the highest MPGe rating in EV mode of any vehicle rated by EPA.  Unlike its predecessor, the Prime runs entirely on electricity in EV mode.  Global sales of the Mitsubishi Outlander P-HEV passed the 100,000 unit milestone in March 2016.  BYD Qin sales in China reached the 50,000 unit milestone in April 2016, becoming the fourth plug-in hybrid to pass that mark. In June 2016, Nissan announced it will introduce a compact range extender car in Japan before March 2017.  The series plug-in hybrid will use a new hybrid system, dubbed e-Power, which debuted with the Nissan Gripz concept crossover showcased at the 2015 Frankfurt Auto Show. PHEVs are based on the same three basic powertrain architectures as conventional electric hybrids: Series hybrids use an internal combustion engine (ICE) to turn a generator, which in turn supplies current to an electric motor, which then rotates the vehicle’s drive wheels.  A battery or supercapacitor pack, or a combination of the two, can be used to store excess charge.  Examples of series hybrids vehicles include the Chevrolet Volt (first generation), Fisker Karma, Renault Kangoo, Elect'Road, Toyota's Japan-only Coaster light-duty passenger bus, Daimler AG's hybrid Orion bus, Opel Flextreme concept car, Swissauto REX VW Polo prototype and many diesel-electric locomotives. With an appropriate balance of components this type can operate over a substantial distance with its full range of power without engaging the ICE.  As is the case for other architectures, series hybrids can operate without recharging as long as there is liquid fuel in the tank. Parallel hybrids, such as Honda's Insight, Civic, and Accord hybrids, can simultaneously transmit power to their drive wheels from two distinct sources—for example, an internal combustion engine and a battery-powered electric drive.  Although most parallel hybrids incorporate an electric motor between the vehicle's engine and transmission, a parallel hybrid can also use its engine to drive one of the vehicle's axles, while its electric motor drives the other axle and/or a generator used for recharging the batteries.  This type is called a road-coupled hybrid.  The Audi Duo plug-in hybrid concept car is an example of this type of parallel hybrid architecture.  Parallel hybrids can be programmed to use the electric motor to substitute for the ICE at lower power demands as well as to substantially increase the power available to a smaller ICE, both of which substantially increase fuel economy compared to a simple ICE vehicle. Series-parallel hybrids have the flexibility to operate in either series or parallel mode.  Hybrid powertrains currently used by Ford, Lexus, Nissan, Chevrolet, and Toyota, which some refer to as “series-parallel with power-split,” can operate in both series and parallel mode at the same time.  As of 2007, most plug-in hybrid conversions of conventional hybrids utilize this architecture.  The Toyota Prius Plug-in Hybrid operates as a series-parallel hybrid. Batteries are DC devices while grid power is AC.  In order to charge the batteries, a DC charger must be utilized.  The charger can be located in several locations: On-board chargers are mounted inside the vehicle.  Since the charger takes up space and adds weight, its power capacity is generally limited by practical considerations, avoiding carrying a more powerful charger that can only be fully utilized at certain locations.  However, carrying the charger along with the vehicle ensures that power will be available anywhere a power connection can be found. Off-board chargers can be as large as needed and mounted at fixed locations, like the garage or dedicated charging stations.  Built with dedicated wiring, these chargers can handle much more power and charge the batteries more quickly.  However, as the output of these chargers is DC, each battery system requires the output to be changed for that car.  Modern charging stations have a system for identifying the voltage of the battery pack and adjusting accordingly. Using electric motor's inverter allows the motor windings to act as the transformer coils, and the existing high-power inverter as the AC-to-DC charger.  As these components are already required on the car, and are designed to handle any practical power capability, they can be used to create a very powerful form of on-board charger with zero additional weight or size.  AC Propulsion uses this charging method, referred to as \"reductive charging\". Regardless of its architecture, a plug-in hybrid may be capable of charge-depleting and charge-sustaining modes.  Combinations of these two modes are termed blended mode or mixed-mode.  These vehicles can be designed to drive for an extended range in all-electric mode, either at low speeds only or at all speeds.  These modes manage the vehicle's battery discharge strategy, and their use has a direct effect on the size and type of battery required: Charge-depleting mode allows a fully charged PHEV to operate exclusively (or depending on the vehicle, almost exclusively, except during hard acceleration) on electric power until its battery state of charge is depleted to a predetermined level, at which time the vehicle's internal combustion engine or fuel cell will be engaged.  This period is the vehicle's all-electric range.  This is the only mode that a battery electric vehicle can operate in, hence their limited range. Charge-sustaining mode is used by production hybrid vehicles (HEVs) today, and combines the operation of the vehicle's two power sources in such a manner that the vehicle is operating as efficiently as possible without allowing the battery state of charge to move outside a predetermined narrow band.  Over the course of a trip in a HEV the state of charge may fluctuate but will have no net change.  The battery in a HEV can thus be thought of as an energy accumulator rather than a fuel storage device.  Once a plug-in hybrid has exhausted its all-electric range in charge-depleting mode, it can switch into charge-sustaining mode automatically. Mixed mode describes a trip in which a combination of the above modes are utilized.  For example, a PHEV-20 Prius conversion may begin a trip with 5 mi of low speed charge-depleting, then get onto a freeway and operate in blended mode for 20 mi , using 10 mi worth of all-electric range at twice the fuel economy.  Finally the driver might exit the freeway and drive for another 5 mi without the internal combustion engine until the full 20 mi of all-electric range are exhausted.  At this point the vehicle can revert to a charge sustaining-mode for another 10 mi until the final destination is reached.  Such a trip would be considered a mixed mode, as multiple modes are employed in one trip.  This contrasts with a charge-depleting trip which would be driven within the limits of a PHEV's all-electric range.  Conversely, the portion of a trip which extends beyond the all-electric range of a PHEV would be driven primarily in charge-sustaining mode, as used by a conventional hybrid. PHEVs typically require deeper battery charging and discharging cycles than conventional hybrids.  Because the number of full cycles influences battery life, this may be less than in traditional HEVs which do not deplete their batteries as fully.  However, some authors argue that PHEVs will soon become standard in the automobile industry.  Design issues and trade-offs against battery life, capacity, heat dissipation, weight, costs, and safety need to be solved.  Advanced battery technology is under development, promising greater energy densities by both mass and volume, and battery life expectancy is expected to increase. The cathodes of some early 2007 lithium-ion batteries are made from lithium-cobalt metal oxide.  This material is expensive, and cells made with it can release oxygen if overcharged.  If the cobalt is replaced with iron phosphates, the cells will not burn or release oxygen under any charge.  The price premium for early 2007 conventional hybrids is about US$5000, some US$3000 of which is for their NiMH battery packs.  At early 2007 gasoline and electricity prices, that would mean a break-even point after six to ten years of operation.  The conventional hybrid premium could fall to US$2000 in five years, with US$1200 or more of that being cost of lithium-ion batteries, providing for a three-year payback.  The payback period may be longer for plug-in hybrids, because of their larger, more expensive batteries. Nickel–metal hydride and lithium-ion batteries can be recycled; Toyota, for example, has a recycling program in place under which dealers are paid a US$200 credit for each battery returned.  However, plug-in hybrids typically use larger battery packs than comparable conventional hybrids, and thus require more resources.  Pacific Gas and Electric Company (PG&E) has suggested that utilities could purchase used batteries for backup and load leveling purposes.  They state that while these used batteries may be no longer usable in vehicles, their residual capacity still has significant value.  More recently, General Motors (GM) has said it has been \"approached by utilities interested in using recycled Volt batteries as a power storage system, a secondary market that could bring down the cost of the Volt and other plug-in vehicles for consumers.\" Lithium iron phosphate (LiFePO) is a class of cathode materials used in lithium iron phosphate batteries that is getting attention from the auto industry.  Valence Technologies produce a lithium iron manganese phosphate (LiFeMnPO) battery with LG Chem selling lithium iron phosphate (LiFePO) batteries for the Chevy Volt and A123 produces a lithium nano-phosphate battery.  The most important merit of this battery type is safety and high-power.  Lithium iron phosphate batteries are one of three major types in LFP family, the other two being nano-phosphate and nano-cocrystalline-olivine. In France, Électricité de France (EDF) and Toyota are installing charging stations for PHEVs on roads, streets and parking lots. EDF is also partnering with Elektromotive, Ltd. to install 250 new charging points over six months from October 2007 in London and elsewhere in the UK.  Recharging points also can be installed for specific uses, as in taxicab stands. Project Better Place began in October 2007 and is working with Renault on development of exchangeable batteries (battery swapping). Ultracapacitors (or \"supercapacitors\") are used in some plug-in hybrids, such as AFS Trinity's concept prototype, to store rapidly available energy with their high power density, in order to keep batteries within safe resistive heating limits and extend battery life.  The CSIRO's UltraBattery combines a supercapacitor and a lead acid battery in a single unit, creating a hybrid car battery that lasts longer, costs less and is more powerful than current technologies used in plug-in hybrid electric vehicles (PHEVs). The optimum battery size varies depending on whether the aim is to reduce oil consumption, running costs, or emissions, but a recent study concluded that \"The best choice of PHEV battery capacity depends critically on the distance that the vehicle will be driven between charges.  Our results suggest that for urban driving conditions and frequent charges every 10 miles or less, a low-capacity PHEV sized with an AER (all electric range) of about 7 miles would be a robust choice for minimizing gasoline consumption, cost, and greenhouse gas emissions.  For less frequent charging, every 20–100 miles, PHEVs release fewer GHGs, but HEVs are more cost effective. \" Retrofit electrification requires only one-fifth the energy required to build a new vehicle.  This is called ACEV-to-PHEV conversion. There are several companies that are converting fossil fuel non-hybrid vehicles (also called all-combustion engine vehicles) to plug-in hybrids: Colorado is going to offer $6,000 credit for PHEV conversions (in addition to a federal 10% credit up to $4,000 for qualifying vehicles). Aftermarket conversion of an existing production hybrid (a charge-maintaining hybrid) to a plug-in hybrid (called CHEV-to-PHEV conversion) typically involves increasing the capacity of the vehicle's battery pack and adding an on-board AC-to-DC charger.  Ideally, the vehicle's powertrain software would be reprogrammed to make full use of the battery pack's additional energy storage capacity and power output. Many early plug-in hybrid electric vehicle conversions have been based on the 2004 or later model Toyota Prius.  Some of the systems have involved replacement of the vehicle's original NiMH battery pack and its electronic control unit.  Others, such as A123 Hymotion, the CalCars Prius+, and the PiPrius, piggyback an additional battery back onto the original battery pack, this is also referred to as Battery Range Extender Modules (BREMs).  Within the electric vehicle conversion community this has been referred to as a \"hybrid battery pack configuration\".  Early lead–acid battery conversions by CalCars demonstrated 10 miles (15 km ) of EV-only and 20 miles (30 km ) of double mileage blended mode range. EDrive Systems use Valence Technology Li-ion batteries and have a claimed 40 to 50 miles (64 to 80 km) of electric range.  Other companies offering plug-in conversions or kits for the Toyota Prius (some of them also for Ford Escape Hybrid) include Hymotion, Hybrids Plus Manzanita Micro and OEMtek BREEZ (PHEV-30).  AFS Trinity's XH-150 claims that it has created a functioning plug-in hybrid with a 40 mi all-electric range and that it has solved the overheating problem that rapid acceleration can cause in PHEVs and extend battery life. The EAA-PHEV project was conceived by CalCars and the Electric Auto Association in October 2005 to accelerate efforts to document existing HEVs and their potential for conversion into PHEVs.  It includes a \"conversion interest\" page.  The Electric Auto Association-PHEV \"Do-It-Yourself\" Open Source community's primary focus is to provide conversion instructions to help guide experienced converters through the process, and to provide a common design that could demonstrate multiple battery technologies.  Many members of organizations such as CalCars and the EAA as well as companies like Hybrids Plus, Hybrid Interfaces of Canada, and Manzanita Micro participate in the development of the project. Plug-In Supply, Inc. of Petaluma, California offers components and assemblies to build the Prius+, the plug-in conversion invented by CalCars.  Their lead–acid battery box assembly forms a complete install package, providing access to the spare tire and containing twenty 12 volt lead–acid batteries and all high voltage components and control electronics.  The \"PbA Battery Box Assembly\" is also available without batteries.  It provides about 10 mi of EV mode range.  Conversion time was reduced by plug-in supply to one day. Oemtek offers a Valence powered lithium iron phosphate conversion that should provide 50 mi of all-electric range.  The Motor Industry Research Association has announced a retrofit hybrid conversion kit that provides removable battery packs that plug into a wall outlet for charging.  Poulsen Hybrid is developing a conversion kit that will add through-the-road plug-in hybrid capability to conventional vehicles by externally mounting electric motors onto two of the wheels. MD-Tech offers a PHEV Kit for hybrid vehicles that acts as a hybrid battery re-charger.  The kit supplies power to the hybrid battery without modifying the powertrain of the hybrid vehicle.  It fits into the back of a hybrid vehicle, originally designed to fit under the rear floor of a Toyota Prius.  The kit uses lithium iron phosphate cells in a battery supplying 4 kWh of energy.  This gives it a range of 30 km in charge-depleting, EV mode and provides a boost of power in blended mode.  The battery charger uses mains electricity and takes 4–5 hours to charge.  The battery supplies power to the DC to DC boost converter which regulates power to the hybrid battery.  The converter output can be adjusted to set voltage and current output.  A proprietary battery management system uses active battery balancing to monitor the battery state of health. With the exception of Tesla Motors, demand for all-electric vehicles, especially in the U.S. market, has been driven by government incentives.  In particular, American sales of the Nissan Leaf have depended on generous incentives and special treatment in the state of Georgia, the top selling Leaf market.  According to international market research, 60% of respondents believe a battery driving range of less than 160 km is unacceptable even though only 2% drive more than that distance per day.  Among popular current all-electric vehicles, only the Tesla (with the most expensive version of the Model S offering a 265 mi range in the U.S. Environmental Protection Agency 5-cycle test) significantly exceeds this threshold.  The Nissan Leaf has an EPA rated range of 75 mi for the 2013 model year. Plug-in hybrids provide the extended range and potential for refueling of conventional hybrids while enabling drivers to utilize battery electric power for at least a significant part of their typical daily driving.  The average trip to or from work in the United States in 2009 was 11.8 mi , while the average distance commuted to work in England and Wales in 2011 was slightly lower at 15 km .  Since building a PHEV with a longer all-electric range adds weight and cost, and reduces cargo and/or passenger space, there is not a specific all-electric range that is optimal.  The accompanying graph shows the observed all-electric range, in miles, for four popular U.S. market plug-in hybrids, as tested by Popular Mechanics magazine. A key design parameter of the Chevrolet Volt was a target of 40 mi for the all-electric range, selected to keep the battery size small and lower costs, and mainly because research showed that 78% of daily commuters in the U.S. travel 40 mi or less.  This target range would allow most travel to be accomplished electrically driven and the assumption was made that charging will take place at home overnight.  This requirement translated using a lithium-ion battery pack with an energy storage capacity of 16 kWh considering that the battery would be used until the state of charge (SOC) of the battery reached 30%. In October 2014 General Motors reported, based on data collected through its OnStar telematics system since Volt deliveries began, and with over 1 billion miles (1.6 billion km) traveled, that Volt owners drive about 62.5% of their trips in all-electric mode.  In May 2016, Ford reported, based on data collected from more than 610 million miles (976 million km) logged by its electrified vehicles through its telematics system, that drivers of these vehicles run an average of 13,500 mi annually on their vehicles, with about half of those miles operating in all-electric mode.  A break down of these figures show an average daily commute of 42 mi for Ford Energi plug-in hybrid drivers.  Ford notes that with the enhanced electric range of the 2017 model year model, the average Fusion Energi commuter could go the entire day using no gasoline, if the car is fully charged both, before leaving for work and before leaving for home.  According to Ford data, currently most customers are likely charging their vehicles only at home. The 2015 edition of the EPA's annual report \"\"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends\"\" estimates the following utility factors for 2015 model year plug-in hybrids to represent the percentage of miles that will be driven using electricity by an average driver, whether in electric only or blended modes: 83% for the BMW i3 REx, 66% for the Chevrolet Volt, 45% for the Ford Energi models, 43% for the McLaren P1, 37% for the BMW i8, and 29% for the Toyota Prius PHV.  A 2014 analysis conducted by the Idaho National Laboratory using a sample of 21,600 all-electric cars and plug-in hybrids, found that Volt owners traveled on average 9,112 miles in all-electric mode (e-miles) per year, while Leaf owners traveled 9,697 e-miles per year, despite the Volt's shorter all-electric range, about half of the Leaf's. Between January and August 2014, a period during which US sales of conventional hybrids slowed, US sales of plug-in hybrids grew from 28,241 to 40,748 compared to the same period in 2013.  US sales of all-electric vehicles also grew during the same period: from 29,917 vehicles in the January to August 2013 period to 40,349 in January to August 2014. Each kilowatt hour of battery capacity in use will displace up to 50 USgal of petroleum fuels per year (gasoline or diesel fuels).  Also, electricity is multi-sourced and, as a result, it gives the greatest degree of energy resilience. The actual fuel economy for PHEVs depends on their powertrain operating modes, their all-electric range, and the amount of driving between charges.  If no gasoline is used the miles per gallon gasoline equivalent (MPG-e) depends only on the efficiency of the electric system.  The first mass production PHEV available in the U.S. market, the 2011 Chevrolet Volt, with an EPA rated all-electric range of 35 mi , and an additional gasoline-only extended range of 344 mi has an EPA combined city/highway fuel economy of 93 MPG-e in all-electric mode, and 37 mpgus in gasoline-only mode, for an overall combined gas-electric fuel economy rating of 60 mpgus equivalent (MPG-e).  The EPA also included in the Volt's fuel economy label a table showing fuel economy and electricity consumed for five different scenarios: 30, 45, 60 and 75 mi driven between a full charge, and a never charge scenario.  According to this table the fuel economy goes up to 168 mpgus equivalent (MPG-e) with 45 mi driven between full charges. For the more comprehensive fuel economy and environment label that will be mandatory in the U.S. beginning in model year 2013, the National Highway Traffic Safety Administration (NHTSA) and Environmental Protection Agency (EPA) issued two separate fuel economy labels for plug-in hybrids because of their design complexity, as PHEVS can operate in two or three operating modes: all-electric, blended, and gasoline-only.  One label is for series hybrid or extended range electric vehicle (like the Chevy Volt), with all-electric and gasoline-only modes; and a second label for blended mode or series-parallel hybrid, that includes a combination of both gasoline and plug-in electric operation; and gasoline only, like a conventional hybrid vehicle. A further advantage of PHEVs is that they have potential to be even more efficient than conventional hybrids because a more limited use of the PHEV's internal combustion engine may allow the engine to be used at closer to its maximum efficiency.  While a Prius is likely to convert fuel to motive energy on average at about 30% efficiency (well below the engine's 38% peak efficiency) the engine of a PHEV-70 would be likely to operate far more often near its peak efficiency because the batteries can serve the modest power needs at times when the combustion engine would be forced to run well below its peak efficiency.  The actual efficiency achieved depends on losses from electricity generation, inversion, battery charging/discharging, the motor controller and motor itself, the way a vehicle is used (its duty cycle), and the opportunities to recharge by connecting to the electrical grid. The Society of Automotive Engineers (SAE) developed their recommended practice in 1999 for testing and reporting the fuel economy of hybrid vehicles and included language to address PHEVs.  An SAE committee is currently working to review procedures for testing and reporting the fuel economy of PHEVs.  The Toronto Atmospheric Fund tested ten retrofitted plug-in hybrid vehicles that achieved an average of 5.8 litres per 100 kilometre or 40.6 miles per gallon over six months in 2008, which was considered below the technology's potential. PHEV batteries also allow for an additional efficiency when converting solar power directly into a DC storage system, as opposed to converting the energy into AC home or building.  DC to DC direct conversion is more efficient, therefore, potentially allowing the more efficient capture of solar energy. In real world testing using normal drivers, some Prius PHEV conversions may not achieve much better fuel economy than HEVs.  For example, a plug-in Prius fleet, each with a 30 mi all-electric range, averaged only 51 mpgus in a 17000 mi test in Seattle, and similar results with the same kind of conversion battery models at Google's RechargeIT initiative.  Moreover, the additional battery pack costs –. The following table compares EPA's estimated out-of-pocket fuel costs and fuel economy ratings of serial production plug-in hybrid electric vehicles rated by EPA as of 2017 expressed in miles per gallon gasoline equivalent (mpg-e), versus the most fuel efficient gasoline-electric hybrid car, the 2016 Toyota Prius Eco (fourth generation), rated 56 mpgus , and EPA's average new 2016 vehicle, which has a fuel economy of 25 mpgus .  The table also shows the fuel efficiency for plug-in hybrids in all-electric mode expressed as KWh/100 mile, the metric used by EPA to rate electric cars before November 2010. A study published in 2014 by researchers from Lamar University, Iowa State University and Oak Ridge National Laboratory compared the operating costs of plug-in hybrid electric vehicles (PHEVs) of various electric ranges (10, 20, 30, and 40 miles) with conventional gasoline vehicles and hybrid-electric vehicles (HEVs) for different payback periods, considering different charging infrastructure deployment levels and gasoline prices.  The study concluded that: One of the main barriers for the general adoption of all-electric cars is the range anxiety factor, the driver's fear of being stranded by a depleted battery before reaching the final destination.  Plug-in hybrids, as opposed to pure electric cars, eliminate the range anxiety concerns because the gasoline engine serves as a back-up to recharge the battery to provide electric power to the electric motor, or to provide propulsion directly.  Access to a regular fuel station guarantees that a PHEV has similar driving ranges as conventional gasoline-powered automobile. s of 2016 , there are five plug-in hybrids in the European market capable of driving around 50 km (under NEDC cycle) on the battery alone, the Audi A3 e-tron, Mitsubishi Outlander P-HEV, Volkswagen Golf GTE, Volkswagen Passat GTE, and Volvo V60 Plug-in Hybrid.  Other models with larger all-electric range are the Chevrolet Volt and Cadillac ELR, sold in the U.S. and Canada, and the BMW i3 REx, available in 49 countries. As a response to range anxiety concerns, BMW is offering an optional limited capability gasoline range extender engine for its all-electric BMW i3.  The range-extender option costs an additional in the United States, (~ ) in France, and (~ ) in the Netherlands.  The range-extender option of the BMW i3 was designed to meet the California Air Resources Board (CARB) regulation for an auxiliary power unit (APU) called REx.  According to rules adopted in March 2012 by CARB, the 2014 BMW i3 with a REx unit fitted will be the first car to qualify as a range-extended battery-electric vehicle or \"BEVx.\"  CARB describes this type of electric vehicle as \"a relatively high-electric range battery-electric vehicle (BEV) to which an APU is added.\"  The unit, which maintains battery charge at about 30% after the pack has been depleted in normal use, is strictly limited in the additional range it can provide. According to BMW, at the beginning of the i3 release, the use of range-extender was much more than the carmaker expected, more than 60%.  Over time it has decreased significantly, with some people almost never using it, and by 2016 it is being regularly used in fewer than 5% of i3s.  In early October 2014, General Motors reported, based on data collected through its OnStar telematics system, that Volt owners in North America have accumulated a total of 1 billion miles (1.6 billion km) traveled, of which, about 62.5% were driven in all-electric mode.  A similar report, issued by GM in August 2016, reported that Volt owners have accumulated almost 1.5 billion miles (2.4 billion km) driven in EV mode, representing 60% of their total miles traveled. In a study published in June 2016, conducted by the Norwegian Institute of Transport Economics, the researchers found that plug-in hybrid owners in Norway drive on average 55% of their annual distance in charge-depleting or all-electric mode (EV mode), and the share goes up to about 63% for work trips.  The share of electric travel is higher for trips to work and in the summer, and lower in the winter.  The average plug-in hybrid user in the survey drives 60% of the total distance in EV mode in the summer and 53% in winter.  The estimate for work trips is higher at 70% in the summer and 59% in winter. One of the advantages of the plug-in hybrid design is that the generator can be completely decoupled from the traction.  Unlike a conventional engine, which operates over a wide variety of power settings and operational conditions, the range extender can be operated under optimum conditions at all times.  High-efficiency power sources that are not suitable for normal automotive use may be perfectly suitable for PEV use.  These include advanced close-cycle steam engines, stirling engines, Wankel engines, and microturbines due primarily to their light weight and small size. The Ontario Medical Association announced that smog is responsible for an estimated 9,500 premature deaths in its province every year.  Plug-in hybrids in emission-free electric mode may contribute to the reduction of smog. PHEVs and fully electric cars may allow for more efficient use of existing electric production capacity, much of which sits idle as operating reserve most of the time.  This assumes that vehicles are charged primarily during off peak periods (i.e., at night), or equipped with technology known as charge control to shut off charging during periods of peak demand.  Another advantage of a plug-in vehicle is their potential ability to load balance or help the grid during peak loads.  This is accomplished with vehicle-to-grid technology.  By using excess battery capacity to send power back into the grid and then recharge during off peak times using cheaper power, such vehicles are actually advantageous to utilities as well as their owners.  Even if such vehicles just led to an increase in the use of nighttime electricity they would even out electricity demand which is typically higher in the daytime, and provide a greater return on capital for electricity infrastructure. In the UK, VTG would need to comply with generation connection standard \"G59/2\", which means that it would need an earth rod at the premises, and would be unable to export more than 17 kW without the network firm's permission (which feeding onto one phase, i.e. for a normal house, would not be given – to maintain a balance of load across the three phases). In October 2005, five Toyota engineers and one Asian AW engineer published an IEEE technical paper detailing a Toyota-approved project to add vehicle-to-grid capability to a Toyota Prius.  Although the technical paper described \"a method for generating voltage between respective lines of neutral points in the generator and motor of the THS-II (Toyota Hybrid System) to add a function for generating electricity\", it did not state whether or not the experimental vehicle could be charged through the circuit, as well.  However, the vehicle was featured in a Toyota Dream House, and a brochure for the exhibit stated that \"the house can supply electricity to the battery packs of the vehicles via the stand in the middle of the garage\", indicating that the vehicle may have been a plug-in hybrid. In November 2005, more than 50 leaders from public power utility companies across the United States met at the Los Angeles Department of Water and Power headquarters to discuss plug-in hybrid and vehicle-to-grid technology.  The event, which was sponsored by the American Public Power Association, also provided an opportunity for association members to plan strategies that public power utility companies could use to promote plug-in hybrid technology.  Greg Hanssen and Peter Nortman of EnergyCS and EDrive attended the two-day session, and during a break in the proceedings, made an impromptu display in the LADWP parking lot of their converted Prius plug-in hybrid. In September 2006, the California Air Resources Board held a Zero Emission Vehicle symposium that included several presentations on V2G technology.  In April 2007, Pacific Gas and Electric Company showcased a PHEV at the Silicon Valley Leadership Alternative Energy Solutions Summit with vehicle-to-grid capability, and demonstrated that it could be used as a source of emergency home power in the event of an electrical power failure.  Regulations intended to protect electricians against power other than from grid sources would need to be changed, or regulations requiring consumers to disconnect from the grid when connected to non-grid sources will be required before such backup power solutions would be feasible. Jon Wellinghoff, from the US Federal Energy Regulatory Commission, coined the term \"Cash-Back Hybrids\" to describe payments to car owners for putting their batteries on the power grid.  Batteries could also be offered in low-cost leasing or renting or by donation (including maintenance) to the car owners by the public utilities, in a vehicle-to-grid agreement. Disadvantages of plug-in hybrids include the additional cost, weight, and size of a larger battery pack.  According to a 2010 study by the National Research Council, the cost of a lithium-ion battery pack is about /kW·h of usable energy, and considering that a PHEV-10 requires about 2.0 kW·h and a PHEV-40 about 8 kW·h, the manufacturer cost of the battery pack for a PHEV-10 is around and it goes up to for a PHEV-40.  According to the same study, even though costs are expected to decline by 35% by 2020, market penetration is expected to be slow and therefore PHEVs are not expected to significantly impact oil consumption or carbon emissions before 2030, unless a fundamental breakthrough in battery technologies occurs. According to the 2010 NRC study, although a mile driven on electricity is cheaper than one driven on gasoline, lifetime fuel savings are not enough to offset plug-ins high upfront costs, and it will take decades before the break even point is achieved.  Furthermore, hundreds of billions of dollars in government subsidies and incentives are likely to be required to achieve a rapid plug-in market penetration in the U.S. A 2013 study by the American Council for an Energy-Efficient Economy reported that battery costs came down from per kilowatt hour in 2007 to per kilowatt hour in 2012.  The U.S. Department of Energy has set cost targets for its sponsored battery research of per kilowatt hour in 2015 and per kilowatt hour by 2022.  Cost reductions through advances in battery technology and higher production volumes will allow plug-in electric vehicles to be more competitive with conventional internal combustion engine vehicles. A study published in 2011 by the Belfer Center, Harvard University, found that the gasoline costs savings of plug-in electric cars over the vehicles’ lifetimes do not offset their higher purchase prices.  This finding was estimated comparing their lifetime net present value at 2010 purchase and operating costs for the U.S. market, and assuming no government subidies.  According to the study estimates, a PHEV-40 is more expensive than a conventional internal combustion engine, while a battery electric vehicle (BEV) is more expensive.  The study also examined how this balance will change over the next 10 to 20 years, assuming that battery costs will decrease while gasoline prices increase.  Under the future scenarios considered, the study found that BEVs will be significantly less expensive than conventional cars ( to cheaper), while PHEVs, will be more expensive than BEVs in almost all comparison scenarios, and only less expensive than conventional cars in a scenario with very low battery costs and high gasoline prices.  BEVs are simpler to build and do not use liquid fuel, while PHEVs have more complicated powertrains and still have gasoline-powered engines. Lithium iron phosphate batteries from Valence Technologies were used in the first plug-in hybrids from CalCars.  They are providing a conversion for the Toyota Prius priced at .  Hymotion also offers a conversion for but their conversion is only 5 kW where Oemtek's is 9 kW. Many authors have assumed that plug-in recharging will take place overnight at home.  However, residents of cities, apartments, dormitories, and townhouses might not have garages or driveways with available power outlets, and they might be less likely to buy plug-ins unless recharging infrastructure is developed.  Electrical outlets or charging stations near their places of residence, or in commercial or public parking lots or streets or workplaces are required for these potential users to gain the full advantage of PHEVs.  Even house dwellers might need to charge at the office or to take advantage of opportunity charging at shopping centers.  However, this infrastructure is not in place today and it will require investments by both the private and public sectors. Several cities in California and Oregon, and particularly San Francisco and other cities in the San Francisco Bay Area and Silicon Valley, as well as some local private firms such as Google and Adobe Systems, already have deployed charging stations and have expansion plans to attend both plug-ins and all-electric cars.  In Google's case, its Mountain View campus has 100 available charging stations for its share-use fleet of converted plug-ins available to its employees.  Solar panels are used to generate the electricity, and this pilot program is being monitored on a daily basis, with performance results published on the RechargeIT website. Increased pollution is expected to occur in some areas with the adoption of PHEVs, but most areas will experience a decrease.  A study by the ACEEE predicts that widespread PHEV use in heavily coal-dependent areas would result in an increase in local net sulfur dioxide and mercury emissions, given emissions levels from most coal plants currently supplying power to the grid.  Although clean coal technologies could create power plants which supply grid power from coal without emitting significant amounts of such pollutants, the higher cost of the application of these technologies may increase the price of coal-generated electricity.  The net effect on pollution is dependent on the fuel source of the electrical grid (fossil or renewable, for example) and the pollution profile of the power plants themselves.  Identifying, regulating and upgrading single point pollution source such as a power plant—or replacing a plant altogether—may also be more practical.  From a human health perspective, shifting pollution away from large urban areas may be considered a significant advantage. According to a 2009 study by The National Academy of Science, \"Electric vehicles and grid-dependent (plug-in) hybrid vehicles showed somewhat higher nonclimate damages than many other technologies.\"  Efficiency of plug-in hybrids is also impacted by the overall efficiency of electric power transmission.  Transmission and distribution losses in the USA were estimated at 7.2% in 1995 and 6.5% in 2007.  By life cycle analysis of air pollution emissions, natural gas vehicles are currently the lowest emitter . Electric utility companies generally do not utilize flat rate pricing.  For example, Pacific Gas and Electric (PG&E) normally charges $0.10 per kilowatt hour (kW·h) for the base tier, but additional tiers are priced as high as $0.30 per kW·h to customers without electric vehicles.  Some utilities offer electric vehicle users a rate tariff that provides discounts for off-peak usage, such as overnight recharging.  PG&E offers a special, discounted rate for plug-in and other electric vehicle customers, the \"Experimental Time-of-Use Low Emission Vehicle rate.\"  That tariff gives people much cheaper rates if they charge at night, especially during the summer months. The additional electrical utilization required to recharge the plug-in vehicles could push many households in areas that do not have off-peak tariffs into the higher priced tier and negate financial benefits.  Without an off-peak charging tariff, one study of a certain PHEV-20 model having an all-electric range of 20 miles, gasoline-fueled efficiency of 52.7 mi/gal U.S., and all-electric efficiency of 4 mi/kW·h, found that household electricity customers who consumed 131%–200% of baseline electricity at $0.220/(kW·h) would see benefits if gasoline was priced above US$2.89/US gal; those that consumed 201%–300% of baseline electricity at $0.303/(kW·h) would only see benefits if gas was priced above $3.98; and households consuming over 300% of baseline electricity at $0.346/(kW·h) would only see benefits if gasoline was priced above $4.55 (USD/gal).  Off-peak tariff rates can lower the break-even point.  The PG&E tariff would change those break-even gasoline prices to US$1.96, $3.17 and $3.80 per gallon, respectively, for the given PHEV and usage pattern in question. Customers under such tariffs could see significant savings by being careful about when the vehicle was charged, for example, by using a timer to restrict charging to off-peak hours.  Thus, an accurate comparison of the benefit requires each household to evaluate its current electrical usage tier and tariffs weighed against the cost of gasoline and the actual observed operational cost of electric mode vehicle operation. Current technology for plug-ins is based on the lithium-ion battery and an electric motor, and the demand for lithium, heavy metals and other rare elements (such as neodymium, boron and cobalt) required for the batteries and powertrain is expected to grow significantly due to the incoming market entrance of plug-ins and electric vehicles in the mid and long term.  Some of the largest world reserves of lithium and other rare metals are located in countries with strong resource nationalism, unstable governments or hostile to U.S. interests, raising concerns about the risk of replacing dependence on foreign oil with a new dependence on hostile countries to supply strategic materials. Even though the metals used in electric vehicle batteries are rare, they can be recycled. Currently, the main deposits of lithium are found in China and South America throughout the Andes mountain chain.  In 2008 Chile was the leading lithium metal producer, followed by Australia, China, and Argentina.  In the United States lithium is recovered from brine pools in Nevada.  Nearly half the world's known reserves are located in Bolivia, and according to the US Geological Survey, Bolivia's Salar de Uyuni desert has 5.4 million tons of lithium, which can be used to make lithium batteries for hybrid and electric vehicles.  Other important reserves are located in Chile, China, and Brazil.  Regarding rare earth elements, most reserves are located in China, which controls the world market for these elements. At low speeds, electric-drive cars produced less roadway noise as compared to vehicles propelled by internal combustion engines. Blind people or the visually impaired consider the noise of combustion engines a helpful aid while crossing streets, hence plug-in electric cars and hybrids could pose an unexpected hazard.  Tests have shown that this is a valid concern, as vehicles operating in electric mode can be particularly hard to hear below 20 mph for all types of road users and not only the visually impaired.  At higher speeds, the sound created by tire friction and the air displaced by the vehicle start to make sufficient audible noise. The Government of Japan, the U.S. Congress, and the European Parliament passed legislation to regulate the minimum level of sound for hybrids and plug-in electric vehicles when operating in electric mode, so that blind people and other pedestrians and cyclists can hear them coming and detect from which direction they are approaching.  s of 2013 , most of the hybrids and plug-in electric cars available in the United States make warning noises using a speaker system. The effect of PHEVs on greenhouse emissions is complex.  Plug-in hybrid vehicles operating on all-electric mode do not emit harmful tailpipe pollutants from the onboard source of power.  The clean air benefit is usually local because depending on the source of the electricity used to recharge the batteries, air pollutant emissions are shifted to the location of the generation plants.  In the same way, PHEVs do not emit greenhouse gases from the onboard source of power, but from the point of view of a well-to-wheel assessment, the extent of the benefit also depends on the fuel and technology used for electricity generation.  From the perspective of a full life cycle analysis, the electricity used to recharge the batteries must be generated from renewable or clean sources such as wind, solar, hydroelectric, or nuclear power for PEVs to have almost none or zero well-to-wheel emissions.  On the other hand, when PEVs are recharged from coal-fired plants, they usually produce slightly more greenhouse gas emissions than internal combustion engine vehicles.  In the case of plug-in hybrid electric vehicle when operating in hybrid mode with assistance of the internal combustion engine, tailpipe and greenhouse emissions are lower in comparison to conventional cars because of their higher fuel economy. There has been much debate over the potential GHG emissions reductions that can be achieved with PHEV.  A study by the Electric Power Research Institute reports that a 338 TW·h or 5.8% increase in power generation needed as a result of PHEV.  In the same report the EPRI also states that CO emissions could increase by 430 million metric tons.  The article concludes: A study by the American Council for an Energy Efficient Economy (ACEEE) predicts that, on average, a typical American driver is expected to achieve about a 15% reduction in net CO2 emissions compared to the driver of a regular hybrid, based on the 2005 distribution of power sources feeding the US electrical grid.  The ACEEE study also predicts that in areas where more than 80% of grid-power comes from coal-burning power plants, local net CO2 emissions will increase, while for PHEVs recharged in areas where the grid is fed by power sources with lower CO2 emissions than the current average, net CO2 emissions associated with PHEVs will decrease correspondingly. A 2007 joint study by the Electric Power Research Institute (EPRI) and the Natural Resources Defense Council (NRDC) similarly found that the introduction of PHEVs into America’s consumer vehicle fleet could achieve significant greenhouse gas emission reductions.  The EPRI-NRDC report estimates that, between 2010 and 2050, a shift toward PHEV use could reduce GHG emissions by 3.4 to 10.4 billion metric tons.  The magnitude of these reductions would ultimately depend on the level of PHEV market penetration and the carbon intensity of the US electricity sector.  In general, PHEVs can be viewed as an element in the \"Pacala and Socolow wedges\" approach which shows a way to stabilize CO2 emissions using a portfolio of existing techniques, including efficient vehicles. A 2008 study at Duke University suggests that for PHEV's to reduce greenhouse gas emissions more than hybrids a carbon pricing signal that encourages the development of low carbon power is needed.  RAND also in 2008 studied the questions of a carbon tax, carbon cap and trade systems, increasing gasoline tax, and providing renewable energy subsidies under various economic conditions and vehicle type availabilities.  RAND found that subsidies were able to provide a smoother transition to new energy sources, especially in the face of energy source price volatility, because subsidies can be structured according to relative costs between renewables and fossil fuel, while taxes and carbon trading schemes alone do not take relative prices of energy into account. The Minnesota Pollution Control Agency found that if Minnesota's fleet of vehicles making lengthy trips were replaced by plug-in hybrids, CO2 emissions per vehicle would likely decrease.  However, unless more than 40% of the electricity used to charge the vehicles were to come from non-polluting sources, replacing the vehicles with non-plug-in hybrids would engender a larger decrease in CO2 emissions.  Plug-in hybrids use less fuel in all cases, and produce much less carbon dioxide in short commuter trips, which is how most vehicles are used.  The difference is such that overall carbon emissions would decrease if all internal combustion vehicles were converted to plug-ins. A study by Kantor, Fowler, Hajimiragha, and ElKamel shows that fuel cell plug-in hybrid vehicles achieve twice as much reduction in greenhouse gas emissions than PHEVs and FCVs.  The study uses the transportation sector in Ontario Canada as a case study, with the maximum allowable number of vehicles being that which can be supported by the electric grid. In 2009 researchers at Argonne National Laboratory adapted their GREET model to conduct a full well-to-wheels (WTW) analysis of energy use and greenhouse gas (GHG) emissions of plug-in hybrid electric vehicles for several scenarios, considering different on-board fuels and different sources of electricity generation for recharging the vehicle batteries.  Three US regions were selected for the analysis, California, New York, and Illinois, as these regions include major metropolitan areas with significant variations in their energy generation mixes.  The full cycle analysis results were also reported for the US generation mix and renewable electricity to examine cases of average and clean mixes, respectively This 2009 study showed a wide spread of petroleum use and GHG emissions among the different fuel production technologies and grid generation mixes.  The following table summarizes the main results: The Argonne study found that PHEVs offered reductions in petroleum energy use as compared with regular hybrid electric vehicles.  More petroleum energy savings and also more GHG emissions reductions were realized as the all-electric range increased, except when electricity used to recharged was dominated by coal or oil-fired power generation.  As expected, electricity from renewable sources realized the largest reductions in petroleum energy use and GHG emissions for all PHEVs as the all-electric range increased.  The study also concluded that plug-in vehicles that employ biomass-based fuels (biomass-E85 and -hydrogen) may not realize GHG emissions benefits over regular hybrids if power generation is dominated by fossil sources. A 2008 study by researchers at Oak Ridge National Laboratory analyzed oil use and greenhouse gas (GHG) emissions of plug-in hybrids relative to hybrid electric vehicles under several scenarios for years 2020 and 2030.  Each type of vehicle was assumed to run 20 mi per day and the HEV was assumed to have a fuel economy of 40 mpgus .  The study considered the mix of power sources for 13 U.S. regions, generally a combination of coal, natural gas and nuclear energy, and to a lesser extend renewable energy.  A 2010 study conducted at Argonne National Laboratory reached similar findings, concluding that PHEVs will reduce oil consumption but could produce very different greenhouse gas emissions for each region depending on the energy mix used to generate the electricity to recharge the plug-in hybrids.  The following table summarizes the main results of the Oak Ridge National Laboratory study for the 2020 scenario: In October 2014, the U.S. Environmental Protection Agency published the 2014 edition of its annual report \"\"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends\".\"  For the first time, the report presents an analysis of the impact of alternative fuel vehicles, with emphasis in plug-in electric vehicles because as their market share is approaching 1%, PEVs began to have a measurable impact on the U.S. overall new vehicle fuel economy and CO emissions. EPA's report included the analysis of 12 all-electric passengers cars and 10 plug-in hybrids available in the market as model year 2014.  For purposes of an accurate estimation of emissions, the analysis took into consideration the differences in operation between those PHEVs like the Chevrolet Volt that can operate in all-electric mode without using gasoline, and those that operate in a blended mode like the Toyota Prius PHV, which uses both energy stored in the battery and energy from the gasoline tank to propel the vehicle, but that can deliver substantial all-electric driving in blended mode.  In addition, since the all-electric range of plug-in hybrids depends on the size of the battery pack, the analysis introduced a utility factor as a projection, on average, of the percentage of miles that will be driven using electricity (in electric only and blended modes) by an average driver.  The following table shows the overall EV/hybrid fuel economy expressed in terms of miles per gallon gasoline equivalent (mpg-e) and the utility factor for the ten MY2014 plug-in hybrids available in the U.S. market.  The study used the utility factor (since in pure EV mode there are no tailpipe emissions) and the EPA best estimate of the CO tailpipe emissions produced by these vehicles in real world city and highway operation based on the EPA 5-cycle label methodology, using a weighted 55% city/45% highway driving.  The results are shown in the following table. In addition, the EPA accounted for the upstream CO emissions associated with the production and distribution of electricity required to charge the PHEVs.  Since electricity production in the United States varies significantly from region to region, the EPA considered three scenarios/ranges with the low end of the range corresponding to the California powerplant emissions factor, the middle of the range represented by the national average powerplant emissions factor, and the upper end of the range corresponding to the powerplant emissions factor for the Rockies.  The EPA estimates that the electricity GHG emission factors for various regions of the country vary from 346 g CO2/kW-hr in California to 986 g CO2/kW-hr in the Rockies, with a national average of 648 g CO2/kW-hr.  The following table shows the tailpipe emissions and the combined tailpipe and upstream emissions for each of the 10 MY 2014 PHEVs available in the U.S. market. Most emission analysis use average emissions rates across regions instead of marginal generation at different times of the day.  The former approach does not take into account the generation mix within interconnected electricity markets and shifting load profiles throughout the day.  An analysis by three economist affiliated with the National Bureau of Economic Research (NBER), published in November 2014, developed a methodology to estimate marginal emissions of electricity demand that vary by location and time of day across the United States.  The study used emissions and consumption data for 2007 through 2009, and used the specifications for the Chevrolet Volt (all-electric range of 35 mi ).  The analysis found that marginal emission rates are more than three times as large in the Upper Midwest compared to the Western U.S., and within regions, rates for some hours of the day are more than twice those for others.  Applying the results of the marginal analysis to plug-in electric vehicles, the NBER researchers found that the emissions of charging PEVs vary by region and hours of the day.  In some regions, such as the Western U.S. and Texas, CO emissions per mile from driving PEVs are less than those from driving a hybrid car.  However, in other regions, such as the Upper Midwest, charging during the recommended hours of midnight to 4 a.m. implies that PEVs generate more emissions per mile than the average car currently on the road.  The results show a fundamental tension between electricity load management and environmental goals as the hours when electricity is the least expensive to produce tend to be the hours with the greatest emissions.  This occurs because coal-fired units, which have higher emission rates, are most commonly used to meet base-level and off-peak electricity demand; while natural gas units, which have relatively low emissions rates, are often brought online to meet peak demand.  This pattern of fuel shifting explains why emission rates tend to be higher at night and lower during periods of peak demand in the morning and evening. The BYD F3DM became the world's first mass-produced plug-in hybrid compact sedan as it went on sale in China to government agencies and corporations on December 15, 2008.  The F3DM had an all-electric range of 100 km .  Sales to the general public began in Shenzhen in March 2010 but because the F3DM nearly doubled the price of cars that run on conventional fuel, BYD Auto was counting on subsidies from the local government to make the plug-in attractive to personal buyers.  The F3DM was sold for 149,800 yuan (about ), and during its first year in the market the F3DM only sold 48 vehicles.  During 2010, 417 units were sold, and cumulative sales in China reached 3,284 units through December 2013.  Production of the BYD F3DM was ended due to low sales, and it was replaced by the BYD Qin. Deliveries of the Chevrolet Volt began in December 2010, initially only in selected U.S. markets, and became available nationwide in November 2011.  The Volt has a United States Environmental Protection Agency rated all electric range of 35 mi .  Deliveries of the 2012 Volt began in Canada in September 2011.  The first deliveries of the Chevrolet Volt in Europe took place on November 30, 2011.  The European version of the Volt, the Opel Ampera, was released to customers in Europe in February 2012.  Deliveries of the right-hand drive Vauxhall Ampera in the UK began in May 2012.  The Holden Volt was released in Australia in December 2012. The Volt/Ampera family of vehicles was the world's best selling plug-in electric car in 2012 with 31,400 units sold, and ranked in the 432nd place among all models sold worldwide in 2012.  The Opel/Vauxhall Ampera was Europe's top selling plug-in electric car in 2012 with 5,268 units representing a market share of 21.5% of the region's plug-in electric passenger car segment.  Combined global Volt/Ampera sales passed the 100,000 unit milestone in October 2015.  Volt sales in the American market passed the 100,000 unit milestone in July 2016. s of 2016 , the Volt family of vehicles, with about 134,500 units sold worldwide, is the world's all-time top selling plug-in hybrid, and it is also the third best selling plug-in electric car ever, after the Nissan Leaf (over 250,000) and the Tesla Model S (over 158,000).  The United States is its main market with 113,489 Volts delivered, followed by Canada with 8,884 units, both through December 2016.  s of 2015 , about 1,750 Volts had been sold in Europe.  s of 2016 , over 10,000 Opel/Vauxhall Amperas had been sold in Europe.  The Netherlands is the leading Ampera market with 5,031 units registered through December 2015, followed by Germany with 1,542 units, and the UK with 1,279 Vauxhall Amperas registered by the end of September 2015. Due to the end of deliveries of Chevrolet vehicles to Europe by the end of 2015 to focus on its core European brands, Opel and Vauxhall, the American-made Chevrolet Volt will not be available in Europe beginning in 2016.  In July 2014, Opel announced that due to the slowdown in sales, the Ampera will be discontinued after the launch of next generation Volt, and between 2014 and 2018, Opel plans to introduce in Europe a successor product in the electric vehicle segment.  Deliveries to retail customers of the second generation Chevrolet Volt began in the U.S. and Canada in October 2015 as a 2016 model year.  Availability in the American market is limited to California and the other 10 states that follow California’s zero emission vehicle rules.  It is scheduled to go on sale as a 2017 model year in the 39 remaining states by the second quarter of 2016.  According to General Motors the second generation Volt has an upgraded powertrain and uses a battery pack with new chemistry that stores 20% more electrical energy and uses fewer cells.  Its improved battery system and drivetrain allow the Volt to deliver an EPA rated all-electric range of 53 mi , and to improve its fuel economy in gasoline-only mode to 42 mpgUS . Retail deliveries of the Fisker Karma began in November 2011.  The Karma had an EPA rated all-electric range of 32 mi .  Production was suspended in November 2012 due to financial difficulties, with about 2,450 Karmas built since 2011.  As a result of flash floods caused by Hurricane Sandy in October 2012, 16 Karmas caught fire and another 330 units were lost when an entire shipment from Europe was flooded while being parked at Port Newark-Elizabeth Marine Terminal.  Fisker Automotive filed for bankruptcy in November 2013, after the U.S. Department of Energy auctioned its debt and sold it to Hybrid Technology LLC.  About 1,600 units were sold in the United States through December 2013.  A total of 533 units were sold in Europe through December 2014.  The Netherlands was the top selling European market for the Karma, with 188 units sold through December 2013. The Toyota Prius Plug-in Hybrid was released in Japan in January 2012, and deliveries in the United States began in late February 2012.  The Prius PHV total all-electric range in blended mode is 11 mi as rated by EPA.  During 2012, its first year in the market, global sales reached 27,181 units.  Production of the first generation Prius Plug-in ended in June 2015.  About 75,400 first generation Prius PHVs were sold worldwide.  The United States led sales with 42,345 units delivered through September 2016.  The second generation Prius plug-in hybrid, the Toyota Prius Prime, was released in the U.S. in November 2016, and the Prius PHV, as it is called in other markets, was released in Japan in February 2017.  European deliveries are slated to begin in March 2017, beginning in the UK. Global cumulative sales of both Prius plug-in generations totaled 79,300 units at the end of January 2017.  The U.S. is the all-time top selling market, with 46,133 units sold through January 2017, including 3,788 second generation Prime vehicles.  Japan ranked next with about 22,100 units, followed by Europe with 10,600 units, both, through January 2017.  The leading European market is the Netherlands with 4,134 units registered as of 30 2015 . The Ford C-Max Energi has an all-electric range of 20 mi and was released in the U.S. by mid October 2012.  The EPA rated the Energi combined city/highway fuel economy in all-electric mode at 88 MPG-e (88 mpgus ), 95 MPG-e (95 mpgus ) for city driving and 81 MPG-e (81 mpgus ) in highway.  s of 2015 , a total of 25,552 units have been sold in the U.S., 525 in Canada, and 224 in the Netherlands, totaling global sales of 22,253 units. The Volvo V60 Plug-in Hybrid, the world's first diesel plug-in hybrid, has an all-electric range of up to 50 km , and a fuel economy of 124 miles per gallon of gasoline equivalent (1.8 l/100 km), with carbon dioxide emissions averaging 49 g/km.  Retail deliveries began in Sweden in late 2012 and the rest of Europe in early 2013.  The diesel PHV is not expected to be available in the U.S.  In September 2012, Volvo announced that the first 1,000 units were sold out before the model year 2013 vehicles were delivered to the dealerships.  The carmaker ramped up production of the 2014 model year to 5,000 units for 2013.  s of 2015 , a total of 15,624 units have been delivered in Europe.  s of 2015 , sales were led by the Netherlands with 11,001 units registered, followed by Sweden with 2,000 units delivered through December 2015.  The V60 PHEV was the top selling plug-in hybrid in Europe accounting for the segment sales during the first eleven months of 2013. The Honda Accord Plug-in Hybrid, with an all-electric range of 13 mi , was released in the U.S. in January 2013 and is available only in California and New York.  A total of 835 units have been sold in the U.S. through September 2014.  The Accord PHEV was introduced in Japan in June 2013 and it is available only for leasing, primarily to corporations and government agencies.  s of 2013 , the Accord PHEV ranked as the third best selling plug-in hybrid in the Japanese market. The Mitsubishi Outlander P-HEV was released in the Japanese market also in January 2013, becoming the world's first SUV plug-in hybrid in the market.  The European version was released in Europe in October 2013.  The introduction in the United States was initially scheduled for 2014, and has been delayed several times.  s of 2015 , U.S. deliveries are scheduled to begin by the third quarter of 2016 as a 2017 model year.  The SUV has an all-electric range of 60 km , and a fuel economy of 157 miles per gallon of gasoline equivalent (MPG-e). Cumulative global sales passed the 100,000 unit milestone in March 2016.  Europe is the leading market with 65,529 units sold, followed by Japan with 33,730 units, and Australia with 2,015.  For two years running, 2014 and 2015, the Outlander P-HEV was the top selling plug-in electric vehicle in Europe.  Also, during 2015 the Outlander plug-in hybrid surpassed the Nissan Leaf as the all-time top selling plug-in passenger car in Europe.  Both in 2014 and 2015, it also ranked as the world's top selling plug-in hybrid, and as the third best selling plug-in car after the all-electric Tesla Model S and Nissan Leaf.  s of 2016 , the Outlander P-HEV, with about 119,500 units sold, ranks as the world's second best-selling plug-in hybrid in history, and fourth top selling plug-in electric vehicle ever.  European sales are led by the Netherlands with 24,572 units registered, followed by the UK with 21,053 units registered, both at the end of March 2016.  In the Dutch market the Outlander P-HEV ranked for two months in-a-row, November and December 2013, as the top selling new car in the country.  s of 2016 , the Outlander P-HEV ranks as the all-time top selling plug-in electric car in the country.  Since March 2015 the Mitsubishi Outlander P-HEV ranks as the all-time top selling plug-in electric vehicle in the UK.  Combined sales of the three top selling countries, Japan, the Netherlands and the UK, represent 78% of the 101,533 Outlander PHEVs sold globally through the end of March 2016. Deliveries of the Ford Fusion Energi began in the United States in February 2013.  The Fusion Energi has an all-electric range of 20 mi and an equivalent fuel economy EPA rating of 88 MPG-e (88 mpgus ).  A mid-cycle refreshed 2017 Fusion Energi is scheduled to be released by mid-2016.  In addition to a new fascia and other technological upgrades, the 2017 model year has more efficient electric motors, allowing the refreshed 2017 model year Fusion Energi to increase its all-electric range to 22 mi , and its EPA rated fuel economy in all-electric mode to 97 miles per gallon gasoline equivalent (MPG-e) (97 mpgus ) for combined city/highway driving.  The EPA rating in hybrid operation rose to 42 mpgUS .  s of 2016 , over 32,000 units have been sold in North America, with 31,471 units delivered in the U.S. and about 655 units in Canada. The limited edition McLaren P1 supercar was released in the UK in October 2013.  The first P1 delivery in the U.S. occurred in May 2014.  The P1 has an all-electric range of 6.2 mi on the combined New European Driving Cycle.  Production is limited to 375 units to maintain exclusivity.  Pricing starts at GB£ ( or ).  With only 12 units manufactured by mid November 2013, the entire P1 production was sold out, with about 75% of the customers opting for some level of customization, raising the average sales price to GB£ ( or ).  Only 20 units have been registered worldwide during the first nine months of 2014.  Switzerland is the top selling market with 12 P1s registered during 2014 up to October. The Porsche Panamera S E-Hybrid was released in the U.S. in October 2013.  The Panamera plug-in hybrid has an all-electric range of 20 mi under the New European Driving Cycle (NEDC) test.  s of 2014 , sales in the U.S. totaled 698 units, followed by the Netherlands with 220 units through June 2014.  Global sales between January and August 2014 totaled over 1,500 units, presenting 9% of all Panamera models sold worldwide and 1.3% of all Porsche vehicles sold during this period. Retail sales of the BYD Qin began in Costa Rica in November 2013, and sales in other Latin American countries were scheduled to begin in 2014.  The start of retail sales in the Chinese market was rescheduled several times, and deliveries began in mid December 2013.  The BYD Qin ranked as the top selling plug-in electric car in China in 2014 with 14,747 units sold, and since ranks as the all-time top selling plug-in electric passenger car in the country.  Again in 2015, the Qin was the best-selling plug-in car in China with 31,898 units sold.  Since its introduction, cumulative sales in China totaled 65,178 plug-in hybrids through September 2016.  Also, the BYD Qin was the world's second best selling plug-in hybrid car in 2015 after the Mitsubishi Outlander P-HEV.  A pure battery electric version, the Qin EV300, was released in China in March 2016. The Cadillac ELR was released to retail customers in the U.S. and Canada in December 2013.  The ELR is a limited production luxury coupé that shares the powertrain of the Chevrolet Volt and has an EPA-rated all-electric range of 37 mi .  Production ended in February 2016.  Cumulative sales in North America totaled about 2,800 units through April 2018.  A total of 2,697 units were delivered in the U.S. through April 2016, and 78 units in Canada through March 2016. The Porsche 918 Spyder, with price starting at was released in Europe in May 2014.  Deliveries in the United States began in June 2014.  Production is limited to 918 units sold as a 2014 model year.  The supercar is capable of reaching 100 mph in all-electric mode and has an EPA rated all-electric range of 12 mi .  The entire production run was sold out by December 2014.  The country with the most orders is the United States with 297 units.  Production is scheduled to end in July 2015.  A total of 105 units have been registered worldwide during the first nine months of 2014.  The United States is the leading market with 57 cars delivered through November 2014. Retail deliveries of the BMW i8 began in Germany in June 2014.  The i8 luxury sports car has an all-electric range of 37 km under the NEDC test.  Pricing for the 2015 BMW i8 destined for the U.S. market starts at ( or GB£ ) before any applicable government incentives.  Global deliveries to retail customers totaled 1,129 units through November 2014. The Volkswagen XL1 was released to retail customers in Germany in June 2014.  The XL1 has an all-electric range of 50 km .  The limited production XL1 is available only in Europe and pricing starts at (~).  VW expects its diesel-powered XL1 to achieve 0.9 l/100 km , becoming the most fuel-efficient car in the world. Sales of the Audi A3 Sportback e-tron began in Europe in August 2014, with the first 227 units were registered in Germany in August 2014.  Global registrations totaled 415 units up to September 2014.  The A3 e-tron has an all-electric range of 50 km Retail sales in the U.S. are scheduled to begin in early 2015.  The first 76 units of the Volkswagen Golf GTE were registered in Germany in August 2014.  The Golf GTE has an all-electric range of 50 km A total of 321 units have been registered in Europe through September 2014. The first units of the Mercedes-Benz S 500 e were also registered in Germany in August 2014.  The S 500 has an all-electric range of 30 km .  Sales in the U.S. began in 2015.  A total of 38 units have been registered up to September 2014.  The first four units of the Porsche Cayenne S E-Hybrid were registered in September 2014.  The plug-in Cayenne has an all-electric range of 14 mi .  A total of 45 units have been sold in the U.S. through November 2014. Retail deliveries of the BYD Tang SUV began in China in June 2015.  The Tang has an all-electric range of 80 km .  The Tang was the top selling plug-in electric car in China in 2016, and also ranked as the world's best-selling plug-in hybrid in 2016 and the world's third best-selling plug-in car in 2016.  A total of 51,077 units have been sold in China through February 2017. The first demo units of the Volkswagen Passat GTE were registered in Germany in January 2015.  The Passat GTE has an all-electric range of 50 km and it is the first plug-in hybrid available as both sedan and station wagon.  A total of 88 units have been registered in Germany through July 2015.  The Volvo XC90 T8 plug-in hybrid was released in Europe in the second quarter of 2015, and in the U.S. in August 2015.  The XC90 T8 has an EPA rated all-electric range of 14 mi , with some gasoline consumption (0.1 gal/100 mi).  According to Volvo, sales of the plug-in variant represent 20% of Volvo XC90 global sales by mid-March 2016.  The Hyundai Sonata PHEV was released in selected markets the United States in November 2015.  The Sonata has an all-electric range of 27 mi . The first units of the Mercedes-Benz GLC 350 e were registered in Germany the second quarter of 2016.  The GLC 350e has an all-electric range of 31 km The first units of the Mercedes-Benz GLE 550e were delivered in the American market in June 2016.  The GLE 550e has an all-electric range of 12 km Retail sales of the BMW 740e iPerformance began in Germany in July 2016.  Deliveries in China began in September 2016.  The 740e has an all-electric range of 14 mi Retail deliveries of the second generation Toyota plug-in hybrid, the Prius Prime began in the U.S. in November 2016.  The Prime has an EPA-rated all-electric range of 25 mi , over twice the range of the first generation model.  The Cadillac CT6 PHEV was released in the Chinese market in December 2016.  Sales in the U.S. are scheduled to begin in the second quarter of 2017.  The Hyundai Ioniq Plug-in was released in February 2017.  The Ioniq Plug-in delivers 50 km in all-electric mode. Before the launch of mass production plug-in hybrids, conversion kits and services were available in the United States to convert production model hybrid electric vehicles to plug-ins.  The regular Toyota Prius has been commercially converted, using these aftermarket kits and tax incentives, to a plug-in hybrid by CalCars and a number of third-party companies.  On a smaller scale, PHEVs have been sold as commercial passenger vans, utility trucks, general and school buses, motorcycles, scooters, and military vehicles.  Hybrid Electric Vehicle Technologies, Inc converts diesel buses to plug-in hybrids, under contract for the Chicago Transit Authority.  Fisher Coachworks is developing a plug-in hybrid, the Fisher GTB-40, which is expected to get about twice the mileage of a regular hybrid electric bus. s of 2015 , the global stock of highway-capable plug-in hybrid electric cars totaled 517,100 units, out of total cumulative global sales of 1.257 million light-duty plug-in electric vehicles (41.1%).  The global ratio between all-electrics (BEVs) and plug-in hybrids (PHEVs) has consistently been 60:40 between 2014 and the first half of 2016, mainly due to the large all-electric market in China.  In the U.S. and Europe, the ratio is approaching a 50:50 split. Global sales of plug-in hybrids grew from over 300 units in 2010 to almost 9,000 in 2011, jumped to over 60,000 in 2012, and reached almost 222,000 in 2015.  s of 2015 , the United States was the world's largest plug-in hybrid car market with a stock of 193,770 units, followed by China with 86,580 vehicles, the Netherlands with 78,160, Japan with 55,470 units, and the UK with 28,250.  About 279,000 light-duty plug-in hybrids were sold in 2016, raising the global stock to almost 800,000 highway legal plug-in hybrid electric cars, out of over 2 million light-duty plug-in electric cars on the world roads at the end of 2016. The Netherlands, Sweden, the UK, and the United States have the largest shares of plug-in hybrid sales as percentage of total plug-in electric passenger vehicle sales.  The Netherlands has the world's largest share of plug-in hybrids among its plug-in electric passenger car stock, with 86,162 plug-in hybrids registered at the end of October 2016, out of 99,945 plug-in electric cars and vans, representing 86.2% of the country's stock of light-duty plug-in electric vehicles.  Sweden ranks next with 16,978 plug-in hybrid cars sold between 2011 and August 2016, representing 71.7% of total plug-in electric passenger car sales registrations.  Plug-in hybrid registrations in the UK between up to August 2016 totaled 45,130 units representing 61.6% of total plug-in car registrations since 2011.  In the United States, plug-in hybrids represent 47.2% of the 506,450 plug-in electric cars sold between 2008 and August 2016. In November 2013 the Netherlands became the first country where a plug-in hybrid topped the monthly ranking of new car sales.  During November sales were led by the Mitsubishi Outlander P-HEV with 2,736 units, capturing a market share of 6.8% of new passenger cars sold that month.  Again in December 2013 the Outlander P-HEV ranked as the top selling new car in the country with 4,976 units, representing a 12.6% market share of new car sales.  These record sales allowed the Netherlands to become the second country, after Norway, where plug-in electric cars have topped the monthly ranking of new car sales.  s of 2013 , the Netherlands was the country with highest plug-in hybrid market concentration, with 1.45 vehicles registered per 1,000 people.  Most of the initial growth of the Dutch plug-in hybrid stock took place in 2013, with 20,164 units sold that year representing a rate of growth of 365% from 2012.  Another surge in plug-in hybrid sales took place in 2015, particularly during the last two months, with 41,226 plug-in hybrids registered in 2015. The dominance of plug-in hybrids in the Netherlands is reflected by the fact that, since their inception in 2011 up until October 2016, five out of the top six registered plug-in electric models are plug-in hybrids.  s of 31 2016 , among all plug-in passenger car registered in the Netherlands, the Mitsubishi Outlander P-HEV leads registrations (24,825), followed by the Volvo V60 Plug-in Hybrid (15,015), the Volkswagen Golf GTE (9,710), the Tesla Model S all-electric car (5,681), the Audi A3 Sportback e-tron (5,227), and the Mercedes-Benz C 350 e (5,092). The following table presents the top ranking countries according to its plug-in hybrid segment market share of total new car sales in 2013: The following table presents the top selling plug-in hybrid models with global sales of around or over 15,000 units since the introduction of the first modern production plug-in hybrid vehicle in December 2008, and summarizes sales in the top selling countries for each model through December 2015: Plug-in vehicles that were developed only for demonstration programs include the Ford Escape Plug-in Hybrid, Volvo V70 Plug-in Hybrid, Suzuki Swift Plug-in, Audi A1 e-tron, Dodge Ram 1500 Plug-in Hybrid, and Volkswagen Golf Variant Twin Drive. PHEVs scheduled for market launch between 2017 and 2018 are the Audi Q7 PHEV, Mercedes-Benz E 350e Plug-in Hybrid, BMW 530e iPerformance, and Honda Clarity Plug-in Hybrid.  In total, Mercedes-Benz plans to introduce 10 new plug-in hybrid models by 2017.  The Mitsubishi ASX Plug-in Hybrid is scheduled for market launch in Europe and other markets in 2017. Several countries have established grants and tax credits for the purchase of new plug-in electric vehicles (PEVs) including plug-in hybrid electric vehicles, and usually the economic incentive depends on battery size.  The U.S. offers a federal income tax credit up to , and several states have additional incentives.  The UK offers a Plug-in Car Grant up to a maximum of GB£ ().  As of April 2011, 15 of the 27 European Union member states provide tax incentives for electrically chargeable vehicles, which includes all Western European countries plus the Czech Republic and Romania.  Also 17 countries levy carbon dioxide related taxes on passenger cars as a disincentive.  The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of all-electric and plug-in hybrid vehicles, hybrid vehicles, and some alternative fuel vehicles. Incentives for the development of PHEVs are included in the Energy Independence and Security Act of 2007.  The Energy Improvement and Extension Act of 2008, signed into law on October 3, 2008, grants a tax credits for the purchase of PHEVs.  President Barack Obama's New Energy for America calls for deployment of 1 million plug-in hybrid vehicles by 2015, and on March 19, 2009, he announced programs directing $2.4 billion to electric vehicle development. The American Recovery and Reinvestment Act of 2009 modifies the tax credits, including a new one for plug-in electric drive conversion kits and for 2 or 3 wheel vehicles.  The ultimate total included in the Act that is going to PHEVs is over $6 billion. In March 2009, as part of the American Recovery and Reinvestment Act, the US Department of Energy announced the release of two competitive solicitations for up to $2 billion in federal funding for competitively awarded cost-shared agreements for manufacturing of advanced batteries and related drive components as well as up to $400 million for transportation electrification demonstration and deployment projects.  This announcement will also help meet the President Barack Obama's goal of putting one million plug-in hybrid vehicles on the road by 2015. Public deployments also include: GM's roadmap for plug-in ready communities includes: consumer incentives to make this early technology more affordable; public and workplace charging infrastructure; consumer-friendly electricity rates and renewable electricity options; government and corporate vehicle purchases; supportive permitting and codes for vehicle charging; and other incentives such as high-occupancy-vehicle (HOV) lanes access Electrification of transport (electromobility) is a priority in the European Union Research Programme.  It also figures prominently in the European Economic Recovery Plan presented November 2008, in the frame of the Green Car Initiative.  DG TREN will support a large European \"electromobility\" project on electric vehicles and related infrastructure with a total budget of around €50 million as part of the Green Car Initiative. Organizations that support plug-in hybrids include the World Wide Fund for Nature, its International Director General James Leape remarked, \"the cars of the future ... should, increasingly, be powered by electricity.\" Also National Wildlife Federation has done a strong endorsement of PHEVs. CalCars (with their PHEV news service and \"What car makers are saying about PHEVs\") is dedicated only to the PHEV and has proposed a Prepayment Plan, where buyers would pay $1,000 to reserve a plug-in car and the federal government would match each payment with $9,000, all of which would go to carmakers.  CalCars is also promoting public funds for conversion of internal combustion engines to plug-in vehicles. Other supportive organizations are Plug In America, the Alliance for Climate Protection, Friends of the Earth, the Rainforest Action Network, Rocky Mountain Institute (Project Get Ready), the San Francisco Bay Area Council, the Apollo Alliance, the Set America Free Coalition, the Silicon Valley Leadership Group, and the Plug-in Hybrid Electric School Bus Project, FPL and Duke Energy has said that by 2020 all new purchases of fleet vehicles will be plug-in hybrid or all-electric. Some battery formats and chemistries (nickel–metal hydride batteries) suitable for use in PHEVs are tightly patented and were not licensed for use by PHEV manufacturers, thereby slowed the development of electric cars and PHEVs before the 2008 Oil Crisis.\n\nEvidence of common descent Evidence of common descent of living organisms has been discovered by scientists researching in a variety of disciplines over many decades, demonstrating the common descent of all life on Earth developing from a last universal common ancestor.  This evidence constructs the theoretical framework on which evolutionary theory rests, demonstrates that evolution does occur, and is able to show the natural processes that led to the emergence of Earth's biodiversity.  Additionally, this evidence supports the modern evolutionary synthesis—the current scientific theory that explains how and why life changes over time.  Evolutionary biologists document evidence of common descent by developing testable predictions, testing hypotheses, and constructing theories that illustrate and describe its causes. Comparison of the DNA genetic sequences of organisms has revealed that organisms that are phylogenetically close have a higher degree of DNA sequence similarity than organisms that are phylogenetically distant.  Genetic fragments such as pseudogenes, regions of DNA that are orthologous to a gene in a related organism, but are no longer active and appear to be undergoing a steady process of degeneration from cumulative mutations support common descent alongside the universal biochemical organization and molecular variance patterns found in all organisms.  Additional genetic information conclusively supports the relatedness of life and has allowed scientists (since the discovery of DNA) to develop phylogenetic trees: a construction of organisms evolutionary relatedness.  It has also led to the development of molecular clock techniques to date taxon divergence times and to calibrate these with the fossil record. Fossils are important for estimating when various lineages developed in geologic time.  As fossilization is an uncommon occurrence, usually requiring hard body parts and death near a site where sediments are being deposited, the fossil record only provides sparse and intermittent information about the evolution of life.  Evidence of organisms prior to the development of hard body parts such as shells, bones and teeth is especially scarce, but exists in the form of ancient microfossils, as well as impressions of various soft-bodied organisms.  The comparative study of the anatomy of groups of animals shows structural features that are fundamentally similar (homologous), demonstrating phylogenetic and ancestral relationships with other organisms, most especially when compared with fossils of ancient extinct organisms.  Vestigial structures and comparisons in embryonic development are largely a contributing factor in anatomical resemblance in concordance with common descent.  Since metabolic processes do not leave fossils, research into the evolution of the basic cellular processes is done largely by comparison of existing organisms' physiology and biochemistry.  Many lineages diverged at different stages of development, so it is possible to determine when certain metabolic processes appeared by comparing the traits of the descendants of a common ancestor. Further evidence comes from the field of biogeography because evolution with common descent provides the best and most thorough explanation for a variety of facts concerning the geographical distribution of plants and animals across the world.  This is especially obvious in the field of insular biogeography.  Combined with the well-established geological theory of plate tectonics, common descent provides a way to combine facts about the current distribution of species with evidence from the fossil record to provide a logically consistent explanation of how the distribution of living organisms has changed over time. The development and spread of antibiotic resistant bacteria provides evidence that evolution due to natural selection is an ongoing process in the natural world.  Natural selection is ubiquitous in all research pertaining to evolution, taking note of the fact that all of the following examples in each section of the article document the process.  Alongside this are observed instances of the separation of populations of species into sets of new species (speciation).  Speciation has been observed directly and indirectly in the lab and in nature.  Multiple forms of such have been described and documented as examples for individual modes of speciation.  Furthermore, evidence of common descent extends from direct laboratory experimentation with the selective breeding of organisms—historically and currently—and other controlled experiments involving many of the topics in the article.  This article summarizes the varying disciplines that provide the evidence for evolution and the common descent of all life on Earth, accompanied by numerous and specialized examples, indicating a compelling concordance of evidence. One of the strongest evidences for common descent comes from the study of gene sequences.  Comparative sequence analysis examines the relationship between the DNA sequences of different species, producing several lines of evidence that confirm Darwin's original hypothesis of common descent.  If the hypothesis of common descent is true, then species that share a common ancestor inherited that ancestor's DNA sequence, as well as mutations unique to that ancestor.  More closely related species have a greater fraction of identical sequence and shared substitutions compared to more distantly related species. The simplest and most powerful evidence is provided by phylogenetic reconstruction.  Such reconstructions, especially when done using slowly evolving protein sequences, are often quite robust and can be used to reconstruct a great deal of the evolutionary history of modern organisms (and even in some instances of the evolutionary history of extinct organisms, such as the recovered gene sequences of mammoths or Neanderthals).  These reconstructed phylogenies recapitulate the relationships established through morphological and biochemical studies.  The most detailed reconstructions have been performed on the basis of the mitochondrial genomes shared by all eukaryotic organisms, which are short and easy to sequence; the broadest reconstructions have been performed either using the sequences of a few very ancient proteins or by using ribosomal RNA sequence . Phylogenetic relationships also extend to a wide variety of nonfunctional sequence elements, including repeats, transposons, pseudogenes, and mutations in protein-coding sequences that do not result in changes in amino-acid sequence.  While a minority of these elements might later be found to harbor function, in aggregate they demonstrate that identity must be the product of common descent rather than common function . All known extant (surviving) organisms are based on the same biochemical processes: genetic information encoded as nucleic acid (DNA, or RNA for many viruses), transcribed into RNA, then translated into proteins (that is, polymers of amino acids) by highly conserved ribosomes. Perhaps most tellingly, the Genetic Code (the \"translation table\" between DNA and amino acids) is the same for almost every organism, meaning that a piece of DNA in a bacterium codes for the same amino acid as in a human cell.  ATP is used as energy currency by all extant life.  A deeper understanding of developmental biology shows that common morphology is, in fact, the product of shared genetic elements.  For example, although camera-like eyes are believed to have evolved independently on many separate occasions, they share a common set of light-sensing proteins (opsins), suggesting a common point of origin for all sighted creatures.  Another noteworthy example is the familiar vertebrate body plan, whose structure is controlled by the homeobox (Hox) family of genes. Comparison of the DNA sequences allows organisms to be grouped by sequence similarity, and the resulting phylogenetic trees are typically congruent with traditional taxonomy, and are often used to strengthen or correct taxonomic classifications.  Sequence comparison is considered a measure robust enough to correct erroneous assumptions in the phylogenetic tree in instances where other evidence is scarce.  For example, neutral human DNA sequences are approximately 1.2% divergent (based on substitutions) from those of their nearest genetic relative, the chimpanzee, 1.6% from gorillas, and 6.6% from baboons. Genetic sequence evidence thus allows inference and quantification of genetic relatedness between humans and other apes.  The sequence of the 16S ribosomal RNA gene, a vital gene encoding a part of the ribosome, was used to find the broad phylogenetic relationships between all extant life.  The analysis, originally done by Carl Woese, resulted in the three-domain system, arguing for two major splits in the early evolution of life.  The first split led to modern Bacteria and the subsequent split led to modern Archaea and Eukaryotes. Some DNA sequences are shared by very different organisms.  It has been predicted by the theory of evolution that the differences in such DNA sequences between two organisms should roughly resemble both the biological difference between them according to their anatomy and the time that had passed since these two organisms have separated in the course of evolution, as seen in fossil evidence.  The rate of accumulating such changes should be low for some sequences, namely those that code for critical RNA or proteins, and high for others that code for less critical RNA or proteins; but for every specific sequence, the rate of change should be roughly constant over time.  These results have been experimentally confirmed.  Two examples are DNA sequences coding for rRNA, which is highly conserved, and DNA sequences coding for fibrinopeptides (amino acid chains that are discarded during the formation of fibrin), which are highly non-conserved. The proteomic evidence also supports the universal ancestry of life.  Vital proteins, such as the ribosome, DNA polymerase, and RNA polymerase, are found in everything from the most primitive bacteria to the most complex mammals.  The core part of the protein is conserved across all lineages of life, serving similar functions.  Higher organisms have evolved additional protein subunits, largely affecting the regulation and protein-protein interaction of the core.  Other overarching similarities between all lineages of extant organisms, such as DNA, RNA, amino acids, and the lipid bilayer, give support to the theory of common descent.  Phylogenetic analyses of protein sequences from various organisms produce similar trees of relationship between all organisms.  The chirality of DNA, RNA, and amino acids is conserved across all known life.  As there is no functional advantage to right- or left-handed molecular chirality, the simplest hypothesis is that the choice was made randomly by early organisms and passed on to all extant life through common descent.  Further evidence for reconstructing ancestral lineages comes from junk DNA such as pseudogenes, \"dead\" genes that steadily accumulate mutations. Pseudogenes, also known as noncoding DNA, are extra DNA in a genome that do not get transcribed into RNA to synthesize proteins.  Some of this noncoding DNA has known functions, but much of it has no known function and is called \"Junk DNA\".  This is an example of a vestige since replicating these genes uses energy, making it a waste in many cases.  A pseudogene can be produced when a coding gene accumulates mutations that prevent it from being transcribed, making it non-functional.  But since it is not transcribed, it may disappear without affecting fitness, unless it has provided some beneficial function as non-coding DNA.  Non-functional pseudogenes may be passed on to later species, thereby labeling the later species as descended from the earlier species. There is also a large body of molecular evidence for a number of different mechanisms for large evolutionary changes, among them: genome and gene duplication, which facilitates rapid evolution by providing substantial quantities of genetic material under weak or no selective constraints; horizontal gene transfer, the process of transferring genetic material to another cell that is not an organism's offspring, allowing for species to acquire beneficial genes from each other; and recombination, capable of reassorting large numbers of different alleles and of establishing reproductive isolation.  The Endosymbiotic theory explains the origin of mitochondria and plastids (\"e.g.\" chloroplasts), which are organelles of eukaryotic cells, as the incorporation of an ancient prokaryotic cell into ancient eukaryotic cell.  Rather than evolving eukaryotic organelles slowly, this theory offers a mechanism for a sudden evolutionary leap by incorporating the genetic material and biochemical composition of a separate species.  Evidence supporting this mechanism has been found in the protist \"Hatena\": as a predator it engulfs a green algae cell, which subsequently behaves as an endosymbiont, nourishing \"Hatena\", which in turn loses its feeding apparatus and behaves as an autotroph. Since metabolic processes do not leave fossils, research into the evolution of the basic cellular processes is done largely by comparison of existing organisms.  Many lineages diverged when new metabolic processes appeared, and it is theoretically possible to determine when certain metabolic processes appeared by comparing the traits of the descendants of a common ancestor or by detecting their physical manifestations.  As an example, the appearance of oxygen in the earth's atmosphere is linked to the evolution of photosynthesis. Evidence for the evolution of \"Homo sapiens\" from a common ancestor with chimpanzees is found in the number of chromosomes in humans as compared to all other members of Hominidae.  All hominidae have 24 pairs of chromosomes, except humans, who have only 23 pairs.  Human chromosome 2 is a result of an end-to-end fusion of two ancestral chromosomes. The evidence for this includes: Chromosome 2 thus presents very strong evidence in favour of the common descent of humans and other apes.  According to J. W. IJdo, \"We conclude that the locus cloned in cosmids c8.1 and c29B is the relic of an ancient telomere-telomere fusion and marks the point at which two ancestral ape chromosomes fused to give rise to human chromosome 2.\" A classic example of biochemical evidence for evolution is the variance of the ubiquitous (i.e. all living organisms have it, because it performs very basic life functions) protein Cytochrome c in living cells.  The variance of cytochrome c of different organisms is measured in the number of differing amino acids, each differing amino acid being a result of a base pair substitution, a mutation.  If each differing amino acid is assumed the result of one base pair substitution, it can be calculated how long ago the two species diverged by multiplying the number of base pair substitutions by the estimated time it takes for a substituted base pair of the cytochrome c gene to be successfully passed on.  For example, if the average time it takes for a base pair of the cytochrome c gene to mutate is N years, the number of amino acids making up the cytochrome c protein in monkeys differ by one from that of humans, this leads to the conclusion that the two species diverged N years ago. The primary structure of cytochrome c consists of a chain of about 100 amino acids.  Many higher order organisms possess a chain of 104 amino acids. The cytochrome c molecule has been extensively studied for the glimpse it gives into evolutionary biology.  Both chicken and turkeys have identical sequence homology (amino acid for amino acid), as do pigs, cows and sheep.  Both humans and chimpanzees share the identical molecule, while rhesus monkeys share all but one of the amino acids: the 66th amino acid is isoleucine in the former and threonine in the latter. What makes these homologous similarities particularly suggestive of common ancestry in the case of cytochrome c, in addition to the fact that the phylogenies derived from them match other phylogenies very well, is the high degree of functional redundancy of the cytochrome c molecule.  The different existing configurations of amino acids do not significantly affect the functionality of the protein, which indicates that the base pair substitutions are not part of a directed design, but the result of random mutations that aren't subject to selection. In addition, Cytochrome b is commonly used as a region of mitochondrial DNA to determine phylogenetic relationships between organisms due to its sequence variability.  It is considered most useful in determining relationships within families and genera.  Comparative studies involving cytochrome b have resulted in new classification schemes and have been used to assign newly described species to a genus, as well as deepen the understanding of evolutionary relationships. Endogenous retroviruses (or ERVs) are remnant sequences in the genome left from ancient viral infections in an organism.  The retroviruses (or virogenes) are always passed on to the next generation of that organism that received the infection.  This leaves the virogene left in the genome.  Because this event is rare and random, finding identical chromosomal positions of a virogene in two different species suggests common ancestry.  Cats (Felidae) present a notable instance of virogene sequences demonstrating common descent.  The standard phylogenetic tree for Felidae have smaller cats (\"Felis chaus\", \"Felis silvestris\", \"Felis nigripes\", and \"Felis catus\") diverging from larger cats such as the subfamily Pantherinae and other carnivores.  The fact that small cats have an ERV where the larger cats do not suggests that the gene was inserted into the ancestor of the small cats after the larger cats had diverged.  Another example of this is with humans and chimps.  Humans contain numerous ERVs that comprise a considerable percentage of the genome.  Sources vary, however, 1% to 8% has been proposed.  Humans and chimps share seven different occurrences of virogenes, while all primates share similar retroviruses congruent with phylogeny. Mathematical models of evolution, pioneered by the likes of Sewall Wright, Ronald Fisher and J. B. S. Haldane and extended via diffusion theory by Motoo Kimura, allow predictions about the genetic structure of evolving populations.  Direct examination of the genetic structure of modern populations via DNA sequencing has allowed verification of many of these predictions.  For example, the Out of Africa theory of human origins, which states that modern humans developed in Africa and a small sub-population migrated out (undergoing a population bottleneck), implies that modern populations should show the signatures of this migration pattern.  Specifically, post-bottleneck populations (Europeans and Asians) should show lower overall genetic diversity and a more uniform distribution of allele frequencies compared to the African population.  Both of these predictions are borne out by actual data from a number of studies. Comparative study of the anatomy of groups of animals or plants reveals that certain structural features are basically similar.  For example, the basic structure of all flowers consists of sepals, petals, stigma, style and ovary; yet the size, colour, number of parts and specific structure are different for each individual species.  The neural anatomy of fossilized remains may also be compared using advanced imaging techniques. Once thought of as a refutation to evolutionary theory, atavisms are \"now seen as potent evidence of how much genetic potential is retained...after a particular structure has disappeared from a species\".  \"Atavisms are the reappearance of a lost character typical of remote ancestors and not seen in the parents or recent ancestors...\" and are an \"[indication] of the developmental plasticity that exists within embryos...\" Atavisms occur because genes for previously existing phenotypical features are often preserved in DNA, even though the genes are not expressed in some or most of the organisms possessing them.  Numerous examples have documented the occurrence of atavisms alongside experimental research triggering their formation.  Due to the complexity and interrelatedness of the factors involved in the development of atavisms, both biologists and medical professionals find it \"difficult, if not impossible, to distinguish [them] from malformations.\" Some examples of atavisms found in the scientific literature include: Evolutionary developmental biology is the biological field that compares the developmental process of different organisms to determine ancestral relationships between species.  A large variety of organism's genomes contain a small fraction of genes that control the organisms development.  Hox genes are an example of these types of nearly universal genes in organisms pointing to an origin of common ancestry.  Embryological evidence comes from the development of organisms at the embryological level with the comparison of different organisms embryos similarity.  Remains of ancestral traits often appear and disappear in different stages of the embryological development process.  Examples include such as hair growth and loss (lanugo) during human development; development and degeneration of a yolk sac; terrestrial frogs and salamanders passing through the larval stage within the egg—with features of typically aquatic larvae—but hatch ready for life on land; and the appearance of gill-like structures (pharyngeal arch) in vertebrate embryo development.  Note that in fish, the arches continue to develop as branchial arches while in humans, for example, they give rise to a variety of structures within the head and neck. If widely separated groups of organisms are originated from a common ancestry, they are expected to have certain basic features in common.  The degree of resemblance between two organisms should indicate how closely related they are in evolution: When a group of organisms share a homologous structure that is specialized to perform a variety of functions to adapt different environmental conditions and modes of life, it is called adaptive radiation.  The gradual spreading of organisms with adaptive radiation is known as divergent evolution. Taxonomy is based on the fact that all organisms are related to each other in nested hierarchies based on shared characteristics.  Most existing species can be organized rather easily in a nested hierarchical classification.  This is evident from the Linnaean classification scheme.  Based on shared derived characters, closely related organisms can be placed in one group (such as a genus), several genera can be grouped together into one family, several families can be grouped together into an order, etc.  The existence of these nested hierarchies was recognized by many biologists before Darwin, but he showed that his theory of evolution with its branching pattern of common descent could explain them.  Darwin described how common descent could provide a logical basis for classification: An evolutionary tree (of Amniota, for example, the last common ancestor of mammals and reptiles, and all its descendants) illustrates the initial conditions causing evolutionary patterns of similarity (e.g., all Amniotes produce an egg that possesses the amnios) and the patterns of divergence amongst lineages (e.g., mammals and reptiles branching from the common ancestry in Amniota).  Evolutionary trees provide conceptual models of evolving systems once thought limited in the domain of making predictions out of the theory.  However, the method of phylogenetic bracketing is used to infer predictions with far greater probability than raw speculation.  For example, paleontologists use this technique to make predictions about nonpreservable traits in fossil organisms, such as feathered dinosaurs, and molecular biologists use the technique to posit predictions about RNA metabolism and protein functions.  Thus evolutionary trees are evolutionary hypotheses that refer to specific facts, such as the characteristics of organisms (e.g., scales, feathers, fur), providing evidence for the patterns of descent, and a causal explanation for modification (i.e., natural selection or neutral drift) in any given lineage (e.g., Amniota).  Evolutionary biologists test evolutionary theory using phylogenetic systematic methods that measure how much the hypothesis (a particular branching pattern in an evolutionary tree) increases the likelihood of the evidence (the distribution of characters among lineages).  The severity of tests for a theory increases if the predictions \"are the least probable of being observed if the causal event did not occur.\"  \"Testability is a measure of how much the hypothesis increases the likelihood of the evidence.\" Evidence for common descent comes from the existence of vestigial structures.  These rudimentary structures are often homologous to structures that correspond in related or ancestral species.  A wide range of structures exist such as mutated and non-functioning genes, parts of a flower, muscles, organs, and even behaviors.  This variety can be found across many different groups of species.  In many cases they are degenerated or underdeveloped.  The existence of vestigial organs can be explained in terms of changes in the environment or modes of life of the species.  Those organs are typically functional in the ancestral species but are now either semi-functional, nonfunctional, or re-purposed. Scientific literature concerning vestigial structures abounds.  One study complied 64 examples of vestigial structures found in the literature across a wide range of disciplines within the 21st century.  The following non-exhaustive list summarizes Senter et al. alongside various other examples: Many different species of insects have mouthparts derived from the same embryonic structures, indicating that the mouthparts are modifications of a common ancestor's original features.  These include a labrum (upper lip), a pair of mandibles, a hypopharynx (floor of mouth), a pair of maxillae, and a labium.  (Fig. 2c) Evolution has caused enlargement and modification of these structures in some species, while it has caused the reduction and loss of them in other species.  The modifications enable the insects to exploit a variety of food materials. Insect mouthparts and antennae are considered homologues of insect legs.  Parallel developments are seen in some arachnids: The anterior pair of legs may be modified as analogues of antennae, particularly in whip scorpions, which walk on six legs.  These developments provide support for the theory that complex modifications often arise by duplication of components, with the duplicates modified in different directions. Similar to the pentadactyl limb in mammals, the earliest dinosaurs split into two distinct orders—the \"saurischia\" and \"ornithischia\".  They are classified as one or the other in accordance with what the fossils demonstrate.  Figure 2d, shows that early \"saurischians\" resembled early \"ornithischians\".  The pattern of the pelvis in all species of dinosaurs is an example of homologous structures.  Each order of dinosaur has slightly differing pelvis bones providing evidence of common descent.  Additionally, modern birds show a similarity to ancient \"saurischian\" pelvic structures indicating the evolution of birds from dinosaurs.  This can also be seen in Figure 5c as the Aves branch off the Theropoda suborder. The pattern of limb bones called pentadactyl limb is an example of homologous structures (Fig. 2e).  It is found in all classes of tetrapods (\"i.e.\" from amphibians to mammals).  It can even be traced back to the fins of certain fossil fishes from which the first amphibians evolved such as tiktaalik.  The limb has a single proximal bone (humerus), two distal bones (radius and ulna), a series of carpals (wrist bones), followed by five series of metacarpals (palm bones) and phalanges (digits).  Throughout the tetrapods, the fundamental structures of pentadactyl limbs are the same, indicating that they originated from a common ancestor.  But in the course of evolution, these fundamental structures have been modified.  They have become superficially different and unrelated structures to serve different functions in adaptation to different environments and modes of life.  This phenomenon is shown in the forelimbs of mammals.  For example: The recurrent laryngeal nerve is a fourth branch of the vagus nerve, which is a cranial nerve.  In mammals, its path is unusually long.  As a part of the vagus nerve, it comes from the brain, passes through the neck down to heart, rounds the dorsal aorta and returns up to the larynx, again through the neck.  (Fig. 2f) This path is suboptimal even for humans, but for giraffes it becomes even more suboptimal.  Due to the lengths of their necks, the recurrent laryngeal nerve may be up to 4 m long, despite its optimal route being a distance of just several inches. The indirect route of this nerve is the result of evolution of mammals from fish, which had no neck and had a relatively short nerve that innervated one gill slit and passed near the gill arch.  Since then, the gill it innervated has become the larynx and the gill arch has become the dorsal aorta in mammals. Similar to the laryngeal nerve in giraffes, the vas deferens is part of the male anatomy of many vertebrates; it transports sperm from the epididymis in anticipation of ejaculation.  In humans, the vas deferens routes up from the testicle, looping over the ureter, and back down to the urethra and penis.  It has been suggested that this is due to the descent of the testicles during the course of human evolution—likely associated with temperature.  As the testicles descended, the vas deferens lengthened to accommodate the accidental \"hook\" over the ureter. When organisms die, they often decompose rapidly or are consumed by scavengers, leaving no permanent evidences of their existence.  However, occasionally, some organisms are preserved.  The remains or traces of organisms from a past geologic age embedded in rocks by natural processes are called fossils.  They are extremely important for understanding the evolutionary history of life on Earth, as they provide direct evidence of evolution and detailed information on the ancestry of organisms.  Paleontology is the study of past life based on fossil records and their relations to different geologic time periods. For fossilization to take place, the traces and remains of organisms must be quickly buried so that weathering and decomposition do not occur.  Skeletal structures or other hard parts of the organisms are the most commonly occurring form of fossilized remains.  There are also some trace \"fossils\" showing moulds, cast or imprints of some previous organisms. As an animal dies, the organic materials gradually decay, such that the bones become porous.  If the animal is subsequently buried in mud, mineral salts infiltrate into the bones and gradually fill up the pores.  The bones harden into stones and are preserved as fossils.  This process is known as petrification.  If dead animals are covered by wind-blown sand, and if the sand is subsequently turned into mud by heavy rain or floods, the same process of mineral infiltration may occur.  Apart from petrification, the dead bodies of organisms may be well preserved in ice, in hardened resin of coniferous trees (figure 3a), in tar, or in anaerobic, acidic peat.  Fossilization can sometimes be a trace, an impression of a form.  Examples include leaves and footprints, the fossils of which are made in layers that then harden. It is possible to decipher how a particular group of organisms evolved by arranging its fossil record in a chronological sequence.  Such a sequence can be determined because fossils are mainly found in sedimentary rock.  Sedimentary rock is formed by layers of silt or mud on top of each other; thus, the resulting rock contains a series of horizontal layers, or strata.  Each layer contains fossils typical for a specific time period when they formed.  The lowest strata contain the oldest rock and the earliest fossils, while the highest strata contain the youngest rock and more recent fossils. A succession of animals and plants can also be seen from fossil discoveries.  By studying the number and complexity of different fossils at different stratigraphic levels, it has been shown that older fossil-bearing rocks contain fewer types of fossilized organisms, and they all have a simpler structure, whereas younger rocks contain a greater variety of fossils, often with increasingly complex structures. For many years, geologists could only roughly estimate the ages of various strata and the fossils found.  They did so, for instance, by estimating the time for the formation of sedimentary rock layer by layer.  Today, by measuring the proportions of radioactive and stable elements in a given rock, the ages of fossils can be more precisely dated by scientists.  This technique is known as radiometric dating. Throughout the fossil record, many species that appear at an early stratigraphic level disappear at a later level.  This is interpreted in evolutionary terms as indicating the times when species originated and became extinct.  Geographical regions and climatic conditions have varied throughout Earth's history.  Since organisms are adapted to particular environments, the constantly changing conditions favoured species that adapted to new environments through the mechanism of natural selection. Despite the relative rarity of suitable conditions for fossilization, an estimated 250,000 fossil species have been named.  The number of individual fossils this represents varies greatly from species to species, but many millions of fossils have been recovered: for instance, more than three million fossils from the last ice age have been recovered from the La Brea Tar Pits in Los Angeles.  Many more fossils are still in the ground, in various geological formations known to contain a high fossil density, allowing estimates of the total fossil content of the formation to be made.  An example of this occurs in South Africa's Beaufort Formation (part of the Karoo Supergroup, which covers most of South Africa), which is rich in vertebrate fossils, including therapsids (reptile-mammal transitional forms).  It has been estimated that this formation contains 800 billion vertebrate fossils.  Palentologists have documented numerous transitional forms and have constructed \"an astonishingly comprehensive record of the key transitions in animal evolution\".  Conducting a survey of the paleontological literature, one would find that there is \"abundant evidence for how all the major groups of animals are related, much of it in the form of excellent transitional fossils\". The fossil record is an important source for scientists when tracing the evolutionary history of organisms.  However, because of limitations inherent in the record, there are not fine scales of intermediate forms between related groups of species.  This lack of continuous fossils in the record is a major limitation in tracing the descent of biological groups.  When transitional fossils are found that show intermediate forms in what had previously been a gap in knowledge, they are often popularly referred to as \"missing links\". There is a gap of about 100 million years between the beginning of the Cambrian period and the end of the Ordovician period.  The early Cambrian period was the period from which numerous fossils of sponges, cnidarians (\"e.g.\", jellyfish), echinoderms (\"e.g.\", eocrinoids), molluscs (\"e.g.\", snails) and arthropods (\"e.g.\", trilobites) are found.  The first animal that possessed the typical features of vertebrates, the \"Arandaspis\", was dated to have existed in the later Ordovician period.  Thus few, if any, fossils of an intermediate type between invertebrates and vertebrates have been found, although likely candidates include the Burgess Shale animal, \"Pikaia gracilens\", and its Maotianshan shales relatives, \"Myllokunmingia\", \"Yunnanozoon\", \"Haikouella lanceolata\", and \"Haikouichthys\". Some of the reasons for the incompleteness of fossil records are: Due to an almost-complete fossil record found in North American sedimentary deposits from the early Eocene to the present, the horse provides one of the best examples of evolutionary history (phylogeny). This evolutionary sequence starts with a small animal called \"Hyracotherium\" (commonly referred to as \"Eohippus\"), which lived in North America about 54 million years ago then spread across to Europe and Asia.  Fossil remains of \"Hyracotherium\" show it to have differed from the modern horse in three important respects: it was a small animal (the size of a fox), lightly built and adapted for running; the limbs were short and slender, and the feet elongated so that the digits were almost vertical, with four digits in the forelimbs and three digits in the hindlimbs; and the incisors were small, the molars having low crowns with rounded cusps covered in enamel. The probable course of development of horses from \"Hyracotherium\" to \"Equus\" (the modern horse) involved at least 12 genera and several hundred species.  The major trends seen in the development of the horse to changing environmental conditions may be summarized as follows: Fossilized plants found in different strata show that the marshy, wooded country in which \"Hyracotherium\" lived became gradually drier.  Survival now depended on the head being in an elevated position for gaining a good view of the surrounding countryside, and on a high turn of speed for escape from predators, hence the increase in size and the replacement of the splayed-out foot by the hoofed foot.  The drier, harder ground would make the original splayed-out foot unnecessary for support.  The changes in the teeth can be explained by assuming that the diet changed from soft vegetation to grass.  A dominant genus from each geological period has been selected (see figure 3e) to show the slow alteration of the horse lineage from its ancestral to its modern form. Prior to 2004, paleontologists had found fossils of amphibians with necks, ears, and four legs, in rock no older than 365 million years old.  In rocks more than 385 million years old they could only find fish, without these amphibian characteristics.  Evolutionary theory predicted that since amphibians evolved from fish, an intermediate form should be found in rock dated between 365 and 385 million years ago.  Such an intermediate form should have many fish-like characteristics, conserved from 385 million years ago or more, but also have many amphibian characteristics as well.  In 2004, an expedition to islands in the Canadian arctic searching specifically for this fossil form in rocks that were 375 million years old discovered fossils of Tiktaalik.  Some years later, however, scientists in Poland found evidence of fossilised tetrapod tracks predating \"Tiktaalik\". Data about the presence or absence of species on various continents and islands (biogeography) can provide evidence of common descent and shed light on patterns of speciation. All organisms are adapted to their environment to a greater or lesser extent.  If the abiotic and biotic factors within a habitat are capable of supporting a particular species in one geographic area, then one might assume that the same species would be found in a similar habitat in a similar geographic area, e.g. in Africa and South America.  This is not the case.  Plant and animal species are discontinuously distributed throughout the world: Even greater differences can be found if Australia is taken into consideration, though it occupies the same latitude as much of South America and Africa.  Marsupials like kangaroos, bandicoots, and quolls make up about half of Australia's indigenous mammal species.  By contrast, marsupials are today totally absent from Africa and form a smaller portion of the mammalian fauna of South America, where opossums, shrew opossums, and the monito del monte occur.  The only living representatives of primitive egg-laying mammals (monotremes) are the echidnas and the platypus.  The short-beaked echidna (Tachyglossus aculeatus) and its subspecies populate Australia, Tasmania, New Guinea, and Kangaroo Island while the long-beaked echidna (Zaglossus bruijni) lives only in New Guinea.  The platypus lives in the waters of eastern Australia.  They have been introduced to Tasmania, King Island, and Kangaroo Island.  These Monotremes are totally absent in the rest of the world.  On the other hand, Australia is missing many groups of placental mammals that are common on other continents (carnivorans, artiodactyls, shrews, squirrels, lagomorphs), although it does have indigenous bats and murine rodents; many other placentals, such as rabbits and foxes, have been introduced there by humans. Other animal distribution examples include bears, located on all continents excluding Africa, Australia and Antarctica, and the polar bear only located solely in the Arctic Circle and adjacent land masses.  Penguins are located only around the South Pole despite similar weather conditions at the North Pole.  Families of sirenians are distributed exclusively around the earth's waters, where manatees are located in western Africa waters, northern South American waters, and West Indian waters only while the related family, the dugongs, are located only in Oceanic waters north of Australia, and the coasts surrounding the Indian Ocean Additionally, the now extinct Steller's sea cow resided in the Bering Sea. The same kinds of fossils are found from areas known to be adjacent to one another in the past but that, through the process of continental drift, are now in widely divergent geographic locations.  For example, fossils of the same types of ancient amphibians, arthropods and ferns are found in South America, Africa, India, Australia and Antarctica, which can be dated to the Paleozoic Era, when these regions were united as a single landmass called Gondwana.  Sometimes the descendants of these organisms can be identified and show unmistakable similarity to each other, even though they now inhabit very different regions and climates. Evidence from island biogeography has played an important and historic role in the development of evolutionary biology.  For purposes of biogeography, islands are divided into two classes.  Continental islands are islands like Great Britain, and Japan that have at one time or another been part of a continent.  Oceanic islands, like the Hawaiian islands, the Galápagos Islands and St. Helena, on the other hand are islands that have formed in the ocean and never been part of any continent.  Oceanic islands have distributions of native plants and animals that are unbalanced in ways that make them distinct from the biotas found on continents or continental islands.  Oceanic islands do not have native terrestrial mammals (they do sometimes have bats and seals), amphibians, or fresh water fish.  In some cases they have terrestrial reptiles (such as the iguanas and giant tortoises of the Galápagos Islands) but often (for example Hawaii) they do not.  This despite the fact that when species such as rats, goats, pigs, cats, mice, and cane toads, are introduced to such islands by humans they often thrive.  Starting with Charles Darwin, many scientists have conducted experiments and made observations that have shown that the types of animals and plants found, and not found, on such islands are consistent with the theory that these islands were colonized accidentally by plants and animals that were able to reach them.  Such accidental colonization could occur by air, such as plant seeds carried by migratory birds, or bats and insects being blown out over the sea by the wind, or by floating from a continent or other island by sea, as for example by some kinds of plant seeds like coconuts that can survive immersion in salt water, and reptiles that can survive for extended periods on rafts of vegetation carried to sea by storms. Many of the species found on remote islands are endemic to a particular island or group of islands, meaning they are found nowhere else on earth.  Examples of species endemic to islands include many flightless birds of New Zealand, lemurs of Madagascar, the Komodo dragon of Komodo, the dragon's blood tree of Socotra, Tuatara of New Zealand, and others.  However, many such endemic species are related to species found on other nearby islands or continents; the relationship of the animals found on the Galápagos Islands to those found in South America is a well-known example.  All of these facts, the types of plants and animals found on oceanic islands, the large number of endemic species found on oceanic islands, and the relationship of such species to those living on the nearest continents, are most easily explained if the islands were colonized by species from nearby continents that evolved into the endemic species now found there. Other types of endemism do not have to include, in the strict sense, islands.  Islands can mean isolated lakes or remote and isolated areas.  Examples of these would include the highlands of Ethiopia, Lake Baikal, fynbos of South Africa, forests of New Caledonia, and others.  Examples of endemic organisms living in isolated areas include the kagu of New Caledonia, cloud rats of the Luzon tropical pine forests of the Philippines, the boojum tree (\"Fouquieria columnaris\") of the Baja California peninsula, the Baikal seal and the omul of Lake Baikal. Oceanic islands are frequently inhabited by clusters of closely related species that fill a variety of ecological niches, often niches that are filled by very different species on continents.  Such clusters, like the finches of the Galápagos, Hawaiian honeycreepers, members of the sunflower family on the Juan Fernandez Archipelago and wood weevils on St. Helena are called adaptive radiations because they are best explained by a single species colonizing an island (or group of islands) and then diversifying to fill available ecological niches.  Such radiations can be spectacular; 800 species of the fruit fly family \"Drosophila\", nearly half the world's total, are endemic to the Hawaiian islands.  Another illustrative example from Hawaii is the silversword alliance, which is a group of thirty species found only on those islands.  Members range from the silverswords that flower spectacularly on high volcanic slopes to trees, shrubs, vines and mats that occur at various elevations from mountain top to sea level, and in Hawaiian habitats that vary from deserts to rainforests.  Their closest relatives outside Hawaii, based on molecular studies, are tarweeds found on the west coast of North America.  These tarweeds have sticky seeds that facilitate distribution by migrant birds.  Additionally, nearly all of the species on the island can be crossed and the hybrids are often fertile, and they have been hybridized experimentally with two of the west coast tarweed species as well.  Continental islands have less distinct biota, but those that have been long separated from any continent also have endemic species and adaptive radiations, such as the 75 lemur species of Madagascar, and the eleven extinct moa species of New Zealand. The biologist Ernst Mayer championed the concept of ring species, claiming that it unequivocally demonstrated the process of speciation.  A ring species is an alternative model to allopatric speciation, “illustrating how new species can arise through ‘circular overlap’, without interruption of gene flow through intervening populations…” Ring species often attract the interests of evolutionary biologists, systematists, and researchers of speciation leading to both thought provoking ideas and confusion concerning their definition.  Contemporary scholars recognize that examples in nature have proved rare due to various factors such as limitations in taxonomic delineation or, “taxonomic zeal”—explained by the fact that taxonomists classify organisms into \"species\", while ring species often cannot fit this definition.  Other reasons such as gene flow interruption from “vicariate divergence” and fragmented populations due to climate instability have also been cited. A great deal of research has been conducted on the topic, and examples have been found and documented in nature.  Debate exists concerning much of the research, with some authors citing evidence against their existence entirely.  The following examples provide evidence that—despite the limited number of concrete, idealized examples in nature—continuums of species do exist and can be found in biological systems.  This is often characterized by the existence of various sub-species level classifications (i.e. clines, ecotypes, groups, varieties, etc.).  Caveats do exist considering that many of the examples have been disputed by researchers conducting subsequent studies and that, \"many of the [proposed] cases have received very little attention from researchers, making it difficult to assess whether they display the characteristics of ideal ring species.\" Examples of species that have been described as exhibiting a ring-like, geographic distribution pattern: The combination of continental drift and evolution can sometimes be used to predict what will be found in the fossil record. \" Glossopteris\" is an extinct species of seed fern plants from the Permian.  \"Glossopteris\" appears in the fossil record around the beginning of the Permian on the ancient continent of Gondwana.  Continental drift explains the current biogeography of the tree.  Present day \"Glossopteris\" fossils are found in Permian strata in southeast South America, southeast Africa, all of Madagascar, northern India, all of Australia, all of New Zealand, and scattered on the southern and northern edges of Antarctica.  During the Permian, these continents were connected as Gondwana (see figure 4c) in agreement with magnetic striping, other fossil distributions, and glacial scratches pointing away from the temperate climate of the South Pole during the Permian. The history of metatherians (the clade containing marsupials and their extinct, primitive ancestors) provides an example of how evolutionary theory and the movement of continents can be combined to make predictions concerning fossil stratigraphy and distribution.  The oldest metatherian fossils are found in present-day China.  Metatherians spread westward into modern North America (still attached to Eurasia) and then to South America, which was connected to North America until around 65 mya.  Marsupials reached Australia via Antarctica about 50 mya, shortly after Australia had split off suggesting a single dispersion event of just one species.  Evolutionary theory suggests that the Australian marsupials descended from the older ones found in the Americas.  Geologic evidence suggests that between 30 and 40 million years ago South America and Australia were still part of the Southern Hemisphere super continent of Gondwana and that they were connected by land that is now part of Antarctica.  Therefore, when combining the models, scientists could predict that marsupials migrated from what is now South America, through Antarctica, and then to present-day Australia between 40 and 30 million years ago.  A first marsupial fossil of the extinct family Polydolopidae was found on Seymour Island on the Antarctic Peninsula in 1982.  Further fossils have subsequently been found, including members of the marsupial orders Didelphimorphia (opossum) and Microbiotheria, as well as ungulates and a member of the enigmatic extinct order Gondwanatheria, possibly \"Sudamerica ameghinoi\". The history of the camel provides an example of how fossil evidence can be used to reconstruct migration and subsequent evolution.  The fossil record indicates that the evolution of camelids started in North America (see figure 4e), from which, six million years ago, they migrated across the Bering Strait into Asia and then to Africa, and 3.5 million years ago through the Isthmus of Panama into South America.  Once isolated, they evolved along their own lines, giving rise to the Bactrian camel and dromedary in Asia and Africa and the llama and its relatives in South America.  Camelids then became extinct in North America at the end of the last ice age. Examples for the evidence for evolution often stem from direct observation of natural selection in the field and the laboratory.  This section is unique in that it provides a narrower context concerning the process of selection.  All of the examples provided prior to this have described the evidence that evolution has occurred, but has not provided the major underlying mechanism: natural selection.  This section explicitly provides evidence that natural selection occurs, has been replicated artificially, and can be replicated in laboratory experiments. Scientists have observed and documented a multitude of events where natural selection is in action.  The most well known examples are antibiotic resistance in the medical field along with better-known laboratory experiments documenting evolution's occurrence.  Natural selection is tantamount to common descent in that long-term occurrence and selection pressures can lead to the diversity of life on earth as found today.  All adaptations—documented and undocumented changes concerned—are caused by natural selection (and a few other minor processes).  It is well established that, \"...natural selection is a ubiquitous part of speciation...\", and is the primary driver of speciation; therefore, the following examples of natural selection \"and\" speciation will often interdepend or correspond with one another.  The examples below are only a small fraction of the actual experiments and observations. Artificial selection demonstrates the diversity that can exist among organisms that share a relatively recent common ancestor.  In artificial selection, one species is bred selectively at each generation, allowing only those organisms that exhibit desired characteristics to reproduce.  These characteristics become increasingly well developed in successive generations.  Artificial selection was successful long before science discovered the genetic basis.  Examples of artificial selection include dog breeding, genetically modified food, flower breeding, and the cultivation of foods such as wild cabbage, and others. Experimental evolution uses controlled experiments to test hypotheses and theories of evolution.  In one early example, William Dallinger set up an experiment shortly before 1880, subjecting microbes to heat with the aim of forcing adaptive changes.  His experiment ran for around seven years, and his published results were acclaimed, but he did not resume the experiment after the apparatus failed. A large-scale example of experimental evolution is Richard Lenski's multi-generation experiment with \"Escherichia coli\".  Lenski observed that some strains of \"E. coli\" evolved a complex new ability, the ability to metabolize citrate, after tens of thousands of generations.  The evolutionary biologist Jerry Coyne commented as a critique of creationism, saying, \"the thing I like most is it says you can get these complex traits evolving by a combination of unlikely events.  That's just what creationists say can't happen.\"  In addition to the metabolic changes, the different bacterial populations were found to have diverged in respect to both morphology (the overall size of the cell) and fitness (of which was measured in competition with the ancestors).  The \"E. coli\" long-term evolution experiment that began in 1988 is still in progress, and has shown adaptations including the evolution of a strain of \"E. coli\" that was able to grow on citric acid in the growth media—a trait absent in all other known forms of \"E. coli\", including the initial strain. Another study involved species of \"Daphnia\" and the enormous increase in lead pollution that occurred during the 20th century.  Lead toxicity reached an all-time high in lakes in the 1970s, and due to leads toxicity to both vertebrates and invertebrates, it could be predicted that an increase in lead pollution would lead to strong selection pressures towards lead tolerance.  In this study, researchers were able to use a method of \"resurrection ecology\" whereby they could hatch decades-old \"Daphnia\" eggs from the time in which lakes were heavily polluted with lead.  The hatchlings in the study were compared to current-day \"Daphnia\", and demonstrated \"dramatic fitness differences between old and modern phenotypes when confronted with a widespread historical environmental stressor\".  Essentially, the modern-day \"Daphnia\" were unable to resist or tolerate high levels of lead (this is due to the huge reduction of lead pollution in 21st century lakes).  The old hatchlings; however, were able to tolerate high lead pollution.  The authors concluded that \"by employing the techniques of resurrection ecology, we were able to show clear phenotypic change over decades...\". A classic example was the phenotypic change, light-to-dark color adaptation, in the peppered moth, due to pollution from the Industrial Revolution in England. The development and spread of antibiotic-resistant bacteria is evidence for the process of evolution of species.  Thus the appearance of vancomycin-resistant \"Staphylococcus aureus\", and the danger it poses to hospital patients, is a direct result of evolution through natural selection.  The rise of \"Shigella\" strains resistant to the synthetic antibiotic class of sulfonamides also demonstrates the generation of new information as an evolutionary process.  Similarly, the appearance of DDT resistance in various forms of \"Anopheles\" mosquitoes, and the appearance of myxomatosis resistance in breeding rabbit populations in Australia, are both evidence of the existence of evolution in situations of evolutionary selection pressure in species in which generations occur rapidly. Additionally, all classes of microbes develop resistance: including fungi (antifungal resistance), viruses (antiviral resistance), protozoa (antiprotozoal resistance), and bacteria (antibiotic resistance).  This is to be expected when considering that all life exhibits universal genetic code and is therefore subject to the process of evolution through its various mechanisms. Another example of organisms adapting to human-caused conditions are Nylon-eating bacteria: a strain of \"Flavobacterium\" that are capable of digesting certain byproducts of nylon 6 manufacturing.  There is scientific consensus that the capacity to synthesize nylonase most probably developed as a single-step mutation that survived because it improved the fitness of the bacteria possessing the mutation.  This is seen as a good example of evolution through mutation and natural selection that has been observed as it occurs and could not have come about until the production of nylon by humans. Both subspecies \"Mimulus aurantiacus puniceus\" (red-flowered) and \"Mimulus aurantiacus australis\" (yellow-flowered) of monkeyflowers are isolated due to the preferences of their hummingbird and hawkmoth pollinators.  The radiation of \"M. aurantiacus\" subspecies are mostly yellow colored; however, both \"M. a.\" ssp.  \"puniceus\" and \"M. a.\" ssp.  \"flemingii\" are red.  Phylogenetic analysis suggests two independent origins of red-colored flowers that arose due to \"cis\"-regulatory mutations in the gene \"MaMyb2\" that is present in all \"M. aurantiacus\" subspecies.  Further research suggested that two independent mutations did not take place, but one \"MaMyb2\" allele was transferred via introgressive hybridization.  This study presents an example of the overlap of research in various disciplines.  Gene isolation and \"cis\"-regulatory functions; phylogenetic analysis; geographic location and pollinator preference; and species hybridization and speciation are just some of the areas in which data can be obtained to document the occurrence of evolution. Like the codfish, human-caused pollution can come in different forms.  Radiotrophic fungi is a perfect example of natural selection taking place after a chemical accident.  Radiotrophic fungi appears to use the pigment melanin to convert gamma radiation into chemical energy for growth and were first discovered in 2007 as black molds growing inside and around the Chernobyl Nuclear Power Plant.  Research at the Albert Einstein College of Medicine showed that three melanin-containing fungi, \"Cladosporium sphaerospermum\", \"Wangiella dermatitidis\", and \"Cryptococcus neoformans\", increased in biomass and accumulated acetate faster in an environment in which the radiation level was 500 times higher than in the normal environment. While studying guppies (\"Poecilia reticulata\") in Trinidad, biologist John Endler detected selection at work on the fish populations.  To rule out alternative possibilities, Endler set up a highly controlled experiment to mimic the natural habitat by constructing ten ponds within a laboratory greenhouse at Princeton University.  Each pond contained gravel to exactly match that of the natural ponds.  After capturing a random sample of guppies from ponds in Trinidad, he raised and mixed them to create similar genetically diverse populations and measured each fish (spot length, spot height, spot area, relative spot length, relative spot height, total patch area, and standard body lengths).  For the experiment he added \"Crenicichla alta\" (\"P. reticulata\"'s main predator) in four of the ponds, \"Rivulus hartii\" (a non-predator fish) in four of the ponds, and left the remaining two ponds empty with only the guppies.  After 10 generations, comparisons were made between each pond's guppy populations and measurements were taken again.  Endler found that the populations had evolved dramatically different color patterns in the control and non-predator pools and drab color patterns in the predator pool.  Predation pressure had caused a selection against standing out from background gravel. In parallel, during this experiment, Endler conducted a field experiment in Trinidad where he caught guppies from ponds where they had predators and relocated them to ponds upstream where the predators did not live.  After 15 generations, Endler found that the relocated guppies had evolved dramatic and colorful patterns.  Essentially, both experiments showed convergence due to similar selection pressures (i.e. predator selection against contrasting color patterns and sexual selection for contrasting color patterns). In a later study by David Reznick, the field population was examined 11 years later after Endler relocated the guppies to high streams.  The study found that the populations has evolved in a number of different ways: bright color patterns, late maturation, larger sizes, smaller litter sizes, and larger offspring within litters.  Further studies of \"P. reticulata\" and their predators in the streams of Trinidad have indicated that varying modes of selection through predation have not only changed the guppies color patterns, sizes, and behaviors, but their life histories and life history patterns. Natural selection is observed in contemporary human populations, with recent findings demonstrating that the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles.  Scientists postulate one of the reasons for the rapid selection of this genetic variant is the lethality of the disease in non-immune persons.  Other reported evolutionary trends in other populations include a lengthening of the reproductive period, reduction in cholesterol levels, blood glucose and blood pressure. A well known example of selection occurring in human populations is lactose tolerance.  Lactose intolerance is the inability to metabolize lactose, because of a lack of the required enzyme lactase in the digestive system.  The normal mammalian condition is for the young of a species to experience reduced lactase production at the end of the weaning period (a species-specific length of time).  In humans, in non-dairy consuming societies, lactase production usually drops about 90% during the first four years of life, although the exact drop over time varies widely.  Lactase activity persistence in adults is associated with two polymorphisms: C/T 13910 and G/A 22018 located in the \"MCM6\" gene.  This gene difference eliminates the shutdown in lactase production, making it possible for members of these populations to continue consumption of raw milk and other fresh and fermented dairy products throughout their lives without difficulty.  This appears to be an evolutionarily recent (around 10,000 years ago [and 7,500 years ago in Europe]) adaptation to dairy consumption, and has occurred independently in both northern Europe and east Africa in populations with a historically pastoral lifestyle. In 1971, ten adult specimens of \"Podarcis sicula\" (the Italian wall lizard) were transported from the Croatian island of Pod Kopište to the island Pod Mrčaru (about 3.5 km to the east).  Both islands lie in the Adriatic Sea near Lastovo, where the lizards founded a new bottlenecked population.  The two islands have similar size, elevation, microclimate, and a general absence of terrestrial predators and the \"P. sicula\" expanded for decades without human interference, even out-competing the (now locally extinct) \"Podarcis melisellensis\" population. In the 1990s, scientists returned to Pod Mrčaru and found that the lizards currently occupying Mrčaru differ greatly from those on Kopište.  While mitochondrial DNA analyses have verified that \"P. sicula\" currently on Mrčaru are genetically very similar to the Kopište source population, the new Mrčaru population of \"P. sicula\" was described as having a larger average size, shorter hind limbs, lower maximal sprint speed and altered response to simulated predatory attacks compared to the original Kopište population.  These population changes in morphology and behavior were attributed to \"relaxed predation intensity\" and greater protection from vegetation on Mrčaru. In 2008, further analysis revealed that the Mrčaru population of \"P. sicula\" have significantly different head morphology (longer, wider, and taller heads) and increased bite force compared to the original Kopište population.  This change in head shape corresponded with a shift in diet: Kopište \"P. sicula\" are primarily insectivorous, but those on Mrčaru eat substantially more plant matter.  The changes in foraging style may have contributed to a greater population density and decreased territorial behavior of the Mrčaru population. Another difference found between the two populations was the discovery, in the Mrčaru lizards, of cecal valves, which slow down food passage and provide fermenting chambers, allowing commensal microorganisms to convert cellulose to nutrients digestible by the lizards.  Additionally, the researchers discovered that nematodes were common in the guts of Mrčaru lizards, but absent from Kopište \"P. sicula\", which do not have cecal valves.  The cecal valves, which occur in less than 1 percent of all known species of scaled reptiles, have been described as an \"adaptive novelty, a brand new feature not present in the ancestral population and newly evolved in these lizards\". A similar study was also done regarding the polycyclic aromatic hydrocarbons (PAHs) that pollute the waters of the Elizabeth River in Portsmouth, Virginia.  This chemical is a product of creosote, a type of tar.  The Atlantic killifish (\"Fundulus heteroclitus\") has evolved a resistance to PAHs involving the AHR gene (the same gene involved in the tomcods).  This particular study focused on the resistance to \"acute toxicity and cardiac teratogenesis\" caused by PAHs. that mutated within the tomcods in the Hudson River. An example involving the direct observation of gene modification due to selection pressures is the resistance to PCBs in codfish.  After General Electric dumped polychlorinated biphenyls (PCBs) in the Hudson River from 1947 through 1976, tomcods (\"Microgadus tomcod\") living in the river were found to have evolved an increased resistance to the compound's toxic effects.  The tolerance to the toxins is due to a change in the coding section of specific gene.  Genetic samples were taken from the cods from 8 different rivers in the New England region: the St. Lawrence River, Miramichi River, Margaree River, Squamscott River, Niantic River, the Shinnecock Basic, the Hudson River, and the Hackensack River.  Genetic analysis found that in the population of tomcods in the four southernmost rivers, the gene AHR2 (aryl hydrocarbon receptor 2) was present as an allele with a difference of two amino acid deletions.  This deletion conferred a resistance to PCB in the fish species and was found in 99% of Hudson River tomcods, 92% in the Hackensack River, 6% in the Niantic River, and 5% in Shinnecock Bay.  This pattern along the sampled bodies of waters infers a direct correlation of selective pressures leading to the evolution of PCB resistance in Atlantic tomcod fish. Urban wildlife is a broad and easily observable case of human-caused selection pressure on wildlife.  With the growth in human habitats, different animals have adapted to survive within these urban environments.  These types of environments can exert selection pressures on organisms, often leading to new adaptations.  For example, the weed \"Crepis sancta\", found in France, has two types of seed, heavy and fluffy.  The heavy ones land nearby to the parent plant, whereas fluffy seeds float further away on the wind.  In urban environments, seeds that float far often land on infertile concrete.  Within about 5–12 generations, the weed evolves to produce significantly heavier seeds than its rural relatives.  Other examples of urban wildlife are rock pigeons and species of crows adapting to city environments around the world; African penguins in Simon's Town; baboons in South Africa; and a variety of insects living in human habitations.  Studies have been conducted and have found striking changes to animals' (more specifically mammals') behavior and physical brain size due to their interactions with human-created environments. Animals that exhibit ecotonal variations allow for research concerning the mechanisms that maintain population differentiation.  A wealth of information about natural selection, genotypic, and phenotypic variation; adaptation and ecomorphology; and social signaling has been acquired from the studies of three species of lizards located in the White Sands desert of New Mexico. \" Holbrookia maculata\", \"Aspidoscelis inornata\", and \"Sceloporus undulatus\" exhibit ecotonal populations that match both the dark soils and the white sands in the region.  Research conducted on these species has found significant phenotypic and genotypic differences between the dark and light populations due to strong selection pressures.  For example, H. maculata exhibits the strongest phenotypic difference (matches best with the substrate) of the light colored population coinciding with the least amount of gene flow between the populations and the highest genetic differences when compared to the other two lizard species. New Mexico’s White Sands are a recent geologic formation (approximately 6000 years old to possibly 2000 years old).  This recent origin of the sands suggests that species exhibiting lighter-colored variations have evolved in a relatively short time frame.  The three lizard species previously mentioned have been found to display variable social signal coloration in coexistence with their ecotonal variants.  Not only have the three species convergently evolved their lighter variants due to the selection pressures from the environment, they’ve also evolved ecomorphological differences: morphology, behavior (in is case, escape behavior), and performance (in this case, sprint speed) collectively.  Roches’ work found surprising results in the escape behavior of \"H. maculata\" and \"S. undulatus\".  When dark morphs were placed on white sands, their startle response was significantly diminished.  This result could be due to varying factors relating to sand temperature or visual acuity; however, regardless of the cause, “…failure of mismatched lizards to sprint could be maladaptive when faced with a predator”. Speciation is the evolutionary process by which new biological species arise.  Biologists research species using different theoretical frameworks for what constitutes a species (see species problem and species complex) and there exists debate with regard to delineation.  Nevertheless, much of the current research suggests that, \"...speciation is a process of emerging genealogical distinctness, rather than a discontinuity affecting all genes simultaneously\" and, in allopatry (the most common form of speciation), \"reproductive isolation is a byproduct of evolutionary change in isolated populations, and thus can be considered an evolutionary accident\".  Speciation occurs as the result of the latter (allopatry); however, a variety of differing agents have been documented and are often defined and classified in various forms (e.g. peripatric, parapatric, sympatric, polyploidization, hybridization, etc.).  Instances of speciation have been observed in both nature and the laboratory.  A.-B Florin and A. Ödeen note that, \"strong laboratory evidence for allopatric speciation is lacking...\"; however, contrary to laboratory studies (focused specifically on models of allopatric speciation), \"speciation most definitely occurs; [and] the vast amount of evidence from nature makes it unreasonable to argue otherwise\".  Coyne and Orr compiled a list of 19 laboratory experiments on \"Drosophila\" presenting examples of allopatric speciation by divergent selection concluding that, \"reproductive isolation in allopatry can evolve as a byproduct of divergent selection\". Research documenting speciation is abundant.  Biologists have documented numerous examples of speciation in nature—with evolution having produced far more species than any observer would consider necessary.  For example, there are well over 350,000 described species of beetles.  Great examples of speciation come from the observations of island biogeography and the process of adaptive radiation, both explained previously.  Evidence of common descent can also be found through paleontological studies of speciation within geologic strata.  The examples described below represent different modes of speciation and provide strong evidence for common descent.  It is important to acknowledge that not all speciation research directly observes divergence from \"start-to-finish\".  This is by virtue of research delimitation and definition ambiguity, and occasionally leads research towards historical reconstructions.  In light of this, examples abound, and the following are by no means exhaustive—comprising only a small fraction of the instances observed.  Once again, take note of the established fact that, \"...natural selection is a ubiquitous part of speciation...\", and is the primary driver of speciation, so; hereinafter, examples of speciation will often interdepend and correspond with selection. Limitations exist within the fossil record when considering the concept of what constitutes a species.  Paleontologists largely rely on a different framework: the morphological species concept.  Due to the absence of information such as reproductive behavior or genetic material in fossils, paleontologists distinguish species by their phenotypic differences.  Extensive investigation of the fossil record has led to numerous theories concerning speciation (in the context of paleontology) with many of the studies suggesting that stasis, punctuation, and lineage branching are common.  In 1995, D. H. Erwin, et al. published a major work—\"New Approaches to Speciation in the Fossil Record\"—which compiled 58 studies of fossil speciation (between 1972 and 1995) finding most of the examples suggesting stasis (involving anagenesis or punctuation) and 16 studies suggesting speciation.  Despite stasis appearing to be the predominate conclusion at first glance, this particular meta-study investigated deeper, concluding that, \"...no single pattern appears dominate...\" with \"...the preponderance of studies illustrating \"both\" stasis and gradualism in the history of a single lineage\".  Many of the studies conducted utilize seafloor sediments that can provide a significant amount of data concerning planktonic microfossils.  The succession of fossils in stratigraphy can be used to determine evolutionary trends among fossil organisms.  In addition, incidences of speciation can be interpreted from the data and numerous studies have been conducted documenting both morphological evolution and speciation. Extensive research on the planktonic foraminifer \"Globorotalia truncatulinoides\" has provided insight into paleobiogeographical and paleoenvironmental studies alongside the relationship between the environment and evolution.  In an extensive study of the paleobiogeography of \"G. truncatulinoides\", researchers found evidence that suggested the formation of a new species (via the sympatric speciation framework).  Cores taken of the sediment containing the three species \"G. crassaformis\", \"G. tosaensis\", and \"G. truncatulinoides\" found that before 2.7 Ma, only \"G. crassaformis\" and \"G. tosaensis\" existed.  A speciation event occurred at that time, whereby intermediate forms existed for quite some time.  Eventually \"G. tosaensis\" disappears from the record (suggesting extinction) but exists as an intermediate between the extant \"G. crassaformis\" and \"G. truncatulinoides\".  This record of the fossils also matched the already existing phylogeny constructed by morphological characters of the three species.  See figure 6a. In a large study of five species of radiolarians (\"Calocycletta caepa\", \"Pterocanium prismatium\", \"Pseudoculous vema\", \"Eucyrtidium calvertense\", and \"Eucyrtidium matuyamai\"), the researchers documented considerable evolutionary change in each lineage.  Alongside this, trends with the closely related species \"E. calvertense\" and \"E. matuyamai\" showed that about 1.9 Mya \"E. calvertense\" invaded a new region of the Pacific, becoming isolated from the main population.  The stratigraphy of this species clearly shows that this isolated population evolved into \"E. Matuyamai\".  It then reinvaded the region of the still-existing and static \"E. calvertense\" population whereby a sudden decrease in body size occurred.  Eventually the invader \"E. matuyamai\" disappeared from the stratum (presumably due to extinction) coinciding with a desistance of size reduction of the \"E. calvertense\" population.  From that point on, the change in size leveled to a constant.  The authors suggest competition-induced character displacement. Researchers conducted measurements on 5,000 \"Rhizosolenia\" (a planktonic diatom) specimens from eight sedimentary cores in the Pacific Ocean.  The core samples spanned two million years and were chronologized using sedimentary magnetic field reversal measurements.  All the core samples yielded a similar pattern of divergence: with a single lineage (\"R. bergonii\") occurring before 3.1 Mya and two morphologically distinct lineages (daughter species: \"R. praebergonii\") appearing after.  The parameters used to measure the samples were consistent throughout each core.  An additional study of the daughter species \"R. praebergonii\" found that, after the divergence, it invaded the Indian Ocean. A recent study was conducted involving the planktonic foraminifer Turborotalia.  The authors extracted “51 stratigraphically ordered samples from a site within the oceanographically stable tropical North Pacific gyre”.  Two hundred individual species were examined using ten specific morphological traits (size, compression index, chamber aspect ratio, chamber inflation, aperture aspect ratio, test height, test expansion, umbilical angle, coiling direction, and the number of chambers in the final whorl).  Utilizing multivariate statistical clustering methods, the study found that the species continued to evolve non-directionally within the Eocene from 45 Ma to about 36 Ma.  However, from 36 Ma to approximately 34 Ma, the stratigraphic layers showed two distinct clusters with significantly defining characteristics distinguishing one another from a single species.  The authors concluded that speciation must have occurred and that the two new species were ancestral to the prior species.  Just as in most of evolutionary biology, this example represents the interdisciplinary nature of the field and the necessary collection of data from various fields (e.g. oceanography, paleontology) and the integration of mathematical analysis (e.g. biometry). There exists evidence for vertebrate speciation despite limitations imposed by the fossil record.  Studies have been conducted documenting similar patterns seen in marine invertebrates.  For example, extensive research documenting rates of morphological change, evolutionary trends, and speciation patterns in small mammals has significantly contributed to the scientific literature; once more, demonstrating that evolution (and speciation) occurred in the past and lends support common ancestry. A study of four mammalian genera: \"Hyopsodus\", \"Pelycodus\", \"Haplomylus\" (three from the Eocene), and \"Plesiadapis\" (from the Paleocene) found that—through a large number of stratigraphic layers and specimen sampling—each group exhibited, \"gradual phyletic evolution, overall size increase, iterative evolution of small species, and character divergence following the origin of each new lineage\".  The authors of this study concluded that speciation was discernible.  In another study concerning morphological trends and rates of evolution found that the European arvicolid rodent radiated into 52 distinct lineages over a time frame of 5 million years while documenting examples of phyletic gradualism, punctuation, and stasis. William R. Rice and George W. Salt found experimental evidence of sympatric speciation in the common fruit fly.  They collected a population of \"Drosophila melanogaster\" from Davis, California and placed the pupae into a habitat maze.  Newborn flies had to investigate the maze to find food.  The flies had three choices to take in finding food.  Light and dark (phototaxis), up and down (geotaxis), and the scent of acetaldehyde and the scent of ethanol (chemotaxis) were the three options.  This eventually divided the flies into 42 spatio-temporal habitats. They then cultured two strains that chose opposite habitats.  One of the strains emerged early, immediately flying upward in the dark attracted to the acetaldehyde.  The other strain emerged late and immediately flew downward, attracted to light and ethanol.  Pupae from the two strains were then placed together in the maze and allowed to mate at the food site.  They then were collected.  A selective penalty was imposed on the female flies that switched habitats.  This entailed that none of their gametes would pass on to the next generation.  After 25 generations of this mating test, it showed reproductive isolation between the two strains.  They repeated the experiment again without creating the penalty against habitat switching and the result was the same; reproductive isolation was produced. A study of the gall-forming wasp species \"Belonocnema treatae\" found that populations inhabiting different host plants (\"Quercus geminata\" and \"Q. Virginiana\") exhibited different body size and gall morphology alongside a strong expression of sexual isolation.  The study hypothesized that \"B. treatae\" populations inhabiting different host plants would show evidence of divergent selection promoting speciation.  The researchers sampled gall wasp species and oak tree localities, measured body size (right hand tibia of each wasp), and counted gall chamber numbers.  In addition to measurements, they conducted mating assays and statistical analyses.  Genetic analysis was also conducted on two mtDNA sites (416 base pairs from cytochrome C and 593 base pairs from cytochrome oxidase ) to “control for the confounding effects of time since divergence among allopatric populations”. In an additional study, the researchers studied two gall wasp species \"B. treatae\" and \"Disholcaspis quercusvirens\" and found strong morphological and behavioral variation among host-associated populations.  This study further confounded prerequisites to speciation. One example of evolution at work is the case of the hawthorn fly, \"Rhagoletis pomonella\", also known as the apple maggot fly, which appears to be undergoing sympatric speciation.  Different populations of hawthorn fly feed on different fruits.  A distinct population emerged in North America in the 19th century some time after apples, a non-native species, were introduced.  This apple-feeding population normally feeds only on apples and not on the historically preferred fruit of hawthorns.  The current hawthorn feeding population does not normally feed on apples.  Some evidence, such as the fact that six out of thirteen allozyme loci are different, that hawthorn flies mature later in the season and take longer to mature than apple flies; and that there is little evidence of interbreeding (researchers have documented a 4–6% hybridization rate) suggests that speciation is occurring. The London Underground mosquito is a species of mosquito in the genus \"Culex\" found in the London Underground.  It evolved from the overground species \"Culex pipiens\".  This mosquito, although first discovered in the London Underground system, has been found in underground systems around the world.  It is suggested that it may have adapted to human-made underground systems since the last century from local above-ground \"Culex pipiens\", although more recent evidence suggests that it is a southern mosquito variety related to \"Culex pipiens\" that has adapted to the warm underground spaces of northern cities. The two species have very different behaviours, are extremely difficult to mate, and with different allele frequency, consistent with genetic drift during a founder event.  More specifically, this mosquito, \"Culex pipiens molestus\", breeds all-year round, is cold intolerant, and bites rats, mice, and humans, in contrast to the above ground species \"Culex pipiens\" that is cold tolerant, hibernates in the winter, and bites only birds.  When the two varieties were cross-bred the eggs were infertile suggesting reproductive isolation. The genetic data indicates that the \"molestus\" form in the London Underground mosquito appears to have a common ancestry, rather than the population at each station being related to the nearest aboveground population (i.e. the \"pipiens\" form).  Byrne and Nichols' working hypothesis was that adaptation to the underground environment had occurred locally in London only once.  These widely separated populations are distinguished by very minor genetic differences, which suggest that the molestus form developed: a single mtDNA difference shared among the underground populations of ten Russian cities; a single fixed microsatellite difference in populations spanning Europe, Japan, Australia, the middle East and Atlantic islands. Debate exists determining when the isthmus of Panama closed.  Much of the evidence supports a closure approximately 2.7 to 3.5 mya using \"...multiple lines of evidence and independent surveys\".  However, a recent study suggests an earlier, transient bridge existed 13 to 15 mya.  Regardless of the timing of the isthmus closer, biologists can study the species on the Pacific and Caribbean sides in, what has been called, \"one of the greatest natural experiments in evolution.\"  Studies of snapping shrimp in the genus \"Alpheus\" have provided direct evidence of allopatric speciation events, and contributed to the literature concerning rates of molecular evolution.  Phylogenetic reconstructions using \"multilocus datasets and coalescent-based analytical methods\" support the relationships of the species in the group and molecular clock techniques support the separation of 15 pairs of \"Alpheus\" species between 3 and 15 million years ago. The botanist Verne Grant pioneered the field of plant speciation with his research and major publications on the topic.  As stated before, many biologists rely on the biological species concept, with some modern researchers utilizing the phylogenetic species concept.  Debate exists in the field concerning which framework should be applied in the research.  Regardless, reproductive isolation is the primary role in the process of speciation and has been studied extensively by biologists in their respective disciplines. Both hybridization and polyploidy have also been found to be major contributors to plant speciation.  With the advent of molecular markers, \"hybridization [is] considerably more frequent than previously believed\".  In addition to these two modes leading to speciation, pollinator preference and isolation, chromosomal rearrangements, and divergent natural selection have become critical to the speciation of plants.  Furthermore, recent research suggests that sexual selection, epigenetic drivers, and the creation of incompatible allele combinations caused by balancing selection also contribute to the formation of new species.  Instances of these modes have been researched in both the laboratory and in nature.  Studies have also suggested that, due to \"the sessile nature of plants... [it increases] the relative importance of ecological speciation...\" Hybridization between two different species sometimes leads to a distinct phenotype.  This phenotype can also be fitter than the parental lineage and as such, natural selection may then favor these individuals.  Eventually, if reproductive isolation is achieved, it may lead to a separate species.  However, reproductive isolation between hybrids and their parents is particularly difficult to achieve and thus hybrid speciation is considered a rare event.  However, hybridization resulting in reproductive isolation is considered an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis.  Polyploids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.  Hybridization without change in chromosome number is called homoploid hybrid speciation.  It is considered very rare but has been shown in \"Heliconius\" butterflies and sunflowers. Polyploid speciation, which involves changes in chromosome number, is a more common phenomenon, especially in plant species. Polyploidy is a mechanism that has caused many rapid speciation events in sympatry because offspring of, for example, tetraploid x diploid matings often result in triploid sterile progeny.  Not all polyploids are reproductively isolated from their parental plants, and gene flow may still occur for example through triploid hybrid x diploid matings that produce tetraploids, or matings between meiotically unreduced gametes from diploids and gametes from tetraploids.  It has been suggested that many of the existing plant and most animal species have undergone an event of polyploidization in their evolutionary history.  Reproduction of successful polyploid species is sometimes asexual, by parthenogenesis or apomixis, as for unknown reasons many asexual organisms are polyploid.  Rare instances of polyploid mammals are known, but most often result in prenatal death. Researchers consider reproductive isolation as key to speciation.  A major aspect of speciation research is to determine the nature of the barriers that inhibit reproduction.  Botanists often consider the zoological classifications of prezygotic and postzygotic barriers as inadequate.  The examples provided below give insight into the process of speciation. The creation of a new allopolyploid species (\"Mimulus peregrinus\") was observed on the banks of the Shortcleuch Water—a river in Leadhills, South Lanarkshire, Scotland.  Parented from the cross of the two species \"Mimulus guttatus\" (containing 14 pairs of chromosomes) and \"Mimulus luteus\" (containing 30-31 pairs from a chromosome duplication), \"M. peregrinus\" has six copies of its chromosomes (caused by the duplication of the sterile hybrid triploid).  Due to the nature of these species, they have the ability to self-fertilize.  Because of its number of chromosomes it is not able to pair with \"M. guttatus\", \"M. luteus\", or their sterile triploid offspring.  \"M. peregrinus\" will either die, producing no offspring, or reproduce with itself effectively leading to a new species. \"Raphanobrassica\" includes all intergeneric hybrids between the genera \"Raphanus\" (radish) and \"Brassica\" (cabbages, etc.).  The \"Raphanobrassica\" is an allopolyploid cross between the radish (\"Raphanus sativus\") and cabbage (\"Brassica oleracea\").  Plants of this parentage are now known as radicole.  Two other fertile forms of \"Raphanobrassica\" are known.  Raparadish, an allopolyploid hybrid between \"Raphanus sativus\" and \"Brassica rapa\" is grown as a fodder crop.  \"Raphanofortii\" is the allopolyploid hybrid between \"Brassica tournefortii\" and \"Raphanus caudatus\".  The \"Raphanobrassica\" is a fascinating plant, because (in spite of its hybrid nature), it is not sterile.  This has led some botanists to propose that the accidental hybridization of a flower by pollen of another species in nature could be a mechanism of speciation common in higher plants. The Welsh groundsel is an allopolyploid, a plant that contains sets of chromosomes originating from two different species.  Its ancestor was \"Senecio × baxteri\", an infertile hybrid that can arise spontaneously when the closely related groundsel (\"Senecio vulgaris\") and Oxford ragwort (\"Senecio squalidus\") grow alongside each other.  Sometime in the early 20th century, an accidental doubling of the number of chromosomes in an \"S. × baxteri\" plant led to the formation of a new fertile species. The York groundsel (\"Senecio eboracensis\") is a hybrid species of the self-incompatible \"Senecio squalidus\" (also known as Oxford ragwort) and the self-compatible \"Senecio vulgaris\" (also known as common groundsel).  Like \"S. vulgaris\", \"S. eboracensis\" is self-compatible, however, it shows little or no natural crossing with its parent species, and is therefore reproductively isolated, indicating that strong breed barriers exist between this new hybrid and its parents.  It resulted from a backcrossing of the F1 hybrid of its parents to \"S. vulgaris\".  \"S. vulgaris\" is native to Britain, while \"S. squalidus\" was introduced from Sicily in the early 18th century; therefore, \"S. eboracensis\" has speciated from those two species within the last 300 years. Other hybrids descended from the same two parents are known.  Some are infertile, such as \"S.\" x \"baxteri\".  Other fertile hybrids are also known, including \"S. vulgaris\" var.  \"hibernicus\", now common in Britain, and the allohexaploid \"S. cambrensis\", which according to molecular evidence probably originated independently at least three times in different locations.  Morphological and genetic evidence support the status of \"S. eboracensis\" as separate from other known hybrids. Kirsten Bomblies et al. from the Max Planck Institute for Developmental Biology discovered that two genes passed down by each parent of the thale cress plant, \"Arabidopsis thaliana\".  When the genes are passed down, it ignites a reaction in the hybrid plant that turns its own immune system against it.  In the parents, the genes were not detrimental, but they evolved separately to react defectively when combined.  To test this, Bomblies crossed 280 genetically different strains of \"Arabidopsis\" in 861 distinct ways and found that 2 percent of the resulting hybrids were necrotic.  Along with allocating the same indicators, the 20 plants also shared a comparable collection of genetic activity in a group of 1,080 genes.  In almost all of the cases, Bomblies discovered that only two genes were required to cause the autoimmune response.  Bomblies looked at one hybrid in detail and found that one of the two genes belonged to the NB-LRR class, a common group of disease resistance genes involved in recognizing new infections.  When Bomblies removed the problematic gene, the hybrids developed normally.  Over successive generations, these incompatibilities could create divisions between different plant strains, reducing their chances of successful mating and turning distinct strains into separate species. \"Tragopogon\" is one example where hybrid speciation has been observed.  In the early 20th century, humans introduced three species of salsify into North America.  These species, the western salsify (\"Tragopogon dubius\"), the meadow salsify (\"Tragopogon pratensis\"), and the oyster plant (\"Tragopogon porrifolius\"), are now common weeds in urban wastelands.  In the 1950s, botanists found two new species in the regions of Idaho and Washington, where the three already known species overlapped.  One new species, \"Tragopogon miscellus\", is a tetraploid hybrid of \"T. dubius\" and \"T. pratensis\".  The other new species, \"Tragopogon mirus\", is also an allopolyploid, but its ancestors were \"T. dubius\" and \"T. porrifolius\".  These new species are usually referred to as \"the Ownbey hybrids\" after the botanist who first described them.  The \"T. mirus\" population grows mainly by reproduction of its own members, but additional episodes of hybridization continue to add to the \"T. mirus\" population. \"T. dubius\" and \"T. pratensis\" mated in Europe but were never able to hybridize.  A study published in March 2011 found that when these two plants were introduced to North America in the 1920s, they mated and doubled the number of chromosomes in there hybrid \"Tragopogon miscellus\" allowing for a \"reset\" of its genes, which in turn, allows for greater genetic variation.  Professor Doug Soltis of the University of Florida said, \"We caught evolution in the act…New and diverse patterns of gene expression may allow the new species to rapidly adapt in new environments\".  This observable event of speciation through hybridization further advances the evidence for the common descent of organisms and the time frame in which the new species arose in its new environment.  The hybridizations have been reproduced artificially in laboratories from 2004 to present day. The bird species, \"Sylvia atricapilla\", commonly referred to as blackcaps, lives in Germany and flies southwest to Spain while a smaller group flies northwest to Great Britain during the winter.  Gregor Rolshausen from the University of Freiburg found that the genetic separation of the two populations is already in progress.  The differences found have arisen in about 30 generations.  With DNA sequencing, the individuals can be assigned to a correct group with an 85% accuracy.  Stuart Bearhop from the University of Exeter reported that birds wintering in England tend to mate only among themselves, and not usually with those wintering in the Mediterranean.  It is still inference to say that the populations will become two different species, but researchers expect it due to the continued genetic and geographic separation. The shortfin molly (\"Poecilia mexicana\") is a small fish that lives in the Sulfur Caves of Mexico.  Years of study on the species have found that two distinct populations of mollies—the dark interior fish and the bright surface water fish—are becoming more genetically divergent.  The populations have no obvious barrier separating the two; however, it was found that the mollies are hunted by a large water bug (\"Belostoma spp\").  Tobler collected the bug and both types of mollies, placed them in large plastic bottles, and put them back in the cave.  After a day, it was found that, in the light, the cave-adapted fish endured the most damage, with four out of every five stab-wounds from the water bugs sharp mouthparts.  In the dark, the situation was the opposite.  The mollies' senses can detect a predator's threat in their own habitats, but not in the other ones.  Moving from one habitat to the other significantly increases the risk of dying.  Tobler plans on further experiments, but believes that it is a good example of the rise of a new species. A remarkable example of natural selection, geographic isolation, and speciation in progress is the relationship between the polar bear (\"Ursus maritimus\") and the brown bear (\"Ursus arctos\").  Considered separate species throughout their ranges; however, it has been documented that they possess the capability to interbreed and produce fertile offspring.  This introgressive hybridization has occurred both in the wild and in captivity and has been documented and verified with DNA testing.  The oldest known fossil evidence of polar bears dates around 130,000 to 110,000 years ago; however, molecular data has revealed varying estimates of divergence time.  Mitochondrial DNA analysis has given an estimate of 150,000 years ago while nuclear genome analysis has shown an approximate divergence of 603,000 years ago.  Recent research using the complete genomes (rather than mtDNA or partial nuclear genomes) establishes the divergence of polar and brown bears between 479-343 thousand years ago.  Despite the differences in divergence rates, molecular research suggests the sister species have undergone a highly complex process of speciation and admixture between the two. Polar bears have acquired significant anatomical and physiological differences from the brown bear that allow it to comfortably survive in conditions that the brown bear likely could not.  Notable examples include the ability to swim sixty miles or more at a time in freezing waters, fur that blends with the snow, and to stay warm in the arctic environment, an elongated neck that makes it easier to keep their heads above water while swimming, and oversized and heavy-matted webbed feet that act as paddles when swimming.  It has also evolved small papillae and vacuole-like suction cups on the soles to make them less likely to slip on the ice, alongside smaller ears for a reduction of heat loss, eyelids that act like sunglasses, accommodations for their all-meat diet, a large stomach capacity to enable opportunistic feeding, and the ability to fast for up to nine months while recycling their urea.  This example presents a macro-evolutionary change involving an amalgamation of several fields of evolutionary biology, e.g. adaptation through natural selection, geographic isolation, speciation, and hybridization. Computer science allows the iteration of self-changing complex systems to be studied, allowing a mathematical understanding of the nature of the processes behind evolution; providing evidence for the hidden causes of known evolutionary events.  The evolution of specific cellular mechanisms like spliceosomes that can turn the cell's genome into a vast workshop of billions of interchangeable parts that can create tools that create tools that create tools that create us can be studied for the first time in an exact way. \"It has taken more than five decades, but the electronic computer is now powerful enough to simulate evolution,\" assisting bioinformatics in its attempt to solve biological problems. Computational evolutionary biology has enabled researchers to trace the evolution of a large number of organisms by measuring changes in their DNA, rather than through physical taxonomy or physiological observations alone.  It has compared entire genomes permitting the study of more complex evolutionary events, such as gene duplication, horizontal gene transfer, and the prediction of factors important in speciation.  It has also helped build complex computational models of populations to predict the outcome of the system over time and track and share information on an increasingly large number of species and organisms. Future endeavors are to reconstruct a now more complex tree of life. Christoph Adami, a professor at the Keck Graduate Institute made this point in \"Evolution of biological complexity\": David J. Earl and Michael W. Deem—professors at Rice University made this point in \"Evolvability is a selectable trait\": \"Computer simulations of the evolution of linear sequences have demonstrated the importance of recombination of blocks of sequence rather than point mutagenesis alone.  Repeated cycles of point mutagenesis, recombination, and selection should allow \"in vitro\" molecular evolution of complex sequences, such as proteins.\"  Evolutionary molecular engineering, also called directed evolution or \"in vitro\" molecular evolution involves the iterated cycle of mutation, multiplication with recombination, and selection of the fittest of individual molecules (proteins, DNA, and RNA).  Natural evolution can be relived showing us possible paths from catalytic cycles based on proteins to based on RNA to based on DNA. Richard Lenski, Charles Ofria, et al. at Michigan State University developed an artificial life computer program with the ability to detail the evolution of complex systems.  The system uses values set to determine random mutations and allows for the effect of natural selection to conserve beneficial traits.  The program was dubbed Avida and starts with an artificial petri dish where organisms reproduce and perform mathematical calculations to acquire rewards of more computer time for replication.  The program randomly adds mutations to copies of the artificial organisms to allow for natural selection.  As the artificial life reproduced, different lines adapted and evolved depending on their set environments.  The beneficial side to the program is that it parallels that of real life at rapid speeds.\n\nChernobyl disaster The Chernobyl disaster, also referred to as the Chernobyl accident, was a catastrophic nuclear accident.  It occurred on 26 April 1986 in the No.4 light water graphite moderated reactor at the Chernobyl Nuclear Power Plant near Pripyat, in what was then part of the Ukrainian Soviet Socialist Republic of the Soviet Union (USSR). The event occurred during a late-night safety test which simulated a station blackout power-failure and in which safety systems were deliberately turned off.  A combination of inherent reactor design flaws and the reactor operators arranging the core in a manner contrary to the checklist for the test, eventually resulted in uncontrolled reaction conditions.  Water flashed into steam generating a destructive steam explosion and a subsequent open-air graphite fire.  This fire produced considerable updrafts for about nine days.  These lofted plumes of fission products into the atmosphere.  The estimated radioactive inventory that was released during this very hot fire phase approximately equalled in magnitude the airborne fission products released in the initial destructive explosion.  Practically all of this radioactive material would then go on to fall-out/precipitate onto much of the surface of the western USSR and Europe. The Chernobyl accident dominates the energy accidents sub-category, of most disastrous nuclear power plant accident in history, both in terms of cost and casualties.  It is one of only two nuclear energy accidents classified as a level 7 event (the maximum classification) on the International Nuclear Event Scale, the other being the Fukushima Daiichi nuclear disaster in Japan in 2011.  The struggle to safeguard against scenarios which were, at many times falsely, perceived as having the potential for greater catastrophe and the later decontamination efforts of the surroundings, ultimately involved over 500,000 workers and cost an estimated 18 billion rubles. During the accident, blast effects caused 2 deaths within the facility, later 134 were hospitalized with acute radiation symptoms, of which 28 firemen and employees died in the days-to-months afterward from the effects of acute radiation syndrome, in addition, approximately 14 cancer deaths amongst this group of initially hospitalized survivors was to follow within the next ten years(1996).  Whilst among the wider population, an excess of 15 childhood thyroid cancer deaths had been documented as of 2011.  It will take further time and investigative funding to definitively determine the elevated relative risk of cancer amongst both the surviving employees and the population at large. The remains of the No.4 reactor building were enclosed in a large cover which was named the \"Object Shelter\". It is often known as the sarcophagus, its purpose is to reduce the spread of radioactivity from the wreckage and to protect the wreckage from the elements. It was not intended as a (radiation shield), it finished by December 1986, at a time when what was left of the reactor was entering the cold shut-down phase; the enclosure was built quickly as occupational safety for the crews of the other undamaged reactors at the power station, with No.3 continuing to produce electricity into 2000. The accident motivated safety upgrades on all remaining Soviet-designed reactors in the RBMK (Chernobyl No.4) family, of which eleven continued to power electric grids as of 2013. The disaster began during a systems test on 26 April 1986 at reactor 4 of the Chernobyl plant near Pripyat and in proximity to the administrative border with Belarus and the Dnieper River.  There was a sudden and unexpected power surge.  When an emergency shutdown was attempted, a much larger spike in power output occurred.  The second spike led to a reactor vessel rupture and a series of steam explosions.  These events exposed the graphite moderator of the reactor to air, causing it to ignite.  The resulting fire sent week long plumes of highly radioactive fallout into the atmosphere over an extensive geographical area, including Pripyat.  The plumes drifted over large parts of the western Soviet Union and Europe.  According to official post-Soviet data, about 60% of the fallout landed in Belarus. 36 hours after the accident Soviet officials enacted a 10-kilometre exclusion zone which resulted in the rapid evacuation of 49,000 people and their animals primarily from the town of Pripyat, the nearest large population centre.  Although not communicated at the time, an immediate evacuation of the town following the accident was not advisable as the road leading out of the town had heavy nuclear fallout hotspots deposited on it.  Up to around that point, the town itself was comparatively safe due to the more favourable wind direction.  Shelter in place was to be the best safety measure to take for the town, before the winds began to change direction. As the plumes and subsequent fallout continued to be generated, the evacuation zone was increased from 10 to 30 km about one week after the accident.  A further 68,000 were evacuated, including from the town of Chernobyl itself.  The surveying and detection of isolated fallout hotspots outside this zone over the following year eventually resulted in 135,000 long-term evacuees in total, accepting to be moved.  The near tripling in the total number of permanently resettled to some 350,000 over the decades, 1986 to 2000, from the most severely contaminated areas, is regarded as largely political in nature, with the majority of the rest evacuated in an effort to redeem loss in trust in the government, which was most common around 1990.  Many thousands of these evacuees would have been \"better off staying home.\"  Risk analysis, supported by DNA biomarkers, has determined that the \"people still living unofficially in the abandoned lands around Chernobyl\" have a lower risk of dying as a result of the elevated doses of radiation in the rural areas than \"if they were exposed to the air pollution health risk in a large city such as nearby Kiev.\" Russia, Ukraine, and Belarus have been burdened with the continuing and substantial decontamination and monthly compensation costs, of the Chernobyl accident.  Although certain initiatives are legitimate, as the director of the UN Development Program Kalman Mizsei noted, “an industry has been built on this unfortunate event,” with a “vast interest in creating a false picture.” The accident raised the already heightened concerns about fission reactors worldwide, and while most concern was focused on those of the same unusual design, hundreds of disparate electric-power reactor proposals, including those under construction at Chernobyl, reactor No.5 and 6, were eventually cancelled.  With the worldwide issue generally being due to the ballooning in costs for new nuclear reactor safety system standards and the legal costs in dealing with the increasingly hostile/anxious public opinion.  There was a precipitous drop in the prior rate of new startups after 1986. The accident also raised concerns about the cavalier safety culture in the Soviet nuclear power industry, slowing industry growth and forcing the Soviet government to become less secretive about its procedures.  The government coverup of the Chernobyl disaster was a catalyst for glasnost, which \"paved the way for reforms leading to the Soviet collapse\". A report by the International Atomic Energy Agency examines the environmental consequences of the accident.  Another UN agency, UNSCEAR, has estimated a global collective dose of radiation exposure from the accident \"equivalent on average to 21 additional days of world exposure to natural background radiation\"; individual doses were far higher than the global mean among those most exposed, including 530,000 primarily male recovery workers (the Liquidators) who averaged an effective dose equivalent to an extra 50 years of typical natural background radiation exposure each. Estimates of the number of deaths that will eventually result from the accident vary enormously; disparities reflect both the lack of solid scientific data and the different methodologies used to quantify mortality—whether the discussion is confined to specific geographical areas or extends worldwide, and whether the deaths are immediate, short term, or long term. In 1994, Thirty-one deaths were directly attributed to the accident, all among the reactor staff and emergency workers.  As of the 2008 UNSCEAR report, the total number of confirmed deaths from radiation was 64 and this is expected to continue to rise. The Chernobyl Forum predicts that the eventual death toll could reach 4,000 among those exposed to the \"highest levels of radiation\" (200,000 emergency workers, 116,000 evacuees and 270,000 residents of the most contaminated areas); this figure is a total causal death toll prediction, combining the deaths of approximately 50 emergency workers who died soon after the accident from acute radiation syndrome, 15 children who have died of thyroid cancer and a future predicted total of 3935 deaths from radiation-induced cancer and leukaemia. In a peer-reviewed paper in the \"International Journal of Cancer\" in 2006, the authors expanded the discussion on those exposed to all of Europe (but following a different conclusion methodology to the Chernobyl Forum study, which arrived at the total predicted death toll of 4,000 after cancer survival rates were factored in) they stated, without entering into a discussion on deaths, that in terms of total excess cancers attributed to the accident: The risk projections suggest that by now [2006] Chernobyl may have caused about 1000 cases of thyroid cancer and 4000 cases of other cancers in Europe, representing about 0.01% of all incident cancers since the accident.  Models predict that by 2065 about 16,000 cases of thyroid cancer and 25,000 cases of other cancers may be expected due to radiation from the accident, whereas several hundred million cancer cases are expected from other causes. Two anti-nuclear advocacy groups have publicized non-peer-reviewed estimates that include mortality estimates for those who were exposed to even smaller amounts of radiation.  The Union of Concerned Scientists(UCS) calculated that, among the hundreds of millions of people exposed worldwide, there will be an eventual 50,000 excess cancer cases, resulting in 25,000 excess cancer deaths, excluding thyroid cancer.  However, these calculations are based on a simple linear no-threshold model multiplication and the misapplication of the collective dose, which the International Commission on Radiological Protection (ICRP) states \"should not be done\" as using the collective dose is \"inappropriate to use in risk projections\". Along similar lines to the UCS approach, the 2006 TORCH report, commissioned by the European Greens political party, likewise simplistically calculates an eventual 30,000 to 60,000 excess cancer deaths in total, around the globe. The Russian founder of that region's chapter of Greenpeace authored a book titled \"\", which suggests that among the billions of people worldwide who were exposed to radioactive contamination from the disaster, nearly a million premature cancer deaths occurred between 1986 and 2004.  Greenpeace itself advocates a figure of at least 200,000 or more.  The book was not peer reviewed prior to its publication, and it has been heavily criticized; of the five reviews published in the academic press, four considered the book severely flawed and contradictory, and one praised it while noting some shortcomings.  The review by M. I. Balonov published by the New York Academy of Sciences concludes that the report is of negative value because it has very little scientific merit while being highly misleading to the lay reader.  It characterized the estimate of nearly a million deaths as more in the realm of science fiction than science. On 26 April 1986, at 01:23 (UTC+3), reactor four suffered a catastrophic power increase, leading to explosions in its core.  This dispersed large quantities of radioactive isotopes into the atmosphere and caused an open-air fire.  The fire increased the emission of radioactive particles, carried by the smoke, as the reactor had not been encased by any kind of hard containment vessel.  The accident occurred during an experiment scheduled to test a potential safety emergency core cooling feature, which took place during a normal shutdown procedure. In steady state operation, a significant fraction (over 6%) of the power from a nuclear reactor is derived not from fission but from the decay heat of its accumulated fission products.  This heat continues for some time after the chain reaction is stopped (e.g., following an emergency SCRAM) and active cooling may be required to prevent core damage.  RBMK reactors like those at Chernobyl use water as a coolant.  Reactor 4 at Chernobyl consisted of about 1,600 individual fuel channels; each required a coolant flow of 28 metric tons (28000 l ) per hour. Since cooling pumps require electricity to cool a reactor after a SCRAM, in the event of a power grid failure, Chernobyl's reactors had three backup diesel generators; these could start up in 15 seconds, but took 60–75 seconds to attain full speed and reach the 5.5megawatt (MW) output required to run one main pump. To solve this one-minute gapconsidered an unacceptable safety riskit had been theorized that rotational energy from the steam turbine (as it wound down under residual steam pressure) could be used to generate the required electrical power.  Analysis indicated that this residual momentum and steam pressure might be sufficient to run the coolant pumps for 45 seconds, bridging the gap between an external power failure and the full availability of the emergency generators. This capability still needed to be confirmed experimentally, and previous tests had ended unsuccessfully.  An initial test carried out in 1982 indicated that the excitation voltage of the turbine-generator was insufficient; it did not maintain the desired magnetic field after the turbine trip.  The system was modified, and the test was repeated in 1984 but again proved unsuccessful.  In 1985, the tests were attempted a third time but also yielded negative results.  The test procedure would be repeated in 1986, and it was scheduled to take place during the maintenance shutdown of Reactor Four. The test focused on the switching sequences of the electrical supplies for the reactor.  The test procedure was expected to begin with an automatic emergency shutdown.  No detrimental effect on the safety of the reactor was anticipated, so the test programme was not formally coordinated with either the chief designer of the reactor (NIKIET) or the scientific manager.  Instead, it was approved only by the director of the plant (and even this approval was not consistent with established procedures). According to the test parameters, the thermal output of the reactor should have been \"no lower\" than 700 MW at the start of the experiment.  If test conditions had been as planned, the procedure would almost certainly have been carried out safely; the eventual disaster resulted from attempts to boost the reactor output once the experiment had been started, which was inconsistent with approved procedure. The Chernobyl power plant had been in operation for two years without the capability to ride through the first 60–75 seconds of a total loss of electric power, and thus lacked an important safety feature.  The station managers presumably wished to correct this at the first opportunity, which may explain why they continued the test even when serious problems arose, and why the requisite approval for the test had not been sought from the Soviet nuclear oversight regulator (even though there was a representative at the complex of 4 reactors). The experimental procedure was intended to run as follows: The conditions to run the test were established before the day shift of 25 April 1986.  The day-shift workers had been instructed in advance and were familiar with the established procedures.  A special team of electrical engineers was present to test the new voltage regulating system.  As planned, a gradual reduction in the output of the power unit was begun at 01:06 on 25 April, and the power level had reached 50% of its nominal 3200 MW thermal level by the beginning of the day shift. At this point, another regional power station unexpectedly went offline, and the Kiev electrical grid controller requested that the further reduction of Chernobyl's output be postponed, as power was needed to satisfy the peak evening demand.  The Chernobyl plant director agreed, and postponed the test.  Despite this delay, preparations for the test not affecting the reactor's power were carried out, including the disabling of the emergency core cooling system or ECCS, a passive/active system of core cooling intended to provide water to the core in a loss-of-coolant accident.  Given the other events that unfolded, the system would have been of limited use, but its disabling as a \"routine\" step of the test is an illustration of the inherent lack of attention to safety for this test.  In addition, had the reactor been shut down for the day as planned, it is possible that more preparation would have been taken in advance of the test. At 23:04, the Kiev grid controller allowed the reactor shutdown to resume.  This delay had some serious consequences: the day shift had long since departed, the evening shift was also preparing to leave, and the night shift would not take over until midnight, well into the job.  According to plan, the test should have been finished during the day shift, and the night shift would only have had to maintain decay heat cooling systems in an otherwise shut-down plant. The night shift had very limited time to prepare for and carry out the experiment.  A further rapid decrease in the power level from 50% was executed during the shift change-over.  Alexander Akimov was chief of the night shift, and Leonid Toptunov was the operator responsible for the reactor's operational regimen, including the movement of the control rods. Toptunov was a young engineer who had worked independently as a senior engineer for approximately three months. The test plan called for a gradual decrease in power output from reactor 4 to a thermal level of 700–1000 MW.  An output of 700 MW was reached at 00:05 on 26 April.  Due to the reactor's production of a fission byproduct, xenon-135, which is a reaction-inhibiting neutron absorber, core power continued to decrease without further operator action—a process known as reactor poisoning.  This continuing decrease in power occurred because in \"steady state operation\", xenon-135 is \"burned off\" as quickly as it is created from decaying iodine-135 by absorbing neutrons from the ongoing chain reaction to become highly stable xenon-136.  When the reactor power was lowered, previously produced high quantities of iodine-135 decayed into the neutron-absorbing xenon-135 faster than the reduced neutron flux could burn it off.  As the reactor power output dropped further, to approximately 500 MW, Toptunov mistakenly inserted the control rods too far—the exact circumstances leading to this are unknown because Akimov and Toptunov both died in the hospital on 10 and 14 May respectively.  This combination of factors put the reactor into an unintended near-shutdown state, with a power output of 30 MW thermal or less. The reactor was now producing 5 percent of the minimum initial power level established as safe for the test.  Control-room personnel decided to restore power by disabling the automatic system governing the control rods and manually extracting the majority of the reactor control rods to their upper limits.  Several minutes elapsed between their extraction and the point that the power output began to increase and subsequently stabilize at 160–200 MW (thermal), a much smaller value than the planned 700 MW.  The rapid reduction in the power during the initial shutdown, and the subsequent operation at a level of less than 200 MW led to increased poisoning of the reactor core by the accumulation of xenon-135.  This restricted any further rise of reactor power, and made it necessary to extract additional control rods from the reactor core in order to counteract the poisoning. The operation of the reactor at the low power level and high poisoning level was accompanied by unstable core temperature and coolant flow, and possibly by instability of neutron flux, which triggered alarms.  The control room received repeated emergency signals regarding the levels in the steam/water separator drums, and large excursions or variations in the flow rate of feed water, as well as from relief valves opened to relieve excess steam into a turbine condenser, and from the neutron power controller.  Between 00:35 and 00:45, emergency alarm signals concerning thermal-hydraulic parameters were ignored, apparently to preserve the reactor power level. When the power level of 200 MW was achieved, preparation for the experiment continued.  As part of the test plan, extra water pumps were activated at 01:05 on 26 April, increasing the water flow.  The increased coolant flow rate through the reactor produced an increase in the inlet coolant temperature of the reactor core (the coolant no longer having sufficient time to release its heat in the turbine and cooling towers), which now more closely approached the nucleate boiling temperature of water, reducing the safety margin. The flow exceeded the allowed limit at 01:19, triggering an alarm of low steam pressure in the steam separators.  At the same time, the extra water flow lowered the overall core temperature and reduced the existing steam voids in the core and the steam separators.  Since water weakly absorbs neutrons (and the higher density of liquid water makes it a better absorber than steam), turning on additional pumps decreased the reactor power further still.  The crew responded by turning off two of the circulation pumps to reduce feedwater flow, in an effort to increase steam pressure, and also to remove more manual control rods to maintain power. All these actions led to an extremely unstable reactor configuration.  Nearly all of the control rods were removed manually, including all but 18 of the \"fail-safe\" manually operated rods of the minimal 28 which were intended to remain fully inserted to control the reactor even in the event of a loss of coolant, out of a total 211 control rods.  While the emergency SCRAM system that would insert all control rods to shut down the reactor could still be activated manually (through the \"AZ-5\" button), the automated system that could do the same had been disabled to maintain the power level, and many other automated and even passive safety features of the reactor had been bypassed.  Further, the reactor coolant pumping had been reduced, which had limited margin so any power excursion would produce boiling, thereby reducing neutron absorption by the water.  The reactor was in an unstable configuration that was outside the safe operating envelope established by the designers.  If anything pushed it into supercriticality, it was unable to recover automatically. At 1:23:04 a.m., the experiment began.  Four of the main circulating pumps (MCP) were active; of the eight total, six are normally active during regular operation.  The steam to the turbines was shut off, beginning a run-down of the turbine generator.  The diesel generators started and sequentially picked up loads; the generators were to have completely picked up the MCPs' power needs by 01:23:43.  In the interim, the power for the MCPs was to be supplied by the turbine generator as it coasted down.  As the momentum of the turbine generator decreased, so did the power it produced for the pumps.  The water flow rate decreased, leading to increased formation of steam voids (bubbles) in the core. Because of the positive void coefficient of the RBMK reactor at low reactor power levels, it was now primed to embark on a positive feedback loop, in which the formation of steam voids reduced the ability of the liquid water coolant to absorb neutrons, which in turn increased the reactor's power output.  This caused yet more water to flash into steam, giving a further power increase.  During almost the entire period of the experiment the automatic control system successfully counteracted this positive feedback, inserting control rods into the reactor core to limit the power rise.  This system had control of only 12 rods, and nearly all others had been manually retracted. At 1:23:40, as recorded by the SKALA centralized control system, a SCRAM (emergency shutdown) of the reactor was initiated.  The SCRAM was started when the EPS-5 button (also known as the AZ-5 button) of the reactor emergency protection system was pressed: this engaged the drive mechanism on all control rods to fully insert them, including the manual control rods that had been withdrawn earlier.  The reason why the EPS-5 button was pressed is not known, whether it was done as an emergency measure in response to rising temperatures, or simply as a routine method of shutting down the reactor upon completion of the experiment. There is a view that the SCRAM may have been ordered as a response to the unexpected rapid power increase, although there is no recorded data proving this.  Some have suggested that the button was not pressed, and instead the signal was automatically produced by the emergency protection system; the SKALA registered a manual SCRAM signal.  In spite of this, the question as to when or even whether the EPS-5 button was pressed has been the subject of debate.  There are assertions that the pressure was caused by the rapid power acceleration at the start, and allegations that the button was not pressed until the reactor began to self-destruct but others assert that it happened earlier and in calm conditions. After the EPS-5 button was pressed, the insertion of control rods into the reactor core began.  The control rod insertion mechanism moved the rods at 0.4 m/s, so that the rods took 18 to 20 seconds to travel the full height of the core, about 7 metres.  A bigger problem was the design of the RBMK control rods, which had graphite neutron moderators attached to boost reactor output when the rod was withdrawn.  Those displacers had a 1.25 m column of water above and below them when the rods were at maximum extration, and lowering the rods displaced the neutron-absorbing water in the lower portion of the reactor with moderating graphite.  As a result, the SCRAM increased the reaction rate in the lower part of the core as the graphite displaced the coolant.  This behaviour was known after the initial insertion of control rods in another RBMK reactor at Ignalina Nuclear Power Plant in 1983 induced a power spike, but as the subsequent SCRAM of that reactor was successful, the information was disseminated but deemed of little importance. A few seconds after the start of the SCRAM, a power spike occurred, and the core overheated, causing some of the fuel rods to fracture, blocking the control rod columns and jamming the control rods at one-third insertion, with the graphite displacers still in the lower part of the core.  Within three seconds the reactor output rose above 530 MW. The subsequent course of events was not registered by instruments; it is known only as a result of mathematical simulation.  Apparently, the power spike caused an increase in fuel temperature and steam buildup, leading to a rapid increase in steam pressure.  This caused the fuel cladding to fail, releasing the fuel elements into the coolant, and rupturing the channels in which these elements were located. Then, according to some estimations, the reactor jumped to around 30,000 MW thermal, ten times the normal operational output.  The last reading on the control panel was 33,000 MW.  It was not possible to reconstruct the precise sequence of the processes that led to the destruction of the reactor and the power unit building, but a steam explosion, like the explosion of a steam boiler from excess vapour pressure, appears to have been the next event.  There is a general understanding that it was explosive steam pressure from the damaged fuel channels escaping into the reactor's exterior cooling structure that caused the explosion that destroyed the reactor casing, tearing off and blasting the 2000-ton upper plate, to which the entire reactor assembly is fastened, through the roof of the reactor building.  This is believed to be the first explosion that many heard.  This explosion ruptured further fuel channels, as well as severing most of the coolant lines feeding the reactor chamber, and as a result the remaining coolant flashed to steam and escaped the reactor core.  The total water loss in combination with a high positive void coefficient further increased the reactor's thermal power. A second, more powerful explosion occurred about two or three seconds after the first; this explosion dispersed the damaged core and effectively terminated the nuclear chain reaction.  This explosion also compromised more of the reactor containment vessel and ejected hot lumps of graphite moderator.  The ejected graphite and the demolished channels still in the remains of the reactor vessel caught fire on exposure to air, greatly contributing to the spread of radioactive fallout and the contamination of outlying areas. According to observers outside Unit 4, burning lumps of material and sparks shot into the air above the reactor.  Some of them fell onto the roof of the machine hall and started a fire.  About 25 percent of the red-hot graphite blocks and overheated material from the fuel channels was ejected.  Parts of the graphite blocks and fuel channels were out of the reactor building.  As a result of the damage to the building an airflow through the core was established by the high temperature of the core.  The air ignited the hot graphite and started a graphite fire. After the larger explosion a number of employees at the power station went outside to get a clearer view of the extent of the damage, one such survivor, Alexander Yuvchenko recounts that once he stopped outside and looked up towards the reactor hall he saw a \"very beautiful\" LASER-like beam of light bluish light, caused by the ionization of air, that appeared to \"flood up into infinity\". There were initially several hypotheses about the nature of the second explosion.  One view was that the second explosion was caused by hydrogen, which had been produced either by the overheated steam-zirconium reaction or by the reaction of red-hot graphite with steam that produced hydrogen and carbon monoxide.  Another hypothesis was that the second explosion was a thermal explosion of the reactor as a result of the uncontrollable escape of fast neutrons caused by the complete water loss in the reactor core.  A third hypothesis was that the explosion was a second steam explosion.  According to this version, the first explosion was a more minor steam explosion in the circulating loop, causing a loss of coolant flow and pressure, that in turn caused the water still in the core to flash to steam.  This second explosion then did the majority of the damage to the reactor and containment building. The force of the second explosion, and the ratio of xenon radioisotopes released during the event, indicate that the second explosion could have been a nuclear power transient; the result of the melting core material, in the absence of its cladding, water coolant and moderator, undergoing runaway prompt criticality similar to the explosion of a fizzled nuclear weapon.  This nuclear excursion released 40 billion joules of energy, the equivalent of about ten tons of TNT.  The analysis indicates that the nuclear excursion was limited to a small portion of the core. Contrary to safety regulations, bitumen, a combustible material, had been used in the construction of the roof of the reactor building and the turbine hall.  Ejected material ignited at least five fires on the roof of the adjacent reactor 3, which was still operating.  It was imperative to put those fires out and protect the cooling systems of reactor 3.  Inside reactor 3, the chief of the night shift, Yuri Bagdasarov, wanted to shut down the reactor immediately, but chief engineer Nikolai Fomin would not allow this.  The operators were given respirators and potassium iodide tablets and told to continue working.  At 05:00, Bagdasarov made his own decision to shut down the reactor, leaving only those operators there who had to work the emergency cooling systems. Approximate radiation intensity levels at different locations at Chernobyl reactor site shortly after the explosion are shown in the below table.  A dose of 500 roentgens (~5 Sv) delivered over five hours is usually lethal for human beings. The radiation levels in the worst-hit areas of the reactor building have been estimated to be 5.6 roentgens per second (R/s), equivalent to more than 20,000 roentgens per hour.  A lethal dose is around 500 roentgens (~5 Gy) over 5 hours, so in some areas, unprotected workers received fatal doses in less than a minute.  However, a dosimeter capable of measuring up to 1000 R/s was buried in the rubble of a collapsed part of the building, and another one failed when turned on.  All remaining dosimeters had limits of 0.001 R/s and therefore read \"off scale\".  Thus, the reactor crew could ascertain only that the radiation levels were somewhere above 0.001 R/s (3.6 R/h), while the true levels were much higher in some areas. Because of the inaccurate low readings, the reactor crew chief Alexander Akimov assumed that the reactor was intact.  The evidence of pieces of graphite and reactor fuel lying around the building was ignored, and the readings of another dosimeter brought in by 04:30 were dismissed under the assumption that the new dosimeter must have been defective.  Akimov stayed with his crew in the reactor building until morning, sending members of his crew to try to pump water into the reactor.  None of them wore any protective gear.  Most, including Akimov, died from radiation exposure within three weeks. Shortly after the accident, firefighters arrived to try to extinguish the fires.  First on the scene was a Chernobyl Power Station firefighter brigade under the command of Lieutenant Volodymyr Pravik, who died on 9 May 1986 of acute radiation sickness.  They were not told how dangerously radioactive the smoke and the debris were, and may not even have known that the accident was anything more than a regular electrical fire: \"We didn't know it was the reactor.  No one had told us.\" Grigorii Khmel, the driver of one of the fire engines, later described what happened: We arrived there at 10 or 15 minutes to two in the morning... We saw graphite scattered about.  Misha asked: \"Is that graphite?\"  I kicked it away.  But one of the fighters on the other truck picked it up.  \"It's hot,\" he said.  The pieces of graphite were of different sizes, some big, some small, enough to pick them up... We didn't know much about radiation.  Even those who worked there had no idea.  There was no water left in the trucks.  Misha filled a cistern and we aimed the water at the top.  Then those boys who died went up to the roof – Vashchik, Kolya and others, and Volodya Pravik... They went up the ladder ... and I never saw them again. Anatoli Zakharov, a fireman stationed in Chernobyl since 1980, offers a different description in 2008: I remember joking to the others, \"There must be an incredible amount of radiation here.  We'll be lucky if we're all still alive in the morning.\" He also said: Of course we knew!  If we'd followed regulations, we would never have gone near the reactor.  But it was a moral obligation – our duty.  We were like kamikaze. The immediate priority was to extinguish fires on the roof of the station and the area around the building containing Reactor No. 4 to protect No. 3 and keep its core cooling systems intact.  The fires were extinguished by 5:00, but many firefighters received high doses of radiation.  The fire inside reactor 4 continued to burn until 10 May 1986; it is possible that well over half of the graphite burned out. The fire was extinguished by a combined effort of helicopters dropping over 5000 metric tons of sand, lead, clay, and neutron-absorbing boron onto the burning reactor and injection of liquid nitrogen.  The Ukrainian filmmaker Vladimir Shevchenko captured film footage of an Mi-8 helicopter as its main rotor collided with a nearby construction crane cable, causing the helicopter to fall near the damaged reactor building and killing its four-man crew.  It is now known that virtually none of the neutron absorbers reached the core. From eyewitness accounts of the firefighters involved before they died (as reported on the CBC television series \"Witness\"), one described his experience of the radiation as \"tasting like metal\", and feeling a sensation similar to that of pins and needles all over his face.  (This is similar to the description given by Louis Slotin, a Manhattan Project physicist who died days after a fatal radiation overdose from a criticality accident.) The explosion and fire threw hot particles of the nuclear fuel and also far more dangerous fission products, radioactive isotopes such as caesium-137, iodine-131, strontium-90 and other radionuclides, into the air: the residents of the surrounding area observed the radioactive cloud on the night of the explosion. Equipment assembled included remote-controlled bulldozers and robot-carts that could detect radioactivity and carry hot debris.  Valery Legasov (first deputy director of the Kurchatov Institute of Atomic Energy in Moscow) said, in 1987: \"But we learned that robots are not the great remedy for everything.  Where there was very high radiation, the robot ceased to be a robot—the electronics quit working.\" With the exception of the fire contained inside Reactor 4, which continued to burn for many days. The nearby city of Pripyat was not immediately evacuated.  The townspeople went about their usual business, completely oblivious to what had just happened.  However, within a few hours of the explosion, dozens of people fell ill.  Later, they reported severe headaches and metallic tastes in their mouths, along with uncontrollable fits of coughing and vomiting. As the plant was run by authorities in Moscow, the government of Ukraine did not receive prompt information on the accident. Valentyna Shevchenko, then Chairman of the Presidium of Verkhovna Rada Supreme Soviet of the Ukrainian SSR, recalls that Ukraine's acting Minister of Internal Affairs Vasyl Durdynets phoned her at work at 9 am to report current affairs; only at the end of the conversation did he add that there had been a fire at the Chernobyl nuclear power plant, but it was extinguished and everything was fine.  When Shevchenko asked \"How are the people?\" , he replied that there was nothing to be concerned about: \"Some are celebrating a wedding, others are gardening, and others are fishing in the Pripyat River\". Shevchenko then spoke over the phone to Volodymyr Shcherbytsky, Head of the Central Committee of the CPU and de facto head of state, who said he anticipated a delegation of the state commission headed by the deputy chairman of the Council of Ministers of USSR. A commission was set up the same day (26 April) to investigate the accident.  It was headed by Valery Legasov, First Deputy Director of the Kurchatov Institute of Atomic Energy, and included leading nuclear specialist Evgeny Velikhov, hydro-meteorologist Yuri Izrael, radiologist Leonid Ilyin and others.  They flew to Boryspil International Airport and arrived at the power plant in the evening of 26 April.  By that time two people had already died and 52 were hospitalized.  The delegation soon had ample evidence that the reactor was destroyed and extremely high levels of radiation had caused a number of cases of radiation exposure.  In the early hours of 27 April, over 24 hours after the initial blast, they ordered the evacuation of Pripyat.  Initially it was decided to evacuate the population for three days; later this was made permanent. By 11:00 on 27 April, buses had arrived in Pripyat to start the evacuation.  The evacuation began at 14:00.  A translated excerpt of the evacuation announcement follows: For the attention of the residents of Pripyat!  The City Council informs you that due to the accident at Chernobyl Power Station in the city of Pripyat the radioactive conditions in the vicinity are deteriorating.  The Communist Party, its officials and the armed forces are taking necessary steps to combat this.  Nevertheless, with the view to keep people as safe and healthy as possible, the children being top priority, we need to temporarily evacuate the citizens in the nearest towns of Kiev region.  For these reasons, starting from April 27, 1986 2 pm each apartment block will be able to have a bus at its disposal, supervised by the police and the city officials.  It is highly advisable to take your documents, some vital personal belongings and a certain amount of food, just in case, with you.  The senior executives of public and industrial facilities of the city has decided on the list of employees needed to stay in Pripyat to maintain these facilities in a good working order.  All the houses will be guarded by the police during the evacuation period.  Comrades, leaving your residences temporarily please make sure you have turned off the lights, electrical equipment and water and shut the windows.  Please keep calm and orderly in the process of this short-term evacuation. To expedite the evacuation, residents were told to bring only what was necessary, and that it would only last approximately three days.  As a result, most personal belongings were left behind, and remain there today.  By 15:00, 53,000 people were evacuated to various villages of the Kiev region.  The next day, talks began for evacuating people from the 10 km zone.  Ten days after the accident, the evacuation area was expanded to 30 km (19 mi).  This \"exclusion zone\" has remained ever since, although its shape has changed and its size has been expanded. These evacuations actually had some economic benefit, moving people to areas of labour shortage in Belarus and Ukraine. Evacuation began long before the accident was publicly known throughout the Union.  Only on 28 April, after radiation levels set off alarms at the Forsmark Nuclear Power Plant in Sweden, over 1000 km from the Chernobyl Plant, did the Soviet Union publicly admit that an accident had occurred. At 21:02 that evening a 20-second announcement was read in the TV news programme \"Vremya\": There has been an accident at the Chernobyl Nuclear Power Plant.  One of the nuclear reactors was damaged.  The effects of the accident are being remedied.  Assistance has been provided for any affected people.  An investigative commission has been set up. This was the entirety of the announcement of the accident.  The Telegraph Agency of the Soviet Union (TASS) then discussed Three Mile Island and other American nuclear accidents, an example of the common Soviet tactic of emphasizing foreign disasters when one occurred in the Soviet Union.  The mention of a commission, however, indicated to observers the seriousness of the incident, and subsequent state radio broadcasts were replaced with classical music, which was a common method of preparing the public for an announcement of a tragedy. Around the same time, ABC News released its report about the disaster. Shevchenko was the first of the Ukrainian state top officials to arrive at the disaster site early on 28 April.  There she spoke with members of medical staff and people, who were calm and hopeful that they could soon return to their homes.  Shevchenko returned home near midnight, stopping at a radiological checkpoint in Vilcha, one of the first that were set up soon after the accident. There was a notification from Moscow that there was no reason to postpone the 1 May International Workers' Day celebrations in Kiev (including the annual parade), but on 30 April a meeting of the Political bureau of the Central Committee of CPU took place to discuss the plan for the upcoming celebration.  Scientists were reporting that the radiological background in Kiev city was normal.  At the meeting, which was finished at 18:00, it was decided to shorten celebrations from the regular 3.5–4 to under 2 hours. Several buildings in Pripyat were officially kept open after the disaster to be used by workers still involved with the plant.  These included the Jupiter Factory which closed in 1996 and the Azure Swimming Pool which closed in 1998. Two floors of bubbler pools beneath the reactor served as a large water reservoir for the emergency cooling pumps and as a pressure suppression system capable of condensing steam in case of a small broken steam pipe; the third floor above them, below the reactor, served as a steam tunnel.  The steam released by a broken pipe was supposed to enter the steam tunnel and be led into the pools to bubble through a layer of water.  After the disaster, the pools and the basement were flooded because of ruptured cooling water pipes and accumulated firefighting water, and constituted a serious steam explosion risk. The smoldering graphite, fuel and other material above, at more than 1200 °C, started to burn through the reactor floor and mixed with molten concrete from the reactor lining, creating corium, a radioactive semi-liquid material comparable to lava.  If this mixture had melted through the floor into the pool of water, it was feared it could have created a serious steam explosion that would have ejected more radioactive material from the reactor.  It became necessary to drain the pool. The bubbler pool could be drained by opening its sluice gates.  However, the valves controlling it were underwater, located in a flooded corridor in the basement.  So volunteers in wetsuits and respirators (for protection against radioactive aerosols) and equipped with dosimeters, entered the knee-deep radioactive water and managed to open the valves.  These were the engineers Alexei Ananenko and Valeri Bezpalov (who knew where the valves were), accompanied by the shift supervisor Boris Baranov.  Upon succeeding and emerging from the water, according to many English language news articles, books and the prominent BBC docudrama \"Surviving Disaster – Chernobyl Nuclear\", the three knew it was a suicide-mission and began suffering from radiation sickness and died soon after.  Some sources also incorrectly claimed that they died there in the plant.  However, research by Andrew Leatherbarrow, author of the 2016 book \"Chernobyl 01:23:40\", determined that the frequently recounted story is a gross exaggeration.  Alexei Ananenko continues to work in the nuclear energy industry, and rebuffs the growth of the Chernobyl media sensationalism surrounding him.  While Valeri Bezpalov was found to still be alive by Leatherbarrow, the elderly 65 year old Baranov had lived until 2005 and had died of heart failure. Once the bubbler pool gates were opened by the Ananenko team, fire brigade pumps were then used to drain the basement.  The operation was not completed until 8 May, after 20,000 metric tons of highly radioactive water were pumped out. With the bubbler pool gone, a meltdown was less likely to produce a powerful steam explosion.  To do so, the molten core would now have to reach the water table below the reactor.  To reduce the likelihood of this, it was decided to freeze the earth beneath the reactor, which would also stabilize the foundations.  Using oil drilling equipment, the injection of liquid nitrogen began on 4 May.  It was estimated that 25 metric tons of liquid nitrogen per day would be required to keep the soil frozen at −100 °C.  This idea was soon scrapped and the bottom room where the cooling system would have been installed was filled with concrete. It is likely that intense alpha radiation hydrolysed the water, generating a low-pH hydrogen peroxide (HO) solution akin to an oxidizing acid.  Conversion of bubbler pool water to HO is confirmed by the presence in the Chernobyl lavas of studtite and metastudtite, the only minerals that contain peroxide. The worst of the radioactive debris was collected inside what was left of the reactor, much of it shoveled in by liquidators wearing heavy protective gear (dubbed \"bio-robots\" by the military); these workers could only spend a maximum of 40 seconds at a time working on the rooftops of the surrounding buildings because of the extremely high doses of radiation given off by the blocks of graphite and other debris.  The reactor itself was covered with bags of sand, lead and boric acid dropped from helicopters: some 5000 metric tons of material were dropped during the week that followed the accident. At the time there was still fear that the reactor could re-enter a self-sustaining nuclear chain-reaction and explode again, and a new containment structure was planned to prevent rain entering and triggering such an explosion, and to prevent further release of radioactive material.  This was the largest civil engineering task in history, involving a quarter of a million construction workers who all reached their official lifetime limits of radiation.  By December 1986, a large concrete sarcophagus had been erected to seal off the reactor and its contents.  A unique \"clean up\" medal was given to the workers. Many of the vehicles used by the liquidators remain parked in a field in the Chernobyl area. During the construction of the sarcophagus, a scientific team re-entered the reactor as part of an investigation dubbed \"Complex Expedition\", to locate and contain nuclear fuel in a way that could not lead to another explosion.  These scientists manually collected cold fuel rods, but great heat was still emanating from the core.  Rates of radiation in different parts of the building were monitored by drilling holes into the reactor and inserting long metal detector tubes.  The scientists were exposed to high levels of radiation and radioactive dust. After six months of investigation, in December 1986, they discovered with the help of a remote camera an intensely radioactive mass in the basement of Unit Four, more than two metres wide and weighing hundreds of tons, which they called \"the elephant's foot\" for its wrinkled appearance.  The mass was composed of sand, glass and a large amount of nuclear fuel that had escaped from the reactor.  The concrete beneath the reactor was steaming hot, and was breached by solidified lava and spectacular unknown crystalline forms termed chernobylite.  It was concluded that there was no further risk of explosion. Liquidators worked under deplorable conditions, poorly informed and with poor protection.  Many, if not most of them, exceeded radiation safety limits.  Some exceeded limits by over 100 times—leading to rapid death. The official contaminated zones became stage to a massive clean-up effort lasting seven months.  The official reason for such early (and dangerous) decontamination efforts, rather than allowing time for natural decay, was that the land must be re-peopled and brought back into cultivation.  Indeed, within fifteen months 75% of the land was under cultivation, even though only a third of the evacuated villages were resettled.  Defence forces must have done much of the work.  Yet this land was of marginal agricultural value.  According to historian David Marples, the administration had a psychological purpose for the clean-up: they wished to forestall panic regarding nuclear energy, and even to restart the Chernobyl power station. There were two official explanations of the accident. The first official explanation of the accident, later acknowledged to be erroneous, was published in August 1986.  It effectively placed the blame on the power plant operators.  To investigate the causes of the accident the IAEA created a group known as the International Nuclear Safety Advisory Group (INSAG), which in its report of 1986, INSAG-1, on the whole also supported this view, based on the data provided by the Soviets and the oral statements of specialists.  In this view, the catastrophic accident was caused by gross violations of operating rules and regulations.  \"During preparation and testing of the turbine generator under run-down conditions using the auxiliary load, personnel disconnected a series of technical protection systems and breached the most important operational safety provisions for conducting a technical exercise.\" The operator error was probably due to their lack of knowledge of nuclear reactor physics and engineering, as well as lack of experience and training.  According to these allegations, at the time of the accident the reactor was being operated with many key safety systems turned off, most notably the Emergency Core Cooling System (ECCS), LAR (Local Automatic control system), and AZ (emergency power reduction system).  Personnel had an insufficiently detailed understanding of technical procedures involved with the nuclear reactor, and knowingly ignored regulations to speed test completion. In this analysis of the causes of the accident, deficiencies in the reactor design and in the operating regulations that made the accident possible were set aside and mentioned only casually.  Serious critical observations covered only general questions and did not address the specific reasons for the accident.  The following general picture arose from these observations.  Several procedural irregularities also helped to make the accident possible.  One was insufficient communication between the safety officers and the operators in charge of the experiment being run that night. The reactor operators disabled safety systems down to the generators, which the test was really about.  The main process computer, SKALA, was running in such a way that the main control computer could not shut down the reactor or even reduce power.  Normally the reactor would have started to insert all of the control rods.  The computer would have also started the \"Emergency Core Protection System\" that introduces 24 control rods into the active zone within 2.5 seconds, which is still slow by 1986 standards.  All control was transferred from the process computer to the human operators. On the subject of the disconnection of safety systems, Valery Legasov said, in 1987, that \"[i]t was like airplane pilots experimenting with the engines in flight\". This view is reflected in numerous publications and also artistic works on the theme of the Chernobyl accident that appeared immediately after the accident, and for a long time remained dominant in the public consciousness and in popular publications. Ukraine has declassified a number of KGB documents from the period between 1971 and 1988 related to the Chernobyl plant, mentioning for example previous reports of structural damages caused by negligence during construction of the plant (such as splitting of concrete layers) that were never acted upon.  They document over 29 emergency situations in the plant during this period, 8 of which were caused by negligence or poor competence on the part of personnel. In 1991 a Commission of the USSR State Committee for the Supervision of Safety in Industry and Nuclear Power reassessed the causes and circumstances of the Chernobyl accident and came to new insights and conclusions.  Based on it, in 1992 the IAEA Nuclear Safety Advisory Group (INSAG) published an additional report, INSAG-7, which reviewed \"that part of the INSAG-1 report in which primary attention is given to the reasons for the accident,\" and was included the USSR State Commission report as Appendix I. In this INSAG report, most of the earlier accusations against staff for breach of regulations were acknowledged to be either erroneous, based on incorrect information obtained in August 1986, or less relevant.  This report reflected another view of the main reasons for the accident, presented in Appendix I.  According to this account, the operators' actions in turning off the Emergency Core Cooling System, interfering with the settings on the protection equipment, and blocking the level and pressure in the separator drum did not contribute to the original cause of the accident and its magnitude, although they may have been a breach of regulations.  Turning off the emergency system designed to prevent the two turbine generators from stopping was not a violation of regulations. Human factors contributed to the conditions that led to the disaster.  These included operating the reactor at a low power level—less than 700 MW—a level documented in the run-down test programme, and operating with a small operational reactivity margin (ORM).  The 1986 assertions of Soviet experts notwithstanding, regulations did not prohibit operating the reactor at this low power level. However, regulations did forbid operating the reactor with a small margin of reactivity.  Yet \"post-accident studies have shown that the way in which the real role of the ORM is reflected in the Operating Procedures and design documentation for the RBMK-1000 is extremely contradictory\", and furthermore, \"ORM was not treated as an operational safety limit, violation of which could lead to an accident\". According to the INSAG-7 Report, the chief reasons for the accident lie in the peculiarities of physics and in the construction of the reactor.  There are two such reasons: Both views were heavily lobbied by different groups, including the reactor's designers, power plant personnel, and the Soviet and Ukrainian governments.  According to the IAEA's 1986 analysis, the main cause of the accident was the operators' actions.  But according to the IAEA's 1993 revised analysis the main cause was the reactor's design.  One reason there were such contradictory viewpoints and so much debate about the causes of the Chernobyl accident was that the primary data covering the disaster, as registered by the instruments and sensors, were not completely published in the official sources. Once again, the human factor had to be considered as a major element in causing the accident.  INSAG notes that both the operating regulations and staff handled the disabling of the reactor protection easily enough: witness the length of time for which the ECCS was out of service while the reactor was operated at half power.  INSAG's view is that it was the operating crew's deviation from the test programme that was mostly to blame.  \"Most reprehensibly, unapproved changes in the test procedure were deliberately made on the spot, although the plant was known to be in a very different condition from that intended for the test.\" As in the previously released report INSAG-1, close attention is paid in report INSAG-7 to the inadequate (at the moment of the accident) \"culture of safety\" at all levels.  Deficiency in the safety culture was inherent not only at the operational stage but also, and to no lesser extent, during activities at other stages in the lifetime of nuclear power plants (including design, engineering, construction, manufacture, and regulation).  The poor quality of operating procedures and instructions, and their conflicting character, put a heavy burden on the operating crew, including the chief engineer.  \"The accident can be said to have flowed from a deficient safety culture, not only at the Chernobyl plant, but throughout the Soviet design, operating and regulatory organizations for nuclear power that existed at that time.\" Although no informing comparisons can be made between the accident and a strictly airburst-fuzed nuclear detonation, as the latter do not produce appreciable \"local\" fallout, it has still been approximated that about four hundred times more radioactive material was released from Chernobyl than by the atomic bombing of Hiroshima.  By contrast the chernobyl accident released about one hundredth to one thousandth of the total amount of radioactivity released during the era of nuclear weapons testing at the height of the Cold War, 1950 – 1960s, with the 1/100th to 1/1000th variance due to trying to make comparisons with different spectrums of isotopes released.  Approximately 100,000 km² of land was significantly contaminated with fallout, with the worst hit regions being in Belarus, Ukraine and Russia.  Slighter levels of contamination were detected over all of Europe except for the Iberian Peninsula. The initial evidence that a major release of radioactive material was affecting other countries came not from Soviet sources, but from Sweden.  On the morning of 28 April workers at the Forsmark Nuclear Power Plant (approximately 1100 km from the Chernobyl site) were found to have radioactive particles on their clothes. It was Sweden's search for the source of radioactivity, after they had determined there was no leak at the Swedish plant, that at noon on 28 April led to the first hint of a serious nuclear problem in the western Soviet Union.  Hence the evacuation of Pripyat on 27 April, 36 hours after the initial explosions, was silently completed before the disaster became known outside the Soviet Union.  The rise in radiation levels had at that time already been measured in Finland, but a civil service strike delayed the response and publication. Contamination from the Chernobyl accident was scattered irregularly depending on weather conditions, much of it deposited on mountainous regions such as the Alps, the Welsh mountains and the Scottish Highlands, where adiabatic cooling caused radioactive rainfall.  The resulting patches of contamination were often highly localized, and water-flows across the ground contributed further to large variations in radioactivity over small areas.  Sweden and Norway also received heavy fallout when the contaminated air collided with a cold front, bringing rain. Rain was purposely seeded over 10,000 km of the Belorussian SSR by the Soviet air force to remove radioactive particles from clouds heading toward highly populated areas.  Heavy, black-coloured rain fell on the city of Gomel.  Reports from Soviet and Western scientists indicate that Belarus received about 60% of the contamination that fell on the former Soviet Union.  However, the 2006 TORCH report stated that half of the volatile particles had landed outside Ukraine, Belarus, and Russia.  A large area in Russia south of Bryansk was also contaminated, as were parts of northwestern Ukraine.  Studies in surrounding countries indicate that over one million people could have been affected by radiation. Recently published data from a long-term monitoring program (The Korma Report II) shows a decrease in internal radiation exposure of the inhabitants of a region in Belarus close to Gomel.  Resettlement may even be possible in prohibited areas provided that people comply with appropriate dietary rules. In Western Europe, precautionary measures taken in response to the radiation included seemingly arbitrary regulations banning the importation of certain foods but not others.  In France some officials stated that the Chernobyl accident had no adverse effects.  Official figures in southern Bavaria in Germany indicated that some wild plant species contained substantial levels of caesium, which were believed to have been passed onto them during their consumption by wild boars, a significant number of which already contained radioactive particles above the allowed level. Like many other releases of radioactivity into the environment, the Chernobyl release was controlled by the physical and chemical properties of the radioactive elements in the core.  Particularly dangerous are the highly radioactive fission products, those with high nuclear decay rates that accumulate in the food chain, such as some of the isotopes of iodine, caesium and strontium.  Iodine-131 and caesium-137 are responsible for most of the radiation exposure received by the general population. Detailed reports on the release of radioisotopes from the site were published in 1989 and 1995, with the latter report updated in 2002. At different times after the accident, different isotopes were responsible for the majority of the external dose.  The remaining quantity of any radioisotope, and therefore the activity of that isotope, after 7 decay half-lives have passed, is less than 1% of its initial magnitude, and it continues to reduce beyond 0.78% after 7 half-lives to 0.098% remaining after 10 half-lives have passed and so on.  (Some radionuclides have decay products that are likewise radioactive, which is not accounted for here.)  The release of radioisotopes from the nuclear fuel was largely controlled by their boiling points, and the majority of the radioactivity present in the core was retained in the reactor. Two sizes of particles were released: small particles of 0.3 to 1.5 micrometres (aerodynamic diameter) and large particles of 10 micrometres.  The large particles contained about 80% to 90% of the released nonvolatile radioisotopes zirconium-95, niobium-95, lanthanum-140, cerium-144 and the transuranic elements, including neptunium, plutonium and the minor actinides, embedded in a uranium oxide matrix. The dose that was calculated is the relative external gamma dose rate for a person standing in the open.  The exact dose to a person in the real world who would spend most of their time sleeping indoors in a shelter and then venturing out to consume an internal dose from the inhalation or ingestion of a radioisotope, requires a personnel specific radiation dose reconstruction analysis. The Chernobyl nuclear power plant is located next to the Pripyat River, which feeds into the Dnieper reservoir system, one of the largest surface water systems in Europe, which at the time supplied water to Kiev's 2.4 million residents, and was still in spring flood when the accident occurred.  The radioactive contamination of aquatic systems therefore became a major problem in the immediate aftermath of the accident.  In the most affected areas of Ukraine, levels of radioactivity (particularly from radionuclides I, Cs and Sr) in drinking water caused concern during the weeks and months after the accident, though officially it was stated that all contaminants had settled to the bottom \"in an insoluble phase\" and would not dissolve for 800–1000 years.  Guidelines for levels of radioiodine in drinking water were temporarily raised to 3,700 Bq/L, allowing most water to be reported as safe, and a year after the accident it was announced that even the water of the Chernobyl plant's cooling pond was within acceptable norms.  Despite this, two months after the disaster the Kiev water supply was abruptly switched from the Dnieper to the Desna River.  Meanwhile, massive silt traps were constructed, along with an enormous 30m-deep underground barrier to prevent groundwater from the destroyed reactor entering the Pripyat River. Bio-accumulation of radioactivity in fish resulted in concentrations (both in western Europe and in the former Soviet Union) that in many cases were significantly above guideline maximum levels for consumption.  Guideline maximum levels for radiocaesium in fish vary from country to country but are approximately 1000 Bq/kg in the European Union.  In the Kiev Reservoir in Ukraine, concentrations in fish were several thousand Bq/kg during the years after the accident. In small \"closed\" lakes in Belarus and the Bryansk region of Russia, concentrations in a number of fish species varied from 100 to 60,000 Bq/kg during the period 1990–92.  The contamination of fish caused short-term concern in parts of the UK and Germany and in the long term (years rather than months) in the affected areas of Ukraine, Belarus, and Russia as well as in parts of Scandinavia. Groundwater was not badly affected by the Chernobyl accident since radionuclides with short half-lives decayed away long before they could affect groundwater supplies, and longer-lived radionuclides such as radiocaesium and radiostrontium were adsorbed to surface soils before they could transfer to groundwater.  However, significant transfers of radionuclides to groundwater have occurred from waste disposal sites in the 30 km exclusion zone around Chernobyl.  Although there is a potential for transfer of radionuclides from these disposal sites off-site (i.e. out of the 30 km exclusion zone), the IAEA Chernobyl Report argues that this is not significant in comparison to current levels of washout of surface-deposited radioactivity. After the disaster, four square kilometres of pine forest directly downwind of the reactor turned reddish-brown and died, earning the name of the \"Red Forest\".  Some animals in the worst-hit areas also died or stopped reproducing.  Most domestic animals were removed from the exclusion zone, but horses left on an island in the Pripyat River 6 km from the power plant died when their thyroid glands were destroyed by radiation doses of 150–200 Sv.  Some cattle on the same island died and those that survived were stunted because of thyroid damage.  The next generation appeared to be normal. A robot sent into the reactor itself has returned with samples of black, melanin-rich radiotrophic fungi that are growing on the reactor's walls. Of the 440,350 wild boar killed in the 2010 hunting season in Germany, over 1000 were found to be contaminated with levels of radiation above the permitted limit of 600 becquerels per kilogram, due to residual radioactivity from Chernobyl. The Norwegian Agricultural Authority reported that in 2009 a total of 18,000 livestock in Norway needed to be given uncontaminated feed for a period of time before slaughter in order to ensure that their meat was safe for human consumption.  This was due to residual radioactivity from Chernobyl in the plants they graze on in the wild during the summer.  1,914 sheep needed to be given uncontaminated feed for a period of time before slaughter during 2012, and these sheep were located in just 18 of Norway's municipalities, a decrease of 17 from the 35 municipalities affected animals were located in during 2011 (117 municipalities were affected during 1986). The after-effects of Chernobyl were expected to be seen for a further 100 years, although the severity of the effects would decline over that period.  Scientists report this is due to radioactive caesium-137 isotopes being taken up by fungi such as \"Cortinarius caperatus\" which is in turn eaten by sheep whilst grazing. The United Kingdom was forced to restrict the movement of sheep from upland areas when radioactive caesium-137 fell across parts of Northern Ireland, Wales, Scotland and northern England.  In the immediate aftermath of the disaster in 1986, a total of 4,225,000 sheep had their movement restricted across a total of 9,700 farms, in order to prevent contaminated meat entering the human food chain.  The number of sheep and the number of farms affected has decreased since 1986, Northern Ireland was released from all restrictions in 2000 and by 2009 369 farms containing around 190,000 sheep remained under the restrictions in Wales, Cumbria and northern Scotland.  The restrictions applying in Scotland were lifted in 2010, whilst those applying to Wales and Cumbria were lifted during 2012, meaning no farms in the UK remain restricted because of Chernobyl fallout. The legislation used to control sheep movement and compensate farmers (farmers were latterly compensated per animal to cover additional costs in holding animals prior to radiation monitoring) was revoked during October and November 2012 by the relevant authorities in the UK. In the aftermath of the accident, 237 people suffered from acute radiation sickness (ARS), of whom 31 died within the first three months. In 2005 the Chernobyl Forum, composed of the IAEA, other UN organizations and the governments of Belarus, Russia and Ukraine, published a report on the radiological environmental and health consequences of the Chernobyl accident. On the death toll of the accident, the report states that 28 emergency workers (\"liquidators\") died from acute radiation syndrome, including beta burns, and 15 patients died from thyroid cancer in the following years, and it roughly estimated that cancer deaths caused by Chernobyl may reach a total of about 4,000 among the 5 million persons residing in the contaminated areas.  The report projected cancer mortality \"increases of less than one per cent\" (~0.3%) on a time span of 80 years, cautioning that this estimate was \"speculative\" since at this time only a few cancer deaths are linked to the Chernobyl disaster.  The report says it is impossible to reliably predict the number of fatal cancers arising from the incident as small differences in assumptions can result in large differences in the estimated health costs.  The report says it represents the consensus view of the eight UN organizations. Of all 66,000 Belarusian emergency workers, by the mid-1990s only 150 (roughly 0.2%) were reported by their government as having died.  In contrast, 5,722 casualties were reported among Ukrainian clean-up workers up to the year 1995, by the National Committee for Radiation Protection of the Ukrainian Population. The four most harmful radionuclides spread from Chernobyl were iodine-131, caesium-134, caesium-137 and strontium-90, with half-lives of 8.02 days, 2.07 years, 30.2 years and 28.8 years respectively.  The iodine was initially viewed with less alarm than the other isotopes, because of its short half-life, but it is highly volatile, and now appears to have travelled furthest and caused the most severe health problems in the short term.  Strontium, on the other hand, is the least volatile of the four, and of main concern in the areas near Chernobyl itself.  Iodine tends to become concentrated in thyroid and milk glands, leading, among other things, to increased incidence of thyroid cancers.  Caesium tends to accumulate in vital organs such as the heart, while strontium accumulates in bones, and may thus be a risk to bone-marrow and lymphocytes. Radiation is most damaging to cells that are actively dividing.  In adult mammals cell division is slow, except in hair follicles, skin, bone marrow and the gastrointestinal tract, which is why vomiting and hair loss are common symptoms of acute radiation sickness. Health in Belarus and Ukraine has shown disturbing trends following the Chernobyl disaster.  In Belarus, incidence of congenital defects had risen by 40% within six years of the accident, to the point that it became the principal cause of infant mortality.  There was a substantial increase in digestive, circulatory, nervous, respiratory and endocrine diseases and cancers, correlated with areas of high radioactive contamination, and in one especially contaminated district of Belarus, 95% of children were in 2005 reported to have at least one chronic illness.  The Ukrainian Ministry of Health estimated in 1993 that roughly 70% of its population were unwell, with large increases in respiratory, blood and nervous system diseases.  By the year 2000, the number of Ukrainians claiming to be radiation 'sufferers' (\"poterpili\") and receiving state benefits had jumped to 3.5 million, or 5% of the population.  Many of these are populations resettled from contaminated zones, or former or current Chernobyl plant workers.  According to IAEA-affiliated scientific bodies, these apparent increases of ill health result partly from economic strains on these countries and poor health-care and nutrition; also, they suggest that increased medical vigilance following the accident has meant that many cases that would previously have gone unnoticed (especially of cancer) are now being registered. Of the approximately 600,000 'liquidators' that were engaged in the Chernobyl clean-up, roughly 50,000 were required to work as 'bio-robots', in conditions of such extreme radiation that electronic robots ceased to operate.  These bio-robots are well-known figures within every village, housing block and work-collective.  Most are prematurely aged and many have died, and leukaemia rates among them are substantially higher than in the wider population.  According to ethnographer Adriana Petryna, birth defects appear to have increased in Ukraine as well.  She describes gross deformities in the Kyev city hospital's neonatal unit, including one infant born to a Chernobyl worker, who had an extra finger, a deformed ear, his trachea missing and his gut external to his body.  Hospital staff were on the whole cooperative, but warned Petryna that she would be forbidden to access any statistics; she could therefore only treat these cases as anecdotal evidence.  Poor or inaccessible statistics has meant that causal connections are very difficult to make in both Belarus and Ukraine.  It has been observed that Belarus in particular actively suppresses or ignores health-related research, a false economy estimated to cost the country ten times more than it saves.  One Belarusian villager describes: \"We had a year once when almost every day there was a funeral.  We must have buried about fifty people that year.  Is it related to radiation?  Who knows.\" Under Soviet rule, the extent of radiation injury was systematically covered up.  Most cases of acute radiation sickness (ARS) were disguised as ‘Vegetovascular dystonia’ (VvD), a Soviet classification for a type of panic disorder with possible symptoms including heart palpitations, sweating, tremors, nausea, hypotension or hypertension, neurosis, spasms and seizures: symptoms which resemble the neurological effects of ARS.  Declassified documents show that the Soviet Health Ministry ordered the systematic misdiagnosis of ARS as VvD, for all people who did not show gross signs of radiation sickness such as burns or hair loss, and for all 'liquidators' who had exceeded their maximum allowable dose.  It appears that up to 17,500 people were intentionally misdiagnosed in this manner.  Subsequent claims for health welfare were denied on the basis of this diagnosis or the application of other psychosocial medical categories (individual poor constitution; psychological self-induction).  A key tool for Soviet denial was the '35 rem concept', whereby it was held that 35 rems was a safe radiation exposure for a lifetime, \"based on international standards\", and since most people near Chernobyl received less than that, their health complaints could be attributed to \"radiophobia\". Both Belarus and Ukraine rely heavily on foreign aid and have been pressured to comply with international views of the disaster.  For instance, in 2002 the World Bank advised Belarus to \"shift its attention from calculating the impact of the accident to developing forward-looking activities directed at economic development and improvement in the quality of life of the affected people\".  Health-related government welfare was blamed for creating \"the sense of victimization and dependency\" and thus exacerbating psychosomatic disorders.  Belarus in particular has complied by ignoring or suppressing scientific research.  Historian David Marples attributes this more to that government's weakness and apathy than a simple desire to avoid health costs. Mutations in both humans and other animals increased following the disaster.  On farms in Narodychi Raion of Ukraine, for instance, in the first four years of the disaster nearly 350 animals were born with gross deformities such as missing or extra limbs, missing eyes, heads or ribs, or deformed skulls; in comparison, only three abnormal births had been registered in the five years prior.  Despite these claims, the World Health Organization states, \"children conceived before or after their father's exposure showed no statistically significant differences in mutation frequencies\".  This statistically insignificant increase was also seen by independent researchers analyzing the children of the liquidators. The two primary individuals involved with the attempt to suggest that the mutation rate amongst animals was, and continues to be, higher in the Chernobyl zone, are the Anders Moller and Timothy Mousseau group.  This group routinely draws large media attention to their sensationalist conclusions, despite their publications always being criticized by a \"phalanx\" of disparate and varying epidemiolgists, as an example of motivated reasoning and poorly conducted research, however these countless rebuttals in contrast do not make for very sensationalist print and therefore go largely unnoticed by the media.  Apart from continuing to publish experimentally unrepeatable and discredited papers, Mousseau routinely gives talks at the Helen Caldicott organized \"Physicians for Social Responsibility\" (PSR) symposiums, an ultra anti-nuclear group, devoted to bring about a \"nuclear free planet\".  Moreover, in years past Moller was previously caught and reprimanded for publishing papers that crossed the scientific \"misconduct\"/\"fraud\" line.  The duo have more recently attempted to publish meta-analyses in which the primary references they weigh-up, analyze and draw their conclusions from, is quite unorthodoxly, their own prior papers along with the discredited book \"\", which failed the peer-review process.  Favorably analyzing ones own research and drawing from references that resoundingly failed the peer-review process, is not considered best scientific practice. Infamously in 1996, geneticist colleagues Ronald Chesser and Robert Baker published a paper on the thriving vole population within the exclusion zone, in which the central conclusion of their work was essentially that \"The mutation rate in these animals is hundreds and probably thousands of times greater than normal\", this claim occurred after they had done a comparison of the mitochondrial DNA of the \"chernobyl voles\" with that of a control group of voles from outside the region.  These alarming conclusions led the paper to appear on the front cover of the prestigious journal \"Nature\", however not long after publication Chesser & Baker discovered a fundamental error in their research in which they had incorrectly classified the species of vole, and therefore were comparing the genetics of two entirely different vole species to start with.  Neither their subsequent corrections, criticism of the Moller and Mousseau barn swallow papers, nor Chesser and Baker's far more rigorous work following their humbling realization, has received the attention, publicity, or economic interest of their initial alarming, yet totally erroneous 1996 paper.  Despite them now being one of the most methodical multi-disciplinaries leading the field, they have trouble finding politicians willing to fund the large studies they would like to see. Following the accident, journalists mistrusted many medical professionals (such as the spokesman from the UK National Radiological Protection Board), and in turn encouraged the public to mistrust them.  Throughout the European continent, due to this media-driven framing of the slight contamination and in nations where abortion is legal, many requests for induced abortions, of otherwise normal pregnancies, were obtained out of fears of radiation from Chernobyl, including an excess number of abortions in Denmark in the months following the accident.  In Greece, following the accident many obstetricians were unable to resist requests from worried pregnant mothers over fears of radiation.  Although it was determined that the effective dose to Greeks would not exceed 1 mSv (100 mrem), a dose much lower than that which could induce embryonic abnormalities or other non-stochastic effects, there was an observed 2500 excess of otherwise wanted pregnancies being terminated, probably out of fear in the mother of radiation risk.  A \"slightly\" above the expected number of requested induced abortions occurred in Italy. Worldwide, an estimated excess of about 150,000 elective abortions may have been performed on otherwise healthy pregnancies out of unfounded fears of radiation from Chernobyl, according to Dr Robert Baker and ultimately a 1987 article published by Linda E. Ketchum in the \"Journal of Nuclear Medicine\" which mentions but does not reference an IAEA source on the matter. The available statistical data excludes the Soviet/Ukraine/Belarus abortion rates, as they are presently unavailable.  From the available data, an increase in the number of abortions in what were healthy developing human offspring in Denmark occurred in the months following the accident, at a rate of about 400 cases.  In Greece, there was an observed 2500 excess of otherwise wanted pregnancies being terminated.  In Italy, a \"slightly\" above the expected number of induced abortions occurred, approximately 100. As the increase in radiation in Denmark was so low...the public debate and anxiety among the pregnant women and their husbands \"caused\" more fetal deaths in Denmark than the accident.  This underlines the importance of public debate, the role of the mass media and of the way in which National Health authorities participate in this debate. No evidence of changes in the prevalence of human deformities/birth congenital anomalies which might be associated with the accident, are apparent in Belarus or the Ukraine, the two republics which had the highest exposure to fallout.  In Sweden, and Finland where no increase in abortion rates occurred, it was likewise determined that \"no association between the temporal and spatial variations in radioactivity and variable incidence of congenital malformations [was found].\"  A similar null increase in the abortion rate and a healthy baseline situation of no increase in birth defects was determined by assessing the Hungarian Congenital Abnormality Registry, Findings also mirrored in Austria.  Larger, \"mainly western European\" data sets approaching a million births in the EUROCAT database, divided into \"exposed\" and control groups were assessed in 1999.  As no Chernobyl impacts were detected, the researchers conclude \"in retrospect the widespread fear in the population about the possible effects of exposure on the unborn fetus was not justified\".  Despite studies from Germany and Turkey, the only robust evidence of negative pregnancy outcomes that transpired after the accident were these elective abortion indirect effects, in Greece, Denmark, Italy etc., due to the anxieties created. In very high doses, it was known at the time that radiation can cause a physiological increase in the rate of pregnancy anomalies, but unlike the dominant linear-no threshold model of radiation and cancer rate increases, it was known, by researchers familiar with both the prior human exposure data and animal testing, that the \"Malformation of organs appears to be a deterministic effect with a threshold dose\" below which, no rate increase is observed.  This teratology (birth defects) issue was discussed by Frank Castronovo of the Harvard Medical School in 1999, publishing a detailed review of dose reconstructions and the available pregnancy data following the Chernobyl accident, inclusive of data from Kiev's two largest obstetrics hospitals.  Castronovo concludes that \"the lay press with newspaper reporters playing up anecdotal stories of children with birth defects\" is, together with dubious studies that show selection bias, the two primary factors causing the persistent belief that Chernobyl increased the background rate of birth defects.  When the vast amount of pregnancy data does not support this perception as no women took part in the most radioactive liquidator operations, no pregnant individuals were exposed to the threshold dose. The liquidators, essentially an all male civil defense emergency workforce, would go on to father normal children, without an increase in developmental anomalies or a statistically significant increase in the frequencies of germline mutations in their progeny.  This normality is similarly seen in the children of the survivors of the Goiana accident. Due in largest part from the ingestion of contaminated dairy products along with the inhalation of the short-lived and therefore highly radioactive isotope, Iodine-131, the 2005 UN collaborative \"Chernobyl Forum\" revealed thyroid cancer among children to be one of the main health impacts from the Chernobyl accident.  In that publication more than 4000 cases were reported, and that there was no evidence of an increase in solid cancers or leukemia.  It said that there was an increase in psychological problems among the affected population.  Dr Michael Repacholi, manager of WHO's Radiation Program reported that the 4000 cases of thyroid cancer resulted in nine deaths. According to UNSCEAR, up to the year 2005, an excess of over 6000 cases of thyroid cancer have been reported.  That is, over the estimated pre-accident baseline thyroid cancer rate, more than 6000 casual cases of thyroid cancer have been reported in children and adolescents exposed at the time of the accident, a number that is expected to increase.  They concluded that there is no other evidence of major health impacts from the radiation exposure. Well-differentiated thyroid cancers are generally treatable, and when treated the five-year survival rate of thyroid cancer is 96%, and 92% after 30 years.  UNSCEAR had reported 15 deaths from thyroid cancer in 2011.  The International Atomic Energy Agency (IAEA) also states that there has been no increase in the rate of birth defects or abnormalities, or solid cancers (such as lung cancer) corroborating UNSCEAR's assessments.  UNSCEAR does raise the possibility of long term genetic defects, pointing to a doubling of radiation-induced minisatellite mutations among children born in 1994.  However, the risk of thyroid cancer associated with the Chernobyl accident is still high according to published studies. The German affiliate of the ultra-anti-nuclear energy organization, the \"International Physicians for the Prevention of Nuclear War\" (IPPNW) attempt to suggest that 10,000 people are affected by thyroid cancer as of 2006 and that 50,000 cases are expected in the future. Fred Mettler, a radiation expert at the University of New Mexico, puts the number of worldwide cancer deaths outside the highly contaminated zone at \"perhaps\" 5000, for a total of 9000 Chernobyl-associated fatal cancers, saying \"the number is small (representing a few percent) relative to the normal spontaneous risk of cancer, but the numbers are large in absolute terms\".  The same report outlined studies based in data found in the Russian Registry from 1991 to 1998 that suggested that \"of 61,000 Russian workers exposed to an average dose of 107 mSv about 5% of all fatalities that occurred may have been due to radiation exposure.\" The report went into depth about the risks to mental health of exaggerated fears about the effects of radiation.  According to the IAEA the \"designation of the affected population as \"victims\" rather than \"survivors\" has led them to perceive themselves as helpless, weak and lacking control over their future\".  The IAEA says that this may have led to behaviour that has caused further health effects. Fred Mettler commented that 20 years later: \"The population remains largely unsure of what the effects of radiation actually are and retain a sense of foreboding.  A number of adolescents and young adults who have been exposed to modest or small amounts of radiation feel that they are somehow fatally flawed and there is no downside to using illicit drugs or having unprotected sex.  To reverse such attitudes and behaviours will likely take years although some youth groups have begun programs that have promise.\"  In addition, disadvantaged children around Chernobyl suffer from health problems that are attributable not only to the Chernobyl accident, but also to the poor state of post-Soviet health systems. The United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR), part of the Chernobyl Forum, have produced their own assessments of the radiation effects.  UNSCEAR was set up as a collaboration between various United Nation bodies, including the World Health Organization, after the atomic bomb attacks on Hiroshima and Nagasaki, to assess the long-term effects of radiation on human health. The number of potential deaths arising from the Chernobyl disaster is heavily debated.  The WHO's prediction of 4000 future cancer deaths in surrounding countries is based on the Linear no-threshold model (LNT), which assumes that the damage inflicted by radiation at low doses is directly proportional to the dose.  Radiation epidemiologist Roy Shore contends that estimating health effects in a population from the LNT model \"is not wise because of the uncertainties\". According to the Union of Concerned Scientists the number of excess cancer deaths worldwide (including all contaminated areas) is approximately 27,000 based on the same LNT. Another study critical of the Chernobyl Forum report was commissioned by Greenpeace, which asserted that the most recently published figures indicate that in Belarus, Russia and Ukraine the accident could have resulted in 10,000–200,000 additional deaths in the period between 1990 and 2004.  The Scientific Secretary of the Chernobyl Forum criticized the report's reliance on non-peer-reviewed locally produced studies.  Although most of the study's sources were from peer-reviewed journals, including many Western medical journals, the higher mortality estimates were from non-peer-reviewed sources, while Gregory Härtl (spokesman for the WHO) suggested that the conclusions were motivated by ideology. \"Chernobyl: Consequences of the Catastrophe for People and the Environment\" is an English translation of the 2007 Russian publication \"Chernobyl\".  It was published in 2009 by the New York Academy of Sciences in their \"Annals of the New York Academy of Sciences\".  It presents an analysis of scientific literature and concludes that medical records between 1986, the year of the accident, and 2004 reflect 985,000 premature deaths as a result of the radioactivity released.  Though, it was impossible to precisely determine what dose the affected people received, knowing the fact that the received doses varied strongly from one individual to the other in the population above which the radioactive cloud travelled, and also knowing the fact that one cannot tell for sure if a cancer in an individual from the former USSR is produced by radiation from the Chernobyl accident or by other social or behavioural factors, such as smoking or alcohol drinking. The authors suggest that most of the deaths were in Russia, Belarus and Ukraine, though others occurred worldwide throughout the many countries that were struck by radioactive fallout from Chernobyl.  The literature analysis draws on over 1000 published titles and over 5000 internet and printed publications discussing the consequences of the Chernobyl disaster.  The authors contend that those publications and papers were written by leading Eastern European authorities and have largely been downplayed or ignored by the IAEA and UNSCEAR. The estimated human health impacts were criticized by M. I. Balonov of the Institute of Radiation Hygiene, St. Petersburg, Russia who described them as biased, drawing from sources which were difficult to independently verify and lacking a proper scientific base.  Balanov expressed his opinion that \"the authors unfortunately did not appropriately analyze the content of the Russian-language publications, for example, to separate them into those that contain scientific evidence and those based on hasty impressions and ignorant conclusions\". According to Kenneth Mossman, a Professor of Health Physics and member of the U.S. Nuclear Regulatory Commission advisory committee, the \"LNT philosophy is overly conservative, and low-level radiation may be less dangerous than commonly believed\".  Yoshihisa Matsumoto, a radiation biologist at the Tokyo Institute of Technology, cites laboratory experiments on animals to suggest there must be a threshold dose below which DNA repair mechanisms can completely repair any radiation damage.  Mossman suggests that the proponents of the current model believe that being conservative is justified due to the uncertainties surrounding low level doses and it is better to have a \"prudent public health policy\". Another significant issue is establishing consistent data on which to base the analysis of the impact of the Chernobyl accident.  Since 1991 large social and political changes have occurred within the affected regions and these changes have had significant impact on the administration of health care, on socio-economic stability, and the manner in which statistical data is collected.  Ronald Chesser, a radiation biologist at Texas Tech University, says that \"the subsequent Soviet collapse, scarce funding, imprecise dosimetry, and difficulties tracking people over the years have limited the number of studies and their reliability\". It is difficult to establish the total economic cost of the disaster.  According to Mikhail Gorbachev, the Soviet Union spent 18 billion rubles (the equivalent of US$18 billion at that time) on containment and decontamination, virtually bankrupting itself.  In Belarus the total cost over 30 years is estimated at US$235 billion (in 2005 dollars).  Ongoing costs are well known; in their 2003–2005 report, The Chernobyl Forum stated that between 5% and 7% of government spending in Ukraine is still related to Chernobyl, while in Belarus over $13 billion is thought to have been spent between 1991 and 2003, with 22% of national budget having been Chernobyl-related in 1991, falling to 6% by 2002.  Much of the current cost relates to the payment of Chernobyl-related social benefits to some 7 million people across the 3 countries. A significant economic impact at the time was the removal of 784320 ha of agricultural land and 694200 ha of forest from production.  While much of this has been returned to use, agricultural production costs have risen due to the need for special cultivation techniques, fertilizers and additives. Politically, the accident gave great significance to the new Soviet policy of glasnost, and helped forge closer Soviet–US relations at the end of the Cold War, through bioscientific cooperation.  The disaster also became a key factor in the Union's eventual 1991 dissolution, and a major influence in shaping the new Eastern Europe. Both Ukraine and Belarus, in their first months of independence, lowered legal radiation thresholds from the Soviet Union's previous, elevated thresholds (from 35 rems per lifetime under the USSR to 7 rems per lifetime in Ukraine and 0.1 rems per year in Belarus).  This required an expansion of territories that were considered contaminated.  In Ukraine, over 500,000 people have now been resettled, many of whom have become applicants for medical and other welfare.  Ukraine also maintains the destroyed reactor, for which it employs a very large workforce in order to keep individual exposure times low.  Many of these workers have since registered disabilities and enrolled for welfare.  In Ukraine, the Chernobyl disaster was an icon of the nationalist movement, symbolic of all that was wrong with the Soviet Union, and welfare became a key platform for winning independence.  Ukraine has since developed a massive and burdensome welfare system that has become increasingly corrupt and ineffective.  It has presented its greatly increased welfare demands since 1991 as a demonstration of its own moral legitimacy, and as an argument for needing foreign aid.  Belarus, on the other hand, was politically weak when it gained independence, and looked to Moscow for guidance; in many ways it has returned to the old Soviet policy of secrecy and denial. Following the accident, questions arose about the future of the plant and its eventual fate.  All work on the unfinished reactors 5 and 6 was halted three years later.  However, the trouble at the Chernobyl plant did not end with the disaster in reactor 4.  The damaged reactor was sealed off and 200 m3 of concrete was placed between the disaster site and the operational buildings.  The work was managed by Grigoriy Mihaylovich Naginskiy, the deputy chief engineer of Installation and Construction Directorate – 90.  The Ukrainian government continued to let the three remaining reactors operate because of an energy shortage in the country. In October 1991, a fire broke out in the turbine building of reactor 2; the authorities subsequently declared the reactor damaged beyond repair, and it was taken offline.  Reactor 1 was decommissioned in November 1996 as part of a deal between the Ukrainian government and international organizations such as the IAEA to end operations at the plant.  On 15 December 2000, then-President Leonid Kuchma personally turned off Reactor 3 in an official ceremony, shutting down the entire site. The Chernobyl reactor is now enclosed in a large concrete sarcophagus, which was built quickly to allow continuing operation of the other reactors at the plant. A New Safe Confinement was to have been built by the end of 2005; however, it has suffered ongoing delays and as of 2010 , when construction finally began, was expected to be completed in 2013.  This was delayed again to 2016, the end of the 30-year lifespan of the sarcophagus.  The structure is being built adjacent to the existing shelter and will be slid into place on rails.  It is to be a metal arch 105 m high and spanning 257 m , to cover both unit 4 and the hastily built 1986 structure.  The Chernobyl Shelter Fund, set up in 1997, has received €810 million from international donors and projects to cover this project and previous work.  It and the Nuclear Safety Account, also applied to Chernobyl decommissioning, are managed by the European Bank for Reconstruction and Development (EBRD). As of 29 November 2016, Reactor No. 4 has been covered by the New Safe Confinement that covers the reactor and the unstable “sarcophagus”.  The huge steel arch was moved into place over several weeks, and the completion of this procedure was celebrated with a ceremony at the site, attended by the Ukrainian president, Petro Poroshenko, diplomats and site workers. By 2002, roughly 15,000 Ukrainian workers were still working within the Zone of Exclusion, maintaining the plant and performing other containment- and research-related tasks, often in dangerous conditions. A handful of Ukrainian scientists work inside the sarcophagus, but outsiders are rarely granted access.  In 2006 an Australian \"60 Minutes\" team led by reporter Richard Carleton and producer Stephen Rice were allowed to enter the sarcophagus for 15 minutes and film inside the control room. On 12 February 2013, a 600 m2 section of the roof of the turbine-building, adjacent to the sarcophagus, collapsed.  At first it was assumed that the roof collapsed because of the weight of snow on it.  However the amount of snow was not exceptional, and the report of a Ukrainian fact-finding panel concluded that the part collapse of the turbine-building was the result of sloppy repair work and aging of the structure.  The report mentioned the possibility that the repaired part of the turbine-building added a larger strain on the total structure than expected, and the braces in the roof were damaged by corrosion and sloppy welding.  Experts such as Valentin Kupny, former deputy director of the nuclear plant, did warn that the complex was on the verge of a collapse, leaving the building in an extremely dangerous condition.  A proposed reinforcement in 2005 was cancelled by a superior official.  After the 12 February incident, radioactivity levels were up to 19 becquerels per cubic meter of air: 12 times normal.  The report assumed radioactive materials from inside the structure spread to the surroundings after the roof collapsed.  All 225 workers employed by the Chernobyl complex and the French company that is building the new shelter were evacuated shortly after the collapse.  According to the managers of the complex, radiation levels around the plant were at normal levels (between 5 and 6 µSv/h) and should not affect workers' health.  According to Kupny the situation was underestimated by the Chernobyl nuclear complex managers, and information was kept secret. s of 2006 , some fuel remained in the reactors at units 1 through 3, most of it in each unit's spent fuel pool, as well as some material in a small spent fuel interim storage facility pond (ISF-1). In 1999 a contract was signed for construction of a radioactive waste management facility to store 25,000 used fuel assemblies from units 1–3 and other operational wastes, as well as material from decommissioning units 1–3 (which will be the first RBMK units decommissioned anywhere).  The contract included a processing facility able to cut the RBMK fuel assemblies and to put the material in canisters, which were to be filled with inert gas and welded shut. The canisters were to be transported to dry storage vaults, where the fuel containers would be enclosed for up to 100 years.  This facility, treating 2500 fuel assemblies per year, would be the first of its kind for RBMK fuel.  However, after a significant part of the storage structures had been built, technical deficiencies in the concept emerged, and the contract was terminated in 2007.  The interim spent fuel storage facility (ISF-2) will now be completed by others by mid-2013. Another contract has been let for a liquid radioactive waste treatment plant, to handle some 35,000 cubic meters of low- and intermediate-level liquid wastes at the site.  This will need to be solidified and eventually buried along with solid wastes on site. In January 2008, the Ukrainian government announced a 4-stage decommissioning plan that incorporates the above waste activities and progresses towards a cleared site According to official estimates, about 95% of the fuel in Reactor 4 at the time of the accident (about 180 metric tons) remains inside the shelter, with a total radioactivity of nearly 18 million curies (670 PBq).  The radioactive material consists of core fragments, dust, and lava-like \"fuel containing materials\" (FCM)—also called \"corium\"—that flowed through the wrecked reactor building before hardening into a ceramic form. Three different lavas are present in the basement of the reactor building: black, brown, and a porous ceramic.  The lava materials are silicate glasses with inclusions of other materials within them.  The porous lava is brown lava that dropped into water and thus cooled rapidly. It is unclear how long the ceramic form will retard the release of radioactivity.  From 1997 to 2002 a series of published papers suggested that the self-irradiation of the lava would convert all 1,200 metric tons into a submicrometre and mobile powder within a few weeks.  But it has been reported that the degradation of the lava is likely to be a slow and gradual process rather than sudden and rapid.  The same paper states that the loss of uranium from the wrecked reactor is only 10 kg per year; this low rate of uranium leaching suggests that the lava is resisting its environment.  The paper also states that when the shelter is improved, the leaching rate of the lava will decrease. Some of the surfaces of the lava flows have started to show new uranium minerals such as čejkaite (Na4(UO2)(CO3)3 ) and uranyl carbonate.  However, the level of radioactivity is such that during 100 years, the lava's self irradiation ( α decays per gram and 2 to of β or γ) will fall short of the level required to greatly change the properties of glass (10 α decays per gram and 10 to 10 Gy of β or γ).  Also the lava's rate of dissolution in water is very low (10 g·cm·day), suggesting that the lava is unlikely to dissolve in water. An area originally extending 30 km in all directions from the plant is officially called the \"zone of alienation\".  It is largely uninhabited, except for about 300 residents who have refused to leave.  The area has largely reverted to forest, and has been overrun by wildlife because of a lack of competition with humans for space and resources.  Even today, radiation levels are so high that the workers responsible for rebuilding the sarcophagus are only allowed to work five hours a day for one month before taking 15 days of rest.  Ukrainian officials estimated the area would not be safe for human life again for another 20,000 years (although by 2016, 187 local Ukrainians had returned and were living permanently in the zone). In 2011 Ukraine opened up the sealed zone around the Chernobyl reactor to tourists who wish to learn more about the tragedy that occurred in 1986. If the forests that have been contaminated by radioactive material catch on fire, they will spread the radioactive material further outwards in the smoke. The Chernobyl Shelter Fund was established in 1997 at the Denver 23rd G8 summit to finance the Shelter Implementation Plan (SIP).  The plan calls for transforming the site into an ecologically safe condition by means of stabilization of the sarcophagus followed by construction of a New Safe Confinement (NSC).  While the original cost estimate for the SIP was US$768 million, the 2006 estimate was $1.2 billion.  The SIP is being managed by a consortium of Bechtel, Battelle, and Électricité de France, and conceptual design for the NSC consists of a movable arch, constructed away from the shelter to avoid high radiation, to be slid over the sarcophagus.  The NSC is expected to be completed in 2015, and will be the largest movable structure ever built. Dimensions: The United Nations Development Programme has launched in 2003 a specific project called the Chernobyl Recovery and Development Programme (CRDP) for the recovery of the affected areas.  The programme was initiated in February 2002 based on the recommendations in the report on Human Consequences of the Chernobyl Nuclear Accident.  The main goal of the CRDP's activities is supporting the Government of Ukraine in mitigating long-term social, economic, and ecological consequences of the Chernobyl catastrophe.  CRDP works in the four most Chernobyl-affected areas in Ukraine: Kyivska, Zhytomyrska, Chernihivska and Rivnenska. The International Project on the Health Effects of the Chernobyl Accident (IPEHCA) was created and received US $20 million, mainly from Japan, in hopes of discovering the main cause of health problems due to I radiation.  These funds were divided among Ukraine, Belarus, and Russia, the three main affected countries, for further investigation of health effects.  As there was significant corruption in former Soviet countries, most of the foreign aid was given to Russia, and no positive outcome from this money has been demonstrated. Chernobyl Children International (CCI) is a United Nations-accredited, non-profit, international development, medical, and humanitarian organization that works with children, families and communities that continue to be affected by the economic outcome of the Chernobyl accident.  The organization's founder and chief executive is Adi Roche.  The CCI was founded in 1991 in response to an appeal from Ukrainian and Belarusian doctors for aid.  Roche then began organizing 'rest and recuperation' holidays for a few Chernobyl children.  Recruiting Irish families who would welcome and care for them, CCI expanded into the United States in 2001. It works closely with the Belarusian government, the United Nations, and many thousands of volunteers worldwide to deliver a broad range of economic supports to the children and the wider community.  It also acts as an advocate for the rights of those affected by the Chernobyl explosion, and engages in research and outreach activities to encourage the rest of the world to remember the victims and understand the long-term impact on their lives. \"The Front Veranda\" (1986), a lithograph by Susan Dorothea White in the National Gallery of Australia, exemplifies worldwide awareness of the event.  \"Heavy Water: A Film for Chernobyl\" was released by Seventh Art in 2006 to commemorate the disaster through poetry and first-hand accounts.  The film secured the Best Short Documentary at Cinequest Film Festival as well as the Rhode Island \"best score\" award along with a screening at Tate Modern. Chernobyl Way is an annual rally run on 26 April by the opposition in Belarus as a remembrance of the Chernobyl disaster. The Chernobyl accident attracted a great deal of interest.  Because of the distrust that many people (both within and outside the USSR) had in the Soviet authorities, a great deal of debate about the situation at the site occurred in the First World during the early days of the event.  Because of defective intelligence based on photographs taken from space, it was thought that unit number three had also suffered a dire accident. Journalists mistrusted many professionals (such as the spokesman from the UK NRPB), and they in turn encouraged the public to mistrust them. In Italy, the Chernobyl accident was reflected in the outcome of the 1987 referendum.  As a result of that referendum, Italy began phasing out its nuclear power plants in 1988, a decision that was effectively reversed in 2008.  A referendum in 2011 reiterated Italians' strong objections to nuclear power, thus abrogating the government's decision of 2008. In Germany, the Chernobyl accident led to the creation of a federal environment ministry, after several states had already created such a post.  The minister was given the authority over reactor safety as well, which the current minister still holds as of 2015.  The events are also credited with strengthening the anti-nuclear power movement, which culminated in the decision to end the use of nuclear power that was made by the 1998–2005 Schröder government. Explanatory notes Citations Sources The source documents relating to the emergency, published in unofficial sources:\n\nMaritime history of California In the California coast, the use of ships and the Pacific Ocean has historically included water craft (such as dugouts, canoes, sailing ships, and steamships), fisheries, shipbuilding, Gold Rush shipping, ports, shipwrecks, naval ships and installations, and lighthouses.  The maritime history of California can be divided into several periods: the Native American period; European exploration period from 1542 to 1769; the Spanish colonial period, 1769 to 1821; the Mexican period, 1821 to 1847; and United States statehood period, which continues to the present day. In the northwest coast of California near the redwood forests several Indian tribes developed large dugout canoes they used for fishing, trade and warfare.  These canoes were constructed by taking a large tree and shaping it with hand tools and fire to a boat's configuration.  A redwood log 4 m long and 240 cm diameter weighs about 2000 kg .  This large weight meant that the logs were selected that required a minimum of movement—usually driftwood or dead fall trees that had been blown over by the wind.  Sometimes logs were cut to length and rolled into water where they could be floated to a selected work area.  The logs were usually cut to length by fire and stone age hand tools and the interior of the canoe was typically burned out with small fires.  The basic procedure was to start a small fire on the tree where it needed shaping, then extinguish it after a short burn.  This would leave one or more centimeters of charred wood where the fire was built that would be easier to remove.  By successively using small fires to char the areas that needed to be worked the logs could be shaped by the crude scrapers and rock, shellfish and horn based tools available.  A finished 4 m long dugout canoe with a nominal 5 cm thickness still weighed over 100 kg .  Most larger dugouts weighed too much to move easily and were usually just pulled up on a beach far enough to get them above high tide.  Constructing these types of dugout canoes took considerable time and skill with stone age tools and fire.  Dugout canoes typically lasted several years. Tule (\"Schoenoplectus acutus\" also called bulrushes) have a thin (~1 cm or 0.5 inch) diameter, rounded green stems that grows to 1 to 3 metres (3–10 ft) tall.  They grow well in marshes, wetlands or at the edges of bodies of water.  The tule stem has a pithy interior filled with spongy tissue packed with air cells—this makes it float well on water as well as a good insulator.  Native Americans used tule for making and thatching huts, baskets, mats, boats, decoys, hats, clothing and shoes.  Tule was typically cut using deer scapula 'saws' that had rough saw like edges cut into them.  Tule has to be handled with care when green to avoid breaking the stem and gains strength when it is partially dried. To make a tule boat, green tule was cut and then spread out in the sun to dry for several days.  Tule canoes were constructed of cut stalks of tule plants bundled together around a willow 'core' for extra strength.  The bundle of tules could be pre-bent as they were being bundled to form a raised prow and stern.  The length of each bundle depends on the size of the boat that were then typically about 10 ft to 15 ft .  The bundle that formed the bottom of the canoe on which the boatman or boatmen sat, knelt or stood was much larger than the others.  To make the sides of the tule canoe two to six tapered bundles were tied to the bottom bundle with grape vines or other native material with extensive lacing at the stern and prow to bend all the tule bundles into a tapered and raised bow and stern.  Tule canoes typically accommodated one to four people.  Tule boats can be quickly built from dried tule, by experienced canoe builders, in less than one day.  Tule boats have a limited useful life before they rot and/or come apart—typically only lasting a few weeks. Several tribes in and around the San Francisco Bay area and in northern California made and used tule canoes (also called balsas).  Bay Miwok, Coast Miwok, Ohlone (Costanoan), Pomo, Klamath, Modoc and several other indigenous natives used the tule plant to make canoes.  Tule canoes were used in ocean lagoons from Tomales Bay and Point Reyes National Seashore south to perhaps Monterey Bay.  Tule–reed boats were used in lakes, bays and slow-moving rivers in much of Northern California.  They were used by the Pomo living in the Laguna de Santa Rosa and Clear Lake, Tule Lake and other areas.  They were common in the San Francisco Bay and on the extensive Sacramento–San Joaquin River Delta and its tributary rivers. These tule canoes were used for transportation to and from their favorite spots for hunting or harvesting salmon, acorns, seeds, berries, shellfish or oysters and other fish or foods.  Extensive beds and shoals of oysters (Ostrea lurida) and other shellfish then lay in shallow water near the shores of San Francisco Bay and Tomales Bay and were a food source used for centuries.  Tule canoes were also used for gathering more tule reeds and for hunting duck or geese which were then often present in the wetlands, etc. in the millions.  Tule canoes were used in collecting aquatic food plants and duck and goose eggs.  Ducks and geese were often hunted from tule canoes with arrows or nets.  Tule canoes were used in fishing with nets, spears or bone fish hooks for several native fish species present in or migrating through the rivers, ocean and bays. The boatman typically sits, kneels or stands in the boat and either paddles it with a double bladed paddle or with his arms in a single person canoe when lying prone.  If the boat was not woven tightly enough, then the boatman would find himself sitting, standing or kneeling in several inches of water.  The tule canoes were often used for transportation to oyster mollusk and other shellfish beds that could be harvested at low tide.  The Emeryville Shellmound or midden composed almost entirely of the inedible shells of different types of shellfish, presumably harvested utilizing tule boats, is an example of the over 400 shell mounds known in the San Francisco Bay area.  These often massive shell mounds (the Emeryville Shellmound was originally reported as being 60 ft high by 350 ft long), were often built up over centuries of shell discards and showed a stable source of easily obtained shellfish utilized for many hundreds of years.  It is believed that shellfish was a major if not the main source of food for many Native American people. To see pictures of tule canoes use the \"image\" option of Google, Bing, etc. and type in \"Tule canoe\" and search--several images are usually found that may be clicked on for more information.  Local conservation groups often have courses in building tule canoes. An ancient maritime culture dating back some 8,000 years, perhaps earlier, has been documented by recent dating of middens on San Clemente Island, some 60 miles offshore Southern California.  Native California peoples lived in large settled villages along the Pacific coastline and on the Channel Islands of California for thousands of years before European contact. In some areas, such as along the Santa Barbara Channel separating the Channel Islands of California from the California coast the Chumash and Tongva people in these villages developed highly sophisticated canoes.  These canoes were used in fishing and in widespread trade between different villages on and off the Channel Islands of California.  Boat construction reached its highest development in California among the Chumash and Tongva people.  Their sewn plank canoes, called a \"tomol\", impressed early explorers of the California coast for its versatility, seaworthiness and size. The canoes were typically made out of planks split from redwood (Sequoioideae) or pine driftwood washed up on the shore.  This driftwood was usually chosen because it was available and usually knot free and easy to work with.  Some of these driftwood logs were selected, cut to length, split, shaped and then their split out planks \"\"sewn\"\" together to form a canoe.  The side planks and canoe bottom were split out of straight knot free logs utilizing whalebone and antler Wedges driven by rock mallets.  The planks were then shaped, trimmed and leveled using flint and seashell tools and shark hide sandpaper.  Where planks needed to be connected holes were bored in the planks using wood drills tipped with chert or bone.  These drilled planks were then connected by \"\"sewing\"\" split and shaped knot free planks together on their ends to get the necessary length.  They were typically fastened together with red milkweed (tok) fiber cords.  After the planks had been shaped and sewn together for length they were carefully shaped, bent and mounted six to eight planks vertically to form the canoe's sides around a large split bottom plank that formed the bottom of the canoe.  Over 20 pieces of shaped wood are used to make a typical tomol.  Once the planks were bent, fitted and lashed together the heart of dry tule rush was forced into the cracks between the planks on the outside of the canoe hull to act as caulking.  All seams between planks, plank ends and holes for cords or thongs were then caulked with 'yop', a mixture of hard tar and pine pitch melted and then boiled.  In many respects their boat construction technique mirrored that utilized for making small wooden boats around the world.  The lack of metal tools and fasteners forced them to use stone age tools and materials. These canoes were built to carry from 3 to 10 people, one of which was usually assigned to bail, and the rest propelled the canoe by using rough oars.  The typical tomol was 12 ft to 24 ft long with a beam of 3 ft to 5 ft .  Sea voyages of over 130 mi have been recorded for these craft.  They fished the sea with fishing nets, harpoons, spears and bone fish hooks.  One of their common net catches were sardines and larger sardines called pilchards—then common in large schools off the California coast.  The Chumash had settlements on the main California coast and on the northern Channel Islands of California.  The Tongva (Gabrielino-Tongva Tribe) had several small settlements on the southern Channel Islands as well as villages on the main California southwest coast. Chumash and Tongva trading expeditions between the mainland and the Channel Islands were common.  Most were to obtain steatite for soapstone bowls and effigy figurines.  The remains of this prehistoric seafaring is being investigated by underwater archaeologists.  At least 25 individual sites have been reported between Ventura, California Beach and Point Conception. In 1539, Francisco de Ulloa under commission from the Viceroyalty of New Spain and New Spain (Mexico) conqueror, Hernán Cortés, explored the Gulf of California to the Colorado River—establishing Baja California as a peninsula.  Ulloa then went 800 mi south down the Baja California peninsula in the Gulf of California and rounding the tip of the peninsula turned north and explored the west coast of the Baja peninsula—perhaps to the 28th parallel (near the Isla Natividad).  Ulloa's sailing ships battered by adverse winds and his men wracked by scurvy, returned to New Spain (Mexico) without exploring further. The first European expedition to explore the upper California coast was led by the Portuguese/Spanish explorer and conquistador João Rodrigues Cabrilho (c. 1499–1543).  Cabrillo shipped for Havana as a young man and joined forces with Hernán Cortés in New Spain in about 1520 as a conquistador crossbow man.  In the conquest of the Aztec capital of Tenochtitlan (Mexico City) in 1521 Cortez directed Cabrillo to build thirteen 40 ft boats to fight on the lake then in the center of Tenochitlan.  Rapidly advancing in rank under Cortez's direction he participated in the conquest of El Salvador and Guatemala and was rewarded by being granted an extensive Encomienda (a feudal grant of land including the occupants on it) controlling vast land and Native American resources in Guatemala.  His success in guiding the Native Americans on his Encomienda in mining gold in Guatemala, made him one of the richest of the conquistadors in Mexico and Guatemala.  Sponsored by Pedro de Alvarado, the Guatemala governor, Cabrillo's directed the building of several small sailing ships in Guatemala—the first on the Pacific coast.  After Alvarado's death in 1541 the new Viceroy of New Spain, Antonio de Mendoza took over control of the shipyards and directed Cabrillo to build three ships and lead an expedition further up the Pacific Coast in search of more rich Native American civilizations like the Aztec and Incas.  They were also to see if there was a shorter way to China—the mythical Strait of Anián (or Northwest Passage) connecting the Pacific Ocean with the Atlantic Ocean. To build the ships the anchors, sails, shipbuilding tools and metal fittings were imported from Spain and then ported by mule and Native American porters across Mexico and then south to Guatemala.  Cabrillo, a former shipbuilder, with his Spanish assistants and Native American workers had the necessary lumber sawed out and assembled to make the first sailing ships built on the America's Pacific coast—in Guatemala.  The ships finished lumber and timbers was sawed out of trees with \"new\" steel saws manned by Native American laborers under the direction of a few Spanish shipbuilders.  The ships built for exploring the Pacific were small open caravels and small bergantina (small open sailboat) built and manned by a mixture of Native Americans and Spanish sailors and conquistadors. The last sailing ships built under Cabrillo's direction were the California exploration fleet: caravels, \"San Salvador\" (about 100 ft long) and the smaller \"Victoria\", and a bergantina (small sail boat or launch), \"San Miguel\".  Cabrillo captained the \"San Salvador\" and Bartolomé Ferrer the \"Victoria\".  These vessels were the first European sailing ships to visit the future state of California. After the California exploration ships were built, Cabrillo and his mixed crews of conquistadors, Spanish and untrained Native American sailors totaling about 200 men, carefully made their way north from Navidad, Mexico up the Pacific coast starting on 17 June 1542.  They took enough supplies to last about two years.  The combination of the south flowing California Current and often opposing winds made progress north up the coast agonizingly slow.  The small, rudely made open boats with only partially trained crews caused the crews to suffer miserably in the storms they encountered on their way.  After landing several times on the Baja California coast for water, wood and whatever supplies they could scrounge they finally, after traveling one hundred and three days, entered San Diego Bay on 28 September 1542.  They continued north up the California coast encountering many Indian villages using Native American \"tomols\" (ocean-going stitched canoes).  The continued north up the coast possibly as far as Point Reyes California. On 23 November 1542, the little fleet limped back down the coast to \"San Salvador\" (identified as today's Santa Catalina Island, California or Santa Rosa Island) to overwinter and make repairs.  There, around Christmas Eve 1542, Cabrillo stepped out of his boat and splintered his shin when he stumbled on a jagged rock.  The injury developed gangrene and he died on 3 January 1543 and was buried in an unknown location.  His second-in-command, Bartolomé Ferrer, brought the remainder of the party back to Barra de Navidad, Mexico where they arrived 14 April 1543.  They had found no gold or silver wealth, no advanced Indian civilization, no agriculture and no Northwest passage.  As a result, California was of little further interest to the Spanish who would basically ignore it for over 220 years. In 1565 the Spanish developed a Manila galleon trade route (also called \"nao de la China\") where they took silver minted in the Potosí area of Peru or in Mexico and traded it for gold, silk, porcelain, spices and other goods from China and other Asian areas including the Spice Islands.  There was a great demand for silver in China.  They also traded for gold objects which could be gotten in China in this time period at a silver:gold exchange rate of about 5:1 whereas the rate in Europe was about 16:1.  The Spanish centered their trade in the Philippines at first around Cebu, which they had recently conquered, and later in Manila after they conquered it.  The trade between the Philippines and Mexico involved using an annual round trip passage of one or more Manila galleons.  These poorly defended galleons left Acapulco Mexico loaded with silver and sailed to the Philippines in about 90 days following what's called now the north equatorial current and trade winds. The higher-latitude Westerlies trade winds and current from east to west at about 30-40 degrees latitude, was not known as a way across the Pacific Ocean until Andrés de Urdaneta's voyage in the 40 ton \"San Lucas\" in 1565.  Returning to Mexico from the Philippines the Manila Galleons went north to about 40 degrees latitude and then turning East they could use the Westerlies trade winds and currents to go east.  They were loaded with a years worth of Oriental trade goods accumulated in the Philippines.  These galleons, after crossing most of the Pacific Ocean, would arrive off the California coast from four to seven months later somewhere near Cape Mendocino (about 300 mi north of San Francisco) at about 40 degrees N. latitude.  They then could turn right and sail south down the California coast utilizing the available winds and the south flowing (≈1 mi/hr (1.6 km/h)) California Current.  The maps and charts were poor and the California coast was often shrouded in fog, so most journeys were well off shore to avoid the Farallon and California Channel Islands.  After sailing about 1500 mi south and passing the Baja Peninsula tip and crossing the Gulf of California they followed the western coast of Mexico to Acapulco, Mexico.  Acapulco was chosen as a home port because of its excellent harbor facilities and its easy access to the city of Veracruz, Mexico on the Caribbean. These galleons were some the largest the Spanish built in the 16th and 17th centuries.  Because of the limited number of ships and the highly profitable cargo they increased ship size up to 1,700 to 2,000 tons and from seven hundred to over one thousand people would take passage back to Acapulco on these vessels.  The Manila galleon trade (See: Spanish treasure fleet) was one of the most persistent, perilous, and profitable commercial enterprises in European colonial history.  This highly profitable trade (profits could reach 200-300%) with an almost annual trip by one to two ships to the Philippines and back down the California coast was continued for over 200 years.  The number of ships was limited by the Spanish Crown which got 20% of all profits.  Because of the high profit and royal taxes smuggling was rampant on these ships.  Because of the harsh trip and high profits most officers and crews only made one trip before finding something else to do.  The ships were mostly built in the Philippines using Filipino laborers to saw out the timber, weave the sails, etc. with Chinese craftsman and blacksmiths doing the ship assembly under the direction of Spanish shipbuilders. The English explorer and privateer Francis Drake sailed along the coast of California in 1579 after capturing two Spanish treasure ships headed for the Philippines in the Pacific.  It is believed that he landed somewhere on the California coast.  There his only surviving ship, the Golden Hind, set up friendly relations with the local Indians and underwent extensive repairs and cleaning of his hull.  Needed supplies of food, water and wood were accumulated by trade and foraging for a trip across the Pacific.  Leaving California he followed Ferdinand Magellan on the second recorded circumnavigation of the world and the first English circumnavigation of the world, being gone from 1577 to 1580.  He returned with several tons of silver and gold.  It is believed Drake put ashore somewhere north of San Francisco.  The exact location of Drake's landing is still undetermined, but a prominent bay on the California coast, Drakes Bay, bears his name.  He claimed the land for England, calling it Nova Albion.  The term \"Nova Albion\" was often used on many European maps to designate territory north of the Spanish Pacific coast settlements.  Spanish maps, explorations etc., of this and later eras were generally not published, being regarded as state secrets by the Spanish monarchy.  As was typical in this era, there were conflicting claims to the same territory, and the Indians who lived there were never consulted. After Thomas Cavendish successfully raided the Manila galleon \"Santa Ana\" off the tip of Baja California in 1587 an attempt was made to explore the coast for a possible town site in California for replenishing and protecting the Manila galleons. Exploration by these Manila galleons met with disaster when the Manila galleon \"San Agustin\" got too close to the Point Reyes, California coast in a storm in 1595 and was shipwrecked.  Subsequently, the Spanish crown decreed that no further exploration or colonization attempts in California would be made with Manila galleons; a years worth of profit from the Philippines could not be risked.  One of the greatest bays on the west coast—San Francisco Bay—escaped outside-the-area knowledge until sited on November 4, 1769. In 1602, 60 years after Cabrillo, the Spaniard Sebastián Vizcaíno, who had been on the \"Santa Ana\" when it was captured by Thomas Cavendish off Cape San Lucas on the Baja peninsula in 1587, explored California's coastline from San Diego as far north as Monterey Bay.  He was looking for a possible town site for replenishing and protecting the annual trip of the Manila Galleon.  Vizcaíno named San Diego Bay and held the first Christian church service recorded in California on the shores of San Diego Bay.  He also put ashore in Monterey, California and made glowing reports of the Monterey Bay area as a possible anchorage for ships with land suitable for growing crops—the California coastal Indians had no agriculture.  He also provided rudimentary charts of the coastal waters, which were used by the Spanish for nearly 200 years. A potential colonial power interested in Alta California was Russia, already established in the Pacific Ocean in Alaska.  Their Maritime Fur Trade originally focused in Alaska started making expeditions to the California for harvesting sea otters and fur seals.  These furs could be traded in China for large profits.  After the conclusion of the Seven Year War between Britain and France and their allies (in U.S. called the French and Indian War) (1754–1763) France was driven out of North America, Spain, Russia and Britain were the only colonial powers left in North America. To prevent Russia or Britain from establishing settlements in California in 1769, the Spanish Visitor General, José de Gálvez, under directions of the Spanish Crown, proceeded to plan a five part expedition to settle Alta California.  Three ships with supplies and men were to go by sea and two expedition by land to start settling Alta California.  Gaspar de Portolà volunteered to command the expedition.  The Catholic Church was represented by Franciscan friar Junípero Serra and his fellow friars.  All five detachments of soldiers, friars and colonists were to meet at the site of San Diego Bay.  The first sailing ship, the \"San Carlos\", sailed from La Paz on 10 January 1769, and the ship \"San Antonio\" sailed on 15 February.  The first land party, led by Fernando Rivera y Moncada, left from the Franciscan Mission San Fernando Velicata on March 24, 1769.  The third vessel, the sailing ship \"San José\", left New Spain later that spring but was lost at sea with no survivors.  With Rivera was Father Juan Crespí, famed diarist of the entire expedition.  The expedition led by Portolà, which included Father Junípero Serra, the President of the Missions, along with a combination of missionaries, settlers, and leather-jacket (leather jackets made of several layers of leather could stop most Indian arrows) soldiers, including José Raimundo Carrillo, left Velicata on May 15, 1769 accompanied by about 46 mules, 200 cows and 140 horses—all that could be spared by the poor Baja Missions.  Fernando Rivera was appointed to command the lead party that would scout out a land route and blaze a trail to San Diego.  Food was short, and the Indians accompanying them were expected to forage for most of what they needed.  Many Indian neophytes died along the way—even more deserted.  On 15 May 1769, the day after Rivera and Crespí reached San Diego Portolà and Serra set out from Velicata.  The two groups traveling from Lower California on foot had to cross about 300 mi of the very dry and rugged Baja California peninsula.  The overland part of the expedition took about 40–51 days to get to San Diego.  All five detachments were to meet at San Diego Bay. The contingent coming by sea, encountered the south flowing California Current and strong head winds and were still straggling in three months after they set sail.  After their arduous journeys, most of the men aboard the ships were ill, chiefly from scurvy, and many had died.  Out of a total of about 219 men who had left Baja California, little more than 100 now survived.  The Spanish settlements of Alta California were the last expansion of Spain's vastly over-extended empire in North America, and they tried to do it with minimal cost and support. A few leather jacket soldiers and Franciscan friars financed by the Catholic Church and Spanish Crown would form the backbone of the proposed settlement of Alta California.  The settlements eventually included: twenty one surviving Missions—typically manned by two to three friars and five to ten soldiers; four military Presidios were built—manned by 10 to 100 soldiers and four small settlements (Pueblos) were set up to grow food for the Presidios. On July 14, 1769, an expedition was dispatched to find the port of Monterey, California.  Not recognizing the Monterey Bay from the description written by Sebastián Vizcaíno almost 200 years prior, the expedition traveled beyond it to what was called San Francisco area.  The exploration party, led by Don Gaspar de Portolà arrived on November 2, 1769, at San Francisco Bay.  One of the greatest ports on the west coast of America had finally become known to non-indigenous people.  The expedition finally returned to San Diego on 24 January 1770 weak with hunger and suffering from scurvy. Without any agricultural crops or experience eating the food on which the Indians subsisted (ground acorns), the shortage of food at San Diego became extremely critical during the first few months of 1770.  They subsisted on some of their cattle (Texas Longhorns), wild geese, fish, and other food exchanged with the Indians for clothing, but the ravages of scurvy continued for there was restricted amounts of food and no understanding of the cause or cure of scurvy then.  A small quantity of corn they had planted grew well—only to be eaten by birds.  Portolá sent Captain Rivera and a small detachment of about 40 men to the Baja California missions in February to obtain more cattle and a pack-train of supplies.  This temporarily eased the drain on San Diego's scant provisions, but within weeks, acute hunger and increased sickness again threatened to force abandonment of the port.  Portolá resolved that if no relief ship arrived by March 19, 1770 they would leave the next morning \"because there were not enough provisions to wait longer and the men had not come to perish from hunger.\"  At three o'clock in the afternoon on March 19, 1770, as if by a miracle, the sails of the sailing ship \"San Antonio\" loaded with relief supplies were discernible on the horizon.  The settlement of Alta California would continue. Late in 1775 Juan Bautista de Anza led a contingent of 240 soldiers, settlers and friars from Sonora Mexico over the Gila River Trail over the Colorado River at the Yuma Crossing and up about 500 mi of Alta California to the San Francisco Bay area where they arrived 28 March 1776.  There the Spanish built the Mission San Francisco de Asís, (or Mission Dolores), the Presidio of San Francisco and Yerba Buena, California (San Francisco).  They came with about 200 leather-jacketed soldiers, and settlers with their families and two Franciscan friars.  They brought with them about 600 horses and mules, 300 Texas Longhorn bulls and cows.  These animals and their descendants were the core of the later cattle and horse herds on the Californio Ranchos.  These soldiers, friars, settlers and livestock came over the Anza Trail from Sonora, Mexico, four years before the trail from New Spain to California was closed for over 40 years by the Quechan people (Yumas)—most new emigrants would have to come by ship. In 1780 the Spanish established two combination missions and pueblos at the Yuma Crossing of the Colorado River: Mission San Pedro y San Pablo de Bicuñer and Mission Puerto de Purísima Concepción.  July 1781 the Yuma (Quechan) Indians, in a dispute with the Spanish destroyed both missions and pueblos—killing 103 soldiers, colonists and Franciscan friars and capturing about 80—mostly women and children.  Despite four expeditions to reassert Spanish control the Yuma Crossing remained under the Quechans' control for the next 40 years—the easiest land route to California was closed.  This restriction caused most settler traffic and supplies to Alta California to come on a 30- to 60-day sailing ship journey form New Spain's towns on the Pacific Ocean.  Because there were only a few settlers and they had essentially nothing to export or trade so there were only a few ships that came to Alta California.  Combined with the Spanish restriction that prohibited non-Spanish shipping the average number of ships going to Alta California from 1770 to 1821 was 2.5 ship/year with 13 years showing no recorded ships. On November 20, 1818 Hippolyte de Bouchard raided the Presidio of Monterey in Monterey, California.  Bouchard, a French revolutionary who later became a citizen of Argentina, is sometimes referred to as California's only pirate, although some Argentines prefer to use the term corsair. Since much of his crew died from scurvy, Bouchard went in search of new crew members in the Sandwich Islands (now Hawaii), and then sailed to the coast near Mission Santa Barbara and threatened the nearby town.  Bouchard and his crew left without attacking after some soldiers from the Presidio of Santa Barbara confronted them, and arranged a prisoner exchange. On December 14, 1818 Bouchard attacked Mission San Juan Capistrano and he and his crew damaged several buildings, including the Governor's house, the King's stores, and the barracks. Even before Mexico gained control of Alta California the onerous Spanish rules against trading with foreigners began to break down as the declining Spanish fleet couldn't enforce their no trading policies.  The Californios, with essentially no industries or manufacturing capabilities, were eager to trade for new commodities, glass, hinges, nails, finished goods, luxury goods and other merchandise.  The Mexican government abolished the no trade with foreign ships policy and soon regular trading trips were being made.  The main products of these California Ranchos were cow hides (called California greenbacks), tallow (rendered fat for making candles and soap) and California/Texas longhorn cattle horns that were traded for other finished goods and merchandise.  This hide-and-tallow trade was mainly carried on by Boston-based ships that traveled for about 200 days in sailing ships about 17000 mi to 18000 mi around Cape Horn to bring finished goods and merchandise to trade with the Californio Ranchos for their hides, tallow and horns.  The cattle and horses that provided the hides, tallow and horns essentially grew wild.  The Californios' hides, tallow and horns provided the necessary trade articles for a mutually beneficial trade.  The first United States, English and Russian trading ships began showing up in California before 1816.  The classic book \"Two Years Before the Mast\" by Richard Henry Dana, Jr. written about the period 1834-36, provides a good first hand account of this trade. From 1825 to 1848 the average number of ships traveling to California increased to about 25 ships per year—a large increase from the average of 2.5 ships per year from 1769 to 1824.  The port of entry for trading purposes was the Alta California Capital, Monterey, California, where custom duties (tariffs) of about 100% were applied.  These high duties gave rise to much bribery and smuggling, as avoiding the tariffs made more money for the ship owners and made the goods less costly to the customers.  Essentially all the cost of the California government (what little there was) was paid for by these tariffs (custom duties).  In this they were much like the United States in 1850, where about 89% of the revenue of its federal government came from import tariffs (also called Customs or ad valorem taxes), although at an average rate of about 20%. By 1846, the province of Alta California had a non-native population of about 1,500 Californio adult men along with about 6,500 women and children, who lived mostly in the southern half of the state around Los Angeles.  Most new California immigrants were adult males and lived in the northern half of California.  Estimates of immigrants vary from 600 to 2,000 by 1846 with more arriving each year.  Estimates of the California Indian population vary from 100,000 to 150,000 in 1846. Before the American Revolution (1775–1783) the colonies that would become the United States had an already developed a strong seafaring tradition in the future New England and Mid-Atlantic states.  The colonies with good access to British shipbuilding experience and technology and with good access to the Atlantic Ocean and extensive forests had already developed an advanced shipbuilding industry even before they gained independence.  They were already building many of the ships used in the extensive British colonial trade as well as whaling and fishing vessels.  Whaling recovered soon after the American Revolutionary War ended in 1783 and the United States whaling industry began to prosper.  The whalers used bases primarily at Nantucket island and New Bedford, Massachusetts.  About ten thousand United States' seamen manned whaling ships on whaling voyages that could last over two years.  The United States grew to become the pre-eminent whaling nation in the world by the 1830s.  From 1835 to 1860 the American whaling fleet averaged about 620 vessels annually with a shipping tonnage aggregating 190,500 tons.  In this time period most of the whalers were whaling in the Pacific Ocean.  From 1835 to 1860 the annual United States whale oil usage averaged 118,000 barrels of sperm oil, 216,000 barrels of other whale oil and 2,323,000 pounds of whalebone (baleen)—with a total average value of over $8,000,000 a year of 1830 dollars.  The whale oil was used primarily in whale oil lamps for illumination at night and for some lubrication purposes.  The baleen was used for corsets, brushes, whips, and other uses that required a strong flexible material—plastic hadn't been invented yet.  Kerosene, when it was introduced in the early 1850s, was much cheaper and easily made by fractional distillation of petroleum.  Kerosene started to rapidly replace whale oil for lighting in the 1850s—saving many more whales than any conservation movement.  The Pacific Ocean whaling ships started getting fresh supplies, water and wood from California and the Sandwich Islands (Hawaii).  Many whaling vessels preferred stopping in the San Francisco Bay Area before stopping to pay the high custom duties (also called tariffs and ad valorem taxes) at Monterey, California wanted by the Californio government—avoiding taxes has a long history.  By 1846 several hundred whaling ships per year were using Hawaii (then called the Sandwich Islands) as a temporary base. Shortly after the United States gained independence in 1783 trade was instituted from East Coast ports with the West Coast to gather furs in the Maritime Fur Trade to trade with China for porcelain, silk, spices etc..  British Royal Navy commander George Vancouver sailed up the west coast past the mouth of the Columbia River and in April 1792 and observed a change in the water's color, which implied there may be a major river emptying into the Pacific.  Later that month, Vancouver encountered the American captain Robert Gray at Grays Harbor—later named that in his honor.  Gray also reported that he had seen the possible entrance to the Columbia a few years earlier and had spent nine days trying but failing to enter the river over its extensive sand bars and turbulent waves; but bad weather forced him to give up.  Gray returned to the river mouth a second time in May 1792.  This time he ordered a small sailboat launched to attempt to find a safe passage across the sand bars and turbulent waves by sounding (measuring the depth with a weight attached to a line of known length) the water depth to find a channel deep enough for his ship.  Finally in the evening of 11 May 1792, Gray's men found a safe channel and he and his crew sailed their ship \"Columbia\" into the estuary of what he named the Columbia River after his ship. Once across the sand bars and turbulent water at the entrance to the river (later called the Columbia Bar) they sailed up the Columbia River several miles while exploring the river.  Gray's find was a significant claim (besides the Lewis and Clark Expedition) put forth by the United States to claim possession of the Oregon Territory. Beginning about 1790 United States ships often sailed along the West Coast to gather furs for trading with China.  They traded steel knives, hatchets, blankets, kettles, whiskey, guns, powder, lead and other trade goods for furs collected by the natives.  An attempt was made by the Spanish in Spanish California to severely restrict trade from San Diego to San Francisco.  The land above San Francisco had no Spanish presence.  The Russian-American Company was created in 1799 as a joint venture between Russian fur traders and United States ship merchants who agreed to purchase fur seal and sea otter furs obtained by the Russians.  The Russian Czar was to get 20% of all profits.  American ships brought food and other supplies to the Russian settlements, assisted in fur hunts, and took furs away.  The company constructed settlements in what are today Alaska, Hawaii and California. The Russian traders after visiting California in 1806 built Fort Ross, California in 1812 on the California coast in Sonoma County, California—just north of San Francisco.  This was the southernmost outpost of the Russian-American Company.  To keep unwanted Spanish intrusion away the Russians built a palisaded fort equipped with several cannons.  Their objective in setting up Ft. Ross was to harvest fur seals and sea otters and grow grain and vegetables for the use of other Russian trading centers in Russian Alaska.  The fur company at Fort Ross typically had a few score Russians with up to 75 Aleut who harvested (usually under some duress) the fur seals and sea otters from their kayaks on or near the Farallon Islands, the Channel Islands of California and in the ports and bays around San Francisco Bay.  The Aleuts of Russian Alaska probably had the largest effect on the Channel islands and its people.  These otter-hunters from the Aleutian Islands set up camps on the surrounding Channel Islands and traded with the native peoples in exchange for permission to hunt otters and seals around the island. American fur trader John Jacob Astor built Fort Astoria on the Columbia River in 1811.  Under pressure brought by the War of 1812 Astor sold the fort in 1813 to what would become The British Hudson's Bay Company.  In 1830 Hudson Bay Co. built a new Fort Vancouver slightly up the Columbia River in the future Washington state.  This fort was the main supply depot for Hudson Bay forts in all the Pacific Northwest.  Within a few years they were growing quantities of wheat, constructed saw and flour mills, and yearly shipped lumber to the Hawaiian Islands and flour to Sitka, Alaska.  They were resupplied every year by two to three ships that brought trading supplies around Cape Horn from England. Sea otters and fur seals were severely depleted on the California coast and islands by the 1820s.  Hudson's Bay Company entered the coast trade in the 1820s with the intention of driving the Americans away.  This was accomplished by about 1840 just as the fur trade industry started dying due to lack of supply and a style change in felt hats—felt was made from fur and the main fur market.  In its late period the maritime fur trade was largely conducted by the British Hudson's Bay Company and the Russian-American Company.  The depleted supply of sea otters and the easy trade with the British in Fort Vancouver for food stuffs led the Russians to abandon Fort Ross in 1841 and sell the cannon and other supplies to John Sutter who was building up Sutter's Fort near Sacramento, California.  The Hudson Bay Company departed from their trading post they had set up in Yerba Buena (San Francisco) in 1845 because of the declining fur trade and the death of their agent there. The United States Exploring Expedition (1838–1842) was an exploring and surveying expedition of the Pacific Ocean (\"the Southern Seas\") conducted by the United States Navy to learn more about the Pacific Ocean and its ports.  The expedition with five ships was authorized by Congress in 1836.  It is sometimes called the \"Ex.  Ex.\"  for short, or \"the Wilkes Expedition\" in honor of its next appointed commanding officer, U.S. Navy Lt. Charles Wilkes (1798–1877).  The expedition was of major importance to the growth of oceanography and cartography of the Pacific.  Two of these ships were lost from accidents—one on the Columbia River in 1841.  From the area of modern-day Portland, Oregon, an overland party headed by George F. Emmons was directed to proceed via an inland route to San Francisco Bay.  This Emmons party traveled south along the Siskiyou Trail, including the Sacramento River, making the first official recorded visit by Americans to and scientific note of Mount Shasta, in northern California.  The Emmons party rejoined the ships, which had sailed south, in San Francisco Bay.  After their return Wilkes published the major scientific works \"Western America, including California and Oregon\" (1849) and \"Theory of the Winds\" (1856). The Pacific Squadron, established 1821, was part of the United States Navy squadron stationed in the Pacific Ocean in the 19th and early 20th centuries.  Initially with no United States ports in the Pacific, they operated out of Naval storeships which provided naval supplies like powder and ammunition and purchased fresh supplies of food, wood and water from local ports of call in California, Hawaiian Islands (called the Sandwich Islands then) and ports and harbors on the Pacific Coast.  The Pacific Squadron was instrumental in the capture of Alta California in the Mexican–American War of 1846 to 1848. The five American navy sailing ships initially stationed in the Pacific had a force of 350-400 U.S. Marines and bluejacket U.S. Navy sailors on board available for deployment and were essentially the only significant United States military force on the Pacific coast in the early months of the Mexican–American War.  The Marines were stationed aboard each warship to assist in close in ship to ship combat for either boarding or repelling boarders and could be detached for use on land.  In addition there were some sailors on each ship that could be detached from each vessel for shore duty and still leave the ship functional though short handed.  Naval gunnery officers typically handled the small cannons deployed as artillery with the sailors and marines. Hostilities between U.S. and Mexican troops commenced on 25 April 1846 with Mexican troops killing and capturing a number of U.S. Army dragoons in the future state of Texas.  The Battle of Palo Alto, the first major battle of the Mexican–American War, was fought on 8 May 1846, a few miles from the modern-day city of Brownsville, Texas.  A force of some 3,400 Mexican troops (a portion of the \"Army of The North\") led by Mexican General Mariano Arista engaged a force of 2,400 United States troops under General Zachary Taylor.  Taylor's forces drove the Mexicans from the field.  The United States Congress responded to these hostilities by issuing a declaration of war against Mexico on 13 May 1846 — the Mexican–American War had begun. Speculating that war with Mexico over Texas and other land was very possible, the U.S. Navy had sent several additional naval vessels to the Pacific in 1845 to protect U.S. interests there.  It took about 200 days, on average, for sailing ships to travel the greater than 17000 mi trip from the East coast around Cape Horn to California.  Initially as the war with Mexico started there were five vessels in the U.S. Navy's Pacific Squadron near California.  In 1846 and 1847, after war was declared, this force was increased to 13 Navy sailing ships—over half the U.S. Navy's available ships.  This would be the last conflict the U.S. Navy fought with only sailing vessels as they rapidly converted to steam ships shortly after this conflict. The former fleet surgeon William M. Wood and John Parrot, the American Consul of Mazatlán, arrived in Guadalajara Mexico on 10 May 1846.  There they heard word of the ongoing hostilities between the U.S. and Mexico forces and sent a message by special courier back to Commodore Sloat then visiting Mazatlán.  On 17 May 1846 this courier's messages informed Commodore Sloat that hostilities between the U.S. and Mexico had commenced.  Commodore (Rear Admiral) John D. Sloat, commander of the U.S. Navy's Pacific Squadron and his fleet of four vessels were then at anchor in the harbor of Mazatlán.  On hearing the news Commodore Sloat dispatched his flagship, the frigate \"Savannah\" (1842) , and the Sloop \"Levant\" (1837) to Monterey harbor where they arrived on 2 July 1846.  They joined the sloop \"Cyane\" (1837) which was already there.  There were U.S. fears that the British might try to annex California to satisfy British creditors.  The British Pacific Station's ships off California were stronger in ships, guns and men. Hearing rumors of possible Californio military action against the newly arrived settlers in California (this had already happened in 1840), some settlers decided to neutralize the small Californio garrison at Sonoma, California.  On 15 June 1846, some thirty settlers, mostly former American citizens, staged a revolt and seized the small Californio garrison in Sonoma without firing a shot.  Initially there was little resistance from anyone in California as they replaced the dysfunctional and ineffective Mexican California government—which already had 40 Presidents in the first 24 years of its existence.  Most settlers and Californios were neutral or actively supported the revolt.  John A. Sutter and his men and supplies at Sutter's Fort joined the revolt.  They raised the \"Bear Flag\" of the California Republic over Sonoma.  The republic was in existence scarcely more than a week before the U.S. Army's John C. Frémont returned and took over on 23 June 1846 from William B. Ide the leader of the Bear Flag Revolt.  The California state flag of today is based on this original Bear Flag and still contains the words \"California Republic\". In 1846 the U.S. Navy was under orders to take over all California ports in the event of war.  There were about 400–500 U.S. Marines and U.S. Navy bluejacket sailors available for possible land action on the Pacific Squadron's ships.  Hearing word of the Bear Flag Revolt in Sonoma and the arrival of the large British 2,600 ton, 600 man, man-of-war HMS \"Collingwood\" , flagship under Sir George S. Seymour, outside Monterey Harbor, Commodore Sloat was finally stirred to action.  On 7 July 1846—seven weeks after war had been declared, Commodore John D. Sloat instructed the Captains of the ships USS \"Savannah\" and sloops USS \"Cyane\" and USS \"Levant\" of the Pacific Squadron in Monterey Bay to occupy Monterey, California—the Alta California capital.  Fifty American marines and about 100 bluejacket sailors landed and captured the city without incident—the few Californio troops formerly there having already evacuated the city.  They raised the flag of the United States without firing a shot.  The only shots fired were a 21 gun salute to the new 28 star U.S. flag fired by each of the U.S. Navy ships in the harbor.  The British ships observed but took no action—getting a message to and from Britain requesting new orders to interfere would have taken from one to two years. The abandoned Presidio and Mission San Francisco de Asís (Mission Dolores) at San Francisco, (then called Yerba Buena), was occupied without firing a shot on 9 July 1846 by U.S. Marines and U.S. Navy sailors from the sloop USS \"Portsmouth\" .  Militia Captain Thomas Fallon led a small force of about 22 men from Santa Cruz, California and captured the small town of Pueblo de San Jose without bloodshed on 11 July 1846.  Fallon received an American flag from Commodore John D. Sloat, and raised it over the pueblo on 14 July.  On 15 July 1846, Commodore (Rear Admiral) John D. Sloat transferred his command of the Pacific Squadron to Commodore Robert F. Stockton when Stockton's ship, the frigate \"Congress\" , arrived from Hawaii.  Stockton, a much more aggressive leader, asked Fremont to form a joint force of Fremont's soldiers, scouts, guides etc. and a volunteer militia—many former Bear Flag Revolters.  This unit called the California Battalion was mustered into U.S. service and were paid regular army wages.  On 19 July, Frémont's newly formed \"California Battalion\" swelled to about 160 men.  These men included Fremont's 30 topographical men and their 30 scouts and hunters, U.S. Marine Lieutenant Archibald H. Gillespie, a U.S. Navy officer to handle their two cannons, a company of Indians trained by Sutter and many other \"permanent\" California settlers from several different countries as well as American settlers.  The California Battalion members were used mainly to garrison and keep order in the rapidly surrendering California towns.  The Navy went down the coast from San Francisco, occupying ports without resistance as they went.  The small pueblo (town) of San Diego surrendered 29 July 1846 without a shot being fired.  The small pueblo of Santa Barbara surrendered without a shot being fired in August 1846.  On 13 August 1846 a joint force of U.S. Marines, bluejacket sailors and parts of Fremont's California Battalion carried by the USS \"Cyane\" entered Los Angeles, California with flags flying and band playing.  Captain Archibald H. Gillespie, (Fremont's second in command), with an inadequate force of 40 to 50 men were left to occupy and keep order in the largest town (about 3,500) in Alta California—Los Angeles. On 11 July the British Royal Navy sloop HMS \"Juno\" enters San Francisco Bay causing Montgomery to man his defenses.  The large British ship, 2,600 ton, man-of-war HMS \"Collingwood\" , flagship under Sir George S. Seymour, also shows up about this time outside Monterey Harbor.  Both British ships observe, but did not enter the conflict. Shortly after 9 July when it became clear the American Navy was taking action, the short-lived Bear Flag Republic was converted into a United States military conflict for possession of California and the Bear Flag (the basis for today's California flag) was replaced by the U.S. flag].  Fremont expeditionary forces joined forces with a volunteer force of California residents to form a small volunteer militia.  The frigate USS \"Congress\" was the flagship of Commodore Robert F. Stockton when he took over as the senior United States military commander in California in late July 1846.  Stockton asked Fremont to muster the troops and volunteers under his command into the California Battalion to help garrison the towns rapidly being captured from the Californios.  Most towns surrendered without a shot being fired.  Fremont's California Battalion members were sworn in and the volunteers paid the regular United States Army salary of $25.00 a month for privates with higher pay for officers.  The California Battalion varied in size with time from about 160 initially to over 450 by January 1847.  Pacific Squadron war ships and storeships served as floating store houses keeping Fremont's volunteer force in the California Battalion supplied with black powder, lead shot and supplies as well as transporting them to different California ports.  The USS \"Cyane\" transported Fremont and about 160 of his men to the small port of San Diego which was captured on 29 July 1846 without a shot being fired. A minor Californio revolt broke out in Los Angeles and the United States force there of 40–50 men evacuated the city for a time.  Later, U.S.forces fought minor scrimmages in the Battle of San Pasqual, the Battle of Dominguez Rancho, and the Battle of Rio San Gabriel.  After the Los Angeles revolt started the California Battalion was expanded to a force of about 400 men.  In early January 1847 a 600-man joint force of U.S. Marine, U.S. Navy bluejacket sailors, General Stephen W. Kearny's 80 U.S. Army dragoons (cavalrymen) and about two companies of Fremont's California Battalion re-occupied Los Angeles after some minor skirmishes—after four months the same U.S. fag again flew over Los Angeles.  The minor armed resistance in California ceased when the Californios signed the Treaty of Cahuenga on 13 January 1847.  The Californios who had wrested control of California from Mexico in 1845 now had a new government. After the Treaty of Cahuenga was signed, the Pacific Squadron then went on to capture all Baja California cities and harbors and sink or capture all the Mexican Pacific Navy they could find.  Baja California was returned to Mexico in subsequent Treaty of Guadalupe Hidalgo negotiations.  More reinforcements of about 320 soldiers (and a few women) of the Mormon Battalion arrived at San Diego, California on 28 January 1847—after hostilities had ceased.  They had been recruited from the Mormon camps on the Missouri River—about 2000 mi away.  These troops were recruited with the understanding they would be discharged in California with their weapons.  Most were discharged before July 1847.  More reinforcements in the form of Colonel Jonathan D. Stevenson's 1st Regiment of New York Volunteers of about 648 men showed up in March–April 1847—again after hostilities had ceased.  Three private merchant ships, \"Thomas H Perkins\", \"Loo Choo\", and \"Susan Drew\", were chartered, and the sloop USS \"Preble\" was assigned convoy detail.  On 26 September the four ships left New York for California.  Fifty men who had been left behind for various reasons sailed on 13 November 1846 on the small storeship  .  The \"Susan Drew\" and \"Loo Choo\" reached Valparaíso, Chile by 20 January 1847 and after getting fresh supplies, water and wood were on their way again by 23 January.  The \"Perkins\" did not stop until San Francisco, reaching port on 6 March 1847.  The \"Susan Drew\" arrived on 20 March 1847 and the \"Loo Choo\" arrived on 26 March 1847, 183 days after leaving New York.  The \"Brutus\" finally arrived on 17 April 1847.  After desertions and deaths in transit, four ships brought Stevenson's 648 men to California.  Initially they took over all of the Pacific Squadron's on-shore military and garrison duties and the Mormon Battalion and California Battalion's garrison duties.  The New York Volunteer companies were deployed from San Francisco in Alta California to La Paz, Mexico in Baja California.  The ship \"Isabella\" sailed from Philadelphia on 16 August 1847, with a detachment of one hundred soldiers, and arrived in California on 18 February 1848, the following year, at about the same time that the ship \"Sweden\" arrived with another detachment of soldiers.  These soldiers were added to the existing companies of Stevenson's 1st Regiment of New York Volunteers.  These troops were recruited with the understanding they would discharged in California.  When gold was discovered in late January 1848, many of Stevenson's troops deserted. The first to hear confirmed information of the California Gold Rush were the people in Oregon, the Sandwich Islands (Hawaii), Mexico, Peru and Chile and they were the first to start flocking to the state in late 1848.  By the end of 1848, some 6,000 Argonauts had come to California.  President James K. Polk made the \"official\" announcement of the discovery of gold in California during his State of the Union Address on 5 December 1848 and displayed about 300 ounces of California gold at the War Department.  Excitement grew as rumors, reports of officers and soldiers in California, newspaper accounts all seemed to confirm that there was a tremendous amount of gold in California—just waiting to be picked up.  Sam Brannan, publisher of the newspaper the \"California Star\" at San Francisco, is regarded as starting the \"Gold Rush\" with stories about the large amount of gold found throughout late 1848 and 1849. These \"forty-niners\" left behind families and jobs in the hope of instant wealth.  A few succeeded handsomely, but the gold fields destroyed some and disappointed many more.  The gold fields were very lucrative with up to $50,000,000 in gold being found every year for several years; but the gold finds were spread very unevenly.  As the easily mined placer gold deposits were worked out the much more capital intensive hard rock mining took over.  Americans and foreigners of many different countries, statuses, classes, and races rushed to California for gold.  Almost all (~96%) were young men under age 40.  Women in the California Gold Rush were initially less than 4% of the population in 1850 and had many opportunities to do new things and take on new tasks in women poor California.  Argonauts, as they were often called, walked over the California Trail or came by sea.  About 80,000 Argonauts arrived in 1849 alone—about 40,000 over the California trail and 40,000 by sea.  In April 1850, a harbor master's estimate counted 62,000 people from across the globe arriving in San Francisco by ship in the preceding 12 months.  Hundreds of ships lay abandoned, anchored in San Francisco Bay, their passengers and crews abandoning the ships to search for gold. A popular concept of the California Gold Rush portrays the overland migration coming by wagon, yet according to the evidence, more people journeyed by sea.  A typical wagon journey took about 140 days while a voyage by paddle wheel steamer, a short land trip and another paddle wheel steamer could be done in as short as 40 days over the Isthmus of Panama or Nicaragua route once the shipping lines were established in about 1850.  Before farms could be set up, cities and industries built, etc. an early maritime traffic in passengers, food, lumber and building supplies were established with Pacific rim countries like Chile, Mexico, Hawaii and the future state of Oregon.  Tools, clothing, and everything needed for an 1850 standard of living for miners and an expanding population would in most cases have to initially be imported from the East coast of the United States or Europe.  Sea transport was about the only way cargo of any kind could be delivered to California.  High value cargo like gold and passengers usually went by the Panama or Nicaragua route.  Bulkier, lower value cargo, usually went by sailing ship around Cape Horn.  A standard sailing ship took an average of about 200 days to go this route while the faster Clipper ships averaged about 140 days.  Carrying any significant amount of goods cross country by wagon over 2000 mi of bad road was a slow, costly process that was seldom done.  Not until the First Transcontinental Railroad was completed in 1869 was there any easy way to move cargo across land to California.  Even today, bulky or heavy cargo is usually sent by ship because it is cheap and efficient, though slower than other methods. (Note: Paddle wheel steamers did not reach California by sea until long after the gold \"rush\" had ended.  See later in this article for the date which was long after the \"rush\") Some enterprising \"Argonauts\" set up businesses to furnish, feed, and entertain the region's growing population.  Some merchants, gamblers, saloon keepers, \"entertainers\", hotel owners, restaurant owners etc. were as likely to prosper as the successful prospectors.  The lack of money (specie) often meant that transactions and wages were paid in several different currencies or in gold dust.  The population of San Francisco boomed as it was the main entry point for sea born travelers and goods of all kinds.  San Francisco by 1850 was declared the main Port of entry in California for all imported goods.  The federal government's got 91% of its income in 1850 from tariffs or custom duties of about 23% on foreign imports.  By 1850 San Francisco had a population of about 20,000 (the largest city in California then) that had swelled to over 36,000 by the 1852 California Census.  The population of California grew from 8,000 in 1846 to about 120,000 in 1850. The vast majority of the California \"Argonauts\" (as they were frequently called) were young Anglo men from the United States.  Those on the East Coast who could afford the trip usually traveled on paddle steamers and occasionally sailing ships to Panama, Nicaragua or Mexico; they then traveled by land to the Pacific and caught another ship to California.  Those who lived in the Mid West usually went by wagon, as many already had a wagon and team and were familiar with wagon travel and often already had or could quickly purchase any additional wagons, animals, supplies and equipment needed for a long wagon trip. (A great many Chinese men came to California, very early in the Gold Rush, having only to cross the Pacific by boat, rather than sail all the way around The Horn or cross the treacherous Isthmus of Panama as many of the Anglo men had to do.) The traffic to California was so heavy that in two years these settlers, combined with those coming by wagon from Salt Lake City (Salt Lake was not yet a city, by any means; it was a \"new\" settlement, where the Mormon leaders had taken their people with the hope of living in peace, free from outsiders), Utah to Los Angeles in winter, the travelers down the Gila River trail in Arizona.  (See Notes:) to make it the 31st state.  All land routes were restricted to the seasons where travel was feasible.  The trip by wagon was a four- to six-month ordeal across over 2000 mi of land from a Missouri River town to California or across the deserts of Arizona or Nevada to California.  Those going by wagon train could not leave until the snow melted and the trails were dry and enough grass had started to feed their livestock—usually early May.  Most of those traveling by wagon already lived in the mid-west and many already had a wagon.  The Salt Lake City, Utah to Los Angeles trail was usually restricted by lack of water to winter.  [The vast majority of the early travelers came from the settlements near the Missouri River who left the river in the Spring when dangers of snow had passed, and traveling by wagon, cart, or mule (more durable than horses) they moved westward.] There were four major routes by sea: paddle steamer (or occasionally sailing ship) to the future countries of Panama, Nicaragua or Mexico, a trip across land to the Pacific and then a trip by paddle steamer to California.  These routes were used by travelers who could afford them being the fastest—about 40–60 days.  The other major sea route was by sea around Cape Horn or the Magellan Straits and on to California—this trip typically took over 200 days and was the main shipping route for merchandise.  Most of those traveling by ship lived on the Eastern seaboard and were acquainted with sea borne commerce and travel. Those traveling by land and sea to California had enough residents in California by 1850 (about 120,000 by corrected 1850 U.S. Census data) (See Notes:) for California in 1850 to become the 31st state. By 1849 the shipping industry was in transition from sail powered boats to steam powered boats and from wood construction to an ever-increasing metal construction.  There were basically three different types of ships being used: standard sailing ships of several different types.  Clippers, and paddle steamers with paddles mounted on the side or rear.  River steam boats typically used rear mounted paddles and had flat bottoms and shallow hulls designed to carry large loads on generally smooth and occasionally shallow rivers.  Ocean-going paddle steamers typically used side-wheeled paddles and used narrower deeper hulls designed to travel in the often stormy weather encountered at sea.  The ship hull design was often based on the clipper ship design with extra bracing to support the loads and strains imposed by the paddle wheels when they encountered rough water. The first paddle-steamer to make a long ocean voyage was the 320 ton 98 ft long  , built in 1819 expressly for packet ship mail and passenger service to and from Liverpool, England.  On 22 May 1819, the watch on the \"Savannah\" sighted Ireland after 23 days at sea.  The Allaire Iron Works of New York supplied \"Savannah's\"<nowiki>'s</nowiki> engine cylinder, while the rest of the engine components and running gear were manufactured by the Speedwell Ironworks of New Jersey.  The 90-horsepower low-pressure engine was of the inclined direct-acting type, with a single 40 in cylinder and a 5 ft stroke.  \"Savannah\"<nowiki>'s</nowiki> engine and machinery were unusually large for their time.  The ship's wrought-iron paddlewheels were 16 feet in diameter with eight buckets per wheel.  For fuel, the vessel carried 75 tons of coal and 25 cords of wood. The SS \"Savannah\" was too small to carry much fuel and the engine was intended only for use in calm weather and to get in and out of harbors.  Under favorable winds the sails alone were able to provide a speed of at least four knots.  The \"Savannah\" was judged not a commercial success and its engine was removed and it was converted back to a regular sailing ship.  By 1848 steamboats built by both United States and British shipbuilders were already in use for mail and passenger service across the Atlantic Ocean—a 3000 mi journey. Since paddle steamers typically required from 5 to 16 tons of coal per day to keep their engines running, they were more expensive to run.  Initially, nearly all seagoing steamboats were equipped with mast and sails to supplement the steam engine power and provide power for occasions when the steam engine needed repair or maintenance.  These steamships typically concentrated on high value cargo, mail and passengers and only had moderate cargo capabilities because of their required loads of coal.  The typical paddle wheel steamship was powered by a coal burning engine that required firemen to shovel the coal to the burners. By 1849 the screw propeller had been invented and was slowly being introduced as iron increasingly was used in ship construction and the stress introduced by propellers could be compensated for.  As the 1800s progressed the timber and lumber needed to make wooden ships got ever more expensive and the iron plate needed for iron ship construction got much cheaper as the massive iron works at Merthyr Tydfil, Wales, for example, got ever more efficient.  The propeller put a lot of stress on the rear of the ships and would not see large spread use till the conversion from wood boats to iron boats was complete—well underway by 1860.  By the 1840s the ocean-going steam ship industry was well established as the Cunard Line and others demonstrated. In 1846 the Oregon boundary dispute was settle with Great Britain and California was conquered in 1847 and annexed in 1848.  The United States was now a Pacific Ocean power.  Starting in 1848 Congress, after the annexation of California but before the California Gold Rush was confirmed there, had subsidized the Pacific Mail Steamship Company with $199,999 to set up regular packet ship, mail, passenger and cargo routes in the Pacific Ocean.  This was to be a regular scheduled route from Panama City, Nicaragua and Mexico to and from San Francisco and Oregon.  Panama City was the Pacific terminus of the Isthmus of Panama trail across Panama.  The Atlantic Ocean mail contract from East Coast cities and New Orleans to and from the Chagres River in Panama was won by the United States Mail Steamship Company whose first paddle wheel steamship, the SS Falcon (1848) was dispatched on 1 December 1848 to the Caribbean (Atlantic) terminus of the Isthmus of Panama trail—the Chagres River. The SS California (1848), the first Pacific Mail Steamship Company paddle wheel steamship, left New York City on 6 October 1848 with only a partial load of her about 60 saloon (about $300 fare) and 150 steerage (about $150 fare) passenger capacity.  Only a few were going all the way to California.  Her crew numbered about 36 men.  She left New York well before confirmed word of the California Gold Rush had reached the East Coast.  Once the California Gold Rush was confirmed by President James Polk in his State of the Union address on 5 December 1848 people started rushing to Panama City to catch the SS California.  The SS \"California\" picked up more passengers in Valparaiso Chile and Panama City Panama and showed up in San Francisco, loaded with about 400 passengers—twice the passengers it had been designed for—on 28 February 1849.  She had left behind about another 400-600 potential passengers still looking for passage from Panama City.  The \"SS California \" had made the trip from Panama and Mexico after steaming around Cape Horn from New York—see SS California (1848). The trips by paddle wheel steamship to Panama and Nicaragua from New York, Philadelphia, Boston, via New Orleans and Havana were about 2600 mi long and took about two weeks.  Trips across the Isthmus of Panama or Nicaragua typically took about one week by native canoe and mule back.  The 4000 mi trip to or from San Francisco to Panama City could be done by paddle wheel steamer in about three weeks.  In addition to this travel time via the Panama route typically had a two- to four-week waiting period to find a ship going from Panama City, Panama to San Francisco before 1850.  It was 1850 before enough paddle wheel steamers were available in the Atlantic and Pacific routes to establish regularly scheduled journeys. Other steamships soon followed and by late 1849 paddle wheel steamships like the SS \"Mckim\" (1848) were carrying miners and their supplies the 125 mi trip from San Francisco up the extensive Sacramento–San Joaquin River Delta to Stockton, California, Marysville, California, Sacramento, etc. to get about 125 mi closer to the gold fields.  Steam powered tugboats and towboats started working in the San Francisco Bay soon after this to expedite shipping in and out of the bay. As the passenger, mail and high value freight business to and from California boomed more and more paddle steamers were brought into service—eleven by the Pacific Mail Steamship Company alone.  The trip to and from California via Panama and paddle wheeled steamers could be done, if there were no waits for shipping, in about 40 days—over 100 days less than by wagon or 160 days less than a trip around Cape Horn.  About 20-30% of the California Argonauts are thought to have returned to their homes, mostly on the East Coast of the United States via Panama—the fastest way home.  Many returned to California after settling their business in the East with their wives, family and/or sweethearts.  Most used the Panama or Nicaragua route till 1855 when the completion of the Panama Railroad made the Panama Route much easier, faster and more reliable.  Between the 1849 and 1869 when the First Transcontinental Railroad was completed across the United States about 800,000 travelers had used the Panama route.  Most of the roughly $50,000,000 of gold found each year in California were shipped East via the Panama route on paddle steamers, mule trains and canoes and later the Panama Railroad across Panama.  After 1855 when the Panama Railroad was completed the Panama Route was by far the quickest and easiest way to get to or from California from the East Coast of the U.S. or Europe.  Most California bound merchandise still used the slower but cheaper Cape Horn sailing ship route.  The sinking of the paddle steamer  (the \"Ship of Gold\")) in a hurricane on 12 September 1857 and the loss of about $2 million in California gold indirectly led to the Panic of 1857. Regular sailing ships that had been developed and refined over centuries of use were the cheapest and the slowest transports available.  There were several types of sailing ships.  They had typically been optimized to carry a large amount of cargo using a small crew of about 20 men and utilized sails in a combination of fore-and-aft rigging and square rigging.  Unless the cargo was time sensitive, they were utilized for nearly all long distance shipping and passenger service.  At the end of the sailing era windjammers were developed to carry large volumes of low value cargo long distances.  Some of the most popular ships were four-masted barques, since the four-masted barque is considered the most efficient rig available because of its ease of handling, small need of manpower, good running capabilities, and good capabilities of rising toward wind. Once in San Francisco the crews often deserted the ships.  The ship owners found little cargo of value to ship back to the East Coast out of California and the ships often went back in ballast with a cargo of useless rocks.  Since many of the ships were older and required expensive maintenance and crews were very hard to find and/or very expensive many hundreds of vessels were simply abandoned or sold at very low cost in Yerba Buena Cove.  Others were converted into store ships or floating warehouses, stores, hotels, prisons, etc..  Some abandoned ships were bought cheap, filled with ballast and sunk on the mud flats at high tide to enlarge the available wharves and docks.  The ships were typically stripped of her upper works and all usable fittings by one of San Francisco's many marine salvage firms of Gold Rush days and then covered with debris and sand as developers filled in the mud flats on the bay and built wharves out to deeper water to accommodate docking ships.  By 1857 nearly all abandoned shipping in the Yerba Buena Cove that had not been re-used was sent to a marine salvage or ship breaking firms where all usable fixtures, anchors, etc. were removed, the copper bottom (about 8000 lb of copper) salvaged and the timbers were removed and set aside for other uses.  The rapidly expanding city of San Francisco needed room to store all of the incoming goods and much larger dockside facilities—there were none when California was annexed.  There initially was not time to build adequate warehouses, wharves, docks on the water front.  In early September 2001, the \"General Harrison\", was discovered at the northwest corner of Battery and Clay streets during construction.  She was built in 1840 in Newburyport, Massachusetts and abandoned sometime before 1850 and turned into a store ship (warehouse).  She was 126.1 ft long, 26.5 ft wide, 13.25 ft in depth and displaced 409 tons.  She was burned to the water line in one of San Francisco's early fires.  The remains, including some of the stores on board, were filled with sand and built over. The average realized speed for the typical sailing ship was about 3.5 mi per hour, the clippers could often reach about 10 mi per hour of realized speed with steam ships averaging about 8 mi per hour. The clippers, developed and mainly used between about 1840 and 1860 were some of the last and \"best\" commercial sailing ships invented.  The clippers had more sails and faster hulls and were some of the fastest sailing ships ever developed.  The clippers required a larger crew to man the larger expanse of sails and typically carried high value cargo with few passenger accommodations.  Under ideal conditions clippers have been logged at over 20 mph —covering over 450 mi in one day.  The average speed over a long journey was about 8 mph .  The typical clipper carried high value, large volume cargo and normally carried only about six passengers.  They competed with the paddle steamers on the shorter Panama, Nicaragua, and Mexican routes.  Because of their shorter runs these paddle steamers were faster but much more costly to run and typically only took high value cargo like passengers, mail and gold shipments.  Clippers averaged about 120 days passage on the about 17000 mi trip between East Coast cities and San Francisco—about 80 days faster travel than the conventional sailing ships. In 1845 the \"Rainbow\", 757 tons OM, the first extreme clipper was launched in New York.  These American clippers were larger vessels designed to sacrifice cargo capacity for speed.  They had a bow lengthened above the water, a drawing out and sharpening of the forward body, and the greatest breadth further aft.  Extreme clippers were built in the period 1845 to 1855. Clippers sometimes took a trip across the Pacific Ocean to Shanghai or some other port in China to pick up a cargo of tea, silk, porcelain, etc. for a profitable return trip to their home port.  The alternative was to return in ballast with a useless cargo of rocks.  Some Clippers were used on the Hawaii to California routes as they shipped mainly food stocks to California. In 1853 the clipper \"Flying Cloud\" sailed from New York City and made San Francisco around Cape Horn in 89 days, 8 hours; a record that stood 136 years until 1989 when the breakthrough-designed sailboat \"Thursday's Child\" completed the passage in 80 days and 20 hours.  The record was once again broken in 2008 by the special built French racing yacht \"Gitana 13\", with a time of 43 days and 38 minutes. By mid-1849 there were steamship lines dropping people from off at the mouth of the Chagres River on the Caribbean side of Panama.  There were then no docking facilities off the Chagres River mouth and passengers had to come ashore in small boats—not an easy task in bad weather.  The east to west transit across the Isthmus of Panama was about 30 mi by native dugout boats (later modified lifeboats were used) up the often wild and dangerous Chagres River and then by mule back for the final 20 mi over the old Spanish trails.  The trails had fallen into serious disrepair after almost 50+ years of little or no maintenance and up to 3 meters of rain each year in the roughly April to December rainy season.  One of the major problems was getting reliable transport of luggage and freight over the Isthmus of Panama.  Even after all arrangements were made and payments made it was not unusual to have to wait weeks longer in Panama City for your luggage to arrive.  A transit from the Atlantic to the Pacific (or from Pacific to Atlantic) would usually take four to eight days by dugout canoe and mule back.  The transit was fraught with dangers and disease.  After the arrival in Panama City the traveler had to wait in a hot, dirty, crowded, disease laden city for their luggage to arrive and then for passage on a paddle steamer or sailing ship headed to California.  One of the main problems initially encountered was getting further passage on a ship to California—there were not enough ships to carry the passengers and cargo that built up in Panama City.  By late 1849 paddle steamer routes had been established to and from Panama City and other ports in Nicaragua and Mexico to San Francisco.  Panama City had a poor harbor and again the ships anchored off shore and a small boat was required to board them. Those going to California by crossing Panama, Nicaragua or Mexico had a much quicker trip than going around Cape Horn or taking the California Trail to California.  A typical trip could be done in about 30–90 days assuming shipping and transit connections could be made.  The biggest handicap the Panama, Nicaragua or Mexico routes had was the wait until scheduled Pacific shipping was established in late 1849 to carry them to California.  These routes also suffered form the risk of catching a possibly fatal disease and having to potentially endure threats of attack by bandits.  Unfortunately, getting shipping from the Pacific ports of Panama, Nicaragua, or Mexico to California was very problematic until mid to late 1849.  Mail, returning gold miners and gold shipments nearly all used the Panama Route.  The number of passengers who used the Panama route in 1849 and 1850 is unknown but must be numbered in the tens of thousands.  In 1851 29,600 passengers used the Panama Route.  This increased to 33,000 passengers in 1852 and dropped to 27,200 passengers in 1853.  by the time the First Transcontinental Railroad was completed in the United States in 1869, it is believed 800,000 travelers had used the Panama Route to get to and from California.  Its estimated that 20% of the original Argonauts left to return home—most via the Panama Route.  Many returned to California with their wives and families. As steamships became available, regular paddle steamer service opened a major mail, passenger and high value cargo link between the two coasts of the United States.  The first transcontinental railroad, the Panama Railroad, was finished in 1855 at a cost of about 5,000 lives and $8,000,000 and provided a major faster link with the East Coast and West Coast.  Regardless of how the miners had originally got to California nearly all returned East via Panama—the easiest and fastest route.  The returning miners and the returning gold were welcomed in the East and the mail and newspapers exchanged soon established strong east-west bonds.  After it was completed in 1855 the ease of use of the Panama Railroad meant that the Panama Route carried most of the high value, time sensitive freight to and from California.  Most female traffic went from east to west over the Panama Route which was the easiest and fasted route after about 1852.  Women in the California Gold Rush were scarce but played a major role in settling California.  Wives' and sweethearts' passage via Panama to California was normally paid for by miners or businessmen who had decided to stay in California.  These women arriving in nearly every ship from Panama soon changed the character of women scarce California as they rapidly outnumbered the prostitutes who had initially flocked there. Money in California was scarce as very few had brought much with them and the costly living expenses soon exhausted much of it.  Shipments of specie from several different countries were brought in but they never seemed to be enough and workers were happy to be paid in an amalgam of several different coins from several different countries.  Loose gold dust was valued at $16.00 per troy ounce then and almost all merchants, bars, gambling dens, brothels etc. had scales available to allow the miners to pay for their purchases in gold.  After passing though many hands in California gold eventually, almost inevitably, went East to pay for merchandise bought in the west and imported from the East.  The primary gold shipment route was via well-guarded paddle steamers to Panama.  Well guarded mule and canoe trips to the mouth of the Chagres River and from their by paddle steamer to New York, etc.  Bandits called the Derienni, initially, often raided these shipments until many of the bandits were caught and hanged—this stopped most of the outbreak of violence.  Gold shipments on Panama paddle steamers often totaled over $1,000,000 per voyage and an accumulative total of over $64,000,000 in gold by 1853. The Mexican route usually involved taking passage in a paddle steamer to Veracruz Mexico, making your way 500 mi across Mexico to Acapulco on Mexico's Pacific coast.  This reversed the path taken by much of the Manila galleons' cargo from Manila which was unloaded at Acapulco and transferred to Veracruz for further shipment to Spain.  The Manila galleons were Spain's main link to the spices from the Spice Islands and silk etc. from China.  By 1849 the paddle steamer had been developed and Veracruz soon became a scheduled stop for many paddle steamers.  One of the main hazards of this route was being accosted by robbers and it was recommended that this passage should by done in groups of 50 or more.  After the Panama Railroad was completed in 1855 with a cost of about 5,000 lives and $8,000,000 the Mexico route was almost shut down. Other longer routes typically involved landing at Tampico, Mexico and then traversing the country to catch a ship in Mazatlán, Mexico.  These routes were used mainly by those who had business in Mexico City or some other Mexican city.  All Mexican travel suffered from the relative lack of steamship service and risk of robberies compared to the Nicaragua and Panama route. Wider than Panama, the Nicaragua route had the advantage of the easily navigated San Juan River and Lake Nicaragua with only a short excursion to the Pacific.  By 1851 trips by steamship to Panama and Nicaragua from New York, Philadelphia, Boston and New Orleans were about 2600 mi long and took about two weeks by steamship.  Commodore Cornelius Vanderbilt established a route through Nicaragua in 1850 that would shorten the water distance between New York and San Francisco by nearly 500 miles (800 km).  The route was by paddle steamer to the San Juan River in Nicaragua and then up the river by small steamboat or native craft to Lake Nicaragua.  After transiting the lake by small boat the travelers could exit and take a stagecoach or mule ride to San Juan del Sur or other city in the Pacific side of Nicaragua.  From there it was a short steamer ride to San Francisco after steamship lines were established.  By early 1852 Vanderbilt was employing seven steamers and offering serious competition to the Panama route. Early travelers til about 1851 had a difficult time knowing if and when a paddle steamer would arrive.  Vanderbilt started his service with steamer \"Independence\" in July 1851 and was soon joined by the steamer \"Prometheus\" and \"Pacific\".  The line prospered offering serious competition to the Pacific Mail Steamship Company.  By 1852 there are a recorded 17,400 Nicaragua crossing passengers compared to 29,600 crossing Panama.  These numbers closed to 24,000 Nicaragua crossings in 1852 to 27,200 Panama crossings in 1853.  As the Panama Railroad progressed across the Isthmus of Panama they took more and more of the traffic and after the railroad was completed in 1855 with a cost of about 5,000 lives and $8,000,000, combined with civil strife in Nicaragua, the Nicaragua route was almost shut down. When news of the California Gold Rush was confirmed late in 1848, many on the East Coast were ready to start on their way to California.  The route was well known because of the many whaling ships that had already traversed Cape Horn on their way to Pacific whaling grounds or ports of call in Hawaii and California.  In most East Coast cities, there were mariners who were well acquainted with the Cape Horn route and who knew precisely how to prepare for a voyage to California.  One of the chief advantages of the Cape Horn route was that they could leave at any time of the year they could find a ship.  There were also travelers nicknamed \"Argonauts\", who foresaw the broad needs and demands of a mining center on the Pacific Coast and who brought along goods of all descriptions and sizes, from needles to knocked-down steamboats, on which they hoped to realize good profits.  Indeed, some had no intention to do actual mining but to earn their fortune by providing goods and services to the miners. Most passengers initially got sea-sick and had a miserable time before getting their sea legs.  The nature of the passage, involving crossing the equator twice, and the length of the average voyage in miles and time (often more than six months) resulted in unusual supply difficulties.  Some ships stocked chickens, pigs or goats to supply fresh eggs and meat some time during the voyage.  Large amounts of fresh water and food (usually salted meat and sea biscuits or hardtack) were required for the trip.  The salted meat (about the only way meat could be preserved then) would have to be soaked overnight in fresh water to leach out the salt and make it semi-edible.  Most Captains understood the causes of scurvy and provided some lemon or lime juice to prevent it.  Despite the monotonous and often tasteless fare, many gained weight on the trip since they had virtually nothing to do except occasional laundry and endless card games.  Storms were a time of high suspense as the storm tossed their ship heavily from side to side and end to end.  Goods, suitcases, etc. not tied down were soon rolling and sliding across the decks.  Clothes were often washed by tying them to a rope and letting them be pulled behind the ship.  Many spent hours fishing but rarely caught anything.  When fish were caught they were a welcome addition to the monotonous diet.  Sleeping was often done in crowded passenger quarters or on the deck—weather permitting.  If the weather was fine and the crew permitted it, many tied a hammock in the rigging and slept outside.  Those who purchased cabin fare usually had a narrow cabin with a door and a cot that was under 5.5 ft long (people were shorter then) and about 1.5 ft wide.  The only extra room was the space under the bed for personal effects and luggage.  Those who paid \"steerage\" fares (about 50% less) slept in common bunk rooms. It was common practice to stop in at least two ports \"en-route\" in order to take on additional supplies of fresh food, water and wood as well as a welcome chance to get off the ships.  Rio de Janeiro or Santa Catarina, Brazil (often called St. Catharine's then) were common stopping places on the Atlantic side of South America.  Some passengers were so desperate for fresh food that they often procured thousands of oranges or other fruit that was ripe while they were there and ate it after they continued on. The Strait of Magellan comprises a navigable sea route immediately south of mainland South America and north of Tierra del Fuego.  The waterway is the most important natural passage between the Pacific and the Atlantic oceans, but it is considered a difficult route to navigate because of the unpredictable winds and currents and the narrowness of the passage. Captains who elected to utilize the Strait of Magellan to bypass Cape Horn and shorten the trip by about 500 mi experienced a passage of from three to six weeks' duration in surroundings so forbidding and monotonous it often provoked despair.  The narrow channels of the straits and the unpredictable currents, tides, and winds were constant hazards, especially to sailing vessels; steam-powered vessels had an easier passage. The other main way around South America was by way of Drake Passage, south of Cape Horn – the body of water between the southern tip of South America at Cape Horn, Chile and the South Shetland Islands of Antarctica.  It connects the southwestern part of the Atlantic Ocean (Scotia Sea) with the southeastern part of the Pacific Ocean and extends into the Southern Ocean.  The passage is named after the 16th-century English privateer, Francis Drake, who was the first to discover it in 1578.  Most sailing ship captains preferred the Drake Passage, which is open water for hundreds of miles, despite being often marked by very rough conditions and high winds.  There is no significant land anywhere around the world at the 55.6 to 62.8 degrees south latitude of the Drake Passage.  This is important to the unimpeded flow of the Antarctic Circumpolar Current, which carries a huge volume of water (about 600 times the flow of the Amazon River) through the passage and around Antarctica. In the Pacific Juan Fernández Islands of Talcahuano, Valparaíso, Chile or Callao, Peru were among the favorite watering and re-stocking ports in the Pacific.  Valparaíso, despite its mediocre harbor, then was the main entrepôt (trans-shipping) port on the Pacific side of South America.  After leaving the South American coast, the ships were buffeted by often baffling and contrary breezes as they traveled across the equatorial trade winds before reaching California waters.  Contrary winds often forced the ships far out into the Pacific – sometimes as distant as 140 degrees west (-140°) longitude before they encountered a favorable on-shore breeze and could sail towards San Francisco Bay.  San Francisco is at 122.5 degrees west (-122.5°) longitude. The last hazards to California-bound vessels were the approach and entrance to San Francisco Bay.  The Farallon Islands off the mouth of San Francisco Bay were the graveyard of several ships, and the narrow, often fog-shrouded opening into San Francisco Bay was always a danger.  Soon after the ship traffic built up, ship pilots who were knowledgeable of the bay were at work boarding incoming (and outgoing) ships and guiding the ships to a safe anchorage in the bay.  By 1851 the tangle of ships in the bay had led to the creation of a harbormaster who dictated where ships could drop anchor.  Once inside San Francisco Bay, vessels were reported and identified to the people of San Francisco by the watchman with a telescope in a tower erected in September 1849 on Telegraph Hill, San Francisco.  The watchman hoisted up the telegraph mast, one semaphore arm for a schooner, two for a brig, three for a ship and two raised about 45 degrees for a paddle steamer.  These signals were soon known by most residents. Once inside the bay and anchored, the next visitors were typically members of the Revenue Cutter Service (predecessor of U.S. Coast Guard).  Founded by Alexander Hamilton on August 4, 1790, the Revenue Cutter Service is the United States' oldest continuous seagoing service and enforced the tariff laws and tariff collection via customs duties (tariffs or ad valorem taxes) on foreign import goods.  In the 1800s about 85–95% of the money collected and used by the Federal Government was from tariff collections.  Customs collections were set up by late 1847 in San Francisco—the designated port of entry for most of California.  Soon after the revenuers had arrived, the ships were often visited by merchants looking for first choice on the arriving goods. The all-sea trip around Cape Horn to California by standard sailing vessels typically took about 200 days (about 6.5 months) and covered 16000 - .  Some trips took almost a year.  The all-sea route enabled enterprising emigrants to ship baggage and supplies they hoped to sell in California for gold dust.  The Cape Horn route was essentially the only route where low, medium or high weight or high volume goods could be shipped cheaply.  Other routes, which often cost significantly more, usually specialized in high value, low weight goods like mail, passengers or gold dust.  Starting out with essentially nothing, goods from the East Coast or Britain were often desperately needed and usually sold for high profits.  The long delay between seeing a market, ordering the goods and shipping the goods made business risky.  Some of the cargoes that were usually profitable were food, liquor, lumber and building supplies.  Ship loads of other types of goods sometimes saturated the marketplace, forcing the goods to be auctioned off at a loss.  Most cargoes included a variety of goods to minimize this problem. When the Central Pacific Railroad started construction of the First Transcontinental Railroad in 1863, all their locomotives, box cars, rails and railroad supplies were shipped via this route.  It was slow, but a ship going around Cape Horn to California could carry high weight and high volume products cheaper than any other route.  Nearly all the ships that were abandoned in San Francisco Bay came by the Cape Horn route.  Since the route back to the East coast was so long and return cargo almost nonexistent, the ships which arrived in San Francisco initially tended to stay there as the crew and passengers abandoned the ship for the gold fields.  As it became clear what was needed in San Francisco and the rest of California, some of the newer ships were put back into service with higher paid crews.  Some were crewed by disillusioned gold diggers seeking a cheap way back home.  Many ships were used for shorter runs to Pacific ports for food supplies or lumber—essentially all that was available then.  During October 1849, 63 vessels left San Francisco; 25 sailed for South America, primarily Valparaíso or Callao; 10 vessels sailed for the Hawaiian islands (20–30 days each way), 9 went to Oregon and 9 to other non-Pacific ports.  The high fares initially charged for paddle steamer passages to Panama induced some captains to allow passengers to work their way back to the East Coast for a low cost return.  This made it possible to put a crew together at a \"reasonable\" cost. News of the California Gold Strike arrived in China by the end of 1848.  Only a few hundred Chinese traveled to California in 1849, but this turned into a steady flood as travel arrangements were worked out by 1851 and later.  The Taiping Revolution in China and the poverty and violence in China induced many Chinese to leave China searching for a way to earn a better living.  Most Chinese, like most early California travelers, came to California with the goal of quickly making enough money to pay for their passage and improve their own and their family's status and lifestyle in China after they returned. Nearly all Chinese immigrants to California were young men with almost no women.  Their wives, families and relatives remained in China.  The Chinese tended to congregate in heavily male small semi-isolated \"China Towns\" wherever they settled.  One of the primary problems they encountered in California was the acute lack of Chinese women—almost none initially immigrated.  This \"problem\" was somewhat alleviated when recruiters ending up procuring or \"buying\" Chinese prostitutes and shipping them to the United States.  There they went to work in brothels or were \"bought\" by wealthy Chinese.  Some of the more affluent Chinese in California could \"buy\" a Chinese mistress for about $300 to $500.  The large population, the common condition of extreme poverty in China and the low status of women in China made recruiting or \"buying\" women for this \"profession\" fairly easy in China. Nearly all these Chinese men initially planned to return to China once they had made enough money.  About half of the initial Chinese immigrants did return to China where their wives and families lived.  Most of the Chinese immigrants booked their passages on ships with the Pacific Mail Steamship Company (founded 1848) or on the American China clippers which often left California empty and looking for a new cargo before returning home.  Nearly all Chinese immigrants neither spoke and understood English nor were they familiar with western culture and life.  While in the United States they had little incentive to assimilate into the dominant culture or learn anything more than rudimentary English language skills.  They did nearly all their business with a few Chinese businessmen that established businesses in California \"China Towns\".  The hostility they experienced from nearly all other cultures in America and their belief that they would return to China often discouraged them from attempting to assimilate. The port of San Francisco initially starting out as only a few ramshackle buildings with a population of about 180 in 1846 grew rapidly to several thousand residents only a few years later.  San Francisco was the nearest seaport to the gold fields with full access to virtually all ongoing sea traffic and freight shipments.  The port of San Francisco boomed and expanded very rapidly to a California state census population of about 32,000 in 1852 (San Francisco—the largest city in the state—U.S. California Census of 1850 was burned in one of the frequent fires in San Francisco).  In addition San Francisco had access to the Sacramento–San Joaquin River Delta which gave access to ship traffic going to Sacramento, Stockton and Marysville, California which were all about 120 mi closer to the goldfields.  Paddle steamers were put in service by late 1849 and provided \"easy\" transport of passengers and freight to Sacramento, banks, bar rooms, gambling establishments, wharfs, warehouses and other needed buildings were built as rapidly as possible. There were many Argonauts and companies of Argonauts who foresaw the broad needs and demands of a rapidly increasing and hopefully wealthy population in California.  Many Argonauts brought along goods of all descriptions and sizes, from needles to knocked-down steamboats, on which they hoped to sell or operate.  Indeed, many immigrants had no intention of panning gold or of digging for it in the mines.  They would rather sell goods and go into a business which the new mining community would support.  After all, gold was the objective; what did it matter whether the means of attaining it were direct or indirect? Ships provided almost the only link for new supplies—overland travel was too difficult and took too long.  One downside that soon developed in California was the long delay in communication between the east and west coast.  It took over 40 days to get a message back to the East Coast of the United States and often over 200–300 days to get new supplies shipped in by sailing ships.  Only high priced and lighter and smaller items could take the shorter and quicker paddle steamer route over the Isthmus of Panama, nearly everything else had to travel the approximate 17000 mi and over 200 day route all sailing ship route around Cape Horn or the Straits of Magellan.  Higher priced, time sensitive goods, were often shipped in the faster clipper sailing ship which could make the trip from New York, Boston, etc. to San Francisco in about 120 days.  Goods when they were shipped from the East coast (and Europe) were in shipload quantities when they arrived.  This often suppressed the local market for that product and some goods would have to be auctioned off at low prices to pay for freight etc. with little or no profit.  Very mixed cargo was typical of early shipments to minimize this problem.  Food, liquors and building supplies nearly always be sold at a profit.  Some business men thrived in this chaotic environment, many others lost their shirts. One of the first and urgent products needed was building supplies, food and other consumables.  Food supplies could be bought and shipped from local Pacific ports to San Francisco, Sacramento, etc.  Soon ships were going to and from Oregon, Hawaii, Mexico, Valparaíso Chile, etc. to get food and building supplies that could be shipped back to San Francisco.  Building supplies, including complete houses, bricks, etc. were shipped from many East Coast ports as well as Britain, etc.--these shipments were seldom time sensitive as to value.  Fairly quickly sawmills were operating in Northern California and Oregon to provide lumber and other wood products.  San Francisco shipping boomed and wharves and piers had to be developed to handle the onslaught of cargo--\"Long Wharf\" was probably the most prominent.  Farmers, laborers, business men, prospective miners, gamblers, 'entertainers' and prostitutes, etc. from around the world flocked to San Francisco. The few Californio ranchers already living in California initially prospered as the sudden increase in the demand for meat drove up the price paid for livestock.  Prices zoomed from the about $2.00 they received for a hide to about $30.00-$40.00 per cow when sold for meat.  Initially most of the Californios prospered.  Taxes, mortgages, squatters and the cost of proving ownership of their enormous land holdings they had got nearly free from the Indians combined with droughts that seriously decreased the size of their herds led many of the about 500 Californio ranch owners to lose some or all of their enormous land holdings within a few years. Later, these food shipments from foreign sources changed mainly to shipments from Oregon and internal shipments in California as agriculture was developed in both states.  Food like molasses, flour, oysters, hams, barrels of salted meat, rice, coffee, tea, eggs (from Mexico); cheese, sugar, coffee, potatoes, onions, limes, coconuts, raisins, almonds.  Some of the goods that were imported by ship included liquor: absinthe, alcohol, ale, beer, whiskey, cognac, cider, champagne, wine, sherry, brandy, claret.  Tools like shovels, picks and gold balances.  Consumables like cigars, cards, candles; clothing: boots, shoes, underwear; pants, shirts, etc.  Fruit like bananas, oranges, and lemons.  Lumber from Oregon and other parts of the U.S. and England (500,000 board feet of lumber and 500,000 bricks from Bath, England in one ship's cargo).  Building materials like nails, bricks, linseed oil, shingles, windows, stoves, lumber, etc..  Miscellaneous items like furniture, wagons, carts, fishing boats, steam engines, etc.  Coal that was needed to power the steam ships.  Livestock like hogs, cows, horses, sheep, chickens, etc.  Completed knocked down houses both metal and wooden.  Some ships carried mostly mail and passengers.  Almost anything could eventually be bought at some price.  As time went on many or indeed most of these supplies were grown or made locally; but some specialized items were nearly always cheaper to import on ships—still true today. San Francisco was designated the official port of entry for all California ports where U.S. Customs (also called tariffs and ad valorem taxs) (averaging about 25%) were collected by the Collector of customs from all ships bearing foreign goods.  The first Collector of customs was Edward H. Harrison appointed by General Kearny.  Shipping boomed from the average of about 25 vessels from 1825 to 1847 to about 793 ships in 1849 and 803 ships in 1850.  All ships were inspected for what goods they carried to collect the appropriate customs.  Passengers disembarking in San Francisco had one of the easier accesses to the gold country since they could from San Francisco take a paddle wheel steamer, after 1849, to Sacramento and several other towns. Starting in 1849 many of the ship crews jumped ship and headed for the gold fields when they reached port.  Soon San Francisco Bay had many hundreds of abandoned ships anchored off shore.  The better ships were re-crewed and put back in the shipping and passenger business.  Others were bought cheap and hauled up on the mud flats and used as store ships, saloons, temporary stores, floating warehouses, homes and a number of other uses.  Many of these re-purposed ships were partially destroyed in one of San Francisco's many fires and ended up as landfill to expand the available land.  The population of San Francisco exploded from about 200 in 1846 to 36,000 in the 1852 California Census.  Unfortunately, the 1850 U.S. Census of San Francisco was burned in one of its frequent fires. In San Francisco initially many people were housed in wooden houses, ships hauled up on the mud flats to serve as homes or businesses, wood-framed canvas tents used for saloons, hotels and boarding houses as well as other flammable structures.  Lighting and heat were provided by burning oil lamps or open fires.  All these canvas and wood structures housing fires, lanterns and candles combined with a lot of drunken gamblers and miners led almost inevitably to many fires.  Most of San Francisco burned down six times in six 'Great Fires' between 1849 and 1852. California has had an extensive fishery since it was discovered over 10,000 years ago.  The Native American inhabitants of California, nearly all hunter-gatherers, harvested many types of fish and shellfish as a regular and often major component of their diet.  Several varieties of salmon and steelhead were some of the mainstays of the California Indians. Indians living in the Northwest coast of California moved and fished along the rivers and California coastal waters using dugout canoes.  Their dugout canoes were laboriously made using fire and stone age tools out of large trees—usually redwoods. Salmon spawned in most rivers and streams in California sometime during the year and were a welcome addition to the diet of the hunter-gatherer California people living near almost all the streams.  Many tribes migrated to a given area along the streams during spawning runs to harvest the fish.  Fish were caught with spears, harpoons, fish nets, fish traps (fishing weirs), hooks and fishing lines, gathering seafood by hand and using specific plant toxins (soaproot, buckeye nuts, and wild cucumber root) to temporarily paralyze the fish so they would float to the surface where could easily be captured.  About the only early competitors for fish was the California grizzly bears who lived in California then and who also liked salmon.  Salmon and other fish were usually eaten almost immediately, smoked or sun dried and stored in woven baskets so they could not spoil and were available to eat nearly year-round.  Acorns gathered each fall were the other staple of most California Indian's diet, The Chumash people and Tongva people used sewen plank canoes (Tomols) to travel across and fish in the seas between the Southwest California Coast and the Channel Islands of California.  Some of their chief catches were sardines (pilchards) who were mentioned several times by the early Spanish explorers.  Sardines are small epipelagic fish (surface water fish to 200m) which then migrated along the California coast in large schools at certain times of the year.  They are an important forage fish for larger forms of marine life and a major fishery in the California waters till the sardine schools greatly diminished due to ocean current temperature changes and over fishing.  The sardines were caught by the California Indians primarily with some kind of net. The Native Americans in the San Francisco Bay constructed the Emeryville Shellmound and over 400 other shellmounds made up of inedible shellfish shells from millions of meals consumed at or near the shellmound sites.  The size of these mounds indicate that they had to have accumulated over hundreds if not thousands of years and indicate a well used and stable food resource.  The inedible shells they threw aside from their catches of shellfish eventually covered some hundreds of thousands of square feet, sometimes tens of feet thick.  Most of the shells are from oysters (Ostrea lurida) which occurred in large oyster reefs throughout the San Francisco Bay area.  How they are harvested is unknown but may have been by hand or by using oyster rakes. Tule (Schoenoplectus acutus) canoes were often used for fishing, moving between shell beds, hunting and fishing sites in the Bay area and the extensive Sacramento–San Joaquin River Delta. During the Spanish colonization and Mexican periods there are no known fisheries developed.  Indeed, they probably contracted as the Chumash people and Tongva people were enticed to move to the Spanish Missions of California and their movement and populations rapidly decreased. During the California Gold Rush there were many new immigrants who were familiar with fishing.  There was a large demand for fresh food including fresh fish and shellfish among the rapidly increasing California population.  Providing fresh food products were one of the most wanted and lucrative trades that developed among the California Argonauts.  The small Californio population before the rush were only able to provide some beef—their main \"product\" before 1850 had been cowhides and tallow.  After the California Gold Rush started developing a market for fresh fish, many Azorean-Portuguese turned from gold mining to fishing.  Fisherman established several small fishing communities up and down the California coast selling fish in towns and cities from San Diego to Eureka.  They built their own small fishing boats using the traditional \"lateen\" sail technology common in the Mediterranean on their fishing boats. After the gold rush started and Chinese immigrants appeared some of the first \"modern\" fishermen in California who started fishing in about 1853.  The Chinese were using sampans built in California to fish for squid, abalone and fish on Monterey Bay.  The Chinese, who came from the coastal Canton region of South China Sea, were able to export roughly two hundred to eight hundred pounds of fresh fish to San Francisco every day or one hundred tons per year.  The Chinese later specialized in squid fishing at night for their Asian markets. In 1899, the sardine fishery collapsed in Italy, energizing the Italian fishermen's immigration to California's fishing villages.  The same year, the first sardine cannery in San Francisco Bay was built.  Sardines, at this time, existed in large schools of millions of fish migrating each year up the California coast to spawn. Fishing technology at Monterey prior to 1905 was archaic and inefficient; the canning process was equally crude.  The unsightliness, odor, and processing waste from harbor canneries dictated that all future canneries would have to locate away from any business or residential district.  To catch more fish the fishermen turned to the more efficient lampra fish net used in Sicily.  The lampra net is set around a school of fish and when both ends are retrieved the vessel tows the net forward, closing the bottom and then top of the net while it scoops up much of the school of fish.  By 1912 70,000 cases of sardines were shipped. Other techniques were developed for \"reducing\" fish heads, tails, guts and skin into meal that could be processed into fertilizer and livestock feed.  \"Reduction\" was cheap because it didn't require much labor, and the market for fishmeal was unlimited.  Monterey became a cannery town.  About 70% of all sardines were ground up and used as fertilizer and lifestock feed with only about 30% canned for human consumption.  The yield of sardines landed in California was about 500,000 tons in 1940 down to only 53 tons in 1953. In 1940 sardines were the most valuable fishing stock in the state.  As the temperature of the ocean dropped the migrating sardine schools largely disappeared after 1950 from California waters and nearly all the canneries shut down.  The ocean temperature has an irregular cycle called the Pacific decadal oscillation turned water temperatures colder in the mid-1940s, driving sardines southward and intensifying the pressures brought on by overfishing.  As the ocean temperatures are cycling higher now there is some evidence that the sardines are starting to return. Today the Monterey Bay Aquarium, displaying many types of marine life, is located on the former site of a sardine cannery on Cannery Row off the Pacific Ocean shoreline in Monterey.  The Monterey Bay National Marine Sanctuary (MBNMS) is a U.S. Federally protected marine area offshore of California's central coast around Monterey Bay. One of the more unusual developments in the exploitation of natural resources was the market for fresh eggs during the Gold Rush leading people to take small boats to the Farallon Islands to collect wild bird eggs.  They sometimes collected up to 500,000 eggs in a year.  The Egg War is the name given to an 1863 armed conflict between rival egging companies on the Farallon Islands, 25 miles west of San Francisco. The fishing vessels first used were powered by wind and oars.  Since \"modern\" fishing in California was developed after 1850 (at about the same time as steamships) there were soon some steam powered fishing vessels being used for longer distance fishing in bigger boats.  The steam was used both for propulsion and also for winching in nets, unloading catches, lifting and lowering anchors etc.  As the diesel engines and petrol engines (gasoline engine) were developed in the early 1900s they were soon the engine of choice.  Diesel engines are now the engine of choice for powering most commercial fishing vessels.  Their economical operation and long lifetimes make their higher initial cost normally well worthwhile.  Today many smaller and sports boats are powered by an outboard motor consisting of a self-contained unit that includes engine, gearbox and propeller or jet drive, designed to be affixed to the outside of the transom.  The outboard motor provides steering control both as a movable rudder and by pivoting over their mountings to control the direction of thrust.  Outboard motors have less than one horsepower to over 200 hp and are relatively easy to remove for service or replacement. Commercial fishing today uses a variety of techniques for fishing.  Fishing rods with baited hooks and fishing lines used in various ways are used for fishing for some particular types of fish.  Fishing using nets like cast nets, hand nets, drift nets, gillnets, seine nets, trawl nets, surrounding nets etc. of various sizes and construction as well as longline fishing with hundreds of hooks on a line fishing both for bottom and pelagic fish (near surface fish) are the most common devices used to catch high yields of fish.  The crab fisheries uses crab pots baited with dead fish to catch crabs. In some parts of the Pacific Northwest, fishing with baited traps is also common.  Common commercial methods for catching shrimp and prawns include bottom trawling, cast nets, seines, shrimp baiting and dip netting.  Bottom trawling often tears up the ocean bottom and can be very destructive to all bottom dwelling fish.  Trawling involves the use of a system of nets deployed on or near the sea floor.  Benthic trawling is towing or dragging a net at the very bottom of the ocean.  Demersal trawling is towing a net just above the benthic zone.  Midwater trawling (pelagic trawling) is trawling, or net fishing nearer the surface of the ocean. For some applications a fish trap is used.  Fish traps or fishing weirs restricts the flow of fish so that they are directed into a trap.  The fish stay alive until they are removed and these techniques can be used to free some types of fish that are preferentially not caught.  Today elaborate fishing trawlers, etc. are all examples of the fishing techniques used today.  To keep the caught fish fresh they are often kept in refrigerated holds or packed in ice. As is typical worldwide of public owned resources, unlimited fishing has led to severe overfishing for some fisheries.  In response to this quotas, catch limits, closed and open seasons and other regulations had to be set in place to control the who, when, how and where questions of fishing.  In 1851—California enacted a law concerning oysters and oyster beds.  In 1852 the first regulation of salmon fishing occurred when fishing weirs or stream fish obstructions were prohibited and closed seasons established.  In 1870 California Board of Fish Commissioners, predecessor to the California Department of Fish and Game was established.  In 1870 the eastern oyster (Crassostrea virginica) was introduced, in 1871 shad, in 1874 Catfish and in 1879 striped bass were all introduced to California waters.  California has about 4,000 lakes and 37000 mi of streams and canals suitable for game fish.  To help fish get around dams fish ladders are constructed to allow them to pass on upstream for spawning etc.  To preserve, protect and enhance existing fishing the California Department of Fish and Game (DFG) tries to keep all fishing laws enforced.  The 720 properties managed by the DFG are: 110 wildlife areas, 130 ecological reserves, 11 marine reserves, 159 public access areas, 21 fish hatcheries and 289 other types of properties.  To help keep California waters stocked with fish in 1870 the first California fish hatcheries were built—mostly trout hatcheries.  Today (2011) there are eight salmon and steelhead hatcheries and 13 trout hatcheries.  Though hatcheries may help some fishing stocks they are no panacea to counteract overfishing, habitat destruction, stream restrictions, water diversions, etc. See: Monterey Bay National Marine Sanctuary link to get a list and links to other protected marine preserves in California. The major types of sport and commercial fish and shellfish now found in California waters are: Abalone, Albacore tuna, Anchovy, Barracuda, Surfperch, Billfishes, Bluefin tuna, Bonito, Cabezone, California halibut, Carp, Catfish, Clams, California corbina, Crabs, Crappie, Croaker, Dungeness crab, Eels, Flounder, Flying fish, Giant sea bass, Greenling, Groundfish (includes Rockfish species), Grouper, Grunion, Halibut, Hardhead, Herring, Hake, Jack mackerel, Kelp Bass, Largemouth bass, Lingcod, Mackerel, Oysters, Pacific shrimp, Perch, Pikeminnow (Squawfish), Prawn, Rock crab, Sablefish, Sacramento blackfish, Salmon, Sardine, Scallops, Scorpionfish, Shark, California sheephead, skate, Shortspine thornyhead, Skipjack tuna, Smallmouth bass, Smelts, Sole, Spider or Sheep crab, Splittail, Spiny lobster, Squid, Steelhead, Striped bass, Sturgeon, Surfperch, Swordfish, Turbot, Trout, Whitefish, Whiting, Yellowtail (fish) See: NOAA Long list of California fish for more specific names: Nearly all fishing is subject to quotas, allowed seasons, licensing, allowed tackle, type and number of lines or net types, excluded (closed) areas, allowed size range, allowed catch size, and other restrictions.  The jurisdictions and roles of several state and federal agencies often overlap in the maritime domain giving rise to an alphabet soup of agencies and jurisdictions.  Each state normally maintains joint jurisdiction over the first 3.4 mi (3 nautical mile) of their coastal waters.  The main agency charged with ensuring deep sea fishing regulations and restrictions in the United States exclusive economic zone (EEZ) of 227 mi (200 nautical miles) off its shores are enforced on the high seas by the United States Coast Guard. The National Oceanic and Atmospheric Administration (NOAA) agency within the United States Department of Commerce is charged with protecting and preserving the nation's living marine resources through scientific research, fisheries management, enforcement and habitat conservation.  The National Oceanic and Atmospheric Administration Fisheries Office for Law Enforcement (NOAA OLE) tries to enforce the about 35 laws and regulations passed by Congress.  NOAA's Office of Law Enforcement (OLE) is responsible for carrying out more than 35 federal statutes and regulations.  The agency's jurisdiction spans more than 11500000 sqkm s ocean in the U.S.'s exclusive economic zone (EEZ) spread over more than 85000 mi of U.S. coastline and the country's 13 National Marine Sanctuaries and its Marine National Monuments.  It and the United States Coast Guard are also responsible for enforcing U.S. treaties and international law governing the high seas and international trade.  With such a large coverage area, it's no wonder that NOAA's Office of Law Enforcement operates joint enforcement agreements with 27 coastal states and partners with other agencies to help get the job done.  Article III, Section 2 of the United States Constitution grants original jurisdiction to U.S. federal courts over admiralty and maritime matters, however that jurisdiction is not exclusive and most maritime cases can be heard in either state or federal courts under the \"saving to suitors\" clause.  NOAA OLE and NOAA Fisheries works within the laws as enacted in the Magnuson-Stevens Act, the Lacey Act Amendments of 1981, the Marine Mammal Protection Act, the National Marine Sanctuaries Act and the Endangered Species Act.  NOAA's Office of Law Enforcement now has 146 special agents and 17 enforcement officers working out of six divisional offices and 52 field offices throughout the United States and U.S. territories.  Many have criticized this meager manpower as grossly inadequate. Overfishing is one of the main problems with many marine fisheries with about 30% of all marine fisheries thought to be over fished.  Inadequate data is one of the main restrictions to finding and instituting reasonable and sustainable limits on many fishing stocks.  To control overfishing NOAA has instituted the National Marine Fisheries Service (NMFS) to set quotas, specify open and closed fisheries and seasons and other limits on what, when and how fish are caught within federal guidelines.  With the help of the six regional science centers, eight regional fisheries management councils, the coastal states and territories, and three interstate fisheries management commissions, These councils have had varying amounts of success, but seem to at least have started the rehabilitation of some fisheries.  United States Fish and Wildlife Service National Fish Passage Program tries to remove barriers blocking the natural migration of fish to historic habitat used for reproduction and growth. Since fisheries are the mainstay of some communities as well as being a $38 billion industry there are many conflicting pressures on controlling fishing.  More data gathered by more people on: bycatch (caught but unusable fish), fish life cycles, fish habitats at different parts of their life cycle, destructive fish harvesting methods, least damaging ways to harvest fish, etc. are needed to make reasonable choices and set quotas, seasons, etc. necessary to preserve our fisheries.  All of this should be set with a maximum of scientific and a minimum of political input.  With today's increasingly efficient fishing techniques and fleets, necessary restrictions are the only thing that will assure a continuing source of fishing related jobs and fish products for our descendants—being responsible stewards of our natural resources is often a difficult job but one we can learn and implement. Mare Island, near the city of Vallejo, California, was first Naval Base in California.  The Napa River forms its eastern side as it enters the Carquinez Strait juncture with the east side of San Pablo Bay.  In 1850, Commodore John Drake Sloat, in charge of a commission to find a California naval base, recommended the island across the Napa River from the settlement of Vallejo; it being \"free from ocean gales and from floods and freshets.\" On November 6, 1850, two months after California was admitted to statehood, President Millard Fillmore reserved Mare Island for government use.  The U.S. Navy Department acted favorably on Commodore Sloat's recommendations and Mare Island was purchased in July 1852, for the sum of $83,410 for the use as a naval shipyard.  Two years later, on 16 September 1854, Mare Island became the first permanent U.S. naval installation on the west coast, with Commodore David G. Farragut, as Mare Island's first base commander.  For more than a century, Mare Island served as the United States Navy's Mare Island Naval Shipyard.  A 508-foot (155 m) drydock was built by the Public Works Department on an excellent rock foundation of cut granite blocks.  The work took nineteen years and was completed in 1891.  During the Spanish–American War, a concrete drydock on wooden piles, 740 feet (230 m) long, was completed after eleven years of work, in 1910.  By 1941, a third drydock had been completed and the drydock number four was under construction.  The ammunitions depot and submarine repair base were modern, fireproof buildings.  A million dollar, three-way vehicle causeway to Vallejo was completed. Before World War II, Mare Island had been in a continual state of upbuilding.  By 1941, new projects included improvements to the central power plant, a new pattern storage building, a large foundry, machine shop, magazine building, paint shop, new administration building, and a huge storehouse.  The yard was expected to be able to repair and paint six to eight large naval vessels at a time.  Several finger piers had recently[when?]  been built, as well as a new shipbuilding wharf, adding one 500-foot (150 m) and a 750-foot (230 m) berth.  It employed 5593 workers at the beginning of 1939, and rapidly increased to 18,500 busily engaged by May 1941, with a monthly payroll of $3,500,000 (1941).  Then came Pearl Harbor.  In 1941, the drafting department had expanded to three buildings accommodating over 400 Naval architects, engineers and draftsmen.  The hospital carried 584 bed patients.  Mare Island became one of the U.S. Navy's ship building sites in World War II specializing in building diesel engine powered submarines—they eventually built 32 of them.  After the war was over Mare Island became a premier site for building nuclear-powered submarines—building 27 of them. In 1969, the US Navy transferred its (Vietnam War) Brown Water Navy Riverine Training Forces from Coronado, California, to Mare Island.  Swift Boats (Patrol Craft Fast-PCF), and PBRs (Patrol Boat River), among other types of riverine craft, conducted boat operations throughout the currently named Napa-Sonoma Marshes State Wildlife Area, which are located on the north and west portions of Mare Island.  Mare Island Naval Base was deactivated during the 1995 cycle of US base closures, but the US Navy Reserves still have access to the water portions of the State Wildlife Area for any riverine warfare training being conducted from their new base in Sacramento, California. In 1996 Mare Island Naval Shipyard was closed. Naval Base San Diego was started on land acquired in 1920.  San Diego has become the home port of the largest naval fleet in the world, and includes two supercarriers, as well as U.S. Marine Corps stations, U.S. Navy ports, and U.S. Coast Guard installations.  Naval Base San Diego is the largest base of the United States Navy on the west coast of the United States, in San Diego, California.  Naval Base San Diego is the principal homeport of the Pacific Fleet, consisting of 54 ships and over 120 tenant commands.  The base is composed of 13 piers stretched over 977 acres of land and 326 acres of water.  The total on base population is 20,000 military personnel and 6,000 civilians. California shipbuilders have built or repaired ships of all types, from battleships to wood sailing ships, from the mid-1850s till today.  In both World War I and World War II several large and small shipyards were built in California especially for war time construction.  Ships were built out of steel, wood and when these were in short supply even out of concrete.  Many of the shipyards built many different types of ships and only the \"major\" builds are included here—see references for more detail and the names of ships.  Shipyards that built only one ship are not included. Mare Island Naval Shipyard (MINS) in Vallejo, California was the premier naval construction site for Navy ships on the West Coast of the United States from about 1855 to 1993. Before World War II, Mare Island had been in a continual state of build up.  By 1941, the yard was expected to be able to repair and paint six to eight large naval vessels at a time.  It employed 5593 workers at the beginning of 1939 which rapidly increased to 18,500 by May 1941, with a monthly payroll of $3,500,000(1941).  Then came the attack on Pearl Harbor.  During World War II, Mare Island specialized in building up the US Navy's submarine forces in the Pacific as well as building other Naval ships. When Congress ordered Mare Island closed down in 1993, the shipyard employed 5,800 workers. Mare Island Naval Shipyard constructed at least eighty-nine seagoing vessels for the United States Navy—including two for the Revenue Cutter Service.  Among the more important ships & boats built were: Mare Island Construction See: Mare Island Naval Shipyard for specific ships. National Steel and Shipbuilding Company, commonly referred to as NASSCO, is a shipyard in San Diego, and a division of General Dynamics.  It is located next to the San Diego Naval base.  What became NASSCO was founded as a small machine shop called California Iron Works (CIW) in 1905.  The machine shop and foundry were renamed National Iron Works (NIW) in 1922 and moved to the San Diego waterfront to build ships in 1944–1945.  In 1949 NIW was renamed National Steel and Shipbuilding Corporation (NASSCO).  The shipyard specializes in maintaining and constructing commercial cargo ships and auxiliary vessels such as minesweepers and LSTs, hospital ships, patrol craft, and cargo vessels for the US Navy and the Military Sealift Command.  It is the largest new construction shipyard on the West Coast of the United States employing more than 4,600 people and is now the only major ship construction yard on the West Coast of the United States. See for write-ups on 70 ships built there. California Shipbuilding Corporation (often called Calship) built 467 Liberty and Victory ships during World War II, including \"Haskell\"-class attack transports.  The Calship shipyard was created at Terminal Island in Los Angeles as part of the World War II shipbuilding effort.  It was initially 8 ways, and increased to 14.  After the war, it was liquidated.  The ships they built were: Kaiser Richmond Shipyards, Richmond, California (a Kaiser facility) had four Richmond Shipyards, located in the city of Richmond, California and another shipyard in Los Angeles.  Kaiser had still other yards in Washington (state) and other states.  They were run by Kaiser-Permanente Metals and Kaiser Shipyards.  The Richmond yards were responsible for constructing more Liberty ships during World War II, 747, than any other shipyards in the United States.  Liberty ships were chosen for mass production because their somewhat obsolete design was relatively simple and their triple expansion piston steam engine components were simple enough that they could be made by several companies that were not highly needed to manufacture other parts.  Ship building was given a high priority for steel and other needed components as the German U-boats till 1944 sunk more ships than could be built by all the shipyards in the United States.  The U.S. shipyards built about 5,926 ships in World War II plus over 100,000 more small craft made for the U.S. Army naval components. Henry J. Kaiser's company had been building cargo ships for the U.S. Maritime Commission in the late 1930s.  In 1940 orders for ships from the British government, already at war with Nazi Germany, allowed for growth.  Kaiser established his first Richmond shipyard, beginning in December 1940.  Eventually building three more in Richmond; each yard with four to eight slips to build ships.  Kaiser-Permanente specialized in mass-producing Liberty ships fast and efficiently and that's all they built till 1944 when they switched to the much more complicated Victory ships and built some tugs and Landing Ship, Tank (LSTs) and other specialized ships in the newly built Yard #4. The following references list individual ships built: These Liberty ships were completed in two-thirds the amount of time and at a quarter of the cost of the average of all other shipyards.  The Liberty ship  was assembled in less than five days as a part of a special competition among shipyards; but by 1944 it was only taking the astonishingly brief time of a little over two weeks to assemble a Liberty ship by standard methods.  They pre-assembled major parts of the ship including the hull sections at various locations in the shipyard and then, when needed, moved them with heavy lift cranes to the shipyard launching site where they welded the pre-built sections together.  After the ships were launched they were finished to their final configuration while afloat and the launch way was available to start building another ship. In 1945, the shipyards were shut down as fast as they had started up four years earlier.  Much of the shoreline previously occupied by the shipyards is now owned by Richmond, California and has been cleaned up and redeveloped under federally assisted \"brownfields\" programs.  The 'Rosie the Riveter'/Home Front World War II National Historical Park was established on the shipyard site to commemorate and interpret the role of the home front in winning World War II. San Francisco Bay has been served by ferries of all types for over 150 years.  Although the construction of the Golden Gate Bridge and the San Francisco–Oakland Bay Bridge led to the decline in the importance of most ferries, some passenger ferries are still in use today for both commuters and tourists, including Golden Gate Ferry and San Francisco Bay Ferry. Ferry service is also available for crossing San Diego Bay from San Diego to Coronado.  Passenger ferries also serve the offshore ports of Avalon and Two Harbors on Santa Catalina Island.  There is no regular vehicle ferry service to Avalon, however, since the city restricts the use of cars and trucks within its borders. The Farallon Islands, Channel Islands of California and the rocky mainland coast have historically provided hazardous navigational obstacles to shipping.  Intermittent fogs and dangerous winds and storms often led ships to rocks, dangerous beaches and islands to be pounded by the Pacific Ocean's Swell and storms.  Fierce currents have always swept in and out of the entrance to the Golden Gate as the tide shifts direction.  More than 140 shipwrecks have been reported in the waters of the Gulf of the Farallones National Marine Sanctuary. One of the first recorded shipwrecks in California is that of the \"San Augustin\", a richly laden Spanish Manila galleon, which was driven ashore in a gale in 1595 in Drake's Bay, northwest of San Francisco. The Honda Point Disaster was the largest peacetime loss of U.S. Navy ships.  Honda Point, also called Point Pedernales, is located on the seacoast of what is now Vandenberg Air Force Base off Point Arguello on the coast in Santa Barbara County, California.  On the evening of 8 September 1923, fourteen ships of Destroyer Squadron 11 were traveling at 20 knots (37 km/h) in formation while navigating by dead reckoning to find the entrance to the sometimes treacherous Santa Barbara Channel.  The squadron was led by Commodore Edward H. Watson, on the flagship destroyer USS \"Delphy\".  All were \"Clemson\"-class destroyers, less than five years old.  At 21:00 hours the ships turned east to course 095, supposedly heading into the entrance of Santa Barbara Channel.  Seven destroyers ran aground at Honda Point, a few miles from the northern side of the Santa Barbara Channel.  Two more destroyers sustained some damage.  Twenty three men died. The state of California keeps a Shipwrecks Database of all known California shipwrecks (1540 ea.)  and their best-known latitude and longitude coordinates, ship type, owner, Captain, etc.--when known.  The definition of a shipwreck included in the database is rather broad including wrecks by running aground on a shore, rocks or reefs, ship explosions, foundering (filling with water and sinking), hitting snags (sunken trees), on board fires, parted lines, etc.--essentially anything that causes damage to the ship.  Many of these ships were repaired and remained in service after their accidents.  These ships, their cargoes, and the mooring systems which restrained them are the physical remains of the maritime history of California. A lighthouse is a tower, building, or other type of structure designed to contain a flashing light to warn of hazards or to aid navigation primarily at night.  The lights now flash on and off in a predetermined sequence to identify which light they are.  Lighthouses are used as an aid to nighttime or fog-bound navigation for ship pilots and Captains at sea or on inland waterways.  They warn of dangerous coastlines, points of land, hazardous shoals, rocks and reefs, or mark ship channels or harbor entrances.  Under clear weather a light can be seen at night about 16 mi .  Now in areas of fog the lights are typically combined with a foghorn.  Before foghorns were developed cannons and/or large bells (rung by clockworks) were used to warn of fog shrouded hazards.  The flashing lights are usually mounted on towers or other prominent structures built on points of land, rocks or shoals near the sea.  Some are built on pilings, caissons or mounted on isolated rocks.  Some lighthouses are mounted on anchored Lightships when no other economical alternative exists. In the 1850s the light was emitted from a system of oil or kerosene lamps.  The light was concentrated and focused with a system of Fresnel lenses.  In the 1850s the light was provided from a burning wick in a whale oil lamp.  Later, as kerosene became available, the light was provided by burning kerosene.  They typically used an Argand lamp which featured a hollow wick in a glass chimney for better, brighter, combustion with a silvered parabolic reflector behind the lamp to direct and intensify the light output.  All oil fired lamps used burning wicks to make the light—giving rise to one of the lighthouse keeper's nickname as \"wickies\" as they spent a great deal of their time trimming the wicks on their lamps in order to keep them burning brightly with minimum sooting.  In the 1850s their lights were rotated using clockworks, usually powered by falling weights attached to chains.  Many lighthouses had vertical shafts in them so the weights could drop the height of the tower.  This provided a longer time period before the weights would have to be pulled up again by the lighthouse keepers to power the rotation mechanism.  Some had to be rewound as often as every two hours. To keep the clockworks wound, refuel the oil needed to keep the light going and keep the lighthouse equipment and windows clean and maintained the lighthouses were typically manned with a lighthouse keepers of from one to five men or women.  Only the fortunate few lighthouses were located where the crews could live in comfort and/or socialize with others.  Because these assignments were often in lonely fog bound locations the crews often rotated on and off duty every few months.  Families sometimes were paid to run a lighthouse that way the husband, wife and children could keep together. Starting in the early 1890s the lights were provided by burning acetylene gas generated in situ from calcium carbide reacting with water.  The acetylene-gas illumination system could reliably be turned on and off automatically, enabling automated unattended lighthouses to be used.  Once electricity became available, often provided by one or more diesel electric generators in remote locations, the light source was gradually converted to electrical power and the clockworks were run by some type of electric motor.  For example, of modern changes, the Chatham Light in Chatham, Massachusetts (near the \"elbow\" of Cape Cod) the light was converted in 1969 from its Fresnel lens light assembly to a Carlisle & Finch DCB-224 rotating light generating over 2.8 million candela.  This is visible for over 20 nautical miles on a clear night and has a self-changing bulb assembly to replace burned-out bulbs. Once widely used, the number of operational lighthouses has declined due to the expense of maintenance and their replacement by modern electronic navigational aids.  Nearly all lighthouses today, that are still being used, are automated to the extent possible with power often provided via solar cells and large batteries in inaccessible areas.  Today's lighthouses are all run by the U.S. Coast Guard the successor to United States Lighthouse Service.  The list of active light houses, lighted beacons, etc. that provide detailed information on aids to navigation with their locations and characteristic signals is currently maintained by the U.S. Coast Guard in its Light List issued each year.  California is presently in Eleventh and Thirteenth Coast Guard district. While the Spanish were in California their shipping was seldom more than ~2.5 ships/year and they almost never had any way to predict when ships would show up and communication was so slow and uncertain that there was seldom any need for something like lighthouses.  When nighttime signals were thought appropriate large fire might be built on the beach.  Essentially there was no nighttime navigation—it was too hazardous.  During nighttime ships kept well off shore till daylight or anchored.  Things only improved slightly when Mexico controlled California as the shipping increased to about 25 ships/year—still too few to make lighthouses or even signal fires almost ever needed.  Ships still kept well off shore till daylight or anchored. When the California Gold Rush started and the number of ships per year jumped to over 700 ships per year and lighthouse technology had advanced far enough, mainly through the introduction of the Fresnel lens and Argand lamps, lighthouses started to become much more useful and feasible.  Several bad shipwrecks showed that there were many hazards to navigation that needed to be marked at night or in fog.  Since from 1790 till well after 1850 the U.S. Federal Government was over 85% financed by import tariffs (also called customs duties or ad valorem taxes) on imported foreign goods of about 25% there was already a steady flow of money collected from California shipping going to Washington.  Since all tariffs were paid by foreign goods shipped into the United States since 1790 by the Revenue Marine (predecessor of the U.S. Coast Guard) closely monitored ship traffic.  Tariffs collected by the Collector of Customs who was charged with inspecting each ship that came into port and collecting the appropriate tariff tax.  The first Collector of Customs in California was Edward H. Harrison appointed by General Kearny in 1848.  To get some of this revenue flowing back to California, California congressmen started petitioning for lighthouses and Congress soon agreed.  By 1850 the East Coast already had a fairly extensive array of light houses so the same technology, developed over decades of use, was transferred to the West Coast of the United States. The firm of Francis A. Gibbons and Francis Kelly was awarded the contract to build California's first seven lighthouses in 1853.  The lighthouse on Alcatraz Island was the first built and was in operation in the San Francisco Bay by 1855—completion was delayed because of the shortage of Fresnel lenses.  Over time more than 45 lighthouses were eventually built along the California coast. Despite the efforts of the brave men and women who were stationed at the Point Reyes lighthouse, ships continued to wreck on the nearby coast.  The Life-Saving Service opened the first of two Life Saving Stations built at Point Reyes in 1889.  The second station, at Drakes Beach, closed in 1968.  The workers stationed there attempted the rescue of victims of storms and shipwrecks.  The incredible danger of their job and the dedication they have to their jobs can be sensed in the U.S. Coast Guard's unofficial motto, The California U.S. Census of 1850 showed 92,597 residents.  To this should be added residents from San Francisco, (the largest city in the state then) Santa Clara, and Contra Costa counties whose censuses were burned up or lost and not included in the totals.  San Francisco's 1850 U.S. census was lost in one of their periodic fires that swept the city six times in their first few years.  Newspaper accounts in 1850 (\"Alta Californian\") gives the population of San Francisco city/county at 21,000; The special California state Census of 1852 finds 6,158 residents of Santa Clara county, 2,786 residents of Contra Costa County and 36,134 in the fast-growing San Francisco city/county.  The \"corrected\" California U.S. 1850 Census is over 120,000.  The 1850 U.S. Census shows 7,765 Hispanic residents that were born in California.  See: U.S. Seventh Census 1850: California which includes the special 1852 California state census\n\nList of Dutch inventions and discoveries The Netherlands had a considerable part in the making of modern society.  The Netherlands and its people have made numerous seminal contributions to the world's civilization, especially in art, science, technology and engineering, economics and finance, cartography and geography, exploration and navigation, law and jurisprudence, thought and philosophy, medicine, and agriculture.  Dutch-speaking people, in spite of their relatively small number, have a significant history of invention, innovation, discovery and exploration.  The following list is composed of objects, (largely) unknown lands, breakthrough ideas/concepts, principles, phenomena, processes, methods, techniques, styles etc., that were discovered or invented (or pioneered) by people from the Netherlands and Dutch-speaking people from the former Southern Netherlands (\"Zuid-Nederlanders\" in Dutch).  Until the fall of Antwerp (1585), the Dutch and Flemish were generally seen as one people. The De Stijl school proposed simplicity and abstraction, both in architecture and painting, by using only straight horizontal and vertical lines and rectangular forms.  Furthermore, their formal vocabulary was limited to the primary colours, red, yellow, and blue and the three primary values, black, white and grey.  De Stijl's principal members were painters Theo van Doesburg (1883–1931), Piet Mondrian (1872–1944), Vilmos Huszár (1884–1960), and Bart van der Leck (1876–1958) and architects Gerrit Rietveld (1888–1964), Robert van 't Hoff (1888–1979) and J.J.P. Oud (1890–1963). Brabantine Gothic, occasionally called Brabantian Gothic, is a significant variant of Gothic architecture that is typical for the Low Countries.  It surfaced in the first half of the 14th century at Saint Rumbold's Cathedral in the City of Mechelen.  The Brabantine Gothic style originated with the advent of the Duchy of Brabant and spread across the Burgundian Netherlands. The Dutch gable was a notable feature of the Dutch-Flemish Renaissance architecture (or Northern Mannerist architecture) that spread to northern Europe from the Low Countries, arriving in Britain during the latter part of the 16th century.  Notable castles/buildings including Frederiksborg Castle, Rosenborg Castle, Kronborg Castle, Børsen, Riga's House of the Blackheads and Gdańsk's Green Gate were built in Dutch-Flemish Renaissance style with sweeping gables, sandstone decorations and copper-covered roofs.  Later Dutch gables with flowing curves became absorbed into Baroque architecture.  Examples of Dutch-gabled buildings can be found in historic cities across Europe such as Potsdam (Dutch Quarter), Friedrichstadt, Gdańsk and Gothenburg.  The style spread beyond Europe, for example Barbados is well known for Dutch gables on its historic buildings.  Dutch settlers in South Africa brought with them building styles from the Netherlands: Dutch gables, then adjusted to the Western Cape region where the style became known as Cape Dutch architecture.  In the Americas and Northern Europe, the West End Collegiate Church (New York City, 1892), the Chicago Varnish Company Building (Chicago, 1895), Pont Street Dutch-style buildings (London, 1800s), Helsingør Station (Helsingør, 1891), and Gdańsk University of Technology's Main Building (Gdańsk, 1904) are typical examples of the Dutch Renaissance Revival (Neo-Renaissance) architecture in the late 19th century. Antwerp Mannerism is the name given to the style of a largely anonymous group of painters from Antwerp in the beginning of the 16th century.  The style bore no direct relation to Renaissance or Italian Mannerism, but the name suggests a peculiarity that was a reaction to the classic style of the early Netherlandish painting.  Antwerp Mannerism may also be used to describe the style of architecture, which is loosely Mannerist, developed in Antwerp by about 1540, which was then influential all over Northern Europe.  The Green Gate (Brama Zielona) in Gdańsk, Poland, is a building which is inspired by the Antwerp City Hall.  It was built between 1568 and 1571 by Regnier van Amsterdam and Hans Kramer to serve as the formal residence of the Polish monarchs when visiting Gdańsk. Cape Dutch architecture is an architectural style found in the Western Cape of South Africa.  The style was prominent in the early days (17th century) of the Cape Colony, and the name derives from the fact that the initial settlers of the Cape were primarily Dutch.  The style has roots in medieval Netherlands, Germany, France and Indonesia.  Houses in this style have a distinctive and recognisable design, with a prominent feature being the grand, ornately rounded gables, reminiscent of features in townhouses of Amsterdam built in the Dutch style. The Amsterdam School (Dutch: \"Amsterdamse School\") flourished from 1910 through about 1930 in the Netherlands.  The Amsterdam School movement is part of international Expressionist architecture, sometimes linked to German Brick Expressionism. The Rietveld Schröder House or Schröder House (Rietveld Schröderhuis in Dutch) in Utrecht was built in 1924 by Dutch architect Gerrit Rietveld.  It became a listed monument in 1976 and a UNESCO World Heritage Site in 2000.  The Rietveld Schröder House constitutes both inside and outside a radical break with tradition, offering little distinction between interior and exterior space.  The rectilinear lines and planes flow from outside to inside, with the same colour palette and surfaces.  Inside is a dynamic, changeable open zone rather than a static accumulation of rooms.  The house is one of the best known examples of \"De Stijl\" architecture and arguably the only true \"De Stijl\" building. The Van Nelle factory was built between 1925 and 1931.  Its most striking feature is its huge glass façades.  The factory was designed on the premise that a modern, transparent and healthy working environment in green surroundings would be good both for production and for workers' welfare.  The factory had a huge impact on the development of modern architecture in Europe and elsewhere.  The Van Nelle Factory is a Dutch national monument (Rijksmonument) and since 2014 has the status of UNESCO World Heritage Site.  The Justification of Outstanding Universal Value was presented in 2013 to the UNESCO World Heritage Committee. An architectural movement started by a generation of new architects during the 1990, among this generation of architects were OMA, MVRDV, UNStudio, Mecanoo, Meyer en Van Schooten and many more.  They started with buildings, which became internationally known for their new and refreshing style.  After which Super Dutch Architecture spread out across the globe. The Dutch door (also known as \"stable door\" or \"half door\") is a type of door divided horizontally in such a fashion that the bottom half may remain shut while the top half opens.  The initial purpose of this door was to keep animals out of farmhouses, while keeping children inside, yet allowing light and air to filter through the open top.  This type of door was common in the Netherlands in the seventeenth century and appears in Dutch paintings of the period.  They were commonly found in Dutch areas of New York and New Jersey (before the American Revolution) and in South Africa. The Red and Blue Chair was designed in 1917 by Gerrit Rietveld.  It represents one of the first explorations by the De Stijl art movement in three dimensions.  It features several Rietveld joints. The Zig-Zag Chair was designed by Rietveld in 1934.  It is a minimalist design without legs, made by 4 flat wooden tiles that are merged in a Z-shape using Dovetail joints.  It was designed for the Rietveld Schröder House in Utrecht. Although oil paint was first used for Buddhist paintings by Indian and Chinese painters sometime between the fifth and tenth centuries, it did not gain notoriety until the 15th century.  Its practice may have migrated westward during the Middle Ages.  Oil paint eventually became the principal medium used for creating artworks as its advantages became widely known.  The transition began with Early Netherlandish painting in northern Europe, and by the height of the Renaissance oil painting techniques had almost completely replaced tempera paints in the majority of Europe.  Early Netherlandish painting (Jan van Eyck in particular) in the 15th century was the first to make oil the default painting medium, and to explore the use of layers and glazes, followed by the rest of Northern Europe, and only then Italy. Glazing is a technique employed by painters since the invention of modern oil painting.  Early Netherlandish painters in the 15th century were the first to make oil the usual painting medium, and explore the use of layers and glazes, followed by the rest of Northern Europe, and only then Italy. Two aspects of realism were rooted in at least two centuries of Dutch tradition: conspicuous textural imitation and a penchant for ordinary and exaggeratedly comic scenes.  Two hundred years before the rise of literary realism, Dutch painters had already made an art of the everyday – pictures that served as a compelling model for the later novelists.  By the mid-1800s, 17th-century Dutch painting figured virtually everywhere in the British and French fiction we esteem today as the vanguard of realism. Hieronymus Bosch is considered one of the prime examples of Pre-Surrealism.  The surrealists relied most on his insights.  In the 20th century, Bosch's paintings (e.g. The Garden of Earthly Delights, The Haywain, The Temptation of St. Anthony and The Seven Deadly Sins and the Four Last Things) were cited by the Surrealists as precursors to their own visions. Still-life painting as an independent genre or specialty first flourished in the Netherlands in the last quarter of the 16th century, and the English term derives from \"stilleven\": \"still life\", which is a calque, while Romance languages (as well as Greek, Polish, Russian and Turkish) tend to use terms meaning \"dead nature\". The term \"landscape\" derives from the Dutch word \"landschap\", which originally meant \"region, tract of land\" but acquired the artistic connotation, \"a picture depicting scenery on land\" in the early 16th century.  After the fall of the Roman Empire, the tradition of depicting pure landscapes declined and the landscape was seen only as a setting for religious and figural scenes.  This tradition continued until the 16th century when artists began to view the landscape as a subject in its own right.  The Dutch Golden Age painting of the 17th century saw the dramatic growth of landscape painting, in which many artists specialized, and the development of extremely subtle realist techniques for depicting light and weather. The Flemish Renaissance painter Pieter Brueghel the Elder chose peasants and their activities as the subject of many paintings.  Genre painting flourished in Northern Europe in his wake.  Adriaen van Ostade, David Teniers, Aelbert Cuyp, Jan Steen, Johannes Vermeer and Pieter de Hooch were among many painters specializing in genre subjects in the Netherlands during the 17th century.  The generally small scale of these artists' paintings was appropriate for their display in the homes of middle class purchasers. Marine painting began in keeping with medieval Christian art tradition.  Such works portrayed the sea only from a bird's eye view, and everything, even the waves, was organized and symmetrical.  The viewpoint, symmetry and overall order of these early paintings underlined the organization of the heavenly cosmos from which the earth was viewed.  Later Dutch artists such as Hendrick Cornelisz Vroom, Cornelius Claesz, Abraham Storck, Jan Porcellis, Simon de Vlieger, Willem van de Velde the Elder, Willem van de Velde the Younger and Ludolf Bakhuizen developed new methods for painting, often from a horizontal point of view, with a lower horizon and more focus on realism than symmetry. The term vanitas is most often associated with still life paintings that were popular in seventeenth-century Dutch art, produced by the artists such as Pieter Claesz.  Common vanitas symbols included skulls (a reminder of the certainty of death); rotten fruit (decay); bubbles, (brevity of life and suddenness of death); smoke, watches, and hourglasses, (the brevity of life); and musical instruments (the brevity and ephemeral nature of life).  Fruit, flowers and butterflies can be interpreted in the same way, while a peeled lemon, as well as the typical accompanying seafood was, like life, visually attractive but with a bitter flavor. Group portraits were produced in great numbers during the Baroque period, particularly in the Netherlands.  Unlike in the rest of Europe, Dutch artists received no commissions from the Calvinist Church which had forbidden such images or from the aristocracy which was virtually non-existent.  Instead, commissions came from civic and businesses associations.  Dutch painter Frans Hals used fluid brush strokes of vivid color to enliven his group portraits, including those of the civil guard to which he belonged.  Rembrandt benefitted greatly from such commissions and from the general appreciation of art by bourgeois clients, who supported portraiture as well as still-life and landscape painting.  Notably, the world's first significant art and dealer markets flourished in Holland at that time. In the 17th century, Dutch painters (especially Frans Hals, Rembrandt, Jan Lievens and Johannes Vermeer) began to create uncommissioned paintings called \"tronies\" that focused on the features and/or expressions of people who were not intended to be identifiable.  They were conceived more for art's sake than to satisfy conventions.  The tronie was a distinctive type of painting, combining elements of the portrait, history, and genre painting.  This was usually a half-length of a single figure which concentrated on capturing an unusual mood or expression.  The actual identity of the model was not supposed to be important, but they might represent a historical figure and be in exotic or historic costume.  In contrast to portraits, \"tronies\" were painted for the open market.  They differ from figurative paintings and religious figures in that they are not restricted to a moral or narrative context.  It is, rather, much more an exploration of the spectrum of human physiognomy and expression and the reflection of conceptions of character that are intrinsic to psychology’s pre-history. Rembrandt lighting is a lighting technique that is used in studio portrait photography. It can be achieved using one light and a reflector, or two lights, and is popular because it is capable of producing images which appear both natural and compelling with a minimum of equipment.  Rembrandt lighting is characterized by an illuminated triangle under the eye of the subject, on the less illuminated side of the face.  It is named for the Dutch painter Rembrandt, who often used this type of lighting in his portrait paintings. The first known mezzotint was done in Amsterdam in 1642 by Utrecht-born German artist Ludwig von Siegen.  He lived in Amsterdam from 1641 to about 1644, when he was supposedly influenced by Rembrandt. The painter and printmaker Jan van de Velde is often credited to be the inventor of the aquatint technique, in Amsterdam around 1650. Pronkstilleven (\"pronk still life\" or \"ostentatious still life\") is a type of banquet piece whose distinguishing feature is a quality of ostentation and splendor.  These still lifes usually depict one or more especially precious objects.  Although the term is a post-17th century invention, this type is characteristic of the second half of the seventeenth century.  It was developed in the 1640s in Antwerp from where it spread quickly to the Dutch Republic.  Flemish artists such as Frans Snyders and Adriaen van Utrecht started to paint still lifes that emphasized abundance by depicting a diversity of objects, fruits, flowers and dead game, often together with living people and animals.  The style was soon adopted by artists from the Dutch Republic.  A leading Dutch representative was Jan Davidsz.  de Heem, who spent a long period of his active career in Antwerp and was one of the founders of the style in Holland. Vincent van Gogh's work is most often associated with Post-Impressionism, but his innovative style had a vast influence on 20th-century art and established what would later be known as Expressionism, also greatly influencing fauvism and early abstractionism.  His impact on German and Austrian Expressionists was especially profound.  \"Van Gogh was father to us all,\" the German Expressionist painter Max Pechstein proclaimed in 1901, when Van Gogh's vibrant oils were first shown in Germany and triggered the artistic reformation, a decade after his suicide in obscurity in France.  In his final letter to Theo, Van Gogh stated that, as he had no children, he viewed his paintings as his progeny.  Reflecting on this, the British art historian Simon Schama concluded that he \"did have a child of course, Expressionism, and many, many heirs.\" Dutch graphic artist Maurits Cornelis Escher, usually referred to as M. C. Escher, is known for his often mathematically inspired woodcuts, lithographs, and mezzotints.  These feature impossible constructions, explorations of infinity, architecture and tessellations. His special way of thinking and rich graphic work has had a continuous influence in science and art, as well as permeating popular culture.  His ideas have been used in fields as diverse as psychology, philosophy, logic, crystallography and topology.  His art is based on mathematical principles like tessellations, spherical geometry, the Möbius strip, unusual perspectives, visual paradoxes and illusions, different kinds of symmetries and impossible objects. \" Gödel, Escher, Bach\" by Douglas Hofstadter discusses the ideas of self-reference and strange loops, drawing on a wide range of artistic and scientific work, including Escher's art and the music of J. S. Bach, to illustrate ideas behind Gödel's incompleteness theorems. Miffy (Nijntje) is a small female rabbit in a series of picture books drawn and written by Dutch artist Dick Bruna. In music, the Franco-Flemish School or more precisely the Netherlandish School refers to the style of polyphonic vocal music composition in the Burgundian Netherlands in the 15th and early 16th centuries, and to the composers who wrote it. The Venetian School of polychoral music was founded by the Netherlandish composer Adrian Willaert. Hardcore or hardcore techno is a subgenre of electronic dance music originating in Europe from the emergent raves in the 1990s.  It was initially designed at Rotterdam in Netherlands, derived from techno. Hardstyle is an electronic dance genre mixing influences from hardtechno and hardcore.  Hardstyle was influenced by gabber.  Hardstyle has its origins in the Netherlands where artists like DJ Zany, Lady Dana, DJ Isaac, DJ Pavo, DJ Luna and The Prophet, who produced hardcore, started experimenting while playing their hardcore records. Holsteins or Holstein-Friesians are a breed of cattle known today as the world's highest-production dairy animals.  Originating in Europe, Holstein-Friesians were bred in the two northern provinces of North Holland and Friesland, and Schleswig-Holstein in what became Germany.  The animals were the regional cattle of the Frisians and the Saxons.  The origins of the breed can be traced to the black cows and white cows of the Batavians and Frisians – migrant tribes who settled the coastal Rhine region more than two thousand years ago. Forerunners to modern Brussels sprouts were likely cultivated in ancient Rome.  Brussels sprouts as we now know them were grown possibly as early as the 13th century in the Low Countries (may have originated in Brussels).  The first written reference dates to 1587.  During the 16th century, they enjoyed a popularity in the Southern Netherlands that eventually spread throughout the cooler parts of Northern Europe. Through history, carrots weren’t always orange.  They were black, purple, white, brown, red and yellow.  Probably orange too, but this was not the dominant colour.  Orange-coloured carrots appeared in the Netherlands in the 16th century.  Dutch farmers in Hoorn bred the color.  They succeeded by cross-breeding pale yellow with red carrots.  It is more likely that Dutch horticulturists actually found an orange rooted mutant variety and then worked on its development through selective breeding to make the plant consistent.  Through successive hybridisation the orange colour intensified.  This was developed to become the dominant species across the world, a sweet orange. Belle de Boskoop is an apple cultivar which, as its name suggests, originated in Boskoop, where it began as a chance seedling in 1856.  There are many variants: Boskoop red, yellow or green.  This rustic apple is firm, tart and fragrant.  Greenish-gray tinged with red, the apple stands up well to cooking.  Generally Boskoop varieties are very high in acid content and can contain more than four times the vitamin C of 'Granny Smith' or 'Golden Delicious'. Karmijn de Sonnaville is a variety of apple bred by Piet de Sonnaville, working in Wageningen in 1949.  It is a cross of Cox's Orange Pippin and Jonathan, and was first grown commercially beginning in 1971.  It is high both in sugars (including some sucrose) and acidity.  It is a triploid, and hence needs good pollination, and can be difficult to grow.  It also suffers from fruit russet, which can be severe.  In Manhart’s book, “apples for the 21st century”, Karmijn de Sonnaville is tipped as a possible success for the future.  Karmijn de Sonnaville is not widely grown in large quantities, but in Ireland, at The Apple Farm, 8 acre it is grown for fresh sale and juice-making, for which the variety is well suited. Elstar apple is an apple cultivar that was first developed in the Netherlands in the 1950s by crossing Golden Delicious and Ingrid Marie apples.  It quickly became popular, especially in Europe and was first introduced to America in 1972.  It remains popular in Continental Europe.  The Elstar is a medium-sized apple whose skin is mostly red with yellow showing.  The flesh is white, and has a soft, crispy texture.  It may be used for cooking and is especially good for making apple sauce.  In general, however, it is used in desserts due to its sweet flavour. The Groasis Waterboxx is a device designed to help grow trees in dry areas.  It was developed by former flower exporter Pieter Hoff, and won \"Popular Science's\" \"Green Tech Best of What's New\" Innovation of the year award for 2010. The Dutch-Frisian geographer Gemma Frisius was the first to propose the use of a chronometer to determine longitude in 1530.  In his book \"On the Principles of Astronomy and Cosmography\" (1530), Frisius explains for the first time how to use a very accurate clock to determine longitude.  The problem was that in Frisius’ day, no clock was sufficiently precise to use his method.  In 1761, the British clock-builder John Harrison constructed the first marine chronometer, which allowed the method developed by Frisius. Triangulation had first emerged as a map-making method in the mid sixteenth century when the Dutch-Frisian mathematician Gemma Frisius set out the idea in his \"Libellus de locorum describendorum ratione\" (\"Booklet concerning a way of describing places\").  Dutch cartographer Jacob van Deventer was among the first to make systematic use of triangulation, the technique whose theory was described by Gemma Frisius in his 1533 book. The modern systematic use of triangulation networks stems from the work of the Dutch mathematician Willebrord Snell (born Willebrord Snel van Royen), who in 1615 surveyed the distance from Alkmaar to Bergen op Zoom, approximately 70 miles (110 kilometres), using a chain of quadrangles containing 33 triangles in all – a feat celebrated in the title of his book \"Eratosthenes Batavus\" (\"The Dutch Eratosthenes\"), published in 1617. The Mercator projection is a cylindrical map projection presented by the Flemish geographer and cartographer Gerardus Mercator in 1569.  It became the standard map projection for nautical purposes because of its ability to represent lines of constant course, known as rhumb lines or loxodromes, as straight segments which conserve the angles with the meridians. Flemish geographer and cartographer Abraham Ortelius generally recognized as the creator of the world's first modern atlas, the \"Theatrum Orbis Terrarum\" (\"Theatre of the World\").  Ortelius's \"Theatrum Orbis Terrarum\" is considered the first true atlas in the modern sense: a collection of uniform map sheets and sustaining text bound to form a book for which copper printing plates were specifically engraved.  It is sometimes referred to as the summary of sixteenth-century cartography. The first printed atlas of nautical charts (\"De Spieghel der Zeevaerdt\" or \"The Mirror of Navigation\" / \"The Mariner's Mirror\") was produced by Lucas Janszoon Waghenaer in Leiden.  This atlas was the first attempt to systematically codify nautical maps.  This chart-book combined an atlas of nautical charts and sailing directions with instructions for navigation on the western and north-western coastal waters of Europe.  It was the first of its kind in the history of maritime cartography, and was an immediate success.  The English translation of Waghenaer's work was published in 1588 and became so popular that any volume of sea charts soon became known as a \"waggoner\", the Anglicized form of Waghenaer's surname. Gerardus Mercator was the first to coin the word \"atlas\" to describe a bound collection of maps through his own collection entitled \"Atlas sive Cosmographicae meditationes de fabrica mvndi et fabricati figvra\".  He coined this name after the Greek god who held the earth in his arms. The Dutch Republic's explorers and cartographers like Pieter Dirkszoon Keyser, Frederick de Houtman, Petrus Plancius and Jodocus Hondius were the pioneers in first systematic charting/mapping of largely unknown southern hemisphere skies in the late 16th century. The constellations around the South Pole were not observable from north of the equator, by Babylonians, Greeks, Chinese or Arabs.  The modern constellations in this region were defined during the Age of Exploration, notably by Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman at the end of sixteenth century.  These twelve Dutch-created represented flora and fauna of the East Indies and Madagascar.  They were depicted by Johann Bayer in his star atlas \"Uranometria\" of 1603.  Several more were created by Nicolas Louis de Lacaille in his star catalogue, published in 1756.  By the end of the Ming Dynasty, Xu Guangqi introduced 23 asterisms of the southern sky based on the knowledge of western star charts.  These asterisms have since been incorporated into the traditional Chinese star maps.  Among the IAU's 88 modern constellations, there are 15 Dutch-created constellations (including Apus, Camelopardalis, Chamaeleon, Columba, Dorado, Grus, Hydrus, Indus, Monoceros, Musca, Pavo, Phoenix, Triangulum Australe, Tucana and Volans). The speculation that continents might have 'drifted' was first put forward by Abraham Ortelius in 1596.  The concept was independently and more fully developed by Alfred Wegener in 1912.  Because Wegener's publications were widely available in German and English and because he adduced geological support for the idea, he is credited by most geologists as the first to recognize the possibility of continental drift.  During the 1960s geophysical and geological evidence for seafloor spreading at mid-oceanic ridges established continental drift as the standard theory or continental origin and an ongoing global mechanism. While making a coloured liquid for a thermometer, Cornelis Drebbel dropped a flask of Aqua regia on a tin window sill, and discovered that stannous chloride makes the color of carmine much brighter and more durable.  Though Drebbel himself never made much from his work, his daughters Anna and Catharina and his sons-in-law Abraham and Johannes Sibertus Kuffler set up a successful dye works.  One was set up in 1643 in Bow, London, and the resulting color was called bow dye. Dutch chemical company DSM invented and patented the Dyneema in 1979.  Dyneema fibres have been in commercial production since 1990 at their plant at Heerlen.  These fibers are manufactured by means of a gel-spinning process that combines extreme strength with incredible softness.  Dyneema fibres, based on ultra-high-molecular-weight polyethylene (UHMWPE), is used in many applications in markets such as life protection, shipping, fishing, offshore, sailing, medical and textiles. In 1962 Philips invented the compact audio cassette medium for audio storage, introducing it in Europe in August 1963 (at the Berlin Radio Show) and in the United States (under the \"Norelco\" brand) in November 1964, with the trademark name \"Compact Cassette\". Laserdisc technology, using a transparent disc, was invented by David Paul Gregg in 1958 (and patented in 1961 and 1990).  By 1969, Philips developed a videodisc in reflective mode, which has great advantages over the transparent mode.  MCA and Philips decided to join forces.  They first publicly demonstrated the videodisc in 1972.  Laserdisc entered the market in Atlanta, on 15 December 1978, two years after the VHS VCR and four years before the CD, which is based on Laserdisc technology.  Philips produced the players and MCA made the discs. The compact disc was jointly developed by Philips (Joop Sinjou) and Sony (Toshitada Doi).  In the early 1970s, Philips' researchers started experiments with \"audio-only\" optical discs, and at the end of the 1970s, Philips, Sony, and other companies presented prototypes of digital audio discs. Bluetooth, a low-energy, peer-to-peer wireless technology was originally developed by Dutch electrical engineer Jaap Haartsen and Swedish engineer Sven Mattisson in the 1990s, working at Ericsson in Lund, Sweden.  It became a global standard of short distance wireless connection. In 1991, NCR Corporation/AT&T Corporation invented the precursor to 802.11 in Nieuwegein.  Dutch electrical engineer Vic Hayes chaired IEEE 802.11 committee for 10 years, which was set up in 1990 to establish a wireless networking standard.  He has been called the father of Wi-Fi (the brand name for products using IEEE 802.11 standards) for his work on IEEE 802.11 (802.11a & 802.11b) standard in 1997. The DVD optical disc storage format was invented and developed by Philips and Sony in 1995. Ambilight, short for \"ambient lighting\", is a lighting system for televisions developed by Philips in 2002. Philips and Sony in 1997 and 2006 respectively, launched the Blu-ray video recording/playback standard. Dijkstra's algorithm, conceived by Dutch computer scientist Edsger Dijkstra in 1956 and published in 1959, is a graph search algorithm that solves the single-source shortest path problem for a graph with non-negative edge path costs, producing a shortest path tree.  Dijkstra's algorithm is so powerful that it not only finds the shortest path from a chosen source to a given destination, it finds all of the shortest paths from the source to all destinations.  This algorithm is often used in routing and as a subroutine in other graph algorithms. Dijkstra's algorithm is considered as one of the most popular algorithms in computer science.  It is also widely used in the fields of artificial intelligence, operational research/operations research, network routing, network analysis, and transportation engineering. Through his fundamental contributions Edsger Dijkstra helped shape the field of computer science.  His groundbreaking contributions ranged from the engineering side of computer science to the theoretical one and covered several areas including compiler construction, operating systems, distributed systems, sequential and concurrent programming, software engineering, and graph algorithms.  Many of his papers, often just a few pages long, are the source of whole new research areas.  Several concepts that are now completely standard in computer science were first identified by Dijkstra and/or bear names coined by him. Edsger Dijkstra's foundational work on concurrency, semaphores, mutual exclusion, deadlock, finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built.  The Edsger W. Dijkstra Prize in Distributed Computing (sponsored jointly by the ACM Symposium on Principles of Distributed Computing and the EATCS International Symposium on Distributed Computing) is given for outstanding papers on the principles of distributed computing, whose significance and impact on the theory and/or practice of distributed computing has been evident for at least a decade. The academic study of concurrent programming (concurrent algorithms in particular) started in the 1960s, with Edsger Dijkstra (1965) credited with being the first paper in this field, identifying and solving mutual exclusion.  A pioneer in the field of concurrent computing, Per Brinch Hansen considers Dijkstra's \"Cooperating Sequential Processes\" (1965) to be the first classic paper in concurrent programming.  As Brinch Hansen notes: ‘Here Dijkstra lays the conceptual foundation for abstract concurrent programming.’ Computer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike physics there were no theoretical concepts or coding systems.  Dijkstra was one of the moving forces behind the acceptance of computer programming as a scientific discipline.  In 1968, computer programming was in a state of crisis.  Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs.  Dijkstra coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy.  As Bertrand Meyer remarked: \"The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction.  Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming.\" Dijkstra's ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects. In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation.  It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST).  The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\" algorithm because its operation resembles that of a railroad shunting yard.  Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report. In 1963/64, during an extended stay at SLAC, Dutch theoretical physicist Martinus Veltman designed the computer program \"Schoonschip\" for symbolic manipulation of mathematical equations, which is now considered the very first computer algebra system. In computer science, mutual exclusion refers to the requirement of ensuring that no two concurrent processes are in their critical section at the same time; it is a basic requirement in concurrency control, to prevent race conditions.  The requirement of mutual exclusion was first identified and solved by Edsger W. Dijkstra in his seminal 1965 paper titled \"Solution of a problem in concurrent programming control\", and is credited as the first topic in the study of concurrent algorithms. The semaphore concept was invented by Dijkstra in 1965 and the concept has found widespread use in a variety of operating systems. In computer science, the sleeping barber problem is a classic inter-process communication and synchronization problem between multiple operating system processes.  The problem is analogous to that of keeping a barber working when there are customers, resting when there are none and doing so in an orderly manner.  The Sleeping Barber Problem was introduced by Edsger Dijkstra in 1965. The Banker's algorithm is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an \"s-state\" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.  The algorithm was developed in the design process for the THE multiprogramming system and originally described (in Dutch) in EWD108.  The name is by analogy with the way that bankers account for liquidity constraints. In computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them.  It was originally formulated in 1965 by Edsger Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals. Soon after, Tony Hoare gave the problem its present formulation. Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming.  Dijkstra attributed the solution to Dutch mathematician Theodorus Dekker in his manuscript on cooperating sequential processes.  It allows two threads to share a single-use resource without conflict, using only shared memory for communication.  Dekker's algorithm is the first published software-only, two-process mutual exclusion algorithm. The THE multiprogramming system was a computer operating system designed by a team led by Edsger W. Dijkstra, described in monographs in 1965–66 and published in 1968. Van Wijngaarden grammar (also vW-grammar or W-grammar) is a two-level grammar that provides a technique to define potentially infinite context-free grammars in a finite number of rules.  The formalism was invented by Adriaan van Wijngaarden to rigorously define some syntactic restrictions that previously had to be formulated in natural language, despite their formal content.  Typical applications are the treatment of gender and number in natural language syntax and the well-definedness of identifiers in programming languages.  The technique was used and developed in the definition of the programming language ALGOL 68.  It is an example of the larger class of affix grammars. In 1968, computer programming was in a state of crisis.  Dijkstra was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs.  Dijkstra coined the phrase \"structured programming\" and during the 1970s this became the new programming orthodoxy.  Structured programming is often regarded as “goto-less programming”.  But as Bertrand Meyer notes, “As the first book on the topic [\"Structured Programming\" by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto.  Its principal message is that programming should be considered a scientific discipline based on mathematical rigor.”  , structured programming – especially in the 1970s and 1980s – significantly influenced the birth of many modern programming languages such as Pascal, C, Modula-2, and Ada.  The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978.  The C++ language was a considerably extended and enhanced version of the popular C (see also: list of C-based programming languages).  Since C++ was developed from a more traditional , it is a 'hybrid language', rather than a pure object-oriented programming language. An \"EPROM\" or \"erasable programmable read only memory\", is a type of memory chip that retains its data when its power supply is switched off.  Development of the EPROM memory cell started with investigation of faulty integrated circuits where the gate connections of transistors had broken.  Stored charge on these isolated gates changed their properties.  The EPROM was invented by the Amsterdam-born Israeli electrical engineer Dov Frohman in 1971, who was awarded US patent 3660819 in 1972. Self-stabilization is a concept of fault-tolerance in distributed computing.  A distributed system that is self-stabilizing will end up in a correct state no matter what state it is initialized with.  That correct state is reached after a finite number of execution steps.  Many years after the seminal paper of Edsger Dijkstra in 1974, this concept remains important as it presents an important foundation for self-managing computer systems and fault-tolerant systems. Self-stabilization became its own area of study in distributed systems research, and Dijkstra set the stage for the next generation of computer scientists such as Leslie Lamport, Nancy Lynch, and Shlomi Dolev.  As a result, Dijkstra's paper received the 2002 ACM PODC Influential-Paper Award (later renamed as \"Dijkstra Prize\" or \"Edsger W. Dijkstra Prize in Distributed Computing\" since 2003). Predicate transformer semantics were introduced by Dijkstra in his seminal paper \"Guarded commands, nondeterminacy and formal derivation of programs\". The Guarded Command Language (GCL) is a language defined by Edsger Dijkstra for predicate transformer semantics.  It combines programming concepts in a compact way, before the program is written in some practical programming language. A \"Van Emde Boas tree\" (or \"Van Emde Boas priority queue\", also known as a \"vEB tree\", is a tree data structure which implements an associative array with \"m\"-bit integer keys.  The vEB tree was invented by a team led by Dutch computer scientist Peter van Emde Boas in 1975. ABC is an imperative general-purpose programming language and programming environment developed at CWI, Netherlands by Leo Geurts, Lambert Meertens, and Steven Pemberton.  It is interactive, structured, high-level, and intended to be used instead of BASIC, Pascal, or AWK.  It is not meant to be a systems-programming language but is intended for teaching or prototyping. The language had a major influence on the design of the Python programming language (as a counterexample); Guido van Rossum, who developed Python, previously worked for several years on the ABC system in the early 1980s. The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system.  The algorithm was proposed by Dijkstra and Scholten in 1980. Smoothsort is a comparison-based sorting algorithm.  It is a variation of heapsort developed by Edsger Dijkstra in 1981.  Like heapsort, smoothsort's upper bound is O(\"n\" log \"n\").  The advantage of smoothsort is that it comes closer to O(\"n\") time if the input is already sorted to some degree, whereas heapsort averages O(\"n\" log \"n\") regardless of the initial sorted state. The Amsterdam Compiler Kit (ACK) is a fast, lightweight and retargetable compiler suite and toolchain developed by Andrew Tanenbaum and Ceriel Jacobs at the Vrije Universiteit in Amsterdam.  It is MINIX's native toolchain.  The ACK was originally closed-source software (that allowed binaries to be distributed for MINIX as a special case), but in April 2003 it was released under an open source BSD license.  It has frontends for programming languages C, Pascal, Modula-2, Occam, and BASIC.  The ACK's notability stems from the fact that in the early 1980s it was one of the first portable compilation systems designed to support multiple source languages and target platforms. EFM (Eight-to-Fourteen Modulation) was invented by Dutch electrical engineer Kees A. Schouhamer Immink in 1985.  EFM is a data encoding technique – formally, a channel code – used by CDs, laserdiscs and pre-Hi-MD MiniDiscs. MINIX (from \"mini-Unix\") is a Unix-like computer operating system based on a microkernel architecture.  Early versions of MINIX were created by Andrew S. Tanenbaum for educational purposes.  Starting with MINIX 3, the primary aim of development shifted from education to the creation of a highly reliable and self-healing microkernel OS.  MINIX is now developed as open-source software.  MINIX was first released in 1987, with its complete source code made available to universities for study in courses and research.  It has been free and open source software since it was re-licensed under the BSD license in April 2000.  Tanenbaum created MINIX at the Vrije Universiteit in Amsterdam to exemplify the principles conveyed in his textbook, \"\" (1987), that Linus Torvalds described as \"the book that launched me to new heights\". Amoeba is a distributed operating system developed by Andrew S. Tanenbaum and others at the Vrije Universiteit in Amsterdam.  The aim of the Amoeba project was to build a timesharing system that makes an entire network of computers appear to the user as a single machine.  The Python programming language was originally developed for this platform. Python is a widely used general-purpose, high-level programming language.  Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than would be possible in languages such as C++ or Java.  The language provides constructs intended to enable clear programs on both a small and large scale.  Python supports multiple programming paradigms, including object-oriented, imperative and functional programming or procedural styles.  It features a dynamic type system and automatic memory management and has a large and comprehensive standard library. Python was conceived in the late 1980s and its implementation was started in December 1989 by Guido van Rossum at CWI in the Netherlands as a successor to the ABC language (itself inspired by SETL) capable of exception handling and interfacing with the Amoeba operating system.  Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, \"benevolent dictator for life\" (BDFL). Vim is a text editor written by the Dutch free software programmer Bram Moolenaar and first released publicly in 1991.  Based on the Vi editor common to Unix-like systems, Vim carefully separated the user interface from editing functions.  This allowed it to be used both from a command line interface and as a standalone application in a graphical user interface. Blender is a professional free and open-source 3D computer graphics software product used for creating animated films, visual effects, art, 3D printed models, interactive 3D applications and video games.  Blender's features include 3D modeling, UV unwrapping, texturing, raster graphics editing, rigging and skinning, fluid and smoke simulation, particle simulation, soft body simulation, sculpting, animating, match moving, camera tracking, rendering, video editing and compositing.  Alongside the modelling features it also has an integrated game engine.  Blender has been successfully used in the media industry in several parts of the world including Argentina, Australia, Belgium, Brazil, Russia, Sweden, and the United States. The Dutch animation studio Neo Geo and Not a Number Technologies (NaN) developed Blender as an in-house application, with the primary author being Ton Roosendaal.  The name \"Blender\" was inspired by a song by Yello, from the album \"Baby\". EFMPlus is the channel code used in DVDs and SACDs, a more efficient successor to EFM used in CDs.  It was created by Dutch electrical engineer Kees A. Schouhamer Immink, who also designed EFM.  It is 6% less efficient than Toshiba's SD code, which resulted in a capacity of 4.7 gigabytes instead of SD's original 5 GB.  The advantage of EFMPlus is its superior resilience against disc damage such as scratches and fingerprints. The Dutch East India Company (Verenigde Oostindische Compagnie, or VOC), founded in 1602, was the world’s first multinational, joint-stock, limited liability corporation – as well as its first government-backed trading cartel.  It was the first company to issue shares of stock and what evolved into corporate bonds.  The VOC was also the first company to actually issue stocks and bonds through a stock exchange.  In 1602, the VOC issued shares that were made tradable on the Amsterdam Stock Exchange.  This invention enhanced the ability of joint-stock companies to attract capital from investors as they could now easily dispose their shares.  The company was known throughout the world as the VOC thanks to its logo featuring those initials, which became the first global corporate brand.  The company's monogram also became the first global logo. The Dutch East India Company was arguably the first megacorporation, possessing quasi-governmental powers, including the ability to wage war, imprison and execute convicts, negotiate treaties, coin money and establish colonies.  Many economic and political historians consider the Dutch East India Company as the most valuable, powerful and influential corporation in the world history. The VOC existed for almost 200 years from its founding in 1602, when the States-General of the Netherlands granted it a 21-year monopoly over Dutch operations in Asia until its demise in 1796.  During those two centuries (between 1602 and 1796), the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods.  By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British) East India Company, the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC.  The VOC enjoyed huge profits from its spice monopoly through most of the 17th century. A Dutch auction is also known as an \"open descending price auction\".  Named after the famous auctions of Dutch tulip bulbs in the 17th century, it is based on a pricing system devised by Nobel Prize–winning economist William Vickrey.  In the traditional Dutch auction, the auctioneer begins with a high asking price which is lowered until some participant is willing to accept the auctioneer's price.  The winning participant pays the last announced price.  Dutch auction is also sometimes used to describe online auctions where several identical goods are sold simultaneously to an equal number of high bidders.  In addition to cut flower sales in the Netherlands, Dutch auctions have also been used for perishable commodities such as fish and tobacco. The Dutch Republic was the birthplace of the first modern art market (\"open art market\" or \"free art market\").  The seventeenth-century Dutch were the pioneering arts marketers, successfully combining art and commerce together as we would recognise it today.  Until the 17th century, commissioning works of art was largely the preserve of the church, monarchs and aristocrats.  The emergence of a powerful and wealthy middle class in Holland, though, produced a radical change in patronage as the new Dutch bourgeoisie bought art.  For the first time, the direction of art was shaped by relatively broadly-based demand rather than religious dogma or royal whim, and the result was a market which today's dealers and collectors would find familiar.  With the creation of the first large-scale open art market, prosperous Dutch merchants, artisans, and civil servants bought paintings and prints in unprecedented numbers.  Foreign visitors were astonished that even modest members of Dutch society such as farmers and bakers owned multiple works of art. The seventeenth-century Dutch businessmen were the pioneers in laying the basis for modern corporate governance.  Isaac Le Maire, an Amsterdam businessman and a sizeable shareholder of the VOC, became the first recorded investor to actually consider the corporate governance's problems.  In 1609, he complained of the VOC's shoddy corporate governance.  On January 24, 1609, Le Maire filed a petition against the VOC, marking the first recorded expression of shareholder activism.  In what is the first recorded corporate governance dispute, Le Maire formally charged that the directors (the VOC's board of directors – the Heeren XVII) sought to “retain another’s money for longer or use it ways other than the latter wishes” and petitioned for the liquidation of the VOC in accordance with standard business practice. The first shareholder revolt happened in 1622, among Dutch East India Company (VOC) investors who complained that the company account books had been “smeared with bacon” so that they might be “eaten by dogs.”  The investors demanded a “reeckeninge,” a proper financial audit.  The 1622 campaign by the shareholders of the VOC is a testimony of genesis of CSR (Corporate Social Responsibility) in which shareholders staged protests by distributing pamphlets and complaining about management self enrichment and secrecy. The construction in 1619 of a train-oil factory on Smeerenburg in the Spitsbergen islands by the Noordsche Compagnie, and the acquisition in 1626 of Manhattan Island by the Dutch West India Company are referred to as the earliest cases of outward foreign direct investment (FDI) in Dutch and world history.  Throughout the seventeenth century, the Dutch East India Company (VOC) and the Dutch West India Company (GWIC/WIC) also began to create trading settlements around the globe.  Their trading activities generated enormous wealth, making the Dutch Republic one of the most prosperous countries of that time.  The Dutch Republic's extensive arms trade occasioned an episode in the industrial development of early-modern Sweden, where arms merchants like Louis de Geer and the Trip brothers, invested in iron mines and iron works, another early example of outward foreign direct investment. It was in the Dutch Republic that some important industries (economic sectors) such as shipbuilding, shipping, printing and publishing were developed on a large-scale export-driven model for the first time in history.  The ship building district of Zaan, near Amsterdam, became the first industrialized area in the world, with around 900 industrial windmills at the end of the 17th century, but there were industrialized towns and cities on a smaller scale also.  Other industries that saw significant growth were papermaking, sugar refining, printing, the linen industry (with spin-offs in vegetable oils, like flax and rape oil), and industries that used the cheap peat fuel, like brewing and ceramics (brickworks, pottery and clay-pipe making). The Dutch shipbuilding industry was of modern dimensions, inclining strongly toward standardised, repetitive methods.  It was highly mechanized and used many labor-saving devices-wind-powered sawmills, powered feeders for saw, block and tackles, great cranes to move heavy timbers-all of which increased productivity.  Dutch shipbuilding benefited from various design innovations which increased carrying capacity and cut costs. Economic historians consider the Netherlands as the first predominantly capitalist nation.  The development of European capitalism began among the city-states of Italy, Flanders, and the Baltic.  It spread to the European interstate system, eventually resulting in the world's first capitalist nation-state, the Dutch Republic of the seventeenth century.  The Dutch were the first to develop capitalism on a nationwide scale (as opposed to earlier city states).  They also played a pioneering role in the emergence of the capitalist world-system.  Simon Schama aptly titled his work \"The Embarrassment of Riches\", capturing the astonishing novelty and success of the commercial revolution in the Dutch Republic. World-systems theorists (including Immanuel Wallerstein and Giovanni Arrighi) often consider the economic primacy of the Dutch Republic in the 17th century as the first capitalist hegemony in world history (followed by hegemonies of the United Kingdom in the 19th century and the United States in the 20th century). The Dutch economic transition from a possession of the Holy Roman Empire in the 1590s to the foremost maritime and economic power in the world has been called the “Dutch Miracle” (or “Dutch Tiger”) by many economic historians, including K. W. Swart.  Until the 18th century, the economy of the Dutch Republic was the most advanced and sophisticated ever seen in history. During their Golden Age, the provinces of the Northern Netherlands rose from almost total obscurity as the poor cousins of the industrious and heavily urbanised southern regions (Southern Netherlands) to become the world leader in economic success. The Netherlands introduced many financial innovations that made it a major economic force – and Amsterdam became the world center for international finance.  Its manufacturing towns grew so quickly that by the middle of the century the Netherlands had supplanted France as the leading industrial nation of the world.” Dutch economist Jan Tinbergen developed the first national comprehensive macroeconomic model, which he first built for the Netherlands and after World War II later applied to the United States and the United Kingdom. The concept of \"fair trade\" has been around for over 40 years, but a formal labelling scheme emerged only in the 1980s.  At the initiative of Mexican coffee farmers, the world's first Fairtrade labeling organisation, Stichting Max Havelaar, was launched in the Netherlands on 15 November 1988 by Nico Roozen, Frans van der Hoff and Dutch ecumenical development agency Solidaridad.  It was branded \"Max Havelaar\" after a fictional Dutch character who opposed the exploitation of coffee pickers in Dutch colonies. An \"exchange\", or \"bourse\", is a highly organized market where (especially) tradable securities, commodities, foreign exchange, futures, and options contracts are sold and bought.  The term \"bourse\" is derived from the 13th-century inn named \"Huis ter Beurze\" in Bruges, Low Countries, where traders and foreign merchants from across Europe conducted business in the late medieval period.  The building, which was established by Robert van der Buerze as a hostelry, had operated from 1285.  Its managers became famous for offering judicious financial advice to the traders and merchants who frequented the building.  This service became known as the \"Beurze Purse\" which is the basis of \"bourse\", meaning an organised place of exchange. The seventeenth-century Dutch merchants laid the foundations for modern stock market that now influences greatly the global economy.  It was in the Dutch Republic that a fully-fledged stock market was established and developed for the first time in history.  The Dutch merchants were also the pioneers in developing the basic techniques of stock trading.  Although bond sales by municipalities and states can be traced to the thirteenth century, the origin of modern stock exchanges that specialize in creating and sustaining secondary markets in corporate securities goes back to the formation of the Dutch East India Company in the year 1602.  Dutch investors were the first to trade their shares at a regular stock exchange.  The Amsterdam Stock Exchange is considered the oldest in the world.  It was established in 1602 by the Dutch East India Company for dealings in its printed stocks and bonds.  Here, the Dutch also pioneered stock futures, stock options, short selling, debt-equity swaps, merchant banking, bonds, unit trusts and other speculative instruments.  Unlike the competing companies, the VOC allowed anyone (including housemaids) to purchase stock in the trading at the fully operational Amsterdam Bourse.  The practice of naked short selling was also invented in the Dutch Republic.  In 1609, Isaac Le Maire, an Amsterdam merchant and a sizeable shareholder of the Dutch East India Company (VOC), became the first recorded short seller in history.  The first recorded ban on short selling also took place in the Dutch Republic in the same year.  In the early 17th century, Dutch merchants invented the common stock – that of the VOC.  Also, the Dutch experienced the first recorded stock market crash in history, the Tulip Mania of 1636–1637.  Since 1602, stock market trading has come a long way.  But basically, the concept and principle of stock market trading is still upheld and is still being implemented up to now. The Dutch Republic (Amsterdam in particular) was the birthplace of the world's first fully functioning financial market, with the birth of a fully fledged capital market.  Capital markets for debt and equity shares are used to raise long-term funds.  New stocks and bonds are sold in primary markets (including initial public offerings) and secondary markets (including stock exchanges).  While the Italian city-states produced the first transferable municipal bonds, they didn't develop the other ingredient necessary to produce a fully fledged capital market: corporate shareholders.  The Dutch East India Company (VOC) became the first company to offer shares of stock to the general public.  Dutch investors were the first to trade their shares at a regular stock exchange.  In 1602 the Dutch East India Company (VOC) established an exchange in Amsterdam where the VOC stocks and bonds could be traded in a secondary market.  The buying and selling of the VOC's securities (including shares and bonds) became the basis of the first official stock market.  The Dutch were also the first to use a fully-fledged capital market (including bond market and stock market) to finance companies (such as the VOC and the WIC).  It was in seventeenth-century Amsterdam that the global securities market began to take on its modern form. What is now known as corporate finance has its modern roots in financial management policies of the Dutch East India Company (VOC) in the 17th century and some basic aspects of modern corporate finance began to appear in financial activities of Dutch businessmen in the early 17th century. The earliest form of a company which issued \"public shares\" was the \"publicani\" during the Roman Republic.  In 1602, the Dutch East India Company (Vereenigde Oost-Indische Compagnie or VOC) became the first modern company to issue shares to the public, thus launching the first modern initial public offering (IPO).  The VOC held the first public offering of shares in history shortly after its founding.  With this first recorded initial public offering (IPO), the VOC brought in 6,424,588 guilders and the company subsequently grew to become the first true transnational corporation in the world. The Dutch were the pioneers in laying the basis for investment banking, allowing the risk of loans to be distributed among thousands of investors in the early seventeenth century. Prior to the 17th century most money was commodity money, typically gold or silver.  However, promises to pay were widely circulated and accepted as value at least five hundred years earlier in both Europe and Asia.  The Song Dynasty was the first to issue generally circulating paper currency, while the Yuan Dynasty was the first to use notes as the predominant circulating medium.  In 1455, in an effort to control inflation, the succeeding Ming Dynasty ended the use of paper money and closed much of Chinese trade.  The medieval European Knights Templar ran an early prototype of a central banking system, as their promises to pay were widely respected, and many regard their activities as having laid the basis for the modern banking system.  As the first public bank to \"offer accounts not directly convertible to coin\", the Bank of Amsterdam (\"Amsterdamsche Wisselbank\" or literally Amsterdam Exchange Bank) established in 1609 is considered to be the precursor to modern central banks, if not the first true central bank.  The Wisselbank's innovations helped lay the foundations for the birth and development of modern central banking systems.  There were earlier banks, especially in the Italian city-states, but the Wisselbank, with its public backing, provided for a scale of operations and stability hitherto unmatched.  Along with a number of subsidiary local banks, it performed many of modern-day central banking functions.  The model of the Wisselbank as a state bank was adapted throughout Europe, including the Bank of Sweden (1668) and the Bank of England (1694).  It occupied a central position in the financial world of its day, providing an effective, efficient and trusted system for national and international payments.  The establishment of the Wisselbank led to the introduction of the concept of bank money – the bank guilder.  Lucien Gillard (2004) calls it the \"European guilder\" (\"le florin européen\"), and Adam Smith devotes many pages to explaining how the bank guilder works (Smith 1776: 446–55).  Considered by many experts to be the first internationally dominant reserve currency of modern times, the Dutch guilder was the dominant currency during the 17th and 18th centuries.  It was just replaced by British pound sterling in the 19th century and the US dollar took the lead just after World War Two and has held it until this day. Financial innovation in Amsterdam took many forms.  In 1609, investors led by Isaac Le Maire formed history's first bear syndicate to engage in short selling, but their coordinated trading had only a modest impact in driving down share prices, which tended to be robust throughout the 17th century. In the first decades of the 17th century, the VOC was the first recorded company ever to pay regular dividends. To encourage investors to buy shares, a promise of an annual payment (called a dividend) was made.  An investor would receive dividends instead interest and the investment was permanent in the form of shares in the company.  Between 1600 and 1800 the Dutch East India Company (VOC) paid annual dividends worth around 18 percent of the value of the shares. In 1656, King Charles X Gustav of Sweden signed two charters creating two private banks under the directorship of Johan Palmstruch (though before having been ennobled he was called Johan Wittmacher or Hans Wittmacher), a Riga-born merchant of Dutch origin.  Palmstruch modeled the banks on those of Amsterdam where he had become a burgher.  The first real European banknote was issued in 1661 by the Stockholms Banco of Johan Palmstruch, a private bank under state charter (precursor to the Sveriges Riksbank, the central bank of Sweden). Joseph de la Vega, also known as Joseph Penso de la Vega, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman.  His 1688 book \"Confusion de Confusiones\" (\"Confusion of Confusions\") explained the workings of the city's stock market.  It was the earliest book about stock trading, taking the form of a dialogue between a merchant, a shareholder and a philosopher.  The book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.  The book has been described as the first precursor of modern behavioural finance, with its descriptions of investor decision-making still reflected in the way some investors operate today, and in 2001 was still rated by the \"Financial Times\" as one of the ten best investment book ever written. The principles of technical analysis are derived from hundreds of years of financial market data.  These principles in a raw form have been studied since the seventeenth century.  Some aspects of technical analysis began to appear in Joseph de la Vega's accounts of the Dutch markets in the late 17th century.  In Asia, technical analysis is said to be a method developed by Homma Munehisa during the early 18th century which evolved into the use of candlestick techniques, and is today a technical analysis charting tool. Josseph de la Vega was in 1688 the first person to give an account of irrational behaviour in financial markets. His 1688 book \"Confusion of Confusions\", has been described as the first precursor of modern behavioural finance, with its descriptions of investor decision-making still reflected in the way some investors operate today. By the first decades of the 18th century, Amsterdam had become the world’s leading financial centre for more than a century, having developed a sophisticated financial system with central banking, fully-fledged capital markets, certain kinds of financial derivatives, and publicly traded multinational corporations.  Amsterdam was the first modern model of an international (global) financial centre that now operated in several countries around the world. In the early 17th century, the Dutch revolutionized domestic and international finance by inventing common stock – that of the Dutch East India Company and founding a proto-central bank, the Wisselbank or Bank of Amsterdam.  In 1609, the Dutch had already had a government bond market for some decades.  Shortly thereafter, the Dutch Republic had in place, in one form or another, all of the key components of a modern financial system: formalized public credit, stable money, elements of a banking system, a central bank of sorts and securities markets.  The Dutch Republic went on to become that century's leading economy. The first investment fund has its roots back in 1774.  A Dutch merchant named Adriaan van Ketwich formed a trust named \"Eendragt Maakt Magt\".  The name of Ketwich's fund translates to \"unity creates strength\".  In response to the financial crisis of 1772–1773, Ketwich’s aim was to provide small investors an opportunity to diversify (Rouwenhorst & Goetzman, 2005).  This investment scheme can be seen as the first near-mutual fund.  In the years following, near-mutual funds evolved and become more diverse and complex. The first mutual funds were established in 1774 in the Netherlands.  Amsterdam-based businessman Abraham van Ketwich (a.k.a. Adriaan van Ketwich) is often credited as the originator of the world's first mutual fund.  The first mutual fund outside the Netherlands was the Foreign & Colonial Government Trust, which was established in London in 1868. Gibbing is the process of preparing salt herring (or soused herring), in which the gills and part of the gullet are removed from the fish, eliminating any bitter taste.  The liver and pancreas are left in the fish during the salt-curing process because they release enzymes essential for flavor.  The fish is then cured in a barrel with one part salt to 20 herring.  Today many variations and local preferences exist on this process.  The process of gibbing was invented by Willem Beuckelszoon (aka Willem Beuckelsz, William Buckels or William Buckelsson), a 14th-century Zealand Fisherman.  The invention of this fish preservation technique led to the Dutch becoming a seafaring power.  This invention created an export industry for salt herring that was monopolized by the Dutch. Many people believe it was the Dutch who invented doughnuts.  A Dutch snack made from potatoes had a round shape like a ball, but, like Gregory's dough balls, needed a little longer time when fried to cook the inside thoroughly.  These potato-balls developed into doughnuts when the Dutch finally made them into ring-shapes reduce frying time. Gin is a spirit which derives its predominant flavour from juniper berries (\"Juniperus communis\").  From its earliest origins in the Middle Ages, gin has evolved over the course of a millennium from a herbal medicine to an object of commerce in the spirits industry.  Gin was developed on the basis of the older Jenever, and become widely popular in Great Britain when William III of Orange, leader of the Dutch Republic, occupied the British throne with his wife Mary.  Today, the gin category is one of the most popular and widely distributed range of spirits, and is represented by products of various origins, styles, and flavour profiles that all revolve around juniper as a common ingredient. The Dutch physician Franciscus Sylvius is often credited with the invention of gin in the mid 17th century, although the existence of genever is confirmed in Massinger's play \"The Duke of Milan\" (1623), when Dr. Sylvius would have been but nine years of age.  It is further claimed that British soldiers who provided support in Antwerp against the Spanish in 1585, during the Eighty Years' War, were already drinking genever (jenever) for its calming effects before battle, from which the term \"Dutch Courage\" is believed to have originated.  The earliest known written reference to genever appears in the 13th century encyclopaedic work \"Der Naturen Bloeme\" (Bruges), and the earliest printed genever recipe from 16th century work \"Een Constelijck Distileerboec\" (Antwerp). A stroopwafel (also known as \"syrup waffle\", \"treacle waffle\" or \"caramel waffle\") is a waffle made from two thin layers of baked batter with a caramel-like syrup filling the middle.  They were first made in Gouda in the 1780s.  The traditional way to eat the stroopwafel is to place it atop of a drinking vessel with a hot beverage (coffee, tea or chocolate) inside that fits the diameter of the waffle.  The heat from the rising steam warms the waffle and slightly softens the inside and makes the waffle soft on one side while still crispy on the other. In 1815, Dutch chemist Coenraad Van Houten introduced alkaline salts to chocolate, which reduced its bitterness.  In the 1820s, Casparus van Houten, Sr. patented an inexpensive method for pressing the fat from roasted cocoa beans.  He created a press to remove about half the natural fat (cacao butter) from chocolate liquor, which made chocolate both cheaper to produce and more consistent in quality.  This innovation introduced the modern era of chocolate.  Van Houten developed the first cocoa powder producing machine in the Netherlands.  Van Houten's machine – a hydraulic press – reduced the cocoa butter content by nearly half.  This created a \"cake\" that could be pulverized into cocoa powder, which was to become the basis of all chocolate products.  The press separated the greasy cocoa butter from cacao seeds, leaving a purer chocolate powder behind.  This powder, much like the instant cocoa powder used today, was easier to stir into milk and water.  As a result, another very important discovery was made: solid chocolate.  By using cocoa powder and low amounts of cocoa butter, it was then possible to manufacture chocolate bar.  The term \"chocolate\" then came to mean solid chocolate, rather than hot chocolate. Dutch-processed chocolate or Dutched chocolate is chocolate that has been treated with an alkalizing agent to modify its color and give it a milder taste compared to \"natural cocoa\" extracted with the Broma process.  It forms the basis for much of modern chocolate, and is used in ice cream, hot cocoa, and baking.  The Dutch process was developed in the early 19th century by Dutch chocolate maker Coenraad Johannes van Houten, whose father Casparus is responsible for the development of the method of removing fat from cacao beans by hydraulic press around 1828, forming the basis for cocoa powder. In 1609, Hugo Grotius, the Dutch jurist who is generally known as the father of modern international law, published his book \"Mare Liberum\" (\"The Free Sea\"), which first formulated the notion of \"freedom of the seas\".  He developed this idea into a legal principle.  It is said to be 'the first, and classic, exposition of the doctrine of the freedom of the seas' which has been the essence and backbone of the modern law of the sea.  It is generally assumed that Grotius first propounded the principle of freedom of the seas, although all countries in the Indian Ocean and other Asian seas accepted the right of unobstructed navigation long before Grotius wrote his \"De Jure Praedae\" (\"On the Law of Spoils\") in the year of 1604.  His work sparked a debate in the seventeenth century over whether states could exclude the vessels of other states from certain waters.  Grotius won this debate, as \"freedom of the seas\" became a universally recognized legal principle, associated with concepts such as communication, trade and peace.  Grotius's notion of the freedom of the seas would persist until the mid-twentieth century, and it continues to be applied even to this day for much of the high seas, though the application of the concept and the scope of its reach is changing. The publication of \"De jure belli ac pacis\" (\"On the Laws of War and Peace\") by Hugo Grotius in 1625 had marked the emergence of international law as an 'autonomous legal science'.  Grotius’s \"On the Law of War and Peace\", published in 1625, is best known as the first systematic treatise on international law, but to thinkers of the seventeenth and eighteenth centuries, it seemed to set a new agenda in moral and political philosophy across the board.  Grotius developed pivotal treatises on freedom of the seas, the law of spoils, the laws of war and peace and he created an autonomous place for international law as its own discipline.  Jean Barbeyrac’s \"Historical and Critical Account of the Science of Morality\", attached to his translation of Samuel von Pufendorf’s \"Law of Nature and Nations\" in 1706, praised Grotius as “the first who broke the ice” of “the \"Scholastic Philosophy\"; which [had] spread itself all over Europe” (1749: 67, 66).  Grotius' truly distinctive contribution to jurisprudence and philosophy of law (public international law or law of nations in particular) was that he secularized natural law.  Grotius had divorced natural law from theology and religion by grounding it solely in the social nature and natural reason of man.  When Grotius, considered by many to be the founder of modern natural law theory (or \"secular natural law\"), said that natural law would retain its validity 'even if God did not exist' (\"etiamsi daremus non esse Deum\"), he was making a clear break with the classical tradition of natural law.  Adam Smith, in lectures delivered in 1762 on the subject of moral philosophy and the law of nations, said that: “Jurisprudence is that science which inquires into the general principles which ought to be the foundation of laws of all nations.  Grotius seems to have been the first who attempted to give the world anything like a regular system of natural jurisprudence, and his treatise, 'On the Laws of War and Peace, ' with all its imperfections, is perhaps at this day the most complete work on this subject.” The Grotian conception of international society became the most distinctive characteristic of the internationalist (or rationalist) tradition in international relations.  This is why it is also called the \"Grotian tradition\".  According to it international politics takes place within international society in which states are bound not only by rules of prudence or expediency but also of morality and law.  Grotius was arguably not the first to formulate such a doctrine.  However, he was first to clearly define the idea of one society of states, governed not by force or warfare but by laws and mutual agreement to enforce those laws.  As many international law scholars noted, the spirit of the Peace of Westphalia (1648) was preceded with the thoughts and ideas of Grotius.  Thomas Franck observed: ‘Since the Reformation, the Peace of Westphalia, and the writings of Hugo Grotius, there has been an explicit assumption that the international system is an association of sovereign states.’  As Hedley Bull declared: ‘The idea of international society which Grotius propounded was given concrete expression in the Peace of Westphalia’, affirming that ‘Grotius must be considered the intellectual father of this first general peace settlement of modern times’. By the end of the seventeenth century, support was growing for some limitation to the seaward extent of territorial waters.  What emerged was the so-called \"cannon shot rule\", which acknowledged the idea that property rights could be acquired by physical occupation and in practice to the effective range of shore-based cannon: about three nautical miles.  The rule was long associated with Cornelis van Bijnkershoek, a Dutch jurist who, especially in his \"De Dominio Maris Dissertatio\" (1702), advocated a middle ground between the extremes of \"Mare Liberum\" and John Selden's \"Mare Clausum\", accepting both the freedom of states to navigate and exploit the resources of the high seas and a right of coastal states to assert wide-ranging rights in a limited marine territory. The Permanent Court of Arbitration (PCA) is an international organization based in The Hague in the Netherlands.  The court was established in 1899 as one of the acts of the first Hague Peace Conference, which makes it the oldest global institution for international dispute resolution.  Its creation is set out under Articles 20 to 29 of the 1899 Hague Convention for the pacific settlement of international disputes, which was a result of the first Hague Peace Conference.  The most concrete achievement of the Conference was the establishment of the PCA as the first institutionalized global mechanism for the settlement of disputes between states.  The PCA encourages the resolution of disputes that involve states, state entities, intergovernmental organizations, and private parties by assisting in the establishment of arbitration tribunals and facilitating their work.  The court offers a wide range of services for the resolution of international disputes which the parties concerned have expressly agreed to submit for resolution under its auspices.  Dutch-Jew legal scholar Tobias Asser's role in the creation of the PCA at the first Hague Peace Conference (1899) earned him the Nobel Peace Prize in 1911. The International Opium Convention, sometimes referred to as the Hague Convention of 1912, signed on 23 January 1912 at The Hague, was the first international drug control treaty and is the core of the international drug control system.  The adoption of the Convention was a turning point in multilateralism, based on the recognition of the transnational nature of the drug problem and the principle of shared responsibility. Denmark was the first state to recognize a legal relationship for same-sex couples, establishing \"registered partnerships\" very much like marriage in 1989.  In 2001, the Netherlands became the first nation in the world to grant same-sex marriages.  The first laws enabling same-sex marriage in modern times were enacted during the first decade of the 21st century.  s of 29 2014 , sixteen countries (Argentina, Belgium, Brazil, Canada, Denmark, France, Iceland, Netherlands, New Zealand, Norway, Portugal, Spain, South Africa, Sweden, United Kingdom, Uruguay) and several sub-national jurisdictions (parts of Mexico and the United States) allow same-sex couples to marry.  Polls in various countries show that there is rising support for legally recognizing same-sex marriage across race, ethnicity, age, religion, political affiliation, and socioeconomic status. The first mechanical clocks, employing the verge escapement mechanism with a foliot or balance wheel timekeeper, were invented in Europe at around the start of the 14th century, and became the standard timekeeping device until the pendulum clock was invented in 1656.  The pendulum clock remained the most accurate timekeeper until the 1930s, when quartz oscillators were invented, followed by atomic clocks after World War 2. A pendulum clock uses a pendulum's arc to mark intervals of time.  From their invention until about 1930, the most accurate clocks were pendulum clocks.  Pendulum clocks cannot operate on vehicles or ships at sea, because the accelerations disrupt the pendulum's motion, causing inaccuracies.  The pendulum clock was invented by Christian Huygens, based on the pendulum introduced by Galileo Galilei.  Although Galileo studied the pendulum as early as 1582, he never actually constructed a clock based on that design.  Christiaan Huygens invented pendulum clock in 1656 and patented the following year.  He contracted the construction of his clock designs to clockmaker Salomon Coster, who actually built the clock. Various authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio.  The thermometer was not a single invention, however, but a development.  However, each inventor and each thermometer was unique – there was no standard scale.  In 1665 Christiaan Huygens suggested using the melting and boiling points of water as standards.  The Fahrenheit scale is now usually defined by two fixed points: the temperature at which water freezes into ice is defined as 32 degrees Fahrenheit (°F), and the boiling point of water is defined to be 212 °F, a 180 degree separation, as defined at sea level and standard atmospheric pressure.  In 1742, Swedish astronomer Anders Celsius created a temperature scale which was the reverse of the scale now known by the name \"Celsius\": \"0\" represented the boiling point of water, while \"100\" represented the freezing point of water.  From 1744 until 1954, 0 °C was defined as the freezing point of water and 100 °C was defined as the boiling point of water, both at a pressure of one standard atmosphere with mercury being the working material. The invention of the mainspring in the early 15th century allowed portable clocks to be built, evolving into the first pocketwatches by the 17th century, but these were not very accurate until the balance spring was added to the balance wheel in the mid 17th century.  Some dispute remains as to whether British scientist Robert Hooke (his was a straight spring) or Dutch scientist Christiaan Huygens was the actual inventor of the balance spring.  Huygens was clearly the first to successfully implement a spiral balance spring in a portable timekeeper.  This is significant because up to that point the pendulum was the most reliable.  This innovation increased watches' accuracy enormously, reducing error from perhaps several hours per day to perhaps 10 minutes per day, resulting in the addition of the minute hand to the face from around 1680 in Britain and 1700 in France. Like the invention of pendulum clock, Huygens' spiral hairspring (balance spring) system of portable timekeepers, helped lay the foundations for the modern watchmaking industry.  The application of the spiral balance spring for watches ushered in a new era of accuracy for portable timekeepers, similar to that which the pendulum had introduced for clocks. From its invention in 1675 by Christiaan Huygens, the spiral hairspring (balance spring) system for portable timekeepers, still used in mechanical watchmaking industry today. Various authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio.  The thermometer was not a single invention, however, but a development.  Though Galileo is often said to be the inventor of the thermometer, what he produced were thermoscopes.  The difference between a thermoscope and a thermometer is that the latter has a scale.  The first person to put a scale on a thermoscope is variously said to be Francesco Sagredo or Santorio Santorio in about 1611 to 1613. Before there was the thermometer, there was the earlier and closely related thermoscope, best described as a thermometer without a temperature scale.  A thermoscope only showed the differences in temperatures, for example, it could show something was getting hotter.  However, the thermoscope did not measure all the data that a thermometer could, for example an exact temperature in degrees.  What can be considered the first modern thermometer, the mercury thermometer with a standardized scale, was invented by German-Dutch scientist Daniel Gabriel Fahrenheit (who had settled in Amsterdam in 1701) in 1714.  Fahrenheit invented the first truly accurate thermometer using mercury instead of alcohol and water mixtures.  He began constructing his own thermometers in 1714, and it was in these that he used mercury for the first time. Various authors have credited the invention of the thermometer to Cornelis Drebbel, Robert Fludd, Galileo Galilei or Santorio Santorio.  The thermometer was not a single invention, however, but a development.  However, each inventor and each thermometer was unique – there was no standard scale.  In 1665 Christiaan Huygens suggested using the melting and boiling points of water as standards, and in 1694 Carlo Renaldini proposed using them as fixed points on a universal scale.  In 1701 Isaac Newton proposed a scale of 12 degrees between the melting point of ice and body temperature.  Finally in 1724 Daniel Gabriel Fahrenheit produced a temperature scale which now (slightly adjusted) bears his name.  He could do this because he manufactured thermometers, using mercury (which has a high coefficient of expansion) for the first time and the quality of his production could provide a finer scale and greater reproducibility, leading to its general adoption.  The Fahrenheit scale was the first widely used temperature scale.  By the end of the 20th century, most countries used the Celsius scale rather than the Fahrenheit scale, though Canada retained it as a supplementary scale used alongside Celsius.  Fahrenheit remains the official scale for Jamaica, the Cayman Islands, Belize, the Bahamas, Palau and the United States and associated territories. The Snellen chart is an eye chart used by eye care professionals and others to measure visual acuity.  Snellen charts are named after Dutch ophthalmologist Hermann Snellen who developed the chart in 1862.  Vision scientists now use a variation of this chart, designed by Ian Bailey and Jan Lovie. Previous to the string galvanometer, scientists used a machine called the capillary electrometer to measure the heart's electrical activity, but this device was unable to produce results at a diagnostic level.  Dutch physiologist Willem Einthoven developed the string galvanometer in the early 20th century, publishing the first registration of its use to record an electrocardiogram in a Festschrift book in 1902.  The first human electrocardiogram was recorded in 1887, however only in 1901 was a quantifiable result obtained from the string galvanometer. In 1922, Dutch astronomer Jan Schilt invented the Schilt photometer, a device that measures the light output of stars and, indirectly, their distances. In the 19th century it became clear that the heart generated electric currents.  The first to systematically approach the heart from an electrical point-of-view was \"Augustus Waller\", working in St Mary's Hospital in Paddington, London.  In 1911 he saw little clinical application for his work.  The breakthrough came when Einthoven, working in Leiden, used his more sensitive string galvanometer, than the \"capillary electrometer\" that Waller used.  Einthoven assigned the letters P, Q, R, S and T to the various deflections that it measured and described the electrocardiographic features of a number of cardiovascular disorders.  He was awarded the 1924 Nobel Prize for Physiology or Medicine for his discovery. Einthoven's triangle is an imaginary formation of three limb leads in a triangle used in electrocardiography, formed by the two shoulders and the pubis.  The shape forms an inverted equilateral triangle with the heart at the center that produces zero potential when the voltages are summed.  It is named after Willem Einthoven, who theorized its existence. When German bombers attacked The Hague in 1940 while Willem Johan Kolff was there, he organised the first blood bank in continental Europe.  It was located in the \"Zuidwal\" hospital in The Hague.  Eleven patients were given blood transfusions in The Hague, six of whom survived.  Donated blood was also used for victims of the bombardment of Rotterdam, whither it was transported by civilian car. An artificial kidney is a machine and its related devices which clean blood for patients who have an acute or chronic failure of their kidneys.  The first artificial kidney was developed by Dutchman Willem Johan Kolff.  The procedure of cleaning the blood by this means is called \"dialysis\", a type of renal replacement therapy that is used to provide an artificial replacement for lost kidney function due to renal failure.  It is a life support treatment and does not treat disease. On 12 December 1957, Kolff implanted an artificial heart into a dog at Cleveland Clinic.  The dog lived for 90 minutes.  In 1967, Dr. Kolff left Cleveland Clinic to start the Division of Artificial Organs at the University of Utah and pursue his work on the artificial heart.  Under his supervision, a team of surgeons, chemists, physicists and bioengineers developed an artificial heart and made it ready for industrial production.  To help manage his many endeavors, Dr. Kolff assigned project managers.  Each project was named after its manager.  Graduate student Robert Jarvik was the project manager for the artificial heart, which was subsequently renamed the \"Jarvik-7\".  Based on lengthy animal trials, this first artificial heart was successfully implanted into the thorax of patient Barney Clark in December 1982.  Clark survived 112 days with the device. The Dutch Republic has been considered by many political and military historians as the first modern (global) sea power.  The United Provinces of the Netherlands was the first state to possess the full triad of foreign commerce, forward bases and merchant and naval fleets.  In the middle of the 17th century the Dutch navy was the most powerful navy in the world.  The Dutch Republic had a commercial fleet that was larger than that of England, France, Germany, Portugal, and Spain combined.  According to Walter Russell Mead, the “modern version of sea power was invented by the Dutch.  The system of global trade, investment, and military power the Dutch built in the seventeenth century was the envy and the wonder of the world at the time, and many of its basic features were adopted by the British and the Americans in subsequent years.”  When the Peter the Great determined to achieve sea power for Imperial Russia, he came to the Dutch Republic to learn about shipbuilding, seamanship and nautical sciences.  During his stay in Holland (1697) the Tsar engaged, with the help of Russian and Dutch assistants, many skilled workers such as builders of locks, fortresses, shipwrights and seamen.  They had to help him with his modernization of Russia.  The best-known sailor who made the journey from the Dutch Republic to Russia was Norwegian-Dutch Cornelius Cruys.  Cruys performed well in Russia and came be regarded as the architect of the Russian Navy.  He became the first commander of the Russian Baltic Fleet and the vice admiral of the Imperial Russian Navy.  Peter the Great designed his new capital on the model of Amsterdam and gave it a Dutch name, Sankt Pieter Burkh (later Germanized into Saint Peterburg).  In St. Petersburg, there is an island which is still called Novaya Gollandiya (literally “New Holland”).  The triangular man-made island took its name after a number of canals and shipbuilding facilities that rendered its appearance similar to Amsterdam.  The Tsar chose to call his island “New Holland”, commemorating his enthusiasm for all things Dutch. The early modern Military Revolution began with reforms inaugurated by Prince Maurice of Nassau with his cousins Count Willem Lodewijk of Nassau-Dillenburg and Count John VII of Nassau during the 1590s.  Maurice developed a system of linear formations (linear tactics), discipline, drill and volley fire based on classical Roman methods that made his army more efficient and his command and control more effective.  He also developed a 43-step drill for firing the musket that was included in an illustrated weapons manual by Jacob de Gheyn II in 1607 (\"Wapenhandelinghe\" or \"Exerise of Arms\").  This became known as the \"Dutch drill\".  It was widely read and emulated in the rest of Europe.  Adopting and perfecting the techniques pioneered by Maurice of Nassau several decades earlier, Gustavus Adolphus repeatedly proved his techniques by defeating the armies of Spain (1630–1632), an empire with resources fantastically larger than Sweden's during the Thirty Years' War.  Descartes served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau, and developed a fascination for practical technology.  Maurice' s military innovations had considerable influences on Descartes' system of philosophy. The Norden bombsight was designed by Carl Norden, a Dutch engineer educated in Switzerland who emigrated to the U.S. in 1904.  In 1920, he started work on the Norden bombsight for the United States Navy.  The first bombsight was produced in 1927.  It was essentially an analog computer, and bombardiers were trained in great secrecy on how to use it.  The device was used to drop bombs accurately from an aircraft, supposedly accurate enough to hit a 100-foot circle from an altitude of 21,000 feet – but under actual combat situations, such an accuracy was never achieved. A submarine snorkel is a device that allows a submarine to operate submerged while still taking in air from above the surface.  It was invented by the Dutchman J.J. Wichers shortly before World War II and copied by the Germans during the war for use by U-Boats. Its common military name is snort. Goalkeeper is a close-in weapon system (CIWS) still in use as of 2015.  It is autonomous and completely automatic short-range defense of ships against highly maneuverable missiles, aircraft and fast maneuvering surface vessels.  Once activated the system automatically performs the entire process from surveillance and detection to destruction, including selection of priority targets. The first (mechanical) metronome was invented by Dietrich Nikolaus Winkel in Amsterdam in 1812, but named (patented) after Johann Maelzel, who took the idea and popularized it. Dutch musician-physicist Adriaan Fokker designed and had built keyboard instruments capable of playing microtonal scales via a generalized keyboard.  The best-known of these is his 31-tone equal-tempered organ, which was installed in Teylers Museum in Haarlem in 1951.  It is commonly called the Fokker organ. The Kraakdoos or Cracklebox is a custom-made battery-powered noise-making electronic device.  It is a small box with six metal contacts on top, which when pressed by fingers generates unusual sounds and tones.  The human body becomes a part of the circuit and determines the range of sounds possible – different players generate different results.  The concept was first conceived by Michel Waisvisz and Geert Hamelberg in the 1960s, and developed further in the 1970s when Waisvisz joined the STEIM foundation in Amsterdam. The Moodswinger is a twelve-string electric zither with an additional third bridge designed by Dutch luthier Yuri Landman.  The rod functions as the third bridge and divides the strings into two sections to add overtones, creating a multiphonic sound. The Springtime is an experimental electric guitar with seven strings and three outputs.  Landman created the instrument in 2008. Neostoicism was a syncretic philosophical movement, joining Stoicism and Christianity.  Neostoicism was founded by Dutch-Flemish humanist Justus Lipsius, who in 1584 presented its rules, expounded in his book \"De Constantia\" (\"On Constancy\"), as a dialogue between Lipsius and his friend Charles de Langhe.  The eleven years (1579–1590) that Lipsius spent in Leiden (Leiden University) were the period of his greatest productivity.  It was during this time that he wrote a series of works designed to revive ancient Stoicism in a form that would be compatible with Christianity.  The most famous of these is \"De Constantia\" (1584).  Neostoicism had a direct influence on many seventeenth and eighteenth-century writers including Montesquieu, Bossuet, Francis Bacon, Joseph Hall, Francisco de Quevedo and Juan de Vera y Figueroa. The rise of modern rationalism in the Dutch Republic, had a profound influence on the 17th-century philosophy.  Descartes is often considered to be the first of the modern rationalists.  Descartes himself had lived in the Dutch Republic for some twenty years (1628–1649) and served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau.  The Dutch Republic was the first country in which Descartes' rationalistic philosophy (Cartesianism) succeeded in replacing Aristotelianism as the academic orthodoxy.  Fritz Berolzheimer considers Hugo Grotius the Descartes of legal philosophy and notes Grotian rationalism's influence on the 17th-century jurisprudence: \"As the Cartesian \"cogito ergo sum\" became the point of departure of rationalistic philosophy, so the establishment of government and law upon reason made Hugo Grotius the founder of an independent and purely rationalistic system of natural law.\"  In the late 1650s Leiden was a place where one could study Cartesian philosophy.  Sometime between 1656 and 1661 it appears that Spinoza did some formal study of philosophy at the University of Leiden.  Philosophy of Spinoza (Spinozism) was an systematic answer to Descartes' famous dualist theory that the body and spirit are separate. Pantheism was popularized in the modern era as both a theology and philosophy based on the work of the 17th-century Dutch Jew philosopher Baruch Spinoza, whose \"Ethics\" was an answer to Descartes' famous dualist theory that the body and spirit are separate.  Spinoza is regarded as the chief source of modern pantheism.  Spinoza held that the two are the same, and this monism is a fundamental quality of his philosophy.  He was described as a \"God-intoxicated man,\" and used the word God to describe the unity of all substance.  Although the term pantheism was not coined until after his death, Spinoza is regarded as its most celebrated advocate. \"European liberalism\", Isaiah Berlin wrote, \"wears the appearance of a single coherent movement, little altered during almost three centuries, founded upon relatively simple foundations, laid by Locke or Grotius or even Spinoza; stretching back to Erasmus and Montaigne...\" As Bertrand Russell noted in his \"A History of Western Philosophy\" (1945): \"Descartes lived in Holland for twenty years (1629–49), except for a few brief visits to France and one to England, all on business.  It is impossible to exaggerate the importance of Holland in the seventeenth century, as the one country where there was freedom of speculation.  Hobbes had to have his books printed there; Locke took refuge there during the five worst years of reaction in England before 1688; Bayle (of the \"Dictionary\") found it necessary to live there; and Spinoza would hardly have been allowed to do his work in any other country.\"  Russell described early liberalism in Europe: \"Early liberalism was a product of England and Holland, and had certain well-marked characteristics.  It stood for religious toleration; it was Protestant, but of a latitudinarian rather than of a fanatical kind; it regarded the wars of religion as silly...\" As Russell Shorto states: “Liberalism has many meanings, but in its classical sense it is a philosophy based on individual freedom.  History has long taught that our modern sensibility comes from the eighteenth century Enlightenment.  In recent decades, historians have seen the Dutch Enlightenment of the seventeenth century as the root of the wider Enlightenment.  And at the center of this sits the city of Amsterdam.”  Amsterdam, to Shorto, was not only the first city in Europe to develop the cultural and political foundations of what we now call liberalism – a society focused on the concerns and comforts of individuals, run by individuals acting together, and tolerant of religion, ethnicity, or other differences – but also an exporter of these beliefs to the rest of Europe and the New World. If Descartes is still considered the father of modern philosophy, Dutch Republic can be called its cradle.  Cartesianism is the name given to the philosophical doctrine of René Descartes.  Descartes is often regarded as the first thinker to emphasize the use of reason to develop the natural sciences.  Cartesianism had been controversial for several years before 1656.  Descartes himself had lived in the Dutch Republic for some twenty years (1628–1649).  Descartes served for a while in the army of the Dutch military leader Prince Maurice of Orange-Nassau, and developed a fascination for practical technology.  In the 1630s, while staying in the Dutch city Deventer, Descartes worked on a text which became published as \"Traite' de l'Homme\" (1664).  Throughout his writing, he used words such as clock, automaton, and self – moving machine as interchangeable constructs.  He postulated an account of the physical world that was thoroughly materialistic.  His mechanical view of nature replaced the organism model which had been popular since the Renaissance.  His \"Discours de la méthode\" (1637) was originally published at Leiden, and his \"Principia philosophiae\" (1644) appeared from the presses at Amsterdam.  In the 1630s and 1640s, Descartes's ideas gained a foothold at the Dutch universities. Spinozism is the monist philosophical system of the Dutch-Jewish philosopher Baruch Spinoza which defines \"God\" as a singular self-subsistent substance, with both matter and thought as its attributes. Affect (\"affectus\" or \"adfectus\" in Latin) is a concept used in the philosophy of Spinoza and elaborated by Henri Bergson, Gilles Deleuze and Félix Guattari that emphasizes bodily experience.  The term \"affect\" is central to what became known as the \"affective turn\" in the humanities and social sciences. Mandeville's paradox is named after Bernard Mandeville, who shows that actions which may be qualified as vicious with regard to individuals have benefits for society as a whole.  This is already clear from the subtitle of his most famous work, \"The Fable of The Bees\": ‘Private Vices, Publick Benefits’.  He states that \"Fraud, Luxury, and Pride must live; Whilst we the Benefits receive.\")  (The Fable of the Bees, ‘The Moral’). Mathematical intuitionism was founded by the Dutch mathematician and philosopher Luitzen Egbertus Jan Brouwer.  In the philosophy of mathematics, intuitionism, or \"neointuitionism\" (opposed to preintuitionism), is an approach where mathematics is considered to be purely the result of the constructive mental activity of humans rather than the discovery of fundamental principles claimed to exist in an objective reality.  That is, logic and mathematics are not considered analytic activities wherein deep properties of objective reality are revealed and applied, but are instead considered the application of internally consistent methods used to realize more complex mental constructs, regardless of their possible independent existence in an objective reality. Devotio Moderna, or Modern Devotion, was a movement for religious reform, calling for apostolic renewal through the rediscovery of genuine pious practices such as humility, obedience and simplicity of life.  It began in the late fourteenth-century, largely through the work of Gerard Groote, and flourished in the Low Countries and Germany in the fifteenth century, but came to an end with the Protestant Reformation.  Gerard Groote, father of the movement, founded the Brethren of the Common Life; after his death, disciples established a house of Augustinian Canons at Windesheim (near Zwolle, Overijssel).  These two communities became the principal exponents of Devotio Moderna.  Martin Luther studied under the Brethren of the Common Life at Magdeburg before going on to the University of Erfurt.  Another famous member of the Brethren of the Common Life was Desiderius Erasmus of Rotterdam. Devotio Moderna, an undogmatic form of piety which some historians have argued helped to pave the road for the Protestant Reformation, is most known today through its influence on Thomas à Kempis, the author of \"The Imitation of Christ\" a book which proved highly influential for centuries. The Mennonites are a Christian group based around the church communities of Anabaptist denominations named after Menno Simons (1496–1561) of Friesland.  Through his writings, Simons articulated and formalized the teachings of earlier Swiss founders.  The teachings of the Mennonites were founded on their belief in both the mission and ministry of Jesus Christ, which they held to with great conviction despite persecution by various Roman Catholic and Protestant states. The Dutch Reformed Church (in Dutch: \"Nederlandse Hervormde Kerk\" or NHK) was a Reformed Christian denomination.  It developed during the Protestant Reformation, with its base in what became known as the Roman Catholic Church.  It was founded in the 1570s and lasted until 2004, the year it merged with the Reformed Churches in the Netherlands and the Evangelical Lutheran Church in the Kingdom of the Netherlands to form the Protestant Church in the Netherlands. Arminianism is based on the theological ideas of Dutch Reformed theologian Jacobus Arminius (1560–1609) and his historic supporters known as the Remonstrants.  His teachings held to the five solae of the Reformation, but they were distinct from the particular teachings of Martin Luther, Zwingli, John Calvin, and other Protestant Reformers.  Arminius (Jacobus Hermanszoon) was a student of Beza (successor of Calvin) at the Theological University of Geneva. Many Christian denominations have been influenced by Arminian views on the will of man being freed by grace prior to regeneration, notably the Baptists in the 16th century, the Methodists in the 18th century and the Seventh-day Adventist Church.  John Wesley was influenced by Arminianism.  Also, Arminianism was an important influence in Methodism, which developed out of the Wesleyan movement.  Some assert that Universalists and Unitarians in the 18th and 19th centuries were theologically linked with Arminianism. The first synagogue of the New World, Kahal Zur Israel Synagogue, is founded in Recife, Brazil by the Dutch Jews.  The Kahal Zur Israel Synagogue in Recife, Brazil, erected in 1636, was the first synagogue erected in the Americas.  Its foundations have been recently discovered, and the 20th-century buildings on the site have been altered to resemble a 17th-century Dutch synagogue. Jansenism was a Catholic theological movement, primarily in France, that emphasized original sin, human depravity, the necessity of divine grace, and predestination.  The movement originated from the posthumously published work (\"Augustinus\") of the Dutch theologian Cornelius Jansen, who died in 1638.  It was first popularized by Jansen's friend Abbot Jean Duvergier de Hauranne, of Saint-Cyran-en-Brenne Abbey, and after Duvergier's death in 1643, was led by Antoine Arnauld.  Through the 17th and into the 18th centuries, Jansenism was a distinct movement within the Catholic Church.  The theological centre of the movement was the convent of Port-Royal Abbey, Paris, which was a haven for writers including Duvergier, Arnauld, Pierre Nicole, Blaise Pascal, and Jean Racine. Congregation Shearith Israel, the Spanish and Portuguese Synagogue in the City of New Amsterdam, was founded in 1654, the first Jewish congregation to be established in North America.  Its founders were twenty-three Jews, mostly of Spanish and Portuguese origin, who had been living in Recife, Brazil.  When the Portuguese defeated the Dutch for control of Recife, and brought with them the Inquisition, the Jews of that area left.  Some returned to Amsterdam, where they had originated.  Others went to places in the Caribbean such as St. Thomas, Jamaica, Surinam and Curaçao, where they founded sister Sephardic congregations.  One group of twenty-three Jews, after a series of unexpected events, landed in New Amsterdam.  After being initially rebuffed by anti-Semitic Governor Peter Stuyvesant, Jews were given official permission to settle in the colony in 1655.  These pioneers fought for their rights and won permission to remain.  This marks the founding of the Congregation Shearith Israel. In 1590 the Dutchmen Hans and Zacharias Janssen (father and son) is sometimes claimed to have invented the first compound microscope. In 1608 Hans Lippershey, Zacharias Janssen and Jacob Metius created the first practical telescope.  Crude telescopes and spyglasses may have been created much earlier, but Lippershey is believed to be the first to apply for a patent, which he failed to secure, after which he made it available for general use.  A description of Lippershey's instrument quickly reached Galileo Galilei, who created a working unit in 1609, with which he made the observations found in his \"Sidereus Nuncius\" of 1610. An aerial telescope is a type of very long focal length refracting telescope, built in the second half of the 17th century, that did not use a tube.  Instead, the objective was mounted on a pole, tree, tower, building or other structure on a swivel ball-joint.  The observer stood on the ground and held the eyepiece, which was connected to the objective by a string or connecting rod.  By holding the string tight and maneuvering the eyepiece, the observer could aim the telescope at objects in the sky.  The idea for this type of telescope may have originated in the late 17th century with the Dutch mathematician, astronomer and physicist Christiaan Huygens and his brother Constantijn Huygens, Jr.. Huygens eyepieces consist of two plano-convex lenses with the plane sides towards the eye separated by an air gap.  The lenses are called the eye lens and the field lens.  The focal plane is located between the two lenses.  It was invented by Christiaan Huygens in the late 1660s and was the first compound (multi-lens) eyepiece.  Huygens discovered that two air spaced lenses can be used to make an eyepiece with zero transverse chromatic aberration.  These eyepieces work well with the very long focal length telescopes (in Huygens day they were used with single element long focal length non-achromatic refracting telescopes, including very long focal length aerial telescopes).  This optical design is now considered obsolete since with today's shorter focal length telescopes the eyepiece suffers from short eye relief, high image distortion, chromatic aberration, and a very narrow apparent field of view.  Since these eyepieces are cheap to make they can often be found on inexpensive telescopes and microscopes.  Because Huygens eyepieces do not contain cement to hold the lens elements, telescope users sometimes use these eyepieces in the role of \"solar projection\", i.e. projecting an image of the Sun onto a screen.  Other cemented eyepieces can be damaged by the intense, concentrated light of the Sun. Van Leeuwenhoek created at least 25 microscopes, of differing types, of which only nine survive.  His simple microscopes were made of silver or copper frames, holding hand-ground lenses.  Those that have survived are capable of magnification up to 275 times.  It is suspected that Van Leeuwenhoek possessed units that could magnify up to 500 times.  Using his handcrafted microscopes, he was the first to observe and describe single-celled organisms, which he originally referred to as \"animalcules\", and which now referred to as micro-organisms or microbes. The cycloid pendulum was invented by Christiaan Huygens in 1673.  Its purpose is to eliminate the lack of isochronism of the ordinary simple pendulum.  This is achieved by making the mass point move on a cycloid instead of a circular arc. The pyrometer, invented by Pieter van Musschenbroek, is a temperature measuring device.  A simple type uses a thermocouple placed either in a furnace or on the item to be measured.  The voltage output of the thermocouple is read from a meter.  Many different types of thermocouple are available, for measuring temperatures from −200 °C to above 1500 °C. A Leyden jar, or Leiden jar, is a device that \"stores\" static electricity between two electrodes on the inside and outside of a glass jar.  It was the original form of a capacitor (originally known as a \"condenser\").  It was invented independently by German cleric Ewald Georg von Kleist on 11 October 1745 and by Dutch scientist Pieter van Musschenbroek of Leiden (Leyden) in 1745–1746.  The invention was named for the city.  The Leyden jar was used to conduct many early experiments in electricity, and its discovery was of fundamental importance in the study of electricity.  Previously, researchers had to resort to insulated conductors of large dimensions to store a charge.  The Leyden jar provided a much more compact alternative.  Like many early electrical devices, there was no particular use for the Leyden jar at first, other than to allow scientists to do a greater variety of electrical experiments.  Benjamin Franklin, for example, used a Leyden jar to store electricity from lightning in his famous kite experiment in 1752.  By doing so he proved that lightning was really electricity. The idea for the Leyden jar was discovered independently by two parties: German scientist and jurist Ewald Georg von Kleist, and Dutchmen Pieter van Musschenbroek and Andreas Cunaeus.  These scientists developed the Leyden jar while working under a theory of electricity that saw electricity as a fluid, and hoped to develop the jar to \"capture\" this fluid.  In 1744 von Kleist lined a glass jar with silver foil, and charged the foil with a friction machine.  Kleist was convinced that a substantial electric charge could be collected when he received a significant shock from the device.  The effects of this \"Kleistian jar\" were independently discovered around the same time by Dutch scientists Pieter van Musschenbroek and Cunaeus at the University of Leiden.  Van Musschenbroek communicated on it with the French scientific community where it was called the Leyden jar. The Eisinga Planetarium (Royal Eise Eisinga Planetarium) was built by Eise Eisinga in his home in Franeker, Friesland.  It took Eisinga seven years to build his planetarium, completing it in 1781.  The orrery still exists and is the world's oldest working planetarium. Kipp's apparatus, also called a Kipp generator, is designed for preparation of small volumes of gases.  It was invented around 1860 by Dutch pharmacist Petrus Jacobus Kipp and widely used in chemical laboratories and for demonstrations in schools into the second half of the 20th century. In optical microscopy many objects such as cell parts in protozoans, bacteria and sperm tails are essentially fully transparent unless stained (and therefore killed).  The difference in densities and composition within these objects however often gives rise to changes in the phase of light passing through them, hence they are sometimes called \"phase objects\".  Using the phase-contrast technique makes these structures visible and allows the study of living specimens.  This phase contrast technique proved to be such an advancement in microscopy that Dutch physicist Frits Zernike was awarded the Nobel Prize in 1953. The magnetic horn (also known as the \"Van der Meer horn\") is a high-current, pulsed focusing device, invented by the Dutch physicist Simon van der Meer at CERN.  It selects pions and focuses them into a sharp beam.  Its original application was in the context of neutrino physics, where beams of pions have to be tightly focused.  When the pions then decay into muons and neutrinos or antineutrinos, an equally well-focused neutrino beam is obtained.  The muons were stopped in a wall of 3000 tons of iron and 1000 tons of concrete, leaving the neutrinos or antineutrinos to reach the Gargamelle bubble chamber. A golf-like game (\"kolf\" in Dutch) is recorded as taking place on 26 February 1297, in a city called Loenen aan de Vecht, where the Dutch played a game with a stick and leather ball.  The winner was whomever hit the ball with the least number of strokes into a target several hundred yards away.  Some scholars argue that this game of putting a small ball in a hole in the ground using clubs was also played in 17th-century Netherlands and that this predates the game in Scotland. The Dutch played a significant role in the history of ice skating (including speed skating and figure skating).  The first feature of ice skating in a work of art was made in the 15th century.  The picture, depicted Saint Lidwina, patron saint of ice skaters, falling on the ice.  Another important aspect is a man seen in the background, who is skating on one leg.  This means that his skates must have had sharp edges similar to those found on modern ice skates. Until the 17th century, ice skating was mostly used for transportation.  Some of the Stuarts (including King Charles II of England) who had fled to the Dutch Republic during the Cromwell Royal reign later returned to Britain, bringing with them the new sport.  Upon his return to England in 1658, the King brought two innovations in ice skating – a pair of iron skates and the Dutch roll.  The Dutch roll was the first form of a gliding or skating motion made possible by the iron skate's two edges.  However, speed skating was the focus of the Dutch, while the English developed modern figure skating. Speed skating, which had developed in the Netherlands in the 17th century, was given a boost by the innovations in skate construction.  Speed skating, or speedskating, is a competitive form of skating in which skaters race each other over a certain distance.  Types of speed skating are long track speed skating, short track speed skating and marathon speed skating.  In the modern Olympic Games, long-track speed skating is usually referred to as just \"speed skating\", while short-track speed skating is known as \"short track\". Sailing, also known as yachting, is a sport in which competitors race from point to point, or around a race course, in sail-powered boats.  Yachting refers to recreational sailing or boating, the specific act of sailing or using other water vessels for sporting purposes.  The invention of sailing is prehistoric, but the racing of sailing boats is believed to have started in the Netherlands some time in the 17th century.  While living in the Dutch Republic, King Charles II of England fell in love with sailing and in 1660, took home the Dutch gifted 66-foot yacht he called \"Mary\".  The sport's popularity spread across the British Isles.  The world's first yacht club was founded in Cork, Ireland in 1720. The International Skating Union (ISU) is the international governing body for competitive ice skating disciplines, including figure skating, synchronized skating, speed skating, and short track speed skating.  It was founded in Scheveningen, Netherlands, in 1892, making it the oldest governing international winter sport federation and one of the oldest international sport federations. The first official World Championships in Speed Skating (open to men only) directly under the auspices of the ISU were held in Amsterdam in 1893. Korfball (Korfbal in Dutch) is a mixed gender team sport, with similarities to netball and basketball.  A team consists of eight players; four female and four male.  A team also includes a coach.  It was founded in the Netherlands in 1902 by Nico Broekhuysen. The Cruijff Turn (also known as \"Cruyff Turn\"), is a famous dribbling trick in football, was perfected by the Dutch football player Johan Cruijff for whom the evasive trick was named.  To make this move, the player first looks to pass or cross the ball.  However, instead of kicking it, he drags the ball behind his planted foot with the inside of his other foot, turns through 180 degrees and accelerates away.  The trick was famously employed by Cruijff in the 1974 FIFA World Cup, first seen in the Dutch match against Sweden and soon widely copied. The foundations for Total Football (Dutch: totaalvoetbal) were laid by Englishman Jack Reynolds who was the manager of AFC Ajax.  Rinus Michels, who played under Reynolds, later became manager of Ajax and refined the concept into what is known today as \"Total Football\" (\"Totaalvoetbal\" in Dutch language), using it in his training for the Ajax Amsterdam squad and the Netherlands national football team in the 1970s.  It was further refined by Stefan Kovacs after Michels left for FC Barcelona.  Johan Cruyff was the system's most famous exponent.  Due to Cruyff's style of play, he is still referred to as \"the total footballer\".  Its cornerstone was a focus on positional interchange.  The invention of \"totaalvoetbal\" helped lay the foundations for the significant successes of Dutch football at both club and international level in the 1970s.  During that decade, the Dutch football rose from almost total obscurity to become a powerhouse in world football.  In an interview published in the 50th anniversary issue of \"World Soccer\" magazine, the captain of the Brazilian team that won the 1970 FIFA World Cup, Carlos Alberto, went on to say: “The only team I’ve seen that did things differently was Holland at the 1974 World Cup in Germany.  Since then everything looks more or less the same to me….  Their ‘carousel’ style of play was amazing to watch and marvellous for the game.” FC Barcelona and the Spanish national football team play a style of football known as \"Tiki-taka\" that has its roots in Total Football.  Johan Cruyff founded Tiki-taka (commonly spelled \"tiqui-taca\" in Spanish) during his time as manager of FC Barcelona (1988–1996).  The style was successfully adopted by the all-conquering Spain national football team (2008–2012) and Pep Guardiola's Barcelona team (2009–2011).  Tiki-taka style differs from Total Football in that it focuses on ball movement rather than positional interchange. The Netherlands revived the construction of canals during the 13th–14th century that had generally been discontinued since the fall of the Roman Empire.  They also contributed in the development of canal construction technology, such as introducing the first flash locks in Europe.  The first pound lock in Europe was built by the Dutch in 1373 at Vreeswijk, where a canal from Utrecht joins the river Lek. Around the 1620s, Cornelis Drebbel developed an automatic temperature control system for a furnace, motivated by his belief that base metals could be turned to gold by holding them at a precise constant temperature for long periods of time.  He also used this temperature regulator in an incubator for hatching chickens. Feedback control has been used for centuries to regulate engineered systems.  In the 17th century, Drebbel invented one of the earliest devices to use feedback, an chicken incubator that used a damper controlled by a thermostat to maintain a constant temperature. The magic lantern is an optical device, an early type of image projector developed in the 17th century.  People have been projecting images using concave mirrors and pin-hole cameras (camera obscura) since Roman times.  But glass lens technology wasn't sufficiently developed to make advanced optical devices (such as telescope and microscope) until the 17th century.  With pinhole cameras and camera obscura it was only possible to project an image of actual scene, such as an image of the sun, on a surface.  The magic lantern on the other hand could project a painted image on a surface, and marks the point where cameras and projectors became two different kinds of devices.  There has been some debate about who the original inventor of the magic lantern is, but the most widely accepted theory is that Christiaan Huygens developed the original device in the late 1650s.  However, other sources give credit to the German priest Athanasius Kircher.  He describes a device such as the magic lantern in his book \"Ars Magna Lucis et Umbrae\".  Huygens is credited because of his major innovation in lantern technology, which was the replacement of images etched on mirrors from earlier lanterns such as Kircher’s with images painted on glass.  This is what paved the way for the use of colour and for double-layered slide projections (generally used to simulate movement). The first allusion to a 'magic lantern' is by Huygens in the 1650s and he is generally credited with inventing it – though he didn't want to admit it, considering it frivolous.  Huygens was the first to describe a fully functioning magic lantern, one he made, and wrote about it in a work in 1659.  Huygens magic lantern has been described as the predecessor of today’s slide projector and the forerunner of the motion picture projector.  Images were hand painted onto the glass slide until the mid-19th century when photographic slides were employed.  Huygens introduced this curiosity to the Danish mathematician Thomas Walgenstein who realized its commercial value for entertainment and traveled through Europe – mostly France and Italy – demonstrating his machine to foreign princes and selling them replicas for their own amusement.  The forerunner of the modern slide projector as well as moving pictures, magic lanterns retained their popularity for centuries and were also the first optical toy to be used for family entertainment in the home. In Amsterdam, the Superintendent of the Fire Brigade, Jan van der Heyden, and his son Nicholaas took firefighting to its next step with the fashioning of the first fire hose in 1673. A gunpowder engine, also known as an \"explosion engine\" or \"Huygens' engine\", is a type of internal combustion engine using gunpowder as its fuel.  It was considered essentially as the first rudimentary internal combustion piston engine.  The concept was first explored during the 17th century, most notably by the Dutch scientist Christiaan Huygens.  In 1678 he outlined a gunpowder engine consisting of a vertical tube containing a piston.  Gunpowder was inserted into the tube and lit through a small hole at the base, like a cannon.  The expanding gasses would drive the piston up the tube until the reached a point near the top.  Here, the piston uncovered holes in the tube that allowed any remaining hot gasses to escape.  The weight of the piston and the vacuum formed by the cooling gasses in the now-closed cylinder drew the piston back into the tube, lifting a test mass to provide power.  According to sources, a single example of this sort of engine was built in 1678 or 79 using a cannon as the cylinder.  The cylinder was held down to a base where the gunpowder sat, making it a breech loading design.  The gasses escaped via two leather tubes attached at the top of the barrel.  When the piston reached them the gasses blew the tubes open, and when the pressure fell, gravity pulled the leather down causing the tubes droop to the side of the cylinder, sealing the holes.  Huygens’ presented a paper on his invention in 1680, \"A New Motive Power by Means of Gunpowder and Air\".  By 1682, the device had successfully shown that a dram (1/16th of an ounce) of gunpowder, in a cylinder seven or eight feet high and fifteen or eighteen inches in diameter, could raise seven or eight boys (or about 1,100 pounds) into the air, who held the end of the rope. The Hollander beater is a machine developed by the Dutch in 1680 to produce pulp from cellulose-containing plant fibers.  It replaced stamp mills for preparing pulp because the Hollander could produce in one day the same quantity of pulp that a stamp mill could produce in eight. In 1783, Maastricht-born chemist Jan Pieter Minckelers used coal gas for lighting and developed the first form of gas lighting. A \"meat slicer\", also called a \"slicing machine\", \"deli slicer\" or simply a \"slicer\", is a tool used in butcher shops and delicatessens to slice meats and cheeses.  The first meat slicer was invented by Wilhelm van Berkel (Wilhelmus Adrianus van Berkel) in Rotterdam in 1898.  Older models of meat slicer may be operated by crank, while newer ones generally use an electric motor. A pentode is an electronic device having five active electrodes.  The term most commonly applies to a three-grid vacuum tube (thermionic valve), which was invented by the Dutchman Bernhard D.H. Tellegen in 1926. Philishave was the brand name for electric shavers manufactured by the Philips Domestic Appliances and Personal Care unit of Philips (in the US, the Norelco name is used).  The Philishave shaver was invented by Philips engineer Alexandre Horowitz, who used rotating cutters instead of the reciprocating cutters that had been used in previous electric shavers. A gyrator is a passive, linear, lossless, two-port electrical network element invented by Tellegen as a hypothetical fifth linear element after the resistor, capacitor, inductor and ideal transformer. Dutch company \"Gatsometer BV\", founded by the 1950s rally driver \"Maurice Gatsonides\", invented the first traffic enforcement camera.  Gatsonides wished to better monitor his speed around the corners of a race track and came up with the device in order to improve his time around the circuit.  The company developed the first radar for use with road traffic and is the world's largest supplier of speed-monitoring camera systems.  Because of this, in some countries speed cameras are sometimes referred to as \"Gatsos\".  They are also sometimes referred to as \"photo radar\", even though many of them do not use radar. The first systems introduced in the late 1960s used film cameras, replaced by digital cameras beginning in the late 1990s. Variomatic is the stepless, fully automatic transmission of the Dutch car manufacturer DAF, originally developed by Hub van Doorne.  The Variomatic was introduced in 1958 (DAF 600), the first automatic gear box made in the Netherlands.  It continues in use in motorscooters. Variomatic was the first commercially successful continuously variable transmissions (CVT). A Red light camera is a traffic enforcement camera that captures an image of a vehicle that enters an intersection against a red traffic light.  By automatically photographing such vehicles, the camera produces evidence that assists authorities in their enforcement of traffic laws.  The first red light camera system was introduced in 1965, using tubes stretched across the road to detect the violation and trigger the camera.  One of the first developers of these red light camera systems was Dutch company Gatsometer BV. Stochastic cooling is a form of particle beam cooling.  It is used in some particle accelerators and storage rings to control the emission of particle beams.  This process uses the electrical signals that the individual charged particles generate in a feedback loop to reduce the tendency of individual particles to move away from other particles in the beam.  This technique was invented and applied at the Intersecting Storage Rings, and later the Super Proton Synchrotron, at CERN in Geneva, Switzerland by Dutch physicist Simon van der Meer.  By increasing the particle density to close to the required energy, this technique improved the beam quality and, inter alia, brought the discovery of W and Z bosons within reach. The clap skate (also called clapskates, slap skates, slapskates) is a type of ice skate used in speed skating.  Clap skates were developed at the Faculty of Human Movement Sciences of the Vrije Universiteit of Amsterdam, led by Gerrit Jan van Ingen Schenau, although the idea is much older.  van Ingen Schenau, who started work on a hinged speed skate in 1979, created his first prototype in 1980 and finished his PhD thesis on the subject in 1981 using the premise that a skater would benefit from extended movement keeping the blade on the ice, allowing the calf muscles more time to exert force. In the 14th century, the Dutch started using wooden platform skates with flat iron bottom runners.  The skates were attached to the skater's shoes with leather straps and poles were used to propel the skater.  Around 1500, the Dutch shifted to a narrow metal double edged blade, so the skater could now push and glide with his feet, eliminating the need for a pole. A herring buss (Dutch: \"Haring Buis\" ) was a type of seagoing fishing vessel, used by Dutch and Flemish herring fishermen in the 15th through early 19th centuries.  The \"Buis\" was first adapted for use as a fishing vessel in the Netherlands, after the invention of gibbing made it possible to preserve herring at sea.  This made longer voyages feasible, and hence enabled Dutch fishermen to follow the herring shoals far from the coasts.  The first herring buss was probably built in Hoorn around 1415.  The last one was built in Vlaardingen in 1841. Originally defined as a light, fast sailing vessel used by the Dutch navy to pursue pirates and other transgressors around and into the shallow waters of the Low Countries.  Later, yachts came to be perceived as luxury, or recreational vessels. Fluyt, a type of sailing vessel originally designed as a dedicated cargo vessel.  Originating from the Netherlands in the 16th century, the vessel was designed to facilitate transoceanic delivery with the maximum of space and crew efficiency.  The inexpensive ship could be built in large numbers.  This ship class was credited with enhancing Dutch competitiveness in international trade and was widely employed by the Dutch East India Company in the 17th and 18th centuries.  The fluyt was a significant factor in the 17th century rise of the Dutch seaborne empire. Cornelis Corneliszoon was the inventor of the wind-powered sawmill.  Prior to the invention of sawmills, boards were rived and planed, or more often sawn by two men with a whipsaw using saddleblocks to hold the log and a pit for the pitman who worked below and got the benefit of sawdust in his eyes.  Sawing was slow and required strong and durable sawmen.  The topsawer had to be the stronger of the two because the saw was pulled in turn by each man, and the lower had the advantage of gravity.  The topsawyer also had to guide the saw to produce a plank of even thickness.  This was often done by following a chalkline. Early sawmills adapted the whipsaw to mechanical power, generally driven by a water wheel to speed up the process.  The circular motion of the wheel was changed to back-and-forth motion of the saw blade by a \"pitman\" thus introducing a term used in many mechanical applications.  A pitman is similar to a crankshaft used in reverse.  A crankshaft converts back-and-forth motion to circular motion. Generally only the saw was powered and the logs had to be loaded and moved by hand.  An early improvement was the development of a movable carriage, also water powered, to steadily advance the log through the saw blade. A schooner is a type of sailing vessel with fore-and-aft sails on two or more masts, the foremast being no taller than the rear mast(s).  Such vessels were first used by the Dutch in the 16th or 17th century (but may not have been called that at the time).  Schooners first evolved from a variety of small two-masted gaff-rigged vessels used in the coast and estuaries of the Netherlands in the late 17th century.  Most were working craft but some pleasure yachts with schooner rigs were built for wealthy merchants and Dutch nobility.  Following arrival of the Dutch-born prince William III the Orange on the British throne, the British Royal Navy built a Royal yacht with a schooner rig in 1695, HMS \"Royal Transport\".  This vessel, captured in a detailed Admiralty model, is the earliest fully documented schooner.  \"Royal Transport\" was quickly noted for its speed and ease of handling and mercantile vessels soon adopted the rig in Europe and in European colonies in North America.  Schooners were immediately popular with colonial traders and fishermen in North America with the first documented reference to a schooner in America appearing in Boston port records in 1716.  North American shipbuilders quickly developed a variety of schooner forms for trading, fishing and privateering.  According to the language scholar Walter William Skeat, the term \"schooner\" comes from \"scoon\", while the \"sch\" spelling comes from the later adoption of the Dutch spelling (\"schoener\").  Another study suggests that a Dutch expression praising ornate schooner yachts in the 17th century, \"een schoone Schip\", may have led to the term \"schooner\" being used by English speakers to describe the early versions of the schooner rig as it evolved in England and America. The Wind chariot or land yacht (Zeilwagen) was designed by Flemish-born mathematician & engineer Simon Stevin for Prince Maurice of Orange.  Land yacht.  It offered a carriage with sails, of which a little model was preserved in Scheveningen until 2012.  Around the year 1600, Stevin, Maurice and twenty-six others used it on the beach between Scheveningen and Petten.  The carriage was propelled solely by force of wind, and traveled faster than horse-drawn vehicles. A replica of reduced scale of Drebbel's submarine built by the team of the TV-series \"Building the Impossible\" (2002). Cornelius Drebbel was the inventor of the first navigable submarine, while working for the British Royal Navy.  He designed and manufactured a steerable submarine with a leather-covered wooden frame.  Between 1620 and 1624 Drebbel successfully built and tested two more, successively larger vessels.  The third model had 6 oars and could carry 16 passengers.  This model was demonstrated to King James I and several thousand Londoners.  The submarine stayed submerged for three hours and could travel from Westminster to Greenwich and back, cruising at a depth of from 12 to .  This submarine was tested many times in the Thames, but never used in battle. In 2002, the British boatbuilder Mark Edwards built a wooden submarine based on the original 17th-century version by Drebbel.  This was shown in the BBC TV programme \"Building the Impossible\" in November 2002.  It is a scale working model of the original and was built using tools and construction methods common in 17th century boat building and was successfully tested under water with two rowers at Dorney Lake, diving beneath the surface and being rowed underwater for 10 minutes.  Legal considerations prevented its use on the River Thames itself. Spyker is credited with building and racing the first ever four-wheel racing car in 1903.  The first four-wheel-drive car, as well as hill-climb racer, with internal combustion engine, the \"Spyker 60 H.P.\", was presented in 1903 by Dutch brothers Jacobus and Hendrik-Jan Spijker of Amsterdam.  The two-seat sports car, which was also the first ever car equipped with a six-cylinder engine, is now an exhibit in the Louwman Collection (the former \"Nationaal Automobiel Museum\") at the Hague in The Netherlands. \"Wilhelmus van Nassouwe\" (\"Het Wilhelmus\") is the national anthem of the Netherlands and is the oldest national anthem in the world.  The anthem was first written down in 1574 (during the Dutch Revolt).  The Japanese anthem, Kimigayo, has the oldest (9th century) lyrics, but a melody was only added in the late 19th century, making it a poem rather than an anthem for most of its lifespan.  Although the Wilhelmus was not officially recognised as the Dutch national anthem until 1932, it has always been popular with parts of the Dutch population and resurfaced on several occasions in the course of Dutch history before gaining its present status. Java Man (\"Homo erectus erectus\") is the name given to hominid fossils discovered in 1891 at Trinil – Ngawi Regency on the banks of the Solo River in East Java, Indonesia, one of the first known specimens of Homo erectus.  Its discoverer, Dutch paleontologist Eugène Dubois, gave it the scientific name Pithecanthropus erectus, a name derived from Greek and Latin roots meaning \"upright ape-man\". Columba is a small, faint constellation named in the late sixteenth century.  Its name is Latin for dove.  It is located just south of Canis Major and Lepus.  Columba was named by Dutch astronomer Petrus Plancius in 1592 in order to differentiate the 'unformed stars' of the large constellation Canis Major.  Plancius first depicted Columba on the small celestial planispheres of his large wall map of 1592.  It is also shown on his smaller world map of 1594 and on early Dutch celestial globes. The first person to record the Novaya Zemlya effect was Gerrit de Veer, a member of Willem Barentsz' ill-fated third expedition into the polar region.  Novaya Zemlya, the archipelago where de Veer first observed the phenomenon, lends its name to the effect. Plancius defined 12 constellations created by Plancius from the observations of Pieter Dirkszoon Keyser and Frederick de Houtman. \"Camelopardalis\" was created by Plancius in 1613 to represent the animal Rebecca rode to marry Isaac in the Bible.  One year later, Jakob Bartsch featured it in his atlas.  Johannes Hevelius gave it the official name of \"Camelopardus\" or \"Camelopardalis\" because he saw the constellation's many faint stars as the spots of a giraffe. \"Monoceros\" is a relatively modern creation.  Its first certain appearance was on a globe created by Plancius in 1612 or 1613.  It was later charted by Bartsch as \"Unicornus\" in his 1624 star chart. In 1655, Huygens became the first person to suggest that Saturn was surrounded by a ring, after Galileo's much less advanced telescope had failed to show rings.  Galileo had reported the anomaly as possibly 3 planets instead of one. In 1655, using a 50 power refracting telescope that he designed himself, Huygens discovered the first of Saturn's moons, Titan. Kapteyn's Star is a class M1 red dwarf about 12.76 light years from Earth in the southern constellation Pictor, and the closest halo star to the Solar System.  With a magnitude of nearly 9 it is visible through binoculars or a telescope.  It had the highest proper motion of any star known until the discovery of Barnard's Star in 1916.  Attention was first drawn to what is now known as Kapteyn's Star by the Dutch astronomer Jacobus Kapteyn, in 1897. In 1904, studying the proper motions of stars, Dutch astronomer Jacobus Kapteyn reported that these were not random, as it was believed in that time; stars could be divided into two streams, moving in nearly opposite directions.  It was later realized that Kapteyn's data had been the first evidence of the rotation of our Galaxy, which ultimately led to the finding of galactic rotation by Bertil Lindblad and Jan Oort. In 1924, Dutch astronomer Jan Oort the galactic halo, a group of stars orbiting the Milky Way but outside the main disk. The Oort constants (discovered by Jan Oort) formula_1 and formula_2 are empirically derived parameters that characterize the local rotational properties of the Milky Way. In 1932, Dutch astronomer Jan Oort became the first person to discover evidence of dark matter.  Oort proposed the substance after measuring the motions of nearby stars in the Milky Way relative to the galactic plane.  He found that the mass of the galactic plane must be more than the mass of the material that can be seen.  A year later (1933), Fritz Zwicky examined the dynamics of clusters of galaxies and found their movements similarly perplexing. The first formal proof of the existence of an atmosphere around Titan came in 1944, when Gerald Kuiper observed Titan with the new McDonald 82 in telescope and discovered spectral signatures on Titan at wavelengths longer than 0.6 μm (micrometers), among which he identified two absorption bands of methane at 6190 and 7250 Å (Kuiper1944).  This discovery was significant not only because it requires a dense atmosphere with a significant fraction of methane, but also because the atmosphere needs to be chemically evolved, since methane requires hydrogen in the presence of carbon, and molecular and atomic hydrogen would have escaped from Titan's weak gravitational field since the formation of the solar system. Using infrared spectrometry, in 1947 the Dutch-American astronomer Gerard Kuiper detected carbon dioxide in the Martian atmosphere, a discovery of biological significance because it is a principal gas in the process of photosynthesis (see also: History of Mars observation).  He was able to estimate that the amount of carbon dioxide over a given area of the surface is double that on the Earth. Miranda is the smallest and innermost of Uranus's five major moons.  It was discovered by Gerard Kuiper on 16 February 1948 at McDonald Observatory. Nereid, also known as Neptune II, is the third-largest moon of Neptune and was its second moon to be discovered, on 1 May 1949, by Gerard Kuiper, on photographic plates taken with the 82-inch telescope at McDonald Observatory. The \"Oort cloud\" or \"Öpik–Oort cloud\", named after Dutch astronomer Jan Oort and Estonian astronomer Ernst Öpik, is a spherical cloud of predominantly icy planetesimals believed to surround the Sun at a distance of up to 50,000 AU .  Further evidence for the existence of the Kuiper belt emerged from the study of comets.  That comets have finite lifespans has been known for some time.  As they approach the Sun, its heat causes their volatile surfaces to sublimate into space, gradually evaporating them.  In order for comets to continue to be visible over the age of the Solar System, they must be replenished frequently.  One such area of replenishment is the Oort cloud, a spherical swarm of comets extending beyond 50,000 AU from the Sun first hypothesised by Dutch astronomer in 1950.  The Oort cloud is believed to be the point of origin of long-period comets, which are those, like Hale–Bopp, with orbits lasting thousands of years. The Kuiper belt was named after Dutch-American astronomer Gerard Kuiper, regarded by many as the father of modern planetary science, though his role in hypothesising it has been heavily contested.  In 1951, he proposed the existence of what is now called the Kuiper Belt, a disk-shaped region of minor planets outside the orbit of Neptune, which also is a source of short-period comets. In the 1660s and 1670s the Dutch Republic-based scientists (in particular Leiden University-based Jan Swammerdam and Nicolas Steno, and Delft-based Regnier de Graaf and Anton van Leeuwenhoek) made key discoveries about animal and human reproduction.  Their research and discoveries contributed greatly to the modern understanding of the female mammalian reproductive system.  Many authors see Regnier de Graaf as the founder of modern reproductive biology (Setchell, 1974).  This is due essentially to his use of convergent scientific methods: meticulous dissections, clinical observations and critical analysis of the available literature (Ankumet al., 1996). Dutch physician & anatomist Regnier de Graaf may have been the first to understand the reproductive function of the Fallopian tubes.  He described the hydrosalpinx, linking its development to female infertility.  de Graaf recognized pathologic conditions of the tubes.  He was aware of tubal pregnancies, and he surmised that the mammalian egg traveled from the ovary to the uterus through the tube. In his \"De Mulierum Organis Generatione Inservientibus\" (1672), de Graaf provided the first thorough description of the female gonad and established that it produced the ovum.  De Graaf used the terminology vesicle or egg (ovum) for what now called the ovarian follicle.  Because the fluid-filled ovarian vesicles had been observed previously by others, including Andreas Vesalius and Falloppio, De Graaf did not claim their discovery.  He noted that he was not the first to describe them, but to describe their development.  De Graaf was the first to observe changes in the ovary before and after mating and describe the corpus luteum.  From the observation of pregnancy in rabbits, he concluded that the follicle contained the oocyte.  The mature stage of the ovarian follicle is called the Graafian follicle in his honour, although others, including Fallopius, had noticed it previously but failed to recognize its reproductive significance. Antonie van Leeuwenhoek is often considered to be the father of microbiology.  Robert Hooke is cited as the first to record microscopic observation of the fruiting bodies of molds, in 1665.  However, the first observation of microbes using a microscope is generally credited to van Leeuwenhoek.  In the 1670s, he observed and researched bacteria and other microorganisms, using a single-lens microscope of his own design. In 1981 the British microscopist Brian J. Ford found that Leeuwenhoek's original specimens had survived in the collections of the Royal Society of London.  They were found to be of high quality, and were all well preserved.  Ford carried out observations with a range of microscopes, adding to our knowledge of Leeuwenhoek's work. Photosynthesis is a fundamental biochemical process in which plants, algae, and some bacteria convert sunlight to chemical energy.  The process was discovered by Jan Ingenhousz in 1779.  The chemical energy is used to drive reactions such as the formation of sugars or the fixation of nitrogen into amino acids, the building blocks for protein synthesis.  Ultimately, nearly all living things depend on energy produced from photosynthesis.  It is also responsible for producing the oxygen that makes animal life possible.  Organisms that produce energy through photosynthesis are called photoautotrophs. Plants are the most visible representatives of photoautotrophs, but bacteria and algae also employ the process. Plant respiration was also discovered by Ingenhousz in 1779. Martinus Beijerinck is considered one of the founders of virology.  In 1898, he published results on his filtration experiments, demonstrating that tobacco mosaic disease is caused by an infectious agent smaller than a bacterium.  His results were in accordance with similar observations made by Dmitri Ivanovsky in 1892.  Like Ivanovsky and Adolf Mayer, predecessor at Wageningen, Beijerinck could not culture the filterable infectious agent.  He concluded that the agent can replicate and multiply in living plants.  He named the new pathogen \"virus\" to indicate its non-bacterial nature.  This discovery is considered to be the beginning of virology. In 1931, Cornelis van Niel made key discoveries explaining the chemistry of photosynthesis.  By studying purple sulfur bacteria and green sulfur bacteria, he was the first scientist to demonstrate that photosynthesis is a light-dependent redox reaction, in which hydrogen reduces carbon dioxide.  Expressed as: where A is the electron acceptor.  His discovery predicted that HO is the hydrogen donor in green plant photosynthesis and is oxidized to O.  The chemical summation of photosynthesis was a milestone in the understanding of the chemistry of photosynthesis.  This was later experimentally verified by Robert Hill. Many naturalists have studied aspects of animal behaviour throughout history.  Ethology has its scientific roots in the work of Charles Darwin and of American and German ornithologists of the late 19th and early 20th century, including Charles O. Whitman, Oskar Heinroth, and Wallace Craig.  The modern discipline of ethology is generally considered to have begun during the 1930s with the work of Dutch biologist Nikolaas Tinbergen and by Austrian biologists Konrad Lorenz and Karl von Frisch. Tinbergen's four questions, named after Nikolaas Tinbergen, one of the founders of modern ethology, are complementary categories of explanations for behaviour.  It suggests that an integrative understanding of behaviour must include both a proximate and ultimate (functional) analysis of behaviour, as well as an understanding of both phylogenetic/developmental history and the operation of current mechanisms. The Vroman effect, named after Leo Vroman, is exhibited by protein adsorption to a surface by blood serum proteins. Flemish physician Jan Baptist van Helmont is sometimes considered the founder of pneumatic chemistry, coining the word \"gas\" and conducting experiments involving gases.  Van Helmont had derived the word “gas” from the Dutch word \"geest\", which means ghost or spirit. Dutch chemist Jacobus Henricus van 't Hoff is generally considered to be one of the founders of the field of stereochemistry.  In 1874, Van 't Hoff built on the work on isomers of German chemist Johannes Wislicenus, and showed that the four valencies of the carbon atom were probably directed in space toward the four corners of a regular tetrahedron, a model which explained how optical activity could be associated with an asymmetric carbon atom.  He shares credit for this with the French chemist Joseph Le Bel, who independently came up with the same idea.  Three months before his doctoral degree was awarded Van 't Hoff published this theory, which today is regarded as the foundation of stereochemistry, first in a Dutch pamphlet in the fall of 1874, and then in the following May in a small French book entitled \"La chimie dans l'espace\".  A German translation appeared in 1877, at a time when the only job Van 't Hoff could find was at the Veterinary School in Utrecht.  In these early years his theory was largely ignored by the scientific community, and was sharply criticized by one prominent chemist, Hermann Kolbe.  However, by about 1880 support for Van 't Hoff's theory by such important chemists as Johannes Wislicenus and Viktor Meyer brought recognition. Jacobus van 't Hoff is also considered as one of the modern founders of the disciple of physical chemistry.  The first scientific journal specifically in the field of physical chemistry was the German journal, \"Zeitschrift für Physikalische Chemie\", founded in 1887 by Wilhelm Ostwald and Van 't Hoff.  Together with Svante Arrhenius, these were the leading figures in physical chemistry in the late 19th century and early 20th century. The Van 't Hoff equation in chemical thermodynamics relates the change in the equilibrium constant, \"K\", of a chemical equilibrium to the change in temperature, \"T\", given the standard enthalpy change, \"ΔH\", for the process.  It was proposed by Dutch chemist Jacobus Henricus van 't Hoff in 1884.  The \"Van 't Hoff equation\" has been widely utilized to explore the changes in state functions in a thermodynamic system.  The \"Van 't Hoff plot\", which is derived from this equation, is especially effective in estimating the change in enthalpy, or total energy, and entropy, or amount of disorder, of a chemical reaction. The van 't Hoff factor formula_3 is a measure of the effect of a solute upon colligative properties such as osmotic pressure, relative lowering in vapor pressure, elevation of boiling point and freezing point depression.  The van 't Hoff factor is the ratio between the actual concentration of particles produced when the substance is dissolved, and the concentration of a substance as calculated from its mass. In carbohydrate chemistry, the Lobry de Bruyn–van Ekenstein transformation is the base or acid-catalyzed transformation of an aldose into the ketose isomer or vice versa, with a tautomeric enediol as reaction intermediate.  The transformation is relevant for the industrial production of certain ketoses and was discovered in 1885 by Cornelis Adriaan Lobry van Troostenburg de Bruyn and Willem Alberda van Ekenstein. The Prins reaction is an organic reaction consisting of an electrophilic addition of an aldehyde or ketone to an alkene or alkyne followed by capture of a nucleophile.  Dutch chemist Hendrik Jacobus Prins discovered two new organic reactions, both now carrying the name Prins reaction.  The first was the addition of polyhalogen compounds to olefins, was found during Prins doctoral research, while the others, the acid-catalyzed addition of aldehydes to olefinic compounds, became of industrial relevance. Dutch physicist Dirk Coster and Hungarian-Swedish chemist George de Hevesy co-discovered \"Hafnium\" (Hf) in 1923, by means of X-ray spectroscopic analysis of zirconium ore.  \"Hafnium' is named after \"Hafnia', the Latin name for Copenhagen (Denmark), where it was discovered. The crystal bar process (also known as \"iodide process\" or the \"van Arkel–de Boer process\") was developed by Dutch chemists Anton Eduard van Arkel and Jan Hendrik de Boer in 1925.  It was the first industrial process for the commercial production of pure ductile metallic zirconium.  It is used in the production of small quantities of ultra-pure titanium and zirconium. Koopmans' theorem states that in closed-shell Hartree–Fock theory, the first ionization energy of a molecular system is equal to the negative of the orbital energy of the highest occupied molecular orbital (HOMO).  This theorem is named after Tjalling Koopmans, who published this result in 1934. Koopmans became a Nobel laureate in 1975, though neither in physics nor chemistry, but in economics. In 1889, Dutch botanist Hugo de Vries published his book \"Intracellular Pangenesis\", in which he postulated that different characters have different hereditary carriers, based on a modified version of Charles Darwin's theory of Pangenesis of 1868.  He specifically postulated that inheritance of specific traits in organisms comes in \"particles\".  He called these units \"pangenes\", a term shortened in 1909 to genes by Danish botanist Wilhelm Johannsen. 1900 marked the \"rediscovery of Mendelian genetics\".  The significance of Gregor Mendel's work was not understood until early in the twentieth century, after his death, when his research was re-discovered by Hugo de Vries, Carl Correns and Erich von Tschermak, who were working on similar problems.  They were unaware of Mendel's work.  They worked independently on different plant hybrids, and came to Mendel's conclusions about the rules of inheritance. The Bushveld Igneous Complex (or BIC) is a large, layered igneous intrusion within the Earth's crust that has been tilted and eroded and now outcrops around what appears to be the edge of a great geological basin, the Transvaal Basin.  Located in South Africa, the BIC contains some of Earth's richest ore deposits.  The complex contains the world's largest reserves of platinum group metals (PGMs), platinum, palladium, osmium, iridium, rhodium, and ruthenium, along with vast quantities of iron, tin, chromium, titanium and vanadium.  The site was discovered around 1897 by Dutch geologist Gustaaf Molengraaff. Descartes (1596–1650) was born in France, but spent most of his adult life in the Dutch Republic.  As Bertrand Russell noted in his \"A History of Western Philosophy\" (1945): \"He lived in Holland for twenty years (1629–49), except for a few brief visits to France and one to England, all on business...\".  In 1637, Descartes published his work on the methods of science, \"Discours de la méthode\" in Leiden.  One of its three appendices was \"La Géométrie\", in which he outlined a method to connect the expressions of algebra with the diagrams of geometry.  It combined both algebra and geometry under one specialty – algebraic geometry, now called analytic geometry, which involves reducing geometry to a form of arithmetic and algebra and translating geometric shapes into algebraic equations. Descartes' \"La Géométrie\" contains Descartes' first introduction of the Cartesian coordinate system. Christiaan Huygens was the first to publish in 1673 (\"Horologium Oscillatorium\") a specific method of determining the evolute and involute of a curve In mathematics, the Korteweg–de Vries equation (KdV equation for short) is a mathematical model of waves on shallow water surfaces.  It is particularly notable as the prototypical example of an exactly solvable model, that is, a non-linear partial differential equation whose solutions can be exactly and precisely specified.  The equation is named for Diederik Korteweg and Gustav de Vries who, in 1895, proposed a mathematical model which allowed to predict the waves behaviour on shallow water surfaces. Brouwer fixed-point theorem is a fixed-point theorem in topology, named after Dutchman Luitzen Brouwer, who proved it in 1911. The hairy ball theorem of algebraic topology states that there is no nonvanishing continuous tangent vector field on even-dimensional \"n\"-spheres.  The theorem was first stated by Henri Poincaré in the late 19th century.  It was first proved in 1912 by Brouwer. The Debye functions are named in honor of Peter Debye, who came across this function (with \"n\" = 3) in 1912 when he analytically computed the heat capacity of what is now called the Debye model. The Kramers–Kronig relations are bidirectional mathematical relations, connecting the real and imaginary parts of any complex function that is analytic in the upper half-plane.  The relation is named in honor of Ralph Kronig and Hendrik Anthony Kramers. Formalized intuitionistic logic was originally developed by Arend Heyting to provide a formal basis for Luitzen Brouwer's programme of intuitionism.  Arend Heyting introduced Heyting algebra (1930) to formalize intuitionistic logic. In mathematics, the Zernike polynomials are a sequence of polynomials that are orthogonal on the unit disk.  Named after Frits Zernike, the Dutch optical physicist, and the inventor of phase contrast microscopy, they play an important role in beam optics. In 1941, Marcel Minnaert invented the Minnaert function, which is used in optical measurements of celestial bodies.  The Minnaert function is a photometric function used to interpret astronomical observations and remote sensing data for the Earth. In 1586, Simon Stevin (Stevinus) derived the mechanical advantage of the inclined plane by an argument that used a string of beads.  Stevin's proof of the law of equilibrium on an inclined plane, known as the \"Epitaph of Stevinus\". Christiaan Huygens stated what is now known as the second of Newton's laws of motion in a quadratic form.  In 1659 he derived the now standard formula for the centripetal force, exerted by an object describing a circular motion, for instance on the string to which it is attached.  In modern notation: with \"m\" the mass of the object, \"v\" the velocity and \"r\" the radius.  The publication of the general formula for this force in 1673 was a significant step in studying orbits in astronomy.  It enabled the transition from Kepler's third law of planetary motion, to the inverse square law of gravitation. Huygens coined the term \"centrifugal force\" in his 1659 \"De Vi Centrifiga\" and wrote of it in his 1673 \"Horologium Oscillatorium\" on pendulums. In 1659, Christiaan Huygens was the first to derive the formula for the period of an ideal mathematical pendulum (with massless rod or cord and length much longer than its swing), in modern notation: with \"T\" the period, \"l\" the length of the pendulum and \"g\" the gravitational acceleration.  By his study of the oscillation period of compound pendulums Huygens made pivotal contributions to the development of the concept of moment of inertia. A tautochrone or isochrone curve is the curve for which the time taken by an object sliding without friction in uniform gravity to its lowest point is independent of its starting point.  The curve is a cycloid, and the time is equal to π times the square root of the radius over the acceleration of gravity.  Christiaan Huygens was the first to discover the tautochronous property (or isochronous property) of the cycloid.  The tautochrone problem, the attempt to identify this curve, was solved by Christiaan Huygens in 1659.  He proved geometrically in his \"Horologium Oscillatorium\", originally published in 1673, that the curve was a cycloid.  Huygens also proved that the time of descent is equal to the time a body takes to fall vertically the same distance as the diameter of the circle which generates the cycloid, multiplied by π⁄2.  The tautochrone curve is the same as the brachistochrone curve for any given starting point.  Johann Bernoulli posed the problem of the brachistochrone to the readers of \"Acta Eruditorum\" in June, 1696.  He published his solution in the journal in May of the following year, and noted that the solution is the same curve as Huygens's tautochrone curve. Christiaan Huygens observed that two pendulum clocks mounted next to each other on the same support often become synchronized, swinging in opposite directions.  In 1665, he reported the results by letter to the Royal Society of London.  It is referred to as \"an odd kind of sympathy\" in the Society's minutes.  This may be the first published observation of what is now called \"coupled oscillations\".  In the 20th century, \"coupled oscillators\" took on great practical importance because of two discoveries: lasers, in which different atoms give off light waves that oscillate in unison, and superconductors, in which pairs of electrons oscillate in synchrony, allowing electricity to flow with almost no resistance.  \"Coupled oscillators\" are even more ubiquitous in nature, showing up, for example, in the synchronized flashing of fireflies and chirping of crickets, and in the pacemaker cells that regulate heartbeats. Flemish anatomist and physician Andreas Vesalius is often referred to as the founder of modern human anatomy for the publication of the seven-volume \"De humani corporis fabrica\" (\"On the Structure of the Human Body\") in 1543. In 1679, van Leeuwenhoek used a microscopes to assess tophaceous material and found that gouty tophi consist of aggregates of needle-shaped crystals, and not globules of chalk as was previously believed. Boerhaave syndrome (also known as \"spontaneous esophageal perforation\" or \"esophageal rupture\") refers to an esophageal rupture secondary to forceful vomiting.  Originally described in 1724 by Dutch physician/botanist Hermann Boerhaave, it is a rare condition with high mortality.  The syndrome was described after the case of a Dutch admiral, Baron Jan von Wassenaer, who died of the condition. Factor V Leiden is an inherited disorder of blood clotting.  It is a variant of human factor V that causes a hypercoagulability disorder.  It is named after the city Leiden, where it was first identified by R. Bertina, et al., in 1994. In 1658 Dutch naturalist Jan Swammerdam was the first person to observe red blood cells under a microscope and in 1695, microscopist Antoni van Leeuwenhoek, also Dutch, was the first to draw an illustration of \"red corpuscles\", as they were called.  No further blood cells were discovered until 1842 when the platelets were discovered. The first person to observe and describe red blood cells was Dutch biologist Jan Swammerdam, who had used an early microscope to study the blood of a frog. A resident of Delft, Anton van Leeuwenhoek, used a high-power single-lens simple microscope to discover the world of micro-organisms.  His simple microscopes were made of silver or copper frames, holding hand-ground lenses were capable of magnification up to 275 times.  Using these he was the first to observe and describe single-celled organisms, which he originally referred to as \"animalcules\", and which now referred to as micro-organisms or microbes. Volvox (1700)- Volvox is a genus of chlorophytes, a type of green algae.  It forms spherical colonies of up to 50,000 cells.  They live in a variety of freshwater habitats, and were first reported by Van Leeuwenhoek in 1700. Biological nitrogen fixation was discovered by Martinus Beijerinck in 1885. \"Rhizobium\" is a genus of Gram-negative soil bacteria that fix nitrogen.  Rhizobium forms an endosymbiotic nitrogen fixing association with roots of legumes and \"Parasponia\".  Martinus Beijerinck in the Netherlands was the first to isolate and cultivate a microorganism from the nodules of legumes in 1888.  He named it \"Bacillus radicicola\", which is now placed in \"Bergey's Manual of Determinative Bacteriology\" under the genus Rhizobium. Martinus Beijerinck discovered the phenomenon of bacterial sulfate reduction, a form of anaerobic respiration.  He learned that bacteria could use sulfate as a terminal electron acceptor, instead of oxygen.  He isolated and described \"Spirillum desulfuricans\" (now called \"Desulfovibrio desulfuricans\"), the first known sulfate-reducing bacterium. In 1898 Beijerinck coined the term \"virus\" to indicate that the causal agent of tobacco mosaic disease was non-bacterial.  Beijerinck discovered what is now known as the tobacco mosaic virus.  He observed that the agent multiplied only in cells that were dividing and he called it a contagium vivum fluidum (\"contagious living fluid\").  Beijerinck's discovery is considered to be the beginning of virology. \"Azotobacter\" is a genus of usually motile, oval or spherical bacteria that form thick-walled cysts and may produce large quantities of capsular slime.  They are aerobic, free-living soil microbes which play an important role in the nitrogen cycle in nature, binding atmospheric nitrogen, which is inaccessible to plants, and releasing it in the form of ammonium ions into the soil.  Apart from being a model organism, it is used by humans for the production of biofertilizers, food additives, and some biopolymers.  The first representative of the genus, \"Azotobacter chroococcum\", was discovered and described in 1901 by the Dutch microbiologist and botanist Martinus Beijerinck. Beijerinck is credited with developing the first enrichment culture, a fundamental method of studying microbes from the environment. Division of the octave into 31 steps arose naturally out of Renaissance music theory; the lesser diesis – the ratio of an octave to three major thirds, 128:125 or 41.06 cents – was approximately a fifth of a tone and a third of a semitone.  In 1666, Lemme Rossi first proposed an equal temperament of this order.  Shortly thereafter, having discovered it independently, scientist Christiaan Huygens wrote about it also.  Since the standard system of tuning at that time was quarter-comma meantone, in which the fifth is tuned to 5, the appeal of this method was immediate, as the fifth of 31-et, at 696.77 cents, is only 0.19 cent wider than the fifth of quarter-comma meantone.  Huygens not only realized this, he went farther and noted that 31-ET provides an excellent approximation of septimal, or 7-limit harmony.  In the twentieth century, physicist, music theorist and composer Adriaan Fokker, after reading Huygens's work, led a revival of interest in this system of tuning which led to a number of compositions, particularly by Dutch composers.  Fokker designed the Fokker organ, a 31-tone equal-tempered organ, which was installed in Teyler's Museum in Haarlem in 1951. Through his fundamental contributions Christiaan Huygens helped shape and lay the foundations of classical mechanics.  His works cover all the fields of mechanics, from the invention of technical devices applicable to different machines to a purely rational knowledge of motion.  Huygens published his results in a classic of the 17th-century mechanics, \"Horologium Oscillatorium\" (1673), that is regarded as one of the three most important work done in mechanics in the 17th century, the other two being Galileo Galilei’s \"Discourses and Mathematical Demonstrations Relating to Two New Sciences\" (1638) and Isaac Newton's \"Philosophiæ Naturalis Principia Mathematica\" (1687).  It is Huygens' major work on pendulums and horology.  As Domenico Bertoloni Meli (2006) notes, \"Horologium Oscillatorium\" was “a masterful combination of sophisticated mathematics and mechanics mixed with a range of practical applications culminating with a new clock aimed at resolving the vexing problem of longitude.” Huygens' groundbreaking research on the nature of light helped lay the foundations of modern optics (physical optics in particular).  Huygens is remembered especially for his wave theory of light, which he first communicated in 1678 to France's Royal Académie des sciences and which he published in 1690 in his \"Treatise on light\".  His argument that light consists of waves now known as the Huygens–Fresnel principle, two centuries later became instrumental in the understanding of wave–particle duality.  The interference experiments of Thomas Young vindicated Huygens' s wave theory in 1801. In 1678, Huygens discovered the polarization of light by double refraction in calcite. In his \"Treatise on light\", Huygens showed how Snell's law of sines could be explained by, or derived from, the wave nature of light, using the Huygens–Fresnel principle. Bernoulli's principle was discovered by Dutch-Swiss mathematician and physicist Daniel Bernoulli and named after him.  It states that for an inviscid flow, an increase in the speed of the fluid occurs simultaneously with a decrease in pressure or a decrease in the fluid's potential energy. In 1785, Ingenhousz described the irregular movement of coal dust on the surface of alcohol and therefore has a claim as discoverer of what came to be known as Brownian motion. The law takes its name from Dutch meteorologist C. H. D. Buys Ballot, who published it in the \"Comptes Rendus\", in November 1857.  While William Ferrel first theorized this in 1856, Buys Ballot was the first to provide an empirical validation.  The law states that in the Northern Hemisphere, if a person stands with his back to the wind, the low pressure area will be on his left, because wind travels counterclockwise around low pressure zones in that hemisphere.  this is approximately true in the higher latitudes and is reversed in the Southern Hemisphere. Spearheaded by Mach and Ostwald, a strong philosophical current that denied the existence of molecules arose towards the end of the 19th century.  The molecular existence was considered unproven and the molecular hypothesis unnecessary.  At the time Van der Waals' thesis was written (1873), the molecular structure of fluids had not been accepted by most physicists, and liquid and vapor were often considered as chemically distinct.  But Van der Waals's work affirmed the reality of molecules and allowed an assessment of their size and attractive strength.  By comparing his equation of state with experimental data, Van der Waals was able to obtain estimates for the actual size of molecules and the strength of their mutual attraction.  The effect of Van der Waals's work on molecular science in the 20th century was direct and fundamental, as is well recognized and documented, due in large part to books by John Rowlinson (1988), and by Kipnis and Yavelov (1996).  By introducing parameters characterizing molecular size and attraction in constructing his equation of state, Van der Waals set the tone for molecular physics (molecular dynamics in particular) of the 20th century.  That molecular aspects such as size, shape, attraction, and multipolar interactions should form the basis for mathematical formulations of the thermodynamic and transport properties of fluids is presently considered an axiom. In 1873, J. D. van der Waals introduced the first equation of state derived by the assumption of a finite volume occupied by the constituent molecules.  The Van der Waals equation is generally regarded as the first somewhat realistic equation of state (beyond the ideal gas law).  Van der Waals noted the non-ideality of gases and attributed it to the existence of molecular or atomic interactions.  His new formula revolutionized the study of equations of state, and was most famously continued via the Redlich-Kwong equation of state (1949) and the Soave modification of Redlich-Kwong.  While the Van der Waals equation is definitely superior to the ideal gas law and does predict the formation of a liquid phase, the agreement with experimental data is limited for conditions where the liquid forms.  Except at higher pressures, the real gases do not obey Van der Waals equation in all ranges of pressures and temperatures.  Despite its limitations, the equation has historical importance, because it was the first attempt to model the behaviour of real gases. The van der Waals forces are named after the scientist who first described them in 1873.  Johannes Diderik van der Waals noted the non-ideality of gases and attributed it to the existence of molecular or atomic interactions.  They are forces that develop between the atoms inside molecules and keep them together.  The Van der Waals forces between molecules, much weaker than chemical bonds but present universally, play a fundamental role in fields as diverse as supramolecular chemistry, structural biology, polymer science, nanotechnology, surface science, and condensed matter physics.  Elucidation of the nature of the Van der Waals forces between molecules has remained a scientific effort from Van der Waals's days to the present. The Van der Waals radius, \"r\" , of an atom is the radius of an imaginary hard sphere which can be used to model the atom for many purposes.  It is named after Johannes Diderik van der Waals, winner of the 1910 Nobel Prize in Physics, as he was the first to recognise that atoms were not simply points and to demonstrate the physical consequences of their size through the van der Waals equation of state. The law of corresponding states was first suggested and formulated by van der Waals in 1880.  This showed that the van der Waals equation of state can be expressed as a simple function of the critical pressure, critical volume and critical temperature.  This general form is applicable to all substances.  The compound-specific constants a and b in the original equation are replaced by universal (compound-independent) quantities.  It was this law that served as a guide during experiments which ultimately led to the liquefaction of hydrogen by James Dewar in 1898 and of helium by Heike Kamerlingh Onnes in 1908. Lorentz ether theory has its roots in Hendrik Lorentz's \"theory of electrons\", which was the final point in the development of the classical aether theories at the end of the 19th and at the beginning of the 20th century.  Lorentz's initial theory created in 1892 and 1895 was based on a completely motionless aether.  Many aspects of Lorentz's theory were incorporated into special relativity with the works of Albert Einstein and Hermann Minkowski. In 1892, Hendrik Lorentz derived the modern form of the formula for the electromagnetic force which includes the contributions to the total force from both the electric and the magnetic fields.  In many textbook treatments of classical electromagnetism, the Lorentz force law is used as the \"definition\" of the electric and magnetic fields E and B. To be specific, the Lorentz force is understood to be the following empirical statement: In the physics of electromagnetism, the Abraham–Lorentz force (also \"Lorentz-Abraham force\") is the recoil force on an accelerating charged particle caused by the particle emitting electromagnetic radiation.  It is also called the \"radiation reaction force\" or the \"self force\". In physics, the Lorentz transformation (or Lorentz transformations) is named after the Dutch physicist Hendrik Lorentz.  It was the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism.  The Lorentz transformation is in accordance with special relativity, but was derived before special relativity.  Early approximations of the transformation were published by Lorentz in 1895.  In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group, and named it after Lorentz. In physics, length contraction (more formally called Lorentz contraction or Lorentz–FitzGerald contraction after Hendrik Lorentz and George FitzGerald) is the phenomenon of a decrease in length measured by the observer, of an object which is traveling at any non-zero velocity relative to the observer.  This contraction is usually only noticeable at a substantial fraction of the speed of light. The Lorentz factor or \"Lorentz term\" is the factor by which time, length, and relativistic mass change for an object while that object is moving.  It is an expression which appears in several equations in special relativity, and it arises from deriving the Lorentz transformations.  The name originates from its earlier appearance in Lorentzian electrodynamics – named after the Dutch physicist Hendrik Lorentz. The Zeeman effect, named after the Dutch physicist Pieter Zeeman, is the effect of splitting a spectral line into several components in the presence of a static magnetic field.  It is analogous to the Stark effect, the splitting of a spectral line into several components in the presence of an electric field.  Also similar to the Stark effect, transitions between different components have, in general, different intensities, with some being entirely forbidden (in the dipole approximation), as governed by the selection rules. Since the distance between the Zeeman sub-levels is a function of the magnetic field, this effect can be used to measure the magnetic field, e.g. that of the Sun and other stars or in laboratory plasmas. The Zeeman effect is very important in applications such as nuclear magnetic resonance spectroscopy, electron spin resonance spectroscopy, magnetic resonance imaging (MRI) and Mössbauer spectroscopy.  It may also be utilized to improve accuracy in atomic absorption spectroscopy. A theory about the magnetic sense of birds assumes that a protein in the retina is changed due to the Zeeman effect. When the spectral lines are absorption lines, the effect is called \"inverse Zeeman effect\". Helium was first liquefied (liquid helium) on 10 July 1908, by Dutch physicist Heike Kamerlingh Onnes.  With the production of liquid helium, it was said that “the coldest place on Earth” was in Leiden. Superconductivity, the ability of certain materials to conduct electricity with little or no resistance, was discovered by Dutch physicist Heike Kamerlingh Onnes. The Einstein–de Haas effect or the \"Richardson effect\" (after Owen Willans Richardson), is a physical phenomenon delineated by Albert Einstein and Wander Johannes de Haas in the mid 1910s, that exposes a relationship between magnetism, angular momentum, and the spin of elementary particles. In thermodynamics and solid state physics, the Debye model is a method developed by Peter Debye in 1912 for estimating the phonon contribution to the specific heat (heat capacity) in a solid.  It treats the vibrations of the atomic lattice (heat) as phonons in a box, in contrast to the Einstein model, which treats the solid as many individual, non-interacting quantum harmonic oscillators.  The Debye model correctly predicts the low temperature dependence of the heat capacity. The geodetic effect (also known as geodetic precession, de Sitter precession or de Sitter effect) represents the effect of the curvature of spacetime, predicted by general relativity, on a vector carried along with an orbiting body.  The geodetic effect was first predicted by Willem de Sitter in 1916, who provided relativistic corrections to the Earth–Moon system's motion. In mathematics and physics, a de Sitter space is the analog in Minkowski space, or spacetime, of a sphere in ordinary, Euclidean space.  The \"n\"-dimensional de Sitter space, denoted dS, is the Lorentzian manifold analog of an \"n\"-sphere (with its canonical Riemannian metric); it is maximally symmetric, has constant positive curvature, and is simply connected for \"n\" at least 3.  The de Sitter space, as well as the anti-de Sitter space is named after Willem de Sitter (1872–1934), professor of astronomy at Leiden University and director of the Leiden Observatory.  Willem de Sitter and Albert Einstein worked in the 1920s in Leiden closely together on the spacetime structure of our universe.  De Sitter space was discovered by Willem de Sitter, and, at the same time, independently by Tullio Levi-Civita. In dynamical systems, a Van der Pol oscillator is a non-conservative oscillator with non-linear damping.  It was originally proposed by Dutch physicist Balthasar van der Pol while he was working at Philips in 1920.  Van der Pol studied a differential equation that describes the circuit of a vacuum tube.  It has been used to model other phenomenon such as human heartbeats by colleague Jan van der Mark. Kramers' opacity law describes the opacity of a medium in terms of the ambient density and temperature, assuming that the opacity is dominated by bound-free absorption (the absorption of light during ionization of a bound electron) or free-free absorption (the absorption of light when scattering a free ion, also called bremsstrahlung).  It is often used to model radiative transfer, particularly in stellar atmospheres.  The relation is named after the Dutch physicist Hendrik Kramers, who first derived the form in 1923. In 1925, Dutch physicists George Eugene Uhlenbeck and Samuel Goudsmit co-discovered the concept of electron spin, which posits an intrinsic angular momentum for all electrons. In 1926, Onnes' student, Dutch physicist Willem Hendrik Keesom, invented a method to freeze liquid helium and was the first person who was able to solidify the noble gas. The Ehrenfest theorem, named after the Austrian-born Dutch-Jew theoretical physicist Paul Ehrenfest at Leiden University. The de Haas–van Alphen effect, often abbreviated to dHvA, is a quantum mechanical effect in which the magnetic moment of a pure metal crystal oscillates as the intensity of an applied magnetic field B is increased.  It was discovered in 1930 by Wander Johannes de Haas and his student P. M. van Alphen. The Shubnikov–de Haas effect (ShdH) is named after Dutch physicist Wander Johannes de Haas and Russian physicist Lev Shubnikov. In quantum mechanics, the Kramers degeneracy theorem states that for every energy eigenstate of a time-reversal symmetric system with half-integer total spin, there is at least one more eigenstate with the same energy.  It was first discovered in 1930 by H. A. Kramers as a consequence of the Breit equation. In 1933, Marcel Minnaert published a solution for the acoustic resonance frequency of a single bubble in water, the so-called Minnaert resonance.  The Minnaert resonance or Minnaert frequency is the acoustic resonance frequency of a single bubble in an infinite domain of water (neglecting the effects of surface tension and viscous attenuation). In quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field.  Dutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947.  After a conversation with Niels Bohr who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948; the former is called the Casimir–Polder force while the latter is the Casimir effect in the narrow sense. Tellegen's theorem is one of the most powerful theorems in network theory.  Most of the energy distribution theorems and extremum principles in network theory can be derived from it.  It was published in 1952 by Bernard Tellegen.  Fundamentally, Tellegen's theorem gives a simple relation between magnitudes that satisfy Kirchhoff's laws of electrical circuit theory. In the early 1970s Simon van der Meer, a Dutch particle physicist at CERN, discovered this technique to concentrate proton and anti-proton beams, leading to the discovery of the W and Z particles.  He won the 1984 Nobel Prize in Physics together with Carlo Rubbia. In 1971, Gerardus 't Hooft, who was completing his PhD under the supervision of Dutch theoretical physicist Martinus Veltman, renormalized Yang–Mills theory.  They showed that if the symmetries of Yang–Mills theory were to be realized in the spontaneously broken mode, referred to as the Higgs mechanism, then Yang–Mills theory can be renormalized.  Renormalization of Yang–Mills theory is considered as a major achievement of twentieth century physics. The holographic principle is a property of string theories and a supposed property of quantum gravity that states that the description of a volume of space can be thought of as encoded on a boundary to the region – preferably a light-like boundary like a gravitational horizon.  In 1993, Dutch theoretical physicist Gerard 't Hooft proposed what is now known as the holographic principle.  It was given a precise string-theory interpretation by Leonard Susskind who combined his ideas with previous ones of 't Hooft and Charles Thorn. During his first journey in 1594, Dutch explorer Willem Barentsz discovered the Orange Islands. On 10 June 1596, Barentsz and Dutchman Jacob van Heemskerk discovered Bear Island, a week before their discovery of Spitsbergen Island. The first undisputedly to have discovered the archipelago is an expedition led by the Dutch mariner Willem Barentsz, who was looking for the Northern Sea Route to China.  He first spotted Bjørnøya on 10 June 1596 and the northwestern tip of Spitsbergen on 17 June.  The sighting of the archipelago was included in the accounts and maps made by the expedition and Spitsbergen was quickly included by cartographers.  The name \"Spitsbergen\", meaning \"pointed mountains\" (from the Dutch \"spits\" – pointed, \"bergen\" – mountains), was at first applied to both the main island and the Svalbard archipelago as a whole. The search for the Northern Sea Route in the 16th century led to its exploration.  Dutch explorer Willem Barentsz reached the west coast of Novaya Zemlya in 1594, and in a subsequent expedition of 1596 rounded the Northern point and wintered on the Northeast coast.  Willem Barents, Jacob van Heemskerck and their crew were blocked by the pack ice in the Kara Sea and forced to winter on the east coast of Novaya Zemlya.  The wintering of the shipwrecked crew in the 'Saved House' was the first successful wintering of Europeans in the High Arctic.  Twelve of the 17 men managed to survive the polar winter (De Veer, 1917).  Barentsz died during the expedition, and may have been buried on the northern island. In 1600 the Dutch navigator Sebald de Weert made the first undisputed sighting of the Falkland Islands.  It was on his homeward leg back to the Netherlands after having left the Straits of Magellan that Sebald De Weert noticed some unnamed and uncharted islands, at least islands that did not exist on his nautical charts.  There he attempted to stop and replenish but was unable to land due to harsh conditions.  The islands Sebald de Weert charted were a small group off the northwest coast of the Falkland Islands and are in fact part of the Falklands.  De Weert then named these islands the “Sebald de Weert Islands” and the Falklands as a whole were known as the Sebald Islands until well into the 18th century. The Dutch ship, Duyfken, led by Willem Janszoon, made the first documented European landing in Australia in 1606.  Although a theory of Portuguese discovery in the 1520s exists, it lacks definitive evidence.  Precedence of discovery has also been claimed for China, France, Spain, India, and even Phoenicia. The Janszoon voyage of 1605–06 led to the first undisputed sighting of Australia by a European was made on 26 February 1606.  Dutch vessel \"Duyfken\", captained by Janszoon, followed the coast of New Guinea, missed Torres Strait, and explored perhaps 350 km of western side of Cape York, in the Gulf of Carpentaria, believing the land was still part of New Guinea.  The Dutch made one landing, but were promptly attacked by Maoris and subsequently abandoned further exploration. The first recorded European sighting of the Australian mainland, and the first recorded European landfall on the Australian continent, are attributed to the Dutch navigator Willem Janszoon.  He sighted the coast of Cape York Peninsula in early 1606, and made landfall on 26 February at the Pennefather River near the modern town of Weipa on Cape York.  The Dutch charted the whole of the western and northern coastlines and named the island continent \"New Holland\" during the 17th century, but made no attempt at settlement. The area that is now Manhattan was long inhabited by the Lenape Indians.  In 1524, Florentine explorer Giovanni da Verrazzano – sailing in service of the king Francis I of France – was the first European to visit the area that would become New York City.  It was not until the voyage of Henry Hudson, an Englishman who worked for the Dutch East India Company, that the area was mapped. At the time of the arrival of the first Europeans in the 17th century, the Hudson Valley was inhabited primarily by the Algonquian-speaking Mahican and Munsee Native American people, known collectively as River Indians.  The first Dutch settlement was in the 1610s at Fort Nassau, a trading post (factorij) south of modern-day Albany, that traded European goods for beaver pelts.  Fort Nassau was later replaced by Fort Orange.  During the rest of the 17th century, the Hudson Valley formed the heart of the New Netherland colony operations, with the New Amsterdam settlement on Manhattan serving as a post for supplies and defense of the upriver operations. The Brouwer Route was a route for sailing from the Cape of Good Hope to Java.  The Route took ships south from the Cape into the Roaring Forties, then east across the Indian Ocean, before turning northwest for Java.  Thus it took advantage of the strong westerly winds for which the Roaring Forties are named, greatly increasing travel speed.  It was devised by Dutch sea explorer Hendrik Brouwer in 1611, and found to halve the duration of the journey from Europe to Java, compared to the previous Arab and Portuguese monsoon route, which involved following the coast of East Africa northwards, sailing through the Mozambique Channel and then across the Indian Ocean, sometimes via India.  The Brouwer Route played a major role in the discovery of the west coast of Australia. After unconfirmed reports of Dutch discovery as early as 1611, the island was named after Dutchman Jan Jacobszoon May van Schellinkhout, who visited the island in July 1614.  As locations of these islands were kept secret by the whalers, Jan Mayen got its current name only in 1620. The name \"Hell Gate\" is a corruption of Dutch phrase \"Hellegat\", which could mean either \"hell's hole\" or \"bright gate/passage\".  It was originally applied to the entirety of the East River.  The strait was described in the journals of Dutch explorer Adriaen Block, who is the first European known to have navigated the strait, during his 1614 voyage aboard the \"Onrust\". The first European to record the existence of Long Island Sound and the Connecticut River was Dutch explorer Adriaen Block, who entered it from the East River in 1614. Fishers Island was called \"Munnawtawkit\" by the Native American Pequot nation.  Block named it \"Visher's Island\" in 1614, after one of his companions.  For the next 25 years, it remained a wilderness, visited occasionally by Dutch traders. On 25 December 1615, Dutch explorers Jacob le Maire and Willem Schouten aboard the Eendracht, discovered Staten Island, close to Cape Horn. On 29 January 1616, they sighted land they called Cape Horn, after the city of Hoorn.  Aboard the Eendracht was the crew of the recently wrecked ship called Hoorn. They discovered Tonga on 21 April 1616 and the Hoorn Islands on 28 April 1616. They discovered New Ireland around May–July 1616. They discovered the Schouten Islands (also known as \"Biak Islands\" or \"Geelvink Islands\") on 24 July 1616. The Schouten Islands (also known as \"Eastern Schouten Islands\" or \"Le Maire Islands\") of Papua New Guinea, were named after Schouten, who visited them in 1616. Hendrik Brouwer's discovery that sailing east from the Cape of Good Hope until land was sighted, and then sailing north along the west coast of Australia was a much quicker route than around the coast of the Indian Ocean made Dutch landfalls on the west coast inevitable.  The first such landfall was in 1616, when Dirk Hartog landed at Cape Inscription on what is now known as Dirk Hartog Island, off the coast of Western Australia, and left behind an inscription on a pewter plate.  In 1697 the Dutch captain Willem de Vlamingh landed on the island and discovered Hartog's plate.  He replaced it with one of his own, which included a copy of Hartog's inscription, and took the original plate home to Amsterdam, where it is still kept in the Rijksmuseum Amsterdam. The first sighting of the Houtman Abrolhos by Europeans was by Dutch VOC ships \"Dordrecht\" and \"Amsterdam\" in 1619, three years after Hartog made the first authenticated sighting of what is now Western Australia, 13 years after the first authenticated voyage to Australia, that of the \"Duyfke]\" in 1606.  Discovery of the islands was credited to Frederick de Houtman, Captain-General of the \"Dordrecht\", as it was Houtman who later wrote of the discovery in a letter to Company directors. The first person to spot Carstensz Pyramid (or Puncak Jaya) is reported to be the Dutch navigator and explorer Jan Carstensz in 1623, for whom the mountain is named.  Carstensz was the first (non-native) to sight the glaciers on the peak of the mountain on a rare clear day.  The sighting went unverified for over two centuries, and Carstensz was ridiculed in Europe when he said he had seen snow and glaciers near the equator.  The snowfield of Puncak Jaya was reached as early as 1909 by a Dutch explorer, Hendrik Albert Lorentz with six of his indigenous Dayak Kenyah porters recruited from the Apo Kayan in Borneo.  The now highest Carstensz Pyramid summit was not climbed until 1962, by an expedition led by the Austrian mountaineer Heinrich Harrer with three other expedition members – the New Zealand mountaineer Philip Temple, the Australian rock climber Russell Kippax, and the Dutch patrol officer Albertus (Bert) Huizenga. The first known European explorer to visit the region was Dutch Willem Janszoon (also known as \"Willem Jansz\") on his 1605–06 voyage.  His fellow countryman, Jan Carstenszoon (also known as \"Jan Carstensz\"), visited in 1623 and named the gulf in honour of Pieter de Carpentier, at that time the Governor-General of Dutch East Indies.  Abel Tasman explored the coast in 1644. The Staaten River is a river in the Cape York Peninsula, Australia that rises more than 200 km to the west of Cairns and empties into the Gulf of Carpentaria.  The river was first named by Carstenszoon in 1623. In 1623 Dutch East India Company captain Willem van Colster sailed into the Gulf of Carpentaria.  Cape Arnhem is named after his ship, the \"Arnhem\", which itself was named after the city of Arnhem. Groote Eylandt was first sighted the \"Arnhem\".  Only in 1644, when Abel Tasman arrived, was the island given a European name, Dutch for \"Large Island\" in an archaic spelling.  The modern Dutch spelling is \"Groot Eiland\". In February 1624, Dutch admiral Jacques l'Hermite discovered the Hermite Islands at Cape Horn. In 1627, Dutch explorers François Thijssen and Pieter Nuyts discovered the south coast of Australia and charted about 1800 km of it between Cape Leeuwin and the Nuyts Archipelago.  François Thijssen, captain of the ship \"'t Gulden Zeepaert\" (The Golden Seahorse), sailed to the east as far as Ceduna in South Australia.  The first known ship to have visited the area is the \"Leeuwin\" (\"Lioness\"), a Dutch vessel that charted some of the nearby coastline in 1622.  The log of the \"Leeuwin\" has been lost, so very little is known of the voyage.  However, the land discovered by the \"Leeuwin\" was recorded on a 1627 map by Hessel Gerritsz: Caert van't Landt van d'Eendracht (\"Chart of the Land of Eendracht\"), which appears to show the coast between present-day Hamelin Bay and Point D’Entrecasteaux.  Part of Thijssen's map shows the islands St Francis and St Peter, now known collectively with their respective groups as the Nuyts Archipelago.  Thijssen's observations were included as soon as 1628 by the VOC cartographer Hessel Gerritsz in a chart of the Indies and New Holland.  This voyage defined most of the southern coast of Australia and discouraged the notion that \"New Holland\", as it was then known, was linked to Antarctica. St Francis Island (originally in Dutch: \"Eyland St. François\") is an island on the south coast of South Australia near Ceduna.  It is now part of the Nuyts Archipelago Wilderness Protection Area.  It was one of the first parts of South Australia to be discovered and named by Europeans, along with St Peter Island.  Thijssen named it after his patron saint, St. Francis. St Peter Island is an island on the south coast of South Australia near Ceduna to the south of Denial Bay.  It is the second largest island in South Australia at about 13 km long.  It was named in 1627 by Thijssen after Pieter Nuyts' patron saint. The Weibbe Hayes Stone Fort, remnants of improvised defensive walls and stone shelters built by Wiebbe Hayes and his men on the West Wallabi Island, are Australia's oldest known European structures, more than 150 years before expeditions to the Australian continent by James Cook and Arthur Phillip. In 1642, Abel Tasman sailed from Mauritius and on 24 November, sighted Tasmania.  He named Tasmania Van Diemen's Land, after Anthony van Diemen, the Dutch East India Company's Governor General, who had commissioned his voyage.  It was officially renamed Tasmania in honour of its first European discoverer on 1 January 1856. Maatsuyker Islands, a group of small islands that are the southernmost point of the Australian continent.  were discovered and named by Tasman in 1642 after a Dutch official.  The main islands of the group are De Witt Island (354 m), Maatsuyker Island (296 m), Flat Witch Island, Flat Top Island, Round Top Island, Walker Island, Needle Rocks and Mewstone. Maria Island was discovered and named in 1642 by Tasman after Maria van Diemen (née van Aelst), wife of Anthony.  The island was known as \"Maria's Isle\" in the early 19th century. Tasman's journal entry for 29 November 1642 records that he observed a rock which was similar to a rock named Pedra Branca off China, presumably referring to the Pedra Branca in the South China Sea. Schouten Island is a 28 km2 island in eastern Tasmania, Australia.  It lies 1.6 kilometres south of Freycinet Peninsula and is a part of Freycinet National Park.  In 1642, while surveying the south-west coast of Tasmania, Tasman named the island after Joost Schouten, a member of the Council of the Dutch East India Company. Tasman also reached Storm Bay, a large bay in the south-east of Tasmania, Australia.  It is the entrance to the Derwent River estuary and the port of Hobart, the capital city of Tasmania.  It is bordered by Bruny Island to the west and the Tasman Peninsula to the east. In 1642, the first Europeans known to reach New Zealand were the crew of Dutch explorer Abel Tasman who arrived in his ships \"Heemskerck\" and \"Zeehaen\".  Tasman anchored at the northern end of the South Island in Golden Bay (he named it Murderers' Bay) in December 1642 and sailed northward to Tonga following a clash with local Māori.  Tasman sketched sections of the two main islands' west coasts.  Tasman called them \"Staten Landt\", after the \"States General of the Netherlands\", and that name appeared on his first maps of the country.  In 1645 Dutch cartographers changed the name to \"Nova Zeelandia\" in Latin, from \"Nieuw Zeeland\", after the Dutch province of \"Zeeland\".  It was subsequently Anglicised as \"New Zealand\" by British naval captain James Cook Various claims have been made that New Zealand was reached by other non-Polynesian voyagers before Tasman, but these are not widely accepted.  Peter Trickett, for example, argues in \"Beyond Capricorn\" that the Portuguese explorer Cristóvão de Mendonça reached New Zealand in the 1520s, and the Tamil bell discovered by missionary William Colenso has given rise to a number of theories, In 1643, still during the same expedition, Tasman discovered Fiji. Tasman discovered Tongatapu and Haʻapai in 1643 commanding two ships, the \"Heemskerck\" and the \"Zeehaen\" commissioned by the Dutch East India Company.  The expedition's goals were to chart the unknown southern and eastern seas and to find a possible passage through the South Pacific and Indian Ocean providing a faster route to Chile. The first European known to visit Sakhalin was Martin Gerritz de Vries, who mapped Cape Patience and Cape Aniva on the island's east coast in 1643. In the summer of 1643, the \"Castricum\", under command of Martin Gerritz de Vries sailed by the southern Kuril Islands, visiting Kunashir, Iturup and Urup, which they named \"Company Island\" and claimed for the Netherlands. Vries Strait or Miyabe Line is a strait between two main islands of the Kurils.  It is located between the northeastern end of the island of Iturup and the southwestern headland of Urup Island, connecting the Sea of Okhotsk on the west with the Pacific Ocean on the east.  The strait is named after de Vries, the first recorded European to explore the area. The Gulf of Patience is a large body of water off the southeastern coast of Sakhalin, Russia, between the main body of Sakhalin Island in the west and Cape Patience in the east.  It is part of the Sea of Okhotsk.  The first Europeans to visit the bay sailed on \"Castricum\".  They named the gulf in memory of the fog that had to clear for them to continue their expedition. The first Europeans known to land on the Rottnest Island were 13 Dutch sailors including Abraham Leeman from the \"Waeckende Boey\" who landed near Bathurst Point on 19 March 1658 while their ship was nearby.  The ship had sailed from Batavia in search of survivors of the missing \"Vergulde Draeck\" which was later found wrecked 80 km north near present-day Ledge Point.  The island was given the name \"Rotte nest\" (meaning \"rat nest\" in the 17th century Dutch language) by Dutch captain Willem de Vlamingh who spent six days exploring the island from 29 December 1696, mistaking the quokkas for giant rats. De Vlamingh led a fleet of three ships, \"De Geelvink\", \"De Nijptang\" and \"Weseltje\" and anchored on the northern side of the island, near The Basin. On 10 January 1697, de Vlamingh ventured up the Swan River.  He and his crew are believed to have been the first Europeans to do so.  He named the \"Swan River\" (\"Zwaanenrivier\" in Dutch) after the large numbers of black swans that he observed there. On Easter Sunday, 5 April 1722, Dutch explorer Jacob Roggeveen discovered Easter Island.  Easter Island is one of the most remote inhabited islands in the world.  The nearest inhabited land (50 residents) is Pitcairn Island 2075 km away, the nearest town with a population over 500 is Rikitea on island Mangareva 2606 km away, and the nearest continental point lies in central Chile, 3512 km away. The name \"Easter Island\" was given by the island's first recorded European visitor, the Dutch explorer Jacob Roggeveen, who encountered it on Easter Sunday (5 April) 1722, while searching for Davis or David's island.  Roggeveen named it \"Paasch-Eyland\" (18th century Dutch for \"Easter Island\").  The island's official Spanish name, \"Isla de Pascua\", also means \"Easter Island\". On 13 June Roggeveen discovered the islands of Samoa. The Orange River was named by Colonel Robert Gordon, commander of the Dutch East India Company garrison at Cape Town, on a trip to the interior in 1779. In 1595, Petrus Plancius, a key promoter to the East Indies expeditions, asked Pieter Dirkszoon Keyser, the chief pilot on the \"Hollandia\", to make observations to fill in the blank area around the south celestial pole on European maps of the southern sky.  Plancius had instructed Keyser to map the skies in the southern hemisphere, which were largely uncharted at the time.  Keyser died in Java the following year but his catalogue of 135 stars, probably measured up with the help of explorer-colleague Frederick de Houtman, was delivered to Plancius, and then those stars were arranged into 12 new southern constellations, letting them be inscribed on a 35-cm celestial globe that was prepared in late 1597 (or early 1598).  This globe was produced in collaboration with the Amsterdam cartographer Jodocus Hondius. Plancius's constellations (mostly referring to animals and subjects described in natural history books and travellers' journals of his day) are Apis the Bee (later changed to Musca by Lacaille), Apus the Bird of Paradise, Chamaeleon, Dorado the Goldfish (or Swordfish), Grus the Crane, Hydrus the Small Water Snake, Indus the Indian, Pavo the Peacock, Phoenix, Triangulum Australe the Southern Triangle, Tucana the Toucan, and Volans the Flying Fish.  The acceptance of these new constellations was assured when Johann Bayer, a German astronomer, included them in his \"Uranometria\" of 1603, the leading star atlas of its day.  These 12 southern constellations are still recognized today by the International Astronomical Union (IAU). Within the thirty-year period the Dutch West India Company controlled the northeast region of Brazil (1624–1654), the seven-year governorship of Count Johan Maurits van Nassau-Siegen was marked by an intense ethnographic exploration.  To that end, Johan Maurits brought from Europe with him a team of artists and scientists who lived in Recife between 1637 and 1644: painter Albert Eckhout (specializing in the human figure), painter Frans Post (landscape painter), natural historian Georg Marcgraf (who also produced drawings and prints), and the physician Willem Piso.  Together with Georg Marcgraf, and originally published by Joannes de Laet, Piso wrote the \"Historia Naturalis Brasiliae\" (1648), an important early western insight into Brazilian flora and fauna, also is the first scientific book about Brazil.  Albert Eckhout, along with the landscape artist Frans Post, was one of two formally trained painters charged with recording the complexity of the local scene.  The seven years Eckhout spent in Brazil constitute an invaluable contribution to the understanding of the European colonization of the New World.  During his stay he created hundreds of oil sketches – mostly from life – of the local flora, fauna and people.  These paintings by Eckhout and the landscapes by Post were among the Europeans' first, introductions to South America. In 1641, Kiliaen van Rensselaer, the director of the Dutch West India Company, hired Adriaen van der Donck (1620–1655) to be his lawyer for his large, semi-independent estate, Rensselaerswijck, in New Netherland.  Until 1645, van der Donck lived in the Upper Hudson River Valley, near Fort Orange (later Albany), where he learned about the Company's fur trade, the Mohawk and Mahican Indians who traded with Dutch, the agriculturist settlers, and the area's plants and animals.  In 1649, after a serious disagreement with the new governor, Peter Stuyvesant, he returned to the Dutch Republic to petition Dutch government.  In 1653, still in the Netherlands waiting for the government to decide his case, Adriaen van der Donck wrote a comprehensive description of the New Netherland's geography and native peoples based on material in his earlier \"Remonstrance\".  The book, \"Beschryvinge van Nieuw-Nederlant\" or \"A Description of New Netherland\" later published in 1655.  This new book was well-crafted to the interests of his audience, consisting of an extensive description of American Indians and their customs, reports on the abundance of the area's agriculture and wealth of its natural resources. Jan Weltevree (1595-?)  is regarded as the first naturalized Westerner to Korea.  Weltevree was a Dutch sailor who arrived on the shores of an island off Joseon’s west coast in 1627 in a shipwreck.  The Joseon Dynasty at that time maintained an isolation policy, so the captured foreigner could not leave the country.  Weltevree took the name \"Bak Yeon\" (also \"Pak Yeon\").  He became an important government official and aided King Hyojong with his keen knowledge of modern weaponry.  His adventures were recorded in the report by Dutch East India Company accountant Hendrik Hamel. Dutch seafarer and VOC's bookkeeper Hendrick Hamel was the first westerner to experience first-hand and write about Korea in Joseon era (1392–1897).  In 1653, Hamel and his men were shipwrecked on Jeju island, and they remained captives in Korea for more than a decade.  The Joseon dynasty was often referred to as the \"Hermit Kingdom\" for its harsh isolationism and closed borders.  The shipwrecked Dutchmen were given some freedom of movement, but were forbidden to leave the country.  After thirteen years (1653–1666), Hamel and seven of his crewmates managed to escape to the VOC trading mission at Dejima (an artificial island in the bay of Nagasaki, Japan), and from there to the Netherlands.  In 1666, three different publishers published his report (\"Journal van de Ongeluckige Voyage van 't Jacht de Sperwer\" or \"An account of the shipwreck of a Dutch vessel on the coast of the isle of Quelpaert together with the description of the kingdom of Corea\"), describing their improbable adventure and giving the first detailed and accurate description of Korea to the western world.\n\nHistory of electromagnetic theory The history of electromagnetic theory begins with ancient measures to understand atmospheric electricity, in particular lightning.  People then had little understanding of electricity, and were unable to explain the phenomena.  Scientific understanding into the nature of electricity grew throughout the eighteenth and nineteenth centuries through the work of researchers such as Ampère, Coulomb, Faraday and Maxwell. In the 19th century it had become clear that electricity and magnetism were related, and their theories were unified: wherever charges are in motion electric current results, and magnetism is due to electric current.  The source for electric field is electric charge, whereas that for magnetic field is electric current (charges in motion). The knowledge of static electricity dates back to the earliest civilizations, but for millennia it remained merely an interesting and mystifying phenomenon, without a theory to explain its behavior and often confused with magnetism.  The ancients were acquainted with rather curious properties possessed by two minerals, amber (Greek: ἤλεκτρον , \"ēlektron\") and magnetic iron ore (μαγνῆτις λίθος \"magnētis lithos\", \"the Magnesian stone, lodestone\").  Amber, when rubbed, attracts lightweight objects, such as feathers; magnetic iron ore has the power of attracting iron. Based on his find of an Olmec hematite artifact in Central America, the American astronomer John Carlson has suggested that \"the Olmec may have discovered and used the geomagnetic lodestone compass earlier than 1000 BC\".  If true, this \"predates the Chinese discovery of the geomagnetic lodestone compass by more than a millennium\".  Carlson speculates that the Olmecs may have used similar artifacts as a directional device for astrological or geomantic purposes, or to orient their temples, the dwellings of the living or the interments of the dead.  The earliest Chinese literature reference to \"magnetism\" lies in a 4th-century BC book called \"Book of the Devil Valley Master\" (鬼谷子): \"The lodestone makes iron come or it attracts it.\" Long before any knowledge of electromagnetism existed, people were aware of the effects of electricity.  Lightning and other manifestations of electricity such as St. Elmo's fire were known in ancient times, but it was not understood that these phenomena had a common origin.  Ancient Egyptians were aware of shocks when interacting with electric fish (such as the electric catfish) or other animals (such as electric eels).  The shocks from animals were apparent to observers since pre-history by a variety of peoples that came into contact with them.  Texts from 2750 BCE by the ancient Egyptians referred to these fish as \"thunderer of the Nile\" and saw them as the \"protectors\" of all the other fish.  Another possible approach to the discovery of the identity of lightning and electricity from any other source, is to be attributed to the Arabs, who before the 15th century used the same Arabic word for lightning (\"barq\") and the electric ray. Thales of Miletus, writing at around 600 BC, noted that rubbing fur on various substances such as amber would cause them to attract specks of dust and other light objects.  Thales wrote on the effect now known as static electricity.  The Greeks noted that if they rubbed the amber for long enough they could even get an electric spark to jump. The electrostatic phenomena was again reported millennia later by Roman and Arabic naturalists and physicians.  Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and torpedo rays. Pliny in his books writes: \"The ancient Tuscans by their learning hold that there are nine gods that send forth lightning and those of eleven sorts.\"  This was in general the early pagan idea of lightning.  The ancients held some concept that shocks could travel along conducting objects.  Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. A number of objects found in Iraq in 1938 dated to the early centuries AD (Sassanid Mesopotamia), called the Baghdad Battery, resembles a galvanic cell and is believed by some to have been used for electroplating.  The claims are controversial because of supporting evidence and theories for the uses of the artifacts, physical evidence on the objects conducive for electrical functions, and if they were electrical in nature.  As a result, the nature of these objects is based on speculation, and the function of these artifacts remains in doubt. Magnetic attraction was once accounted for by Aristotle and Thales as the working of a soul in the stone. In the 11th century, the Chinese scientist Shen Kuo (1031–1095) was the first person to write of the magnetic needle compass and that it improved the accuracy of navigation by employing the astronomical concept of true north \"(Dream Pool Essays\", AD 1088 ), and by the 12th century the Chinese were known to use the lodestone compass for navigation.  In 1187, Alexander Neckam was the first in Europe to describe the compass and its use for navigation. Magnetism was one of the few sciences which progressed in medieval Europe; for in the thirteenth century Peter Peregrinus, a native of Maricourt in Picardy, made a discovery of fundamental importance.  The French 13th century scholar conducted experiments on magnetism and wrote the first extant treatise describing the properties of magnets and pivoting compass needles.  The dry compass was invented around 1300 by Italian inventor Flavio Gioja. Archbishop Eustathius of Thessalonica, Greek scholar and writer of the 12th century, records that \"Woliver\", king of the Goths, was able to draw sparks from his body.  The same writer states that a certain philosopher was able while dressing to draw sparks from his clothes, a result seemingly akin to that obtained by Robert Symmer in his silk stocking experiments, a careful account of which may be found in the 'Philosophical Transactions,' 1759. Italian physician Gerolamo Cardano wrote about electricity in \"De Subtilitate\" (1550) distinguishing, perhaps for the first time, between electrical and magnetic forces. Toward the late 16th century, a physician of Queen Elizabeth's time, Dr. William Gilbert, in \"De Magnete\", expanded on Cardano's work and invented the New Latin word \"electricus\" from ἤλεκτρον (\"ēlektron\"), the Greek word for \"amber\".  Gilbert, a native of Colchester, Fellow of St John's College, Cambridge, and sometime President of the College of Physicians, was one of the earliest and most distinguished English men of science — a man whose work Galileo thought enviably great.  He was appointed Court physician, and a pension was settled on him to set him free to continue his researches in Physics and Chemistry. Gilbert undertook a number of careful electrical experiments, in the course of which he discovered that many substances other than amber, such as sulphur, wax, glass, etc., were capable of manifesting electrical properties.  Gilbert also discovered that a heated body lost its electricity and that moisture prevented the electrification of all bodies, due to the now well-known fact that moisture impaired the insulation of such bodies.  He also noticed that electrified substances attracted all other substances indiscriminately, whereas a magnet only attracted iron.  The many discoveries of this nature earned for Gilbert the title of \"founder of the electrical science\".  By investigating the forces on a light metallic needle, balanced on a point, he extended the list of electric bodies, and found also that many substances, including metals and natural magnets, showed no attractive forces when rubbed.  He noticed that dry weather with north or east wind was the most favourable atmospheric condition for exhibiting electric phenomena—an observation liable to misconception until the difference between conductor and insulator was understood. Gilbert's work was followed up by Robert Boyle (1627–1691), the famous natural philosopher who was once described as \"father of Chemistry, and uncle of the Earl of Cork.\"  Boyle was one of the founders of the Royal Society when it met privately in Oxford, and became a member of the Council after the Society was incorporated by Charles II.  in 1663.  He worked frequently at the new science of electricity, and added several substances to Gilbert's list of electrics.  He left a detailed account of his researches under the title of \"Experiments on the Origin of Electricity\".  Boyle, in 1675, stated that electric attraction and repulsion can act across a vacuum.  One of his important discoveries was that electrified bodies in a vacuum would attract light substances, thus indicating that the electrical effect did not depend upon the air as a medium.  He also added resin to the then known list of electrics. This was followed in 1660 by Otto von Guericke, who invented an early electrostatic generator.  By the end of the 17th Century, researchers had developed practical means of generating electricity by friction with an electrostatic generator, but the development of electrostatic machines did not begin in earnest until the 18th century, when they became fundamental instruments in the studies about the new science of electricity. The first usage of the word \"electricity\" is ascribed to Sir Thomas Browne in his 1646 work, \"Pseudodoxia Epidemica\". The first appearance of the term \"electromagnetism\" on the other hand comes from an earlier date: 1641. \"Magnes\", by the Jesuit luminary Athanasius Kircher, carries on page 640 the provocative chapter-heading: \"\"Elektro-magnetismos\" i.e. On the Magnetism of amber, or electrical attractions and their causes\" (ηλεκτρο-μαγνητισμος \"id est sive De Magnetismo electri, seu electricis attractionibus earumque causis\"). The electric machine was subsequently improved by Francis Hauksbee, his student Litzendorf, and by Prof. Georg Matthias Bose, about 1750.  Litzendorf, researching for Christian August Hausen, substituted a glass ball for the sulphur ball of Guericke.  Bose was the first to employ the \"prime conductor\" in such machines, this consisting of an iron rod held in the hand of a person whose body was insulated by standing on a block of resin.  Ingenhousz, during 1746, invented electric machines made of plate glass.  Experiments with the electric machine were largely aided by the discovery of the property of a glass plate, when coated on both sides with tinfoil, of accumulating a charge of electricity when connected with a source of electromotive force.  The electric machine was soon further improved by Andrew Gordon, a Scotsman, Professor at Erfurt, who substituted a glass cylinder in place of a glass globe; and by Giessing of Leipzig who added a \"rubber\" consisting of a cushion of woollen material.  The collector, consisting of a series of metal points, was added to the machine by Benjamin Wilson about 1746, and in 1762, John Canton of England (also the inventor of the first pith-ball electroscope) improved the efficiency of electric machines by sprinkling an amalgam of tin over the surface of the rubber. In 1729, Stephen Gray conducted a series of experiments that demonstrated the difference between conductors and non-conductors (insulators), showing amongst other things that a metal wire and even pack thread conducted electricity, whereas silk did not.  In one of his experiments he sent an electric current through 800 feet of hempen thread which was suspended at intervals by loops of silk thread.  When he tried to conduct the same experiment substituting the silk for finely spun brass wire, he found that the electric current was no longer carried throughout the hemp cord, but instead seemed to vanish into the brass wire.  From this experiment he classified substances into two categories: \"electrics\" like glass, resin and silk and \"non-electrics\" like metal and water.  \"Non-electrics\" conducted charges while \"electrics\" held the charge. Intrigued by Gray's results, in 1732, C. F. du Fay began to conduct several experiments.  In his first experiment, Du Fay concluded that all objects except metals, animals, and liquids could be electrified by rubbing and that metals, animals and liquids could be electrified by means of an electric machine, thus discrediting Gray's \"electrics\" and \"non-electrics\" classification of substances. In 1737 Du Fay and Hauksbee independently discovered what they believed to be two kinds of frictional electricity; one generated from rubbing glass, the other from rubbing resin.  From this, Du Fay theorized that electricity consists of two electrical fluids, \"vitreous\" and \"resinous\", that are separated by friction and that neutralize each other when combined.  This picture of electricity was also supported by Christian Gottlieb Kratzenstein in his theoretical and experimental works.  The two-fluid theory would later give rise to the concept of \"positive\" and \"negative\" electrical charges devised by Benjamin Franklin. The Leyden jar, a type of capacitor for electrical energy in large quantities, was invented independently by Ewald Georg von Kleist on 11 October 1744 and by Pieter van Musschenbroek in 1745—1746 at Leiden University (the latter location giving the device its name).  William Watson, when experimenting with the Leyden jar, discovered in 1747 that a discharge of static electricity was equivalent to an electric current.  Capacitance was first observed by Von Kleist of Leyden in 1754.  Von Kleist happened to hold, near his electric machine, a small bottle, in the neck of which there was an iron nail.  Touching the iron nail accidentally with his other hand he received a severe electric shock.  In much the same way Musschenbroeck assisted by Cunaens received a more severe shock from a somewhat similar glass bottle.  Sir William Watson of England greatly improved this device, by covering the bottle, or jar, outside and in with tinfoil.  This piece of electrical apparatus will be easily recognized as the well-known Leyden jar, so called by the Abbot Nollet of Paris, after the place of its discovery. In 1741, John Ellicott \"proposed to measure the strength of electrification by its power to raise a weight in one scale of a balance while the other was held over the electrified body and pulled to it by its attractive power\". As early as 1746, Jean-Antoine Nollet (1700–1770) had performed experiments on the propagation speed of electricity.  By involving 200 monks connected from hand to hand by a 7-m iron wire so as to form a circle of about 1.6 km, he was able to prove that this speed is finite, even though very high.  In 1749, Sir William Watson conducted numerous experiments to ascertain the velocity of electricity in a wire.  These experiments, although perhaps not so intended, also demonstrated the possibility of transmitting signals to a distance by electricity.  In these experiments, the signal appeared to travel the 12,276-foot length of the insulated wire instantaneously.  Le Monnier in France had previously made somewhat similar experiments, sending shocks through an iron wire 1,319 feet long. About 1750, first experiments in electrotherapy were made.  Various experimenters made tests to ascertain the physiological and therapeutical effects of electricity.  Typical for this effort was Kratzenstein in Halle who in 1744 wrote a treatise on the subject.  Demainbray in Edinburgh examined the effects of electricity upon plants and concluded that the growth of two myrtle trees was quickened by electrification.  These myrtles were electrified \"during the whole month of October, 1746, and they put forth branches and blossoms sooner than other shrubs of the same kind not electrified.\" .  Abbé Ménon in France tried the effects of a continued application of electricity upon men and birds and found that the subjects experimented on lost weight, thus apparently showing that electricity quickened the excretions.  The efficacy of electric shocks in cases of paralysis was tested in the county hospital at Shrewsbury, England, with rather poor success. Benjamin Franklin promoted his investigations of electricity and theories through the famous, though extremely dangerous, experiment of having his son fly a kite through a storm-threatened sky.  A key attached to the kite string sparked and charged a Leyden jar, thus establishing the link between lightning and electricity.  Following these experiments, he invented a lightning rod.  It is either Franklin (more frequently) or Ebenezer Kinnersley of Philadelphia (less frequently) who is considered to have established the convention of positive and negative electricity. Theories regarding the nature of electricity were quite vague at this period, and those prevalent were more or less conflicting.  Franklin considered that electricity was an imponderable fluid pervading everything, and which, in its normal condition, was uniformly distributed in all substances.  He assumed that the electrical manifestations obtained by rubbing glass were due to the production of an excess of the electric fluid in that substance and that the manifestations produced by rubbing wax were due to a deficit of the fluid.  This explanation was opposed by supporters of the \"two-fluid\" theory like Robert Symmer in 1759.  In this theory, the vitreous and resinous electricities were regarded as imponderable fluids, each fluid being composed of mutually repellent particles while the particles of the opposite electricities are mutually attractive.  When the two fluids unite as a result of their attraction for one another, their effect upon external objects is neutralized.  The act of rubbing a body decomposes the fluids, one of which remains in excess on the body and manifests itself as vitreous or resinous electricity. Up to the time of Franklin's historic kite experiment, the identity of the electricity developed by rubbing and by electrostatic machines (frictional electricity) with lightning had not been generally established.  Dr. Wall, Abbot Nollet, Hauksbee, Stephen Gray and John Henry Winkler had indeed suggested the resemblance between the phenomena of \"electricity\" and \"lightning\", Gray having intimated that they only differed in degree.  It was doubtless Franklin, however, who first proposed tests to determine the sameness of the phenomena.  In a letter to Peter Comlinson of London, on 19 October 1752, Franklin, referring to his kite experiment, wrote, \"At this key the phial (Leyden jar) may be charged; and from the electric fire thus obtained spirits may be kindled, and all the other electric experiments be formed which are usually done by the help of a rubbed glass globe or tube, and thereby the sameness of the electric matter with that of lightning be completely demonstrated.\" On 10 May 1742 Thomas-François Dalibard, at Marley (near Paris), using a vertical iron rod 40 feet long, obtained results corresponding to those recorded by Franklin and somewhat prior to the date of Franklin's experiment.  Franklin's important demonstration of the sameness of frictional electricity and lightning doubtless added zest to the efforts of the many experimenters in this field in the last half of the 18th century, to advance the progress of the science. Franklin's observations aided later scientists such as Michael Faraday, Luigi Galvani, Alessandro Volta, André-Marie Ampère and Georg Simon Ohm, whose collective work provided the basis for modern electrical technology and for whom fundamental units of electrical measurement are named.  Others who would advance the field of knowledge included William Watson, Georg Matthias Bose, Smeaton, Louis-Guillaume Le Monnier, Jacques de Romas, Jean Jallabert, Giovanni Battista Beccaria, Tiberius Cavallo, John Canton, Robert Symmer, Abbot Nollet, John Henry Winkler, Richman , Dr. Wilson , Kinnersley, Joseph Priestley, Franz Aepinus, Edward Hussey Délavai, Henry Cavendish, and Charles-Augustin de Coulomb.  Descriptions of many of the experiments and discoveries of these early electrical scientists may be found in the scientific publications of the time, notably the \"Philosophical Transactions\", \"Philosophical Magazine\", \"Cambridge Mathematical Journal\", \"Young's Natural Philosophy\", Priestley's \"History of Electricity\", Franklin's \"Experiments and Observations on Electricity\", Cavalli's \"Treatise on Electricity\" and De la Rive's \"Treatise on Electricity\". Henry Elles was one of the first people to suggest links between electricity and magnetism.  In 1757 he claimed that he had written to the Royal Society in 1755 about the links between electricity and magnetism, asserting that \"there are some things in the power of magnetism very similar to those of electricity\" but he did \"not by any means think them the same\".  In 1760 he similarly claimed that in 1750 he had been the first \"to think how the electric fire may be the cause of thunder\".  Among the more important of the electrical research and experiments during this period were those of Franz Aepinus, a noted German scholar (1724–1802) and Henry Cavendish of London, England. Franz Aepinus is credited as the first to conceive of the view of the reciprocal relationship of electricity and magnetism.  In his work \"Tentamen Theoria Electricitatis et Magnetism\", published in Saint Petersburg in 1759, he gives the following amplification of Franklin's theory, which in some of its features is measurably in accord with present-day views: \"The particles of the electric fluid repel each other, attract and are attracted by the particles of all bodies with a force that decreases in proportion as the distance increases; the electric fluid exists in the pores of bodies; it moves unobstructedly through non-electric (conductors), but moves with difficulty in insulators; the manifestations of electricity are due to the unequal distribution of the fluid in a body, or to the approach of bodies unequally charged with the fluid.\"  Aepinus formulated a corresponding theory of magnetism excepting that, in the case of magnetic phenomena, the fluids only acted on the particles of iron.  He also made numerous electrical experiments apparently showing that, in order to manifest electrical effects, tourmaline must be heated to between 37.5°С and 100 °C.  In fact, tourmaline remains unelectrified when its temperature is uniform, but manifests electrical properties when its temperature is rising or falling.  Crystals that manifest electrical properties in this way are termed pyroelectric; along with tourmaline, these include sulphate of quinine and quartz. Henry Cavendish independently conceived a theory of electricity nearly akin to that of Aepinus.  In 1784, he was perhaps the first to utilize an electric spark to produce an explosion of hydrogen and oxygen in the proper proportions that would create pure water.  Cavendish also discovered the inductive capacity of dielectrics (insulators), and, as early as 1778, measured the specific inductive capacity for beeswax and other substances by comparison with an air condenser. Around 1784 C. A. Coulomb devised the torsion balance, discovering what is now known as Coulomb's law: the force exerted between two small electrified bodies varies inversely as the square of the distance, not as Aepinus in his theory of electricity had assumed, merely inversely as the distance.  According to the theory advanced by Cavendish, \"the particles attract and are attracted inversely as some less power of the distance than the cube.\"\"  A large part of the domain of electricity became virtually annexed by Coulomb's discovery of the law of inverse squares. Through the experiments of William Watson and others proving that electricity could be transmitted to a distance, the idea of making practical use of this phenomenon began, around 1753, to engross the minds of inquisitive people.  To this end, suggestions as to the employment of electricity in the transmission of intelligence were made.  The first of the methods devised for this purpose was probably that of Georges Lesage in 1774.  This method consisted of 24 wires, insulated from one another and each having had a pith ball connected to its distant end.  Each wire represented a letter of the alphabet.  To send a message, a desired wire was charged momentarily with electricity from an electric machine, whereupon the pith ball connected to that wire would fly out.  Other methods of telegraphing in which frictional electricity was employed were also tried, some of which are described in the history on the telegraph. The era of galvanic or voltaic electricity represented a revolutionary break from the historical focus on frictional electricity.  Alessandro Volta discovered that chemical reactions could be used to create positively charged anodes and negatively charged cathodes.  When a conductor was attached between these, the difference in the electrical potential (also known as voltage) drove a current between them through the conductor.  The potential difference between two points is measured in units of volts in recognition of Volta's work. The first mention of voltaic electricity, although not recognized as such at the time, was probably made by Johann Georg Sulzer in 1767, who, upon placing a small disc of zinc under his tongue and a small disc of copper over it, observed a peculiar taste when the respective metals touched at their edges.  Sulzer assumed that when the metals came together they were set into vibration, acting upon the nerves of the tongue to produce the effects noticed.  In 1790, Prof. Luigi Alyisio Galvani of Bologna, while conducting experiments on \"animal electricity\", noticed the twitching of a frog's legs in the presence of an electric machine.  He observed that a frog's muscle, suspended on an iron balustrade by a copper hook passing through its dorsal column, underwent lively convulsions without any extraneous cause, the electric machine being at this time absent. To account for this phenomenon, Galvani assumed that electricity of opposite kinds existed in the nerves and muscles of the frog, the muscles and nerves constituting the charged coatings of a Leyden jar.  Galvani published the results of his discoveries, together with his hypothesis, which engrossed the attention of the physicists of that time.  The most prominent of these was Volta, professor of physics at Pavia, who contended that the results observed by Galvani were the result of the two metals, copper and iron, acting as electromotors, and that the muscles of the frog played the part of a conductor, completing the circuit.  This precipitated a long discussion between the adherents of the conflicting views.  One group agreed with Volta that the electric current was the result of an electromotive force of contact at the two metals; the other adopted a modification of Galvani's view and asserted that the current was the result of a chemical affinity between the metals and the acids in the pile.  Michael Faraday wrote in the preface to his \"Experimental Researches\", relative to the question of whether metallic contact is productive of a part of the electricity of the voltaic pile: \"I see no reason as yet to alter the opinion I have given; ... but the point itself is of such great importance that I intend at the first opportunity renewing the inquiry, and, if I can, rendering the proofs either on the one side or the other, undeniable to all.\" Even Faraday himself, however, did not settle the controversy, and while the views of the advocates on both sides of the question have undergone modifications, as subsequent investigations and discoveries demanded, up to 1918 diversity of opinion on these points continued to crop out.  Volta made numerous experiments in support of his theory and ultimately developed the pile or battery, which was the precursor of all subsequent chemical batteries, and possessed the distinguishing merit of being the first means by which a prolonged continuous current of electricity was obtainable.  Volta communicated a description of his pile to the Royal Society of London and shortly thereafter Nicholson and Cavendish (1780) produced the decomposition of water by means of the electric current, using Volta's pile as the source of electromotive force. In 1800 Alessandro Volta constructed the first device to produce a large electric current, later known as the electric battery.  Napoleon, informed of his works, summoned him in 1801 for a command performance of his experiments.  He received many medals and decorations, including the Légion d'honneur. Davy in 1806, employing a voltaic pile of approximately 250 cells, or couples, decomposed potash and soda, showing that these substances were respectively the oxides of potassium and sodium, metals which previously had been unknown.  These experiments were the beginning of electrochemistry, the investigation of which Faraday took up, and concerning which in 1833 he announced his important law of electrochemical equivalents, viz.: \"\"The same quantity of electricity — that is, the same electric current — decomposes chemically equivalent quantities of all the bodies which it traverses; hence the weights of elements separated in these electrolytes are to each other as their chemical equivalents\".\"  Employing a battery of 2,000 elements of a voltaic pile Humphry Davy in 1809 gave the first public demonstration of the electric arc light, using for the purpose charcoal enclosed in a vacuum. Somewhat important to note, it was not until many years after the discovery of the voltaic pile that the sameness of annual and frictional electricity with voltaic electricity was clearly recognized and demonstrated.  Thus as late as January 1833 we find Faraday writing in a paper on the electricity of the electric ray. \" \"After an examination of the experiments of Walsh, Ingenhousz, Henry Cavendish, Sir H. Davy, and Dr. Davy, no doubt remains on my mind as to the identity of the electricity of the torpedo with common \"(frictional)\" and voltaic electricity; and I presume that so little will remain on the mind of others as to justify my refraining from entering at length into the philosophical proof of that identity.  The doubts raised by Sir Humphry Davy have been removed by his brother, Dr. Davy; the results of the latter being the reverse of those of the former.  ... The general conclusion which must, I think, be drawn from this collection of facts \"(a table showing the similarity, of properties of the diversely named electricities)\" is, that electricity, whatever may be its source, is identical in its nature\".\" It is proper to state, however, that prior to Faraday's time the similarity of electricity derived from different sources was more than suspected.  Thus, William Hyde Wollaston, wrote in 1801: \"\"This similarity in the means by which both electricity and galvanism (voltaic electricity) appear to be excited in addition to the resemblance that has been traced between their effects shows that they are both essentially the same and confirm an opinion that has already been advanced by others, that all the differences discoverable in the effects of the latter may be owing to its being less intense, but produced in much larger quantity\".\"  In the same paper Wollaston describes certain experiments in which he uses very fine wire in a solution of sulphate of copper through which he passed electric currents from an electric machine.  This is interesting in connection with the later day use of almost similarly arranged fine wires in electrolytic receivers in wireless, or radio-telegraphy. In the first half of the 19th century many very important additions were made to the world's knowledge concerning electricity and magnetism.  For example, in 1819 Hans Christian Ørsted of Copenhagen discovered the deflecting effect of an electric current traversing a wire upon- a suspended magnetic needle. This discovery gave a clue to the subsequently proved intimate relationship between electricity and magnetism which was promptly followed up by Ampère who shortly thereafter (1821) announced his celebrated theory of electrodynamics, relating to the force that one current exerts upon another, by its electro-magnetic effects, namely Ampere brought a multitude of phenomena into theory by his investigations of the mechanical forces between conductors supporting currents and magnets. The German physicist Seebeck discovered in 1821 that when heat is applied to the junction of two metals that had been soldered together an electric current is set up.  This is termed thermoelectricity.  Seebeck's device consists of a strip of copper bent at each end and soldered to a plate of bismuth.  A magnetic needle is placed parallel with the copper strip.  When the heat of a lamp is applied to the junction of the copper and bismuth an electric current is set up which deflects the needle. Around this time, Siméon Denis Poisson attacked the difficult problem of induced magnetization, and his results, though differently expressed, are still the theory, as a most important first approximation.  It was in the application of mathematics to physics that his services to science were performed.  Perhaps the most original, and certainly the most permanent in their influence, were his memoirs on the theory of electricity and magnetism, which virtually created a new branch of mathematical physics. George Green wrote \"An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\" in 1828.  The essay introduced several important concepts, among them a theorem similar to the modern Green's theorem, the idea of potential functions as currently used in physics, and the concept of what are now called Green's functions. George Green was the first person to create a mathematical theory of electricity and magnetism and his theory formed the foundation for the work of other scientists such as James Clerk Maxwell, William Thomson, and others. Peltier in 1834 discovered an effect opposite to thermoelectricity, namely, that when a current is passed through a couple of dissimilar metals the temperature is lowered or raised at the junction of the metals, depending on the direction of the current.  This is termed the Peltier effect.  The variations of temperature are found to be proportional to the strength of the current and not to the square of the strength of the current as in the case of heat due to the ordinary resistance of a conductor.  This second law is the C^2R law, discovered experimentally in 1841 by the English physicist Joule.  In other words, this important law is that the heat generated in any part of an electric circuit is directly proportional to the product of the resistance of this part of the circuit and to the square of the strength of current flowing in the circuit. In 1822 Johann Schweigger devised the first galvanometer.  This instrument was subsequently much improved by Wilhelm Weber (1833).  In 1825 William Sturgeon of Woolwich, England, invented the horseshoe and straight bar electromagnet, receiving therefor the silver medal of the Society of Arts.  In 1837 Carl Friedrich Gauss and Weber (both noted workers of this period) jointly invented a reflecting galvanometer for telegraph purposes.  This was the forerunner of the Thomson reflecting and other exceedingly sensitive galvanometers once used in submarine signaling and still widely employed in electrical measurements.  Arago in 1824 made the important discovery that when a copper disc is rotated in its own plane, and if a magnetic needle be freely suspended on a pivot over the disc, the needle will rotate with the disc.  If on the other hand the needle is fixed it will tend to retard the motion of the disc.  This effect was termed Arago's rotations. Futile attempts were made by Charles Babbage, Peter Barlow, John Herschel and others to explain this phenomenon.  The true explanation was reserved for Faraday, namely, that electric currents are induced in the copper disc by the cutting of the magnetic lines of force of the needle, which currents in turn react on the needle.  Georg Simon Ohm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book \"Die galvanische Kette, mathematisch bearbeitet\". He drew considerable inspiration from Fourier's work on heat conduction in the theoretical explanation of his work.  For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant potential difference.  He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature.  He then added test wires of varying length, diameter, and material to complete the circuit.  He found that his data could be modeled through a simple equation with variable composed of the reading from a galvanometer, the length of the test conductor, thermocouple junction temperature, and a constant of the entire setup.  From this, Ohm determined his law of proportionality and published his results.  In 1827, he announced the now famous law that bears his name, that is: Electromotive force = Current × Resistance Ohm brought into order a host of puzzling facts connecting electromotive force and electric current in conductors, which all previous electricians had only succeeded in loosely binding together qualitatively under some rather vague statements.  Ohm found that the results could be summed up in such a simple law and by Ohm's discovery a large part of the domain of electricity became annexed to theory. The discovery of electromagnetic induction was made almost simultaneously, although independently, by Michael Faraday, who was first to make the discovery in 1831, and Joseph Henry in 1832.  Henry's discovery of self-induction and his work on spiral conductors using a copper coil were made public in 1835, just before those of Faraday. In 1831 began the epoch-making researches of Michael Faraday, the famous pupil and successor of Humphry Davy at the head of the Royal Institution, London, relating to electric and electromagnetic induction.  The remarkable researches of Faraday, the \"prince of experimentalists\", on electrostatics and electrodynamics and the induction of currents.  These were rather long in being brought from the crude experimental state to a compact system, expressing the real essence.  Faraday was not a competent mathematician, but had he been one, he would have been greatly assisted in his researches, have saved himself much useless speculation, and would have anticipated much later work.  He would, for instance, knowing Ampere's theory, by his own results have readily been led to Neumann's theory, and the connected work of Helmholtz and Thomson.  Faraday's studies and researches extended from 1831 to 1855 and a detailed description of his experiments, deductions and speculations are to be found in his compiled papers, entitled Experimental Researches in Electricity.'  Faraday was by profession a chemist.  He was not in the remotest degree a mathematician in the ordinary sense — indeed it is a question if in all his writings there is a single mathematical formula. The experiment which led Faraday to the discovery of electromagnetic induction was made as follows: He constructed what is now and was then termed an induction coil, the primary and secondary wires of which were wound on a wooden bobbin, side by side, and insulated from one another.  In the circuit of the primary wire he placed a battery of approximately 100 cells.  In the secondary wire he inserted a galvanometer.  On making his first test he observed no results, the galvanometer remaining quiescent, but on increasing the length of the wires he noticed a deflection of the galvanometer in the secondary wire when the circuit of the primary wire was made and broken.  This was the first observed instance of the development of electromotive force by electromagnetic induction. He also discovered that induced currents are established in a second closed circuit when the current strength is varied in the first wire, and that the direction of the current in the secondary circuit is opposite to that in the first circuit.  Also that a current is induced in a secondary circuit when another circuit carrying a current is moved to and from the first circuit, and that the approach or withdrawal of a magnet to or from a closed circuit induces momentary currents in the latter.  In short, within the space of a few months Faraday discovered by experiment virtually all the laws and facts now known concerning electro-magnetic induction and magneto-electric induction.  Upon these discoveries, with scarcely an exception, depends the operation of the telephone, the dynamo machine, and incidental to the dynamo electric machine practically all the gigantic electrical industries of the world, including electric lighting, electric traction, the operation of electric motors for power purposes, and electro-plating, electrotyping, etc. In his investigations of the peculiar manner in which iron filings arrange themselves on a cardboard or glass in proximity to the poles of a magnet, Faraday conceived the idea of magnetic \"lines of force\" extending from pole to pole of the magnet and along which the filings tend to place themselves.  On the discovery being made that magnetic effects accompany the passage of an electric current in a wire, it was also assumed that similar magnetic lines of force whirled around the wire.  For convenience and to account for induced electricity it was then assumed that when these lines of force are \"\"cut\"\" by a wire in passing across them or when the lines of force in rising and falling cut the wire, a current of electricity is developed, or to be more exact, an electromotive force is developed in the wire that sets up a current in a closed circuit.  Faraday advanced what has been termed the \"molecular theory of electricity\" which assumes that electricity is the manifestation of a peculiar condition of the molecule of the body rubbed or the ether surrounding the body.  Faraday also, by experiment, discovered paramagnetism and diamagnetism, namely, that all solids and liquids are either attracted or repelled by a magnet.  For example, iron, nickel, cobalt, manganese, chromium, etc., are paramagnetic (attracted by magnetism), whilst other substances, such as bismuth, phosphorus, antimony, zinc, etc., are repelled by magnetism or are diamagnetic. Brugans of Leyden in 1778 and Le Baillif and Becquerel in 1827 had previously discovered diamagnetism in the case of bismuth and antimony.  Faraday also rediscovered specific inductive capacity in 1837, the results of the experiments by Cavendish not having been published at that time.  He also predicted the retardation of signals on long submarine cables due to the inductive effect of the insulation of the cable, in other words, the static capacity of the cable.  In 1816 telegraph pioneer Francis Ronalds had also observed signal retardation on his buried telegraph lines, attributing it to induction. The 25 years immediately following Faraday's discoveries of electromagnetic induction were fruitful in the promulgation of laws and facts relating to induced currents and to magnetism.  In 1834 Heinrich Lenz and Moritz von Jacobi independently demonstrated the now familiar fact that the currents induced in a coil are proportional to the number of turns in the coil.  Lenz also announced at that time his important law that, in all cases of electromagnetic induction the induced currents have such a direction that their reaction tends to stop the motion that produces them, a law that was perhaps deducible from Faraday's explanation of Arago's rotations. The induction coil was first designed by Nicholas Callan in 1836.  In 1845 Joseph Henry, the American physicist, published an account of his valuable and interesting experiments with induced currents of a high order, showing that currents could be induced from the secondary of an induction coil to the primary of a second coil, thence to its secondary wire, and so on to the primary of a third coil, etc.  Heinrich Daniel Ruhmkorff further developed the induction coil, the Ruhmkorff coil was patented in 1851, and he utilized long windings of copper wire to achieve a spark of approximately 2 inches (50 mm) in length.  In 1857, after examining a greatly improved version made by an American inventor, Edward Samuel Ritchie, Ruhmkorff improved his design (as did other engineers), using glass insulation and other innovations to allow the production of sparks more than 300 mm long. Up to the middle of the 19th century, indeed up to about 1870, electrical science was, it may be said, a sealed book to the majority of electrical workers.  Prior to this time a number of handbooks had been published on electricity and magnetism, notably Auguste de La Rive's exhaustive ' \"Treatise on Electricity\",' in 1851 (French) and 1853 (English); August Beer's \"Einleitung in die Elektrostatik, die Lehre vom Magnetismus und die Elektrodynamik\", Wiedemann's ' \"Galvanismus\",' and Reiss' \"'Reibungsal-elektricitat\".'  But these works consisted in the main in details of experiments with electricity and magnetism, and but little with the laws and facts of those phenomena.  Henry d'Abria published the results of some researches into the laws of induced currents, but owing to their complexity of the investigation it was not productive of very notable results.  Around the mid-19th century, Fleeming Jenkin's work on ' \"Electricity and Magnetism\" ' and Clerk Maxwell's ' \"Treatise on Electricity and Magnetism\" ' were published. These books were departures from the beaten path.  As Jenkin states in the preface to his work the science of the schools was so dissimilar from that of the practical electrician that it was quite impossible to give students sufficient, or even approximately sufficient, textbooks.  A student he said might have mastered de la Rive's large and valuable treatise and yet feel as if in an unknown country and listening to an unknown tongue in the company of practical men.  As another writer has said, with the coming of Jenkin's and Maxwell's books all impediments in the way of electrical students were removed, \"\"the full meaning of Ohm's law becomes clear; electromotive force, difference of potential, resistance, current, capacity, lines of force, magnetization and chemical affinity were measurable, and could be reasoned about, and calculations could be made about them with as much certainty as calculations in dynamics\"\". About 1850, Kirchhoff published his laws relating to branched or divided circuits.  He also showed mathematically that according to the then prevailing electrodynamic theory, electricity would be propagated along a perfectly conducting wire with the velocity of light.  Helmholtz investigated mathematically the effects of induction upon the strength of a current and deduced therefrom equations, which experiment confirmed, showing amongst other important points the retarding effect of self-induction under certain conditions of the circuit. In 1853, Sir William Thomson (later Lord Kelvin) predicted as a result of mathematical calculations the oscillatory nature of the electric discharge of a condenser circuit.  To Henry, however, belongs the credit of discerning as a result of his experiments in 1842 the oscillatory nature of the Leyden jar discharge.  He wrote: \"The phenomena require us to admit the existence of a principal discharge in one direction, and then several reflex actions backward and forward, each more feeble than the preceding, until the equilibrium is obtained\".  These oscillations were subsequently observed by B. W. Feddersen (1857) who using a rotating concave mirror projected an image of the electric spark upon a sensitive plate, thereby obtaining a photograph of the spark which plainly indicated the alternating nature of the discharge.  Sir William Thomson was also the discoverer of the electric convection of heat (the \"Thomson\" effect).  He designed for electrical measurements of precision his quadrant and absolute electrometers.  The reflecting galvanometer and siphon recorder, as applied to submarine cable signaling, are also due to him. About 1876 the American physicist Henry Augustus Rowland of Baltimore demonstrated the important fact that a static charge carried around produces the same magnetic effects as an electric current.  The Importance of this discovery consists in that it may afford a plausible theory of magnetism, namely, that magnetism may be the result of directed motion of rows of molecules carrying static charges. After Faraday's discovery that electric currents could be developed in a wire by causing it to cut across the lines of force of a magnet, it was to be expected that attempts would be made to construct machines to avail of this fact in the development of voltaic currents.  The first machine of this kind was due to Hippolyte Pixii, 1832.  It consisted of two bobbins of iron wire, opposite which the poles of a horseshoe magnet were caused to rotate.  As this produced in the coils of the wire an alternating current, Pixii arranged a commutating device (commutator) that converted the alternating current of the coils or armature into a direct current in the external circuit.  This machine was followed by improved forms of magneto-electric machines due to Ritchie, Saxton, Clarke 1834, Stohrer 1843, Nollet 1849, Shepperd 1856, Van Maldern, Siemens, Wilde and others. A notable advance in the art of dynamo construction was made by Mr. S. A. Varley in 1866 and by Dr. Charles William Siemens and Mr. Charles Wheatstone, who independently discovered that when a coil of wire, or armature, of the dynamo machine is rotated between the poles (or in the \"field\") of an electromagnet, a weak current is set up in the coil due to residual magnetism in the iron of the electromagnet, and that if the circuit of the armature be connected with the circuit of the electromagnet, the weak current developed in the armature increases the magnetism in the field.  This further increases the magnetic lines of force in which the armature rotates, which still further increases the current in the electromagnet, thereby producing a corresponding increase in the field magnetism, and so on, until the maximum electromotive force which the machine is capable of developing is reached.  By means of this principle the dynamo machine develops its own magnetic field, thereby much increasing its efficiency and economical operation.  Not by any means, however, was the dynamo electric machine perfected at the time mentioned. In 1860 an important improvement had been made by Dr. Antonio Pacinotti of Pisa who devised the first electric machine with a ring armature.  This machine was first used as an electric motor, but afterward as a generator of electricity.  The discovery of the principle of the reversibility of the dynamo electric machine (variously attributed to Walenn 1860; Pacinotti 1864 ; Fontaine, Gramme 1873; Deprez 1881, and others) whereby it may be used as an electric motor or as a generator of electricity has been termed one of the greatest discoveries of the 19th century. In 1872 the drum armature was devised by Hefner-Alteneck.  This machine in a modified form was subsequently known as the Siemens dynamo.  These machines were presently followed by the Schuckert, Gulcher, Fein, Brush, Hochhausen, Edison and the dynamo machines of numerous other inventors.  In the early days of dynamo machine construction the machines were mainly arranged as direct current generators, and perhaps the most important application of such machines at that time was in electro-plating, for which purpose machines of low voltage and large current strength were employed. Beginning about 1887 alternating current generators came into extensive operation and the commercial development of the transformer, by means of which currents of low voltage and high current strength are transformed to currents of high voltage and low current strength, and vice versa, in time revolutionized the transmission of electric power to long distances.  Likewise the introduction of the rotary converter (in connection with the \"step-down\" transformer) which converts alternating currents into direct currents (and vice versa) has effected large economies in the operation of electric power systems. Before the introduction of dynamo electric machines, voltaic, or primary, batteries were extensively used for electro-plating and in telegraphy.  There are two distinct types of voltaic cells, namely, the \"open\" and the \"closed\", or \"constant\", type.  The open type in brief is that type which operated on closed circuit becomes, after a short time, polarized; that is, gases are liberated in the cell which settle on the negative plate and establish a resistance that reduces the current strength.  After a brief interval of open circuit these gases are eliminated or absorbed and the cell is again ready for operation.  Closed circuit cells are those in which the gases in the cells are absorbed as quickly as liberated and hence the output of the cell is practically uniform.  The Leclanché and Daniell cells, respectively, are familiar examples of the \"open\" and \"closed\" type of voltaic cell.  The \"open\" cells are used very extensively at present, especially in the dry cell form, and in annunciator and other open circuit signal systems.  Batteries of the Daniell or \"gravity\" type were employed almost generally in the United States and Canada as the source of electromotive force in telegraphy before the dynamo machine became available, and still are largely used for this service or as \"local\" cells.  Batteries of the \"gravity\" and the Edison-Lalande types are still much used in \"closed circuit\" systems. In the late 19th century, the term luminiferous aether, meaning light-bearing aether, was a conjectured medium for the propagation of light.  The word \"aether\" stems via Latin from the Greek αιθήρ, from a root meaning to kindle, burn, or shine.  It signifies the substance which was thought in ancient times to fill the upper regions of space, beyond the clouds. In 1864 James Clerk Maxwell of Edinburgh announced his electromagnetic theory of light, which was perhaps the greatest single step in the world's knowledge of electricity.  Maxwell had studied and commented on the field of electricity and magnetism as early as 1855/6 when \"On Faraday's lines of force\" was read to the Cambridge Philosophical Society.  The paper presented a simplified model of Faraday's work, and how the two phenomena were related.  He reduced all of the current knowledge into a linked set of differential equations with 20 equations in 20 variables.  This work was later published as \"On Physical Lines of Force\" in March 1861.  In order to determine the force which is acting on any part of the machine we must find its momentum, and then calculate the rate at which this momentum is being changed.  This rate of change will give us the force.  The method of calculation which it is necessary to employ was first given by Lagrange, and afterwards developed, with some modifications, by Hamilton's equations.  It is usually referred to as Hamilton's principle; when the equations in the original form are used they are known as Lagrange's equations.  Now Maxwell logically showed how these methods of calculation could be applied to the electro-magnetic field.  The energy of a dynamical system is partly kinetic, partly potential.  Maxwell supposes that the magnetic energy of the field is kinetic energy, the electric energy potential. Around 1862, while lecturing at King's College, Maxwell calculated that the speed of propagation of an electromagnetic field is approximately that of the speed of light.  He considered this to be more than just a coincidence, and commented \"\"We can scarcely avoid the conclusion that light consists in the transverse undulations of the same medium which is the cause of electric and magnetic phenomena.\"\" Working on the problem further, Maxwell showed that the equations predict the existence of waves of oscillating electric and magnetic fields that travel through empty space at a speed that could be predicted from simple electrical experiments; using the data available at the time, Maxwell obtained a velocity of 310,740,000 m/s.  In his 1864 paper \"A Dynamical Theory of the Electromagnetic Field\", Maxwell wrote, \"The agreement of the results seems to show that light and magnetism are affections of the same substance, and that light is an electromagnetic disturbance propagated through the field according to electromagnetic laws\". As already noted herein Faraday, and before him, Ampère and others, had inklings that the luminiferous ether of space was also the medium for electric action.  It was known by calculation and experiment that the velocity of electricity was approximately 186,000 miles per second; that is, equal to the velocity of light, which in itself suggests the idea of a relationship between -electricity and \"light.\"  A number of the earlier philosophers or mathematicians, as Maxwell terms them, of the 19th century, held the view that electromagnetic phenomena were explainable by action at a distance.  Maxwell, following Faraday, contended that the seat of the phenomena was in the medium.  The methods of the mathematicians in arriving at their results were synthetical while Faraday's methods were analytical.  Faraday in his mind's eye saw lines of force traversing all space where the mathematicians saw centres of force attracting at a distance.  Faraday sought the seat of the phenomena in real actions going on in the medium; they were satisfied that they had found it in a power of action at a distance on the electric fluids. Both of these methods, as Maxwell points out, had succeeded in explaining the propagation of light as an electromagnetic phenomenon while at the same time the fundamental conceptions of what the quantities concerned are, radically differed.  The mathematicians assumed that insulators were barriers to electric currents; that, for instance, in a Leyden jar or electric condenser the electricity was accumulated at one plate and that by some occult action at a distance electricity of an opposite kind was attracted to the other plate. Maxwell, looking further than Faraday, reasoned that if light is an electromagnetic phenomenon and is transmissible through dielectrics such as glass, the phenomenon must be in the nature of electromagnetic currents in the dielectrics.  He therefore contended that in the charging of a condenser, for instance, the action did not stop at the insulator, but that some \"displacement\" currents are set up in the insulating medium, which currents continue until the resisting force of the medium equals that of the charging force.  In a closed conductor circuit, an electric current is also a displacement of electricity. The conductor offers a certain resistance, akin to friction, to the displacement of electricity, and heat is developed in the conductor, proportional to the square of the current(as already stated herein), which current flows as long as the impelling electric force continues.  This resistance may be likened to that met with by a ship as it displaces in the water in its progress.  The resistance of the dielectric is of a different nature and has been compared to the compression of multitudes of springs, which, under compression, yield with an increasing back pressure, up to a point where the total back pressure equals the initial pressure.  When the initial pressure is withdrawn the energy expended in compressing the \"springs\" is returned to the circuit, concurrently with the return of the springs to their original condition, this producing a reaction in the opposite direction.  Consequently, the current due to the displacement of electricity in a conductor may be continuous, while the displacement currents in a dielectric are momentary and, in a circuit or medium which contains but little resistance compared with capacity or inductance reaction, the currents of discharge are of an oscillatory or alternating nature. Maxwell extended this view of displacement currents in dielectrics to the ether of free space.  Assuming light to be the manifestation of alterations of electric currents in the ether, and vibrating at the rate of light vibrations, these vibrations by induction set up corresponding vibrations in adjoining portions of the ether, and in this way the undulations corresponding to those of light are propagated as an electromagnetic effect in the ether.  Maxwell's electromagnetic theory of light obviously involved the existence of electric waves in free space, and his followers set themselves the task of experimentally demonstrating the truth of the theory.  By 1871, he presented the \"Remarks on the mathematical classification of physical quantities\". In 1887, the German physicist Heinrich Hertz in a series of experiments proved the actual existence of electromagnetic waves, showing that transverse free space electromagnetic waves can travel over some distance as predicted by Maxwell and Faraday.  Hertz published his work in a book titled: \"Electric waves: being researches on the propagation of electric action with finite velocity through space\".  The discovery of electromagnetic waves in space led to the development of radio in the closing years of the 19th century. The electron as a unit of charge in electrochemistry was posited by G. Johnstone Stoney in 1874, who also coined the term \"electron\" in 1894.  Plasma was first identified in a Crookes tube, and so described by Sir William Crookes in 1879 (he called it \"radiant matter\").  The place of electricity in leading up to the discovery of those beautiful phenomena of the Crookes Tube (due to Sir William Crookes), viz., Cathode rays, and later to the discovery of Roentgen or X-rays, must not be overlooked, since without electricity as the excitant of the tube the discovery of the rays might have been postponed indefinitely.  It has been noted herein that Dr. William Gilbert was termed the founder of electrical science.  This must, however, be regarded as a comparative statement. Oliver Heaviside was a self-taught scholar who reformulated Maxwell's field equations in terms of electric and magnetic forces and energy flux, and independently co-formulated vector analysis.  His series of articles continued the work entitled \"\"Electromagnetic Induction and its Propagation\"\", commenced in \"The Electrician\" in 1885 to nearly 1887 (ed., the latter part of the work dealing with the propagation of electromagnetic waves along wires through the dielectric surrounding them), when the great pressure on space and the want of readers appeared to necessitate its abrupt discontinuance.  (A straggler piece appeared December 31, 1887.)  He wrote an interpretation of the transcendental formulae of electromagnetism.  Following the real object of true naturalists when they employ mathematics to assist them, he wrote to find out the connections of known phenomena, and by deductive reasoning, to obtain a knowledge of electromagnetic phenomena.  Although at odds with the scientific establishment for most of his life, Heaviside changed the face of mathematics and science for years to come. Of the changes in the field of electromagnetic theory, certain conclusions from \"Electro-Magnetic Theory\" by Heaviside are, if not drawn, at least indicated in this book.  Two of them may be stated as follows: The ultimate results of his work are twofold.  (1) The first ultimate result is purely mathematical, which is important only to those who study mathematical physics.  The system of \"vectorial algebra\" as developed by Mr. Heaviside was used because of ease for physical investigations to the methods of quaternions.  (2) The second ultimate result is physical.  It consists in more closely uniting the more recondite problems of telegraphy, telephony, \"Teslaic phenomena\" and \"Hertzian phenomena\" with the fundamental properties of the aether.  In elucidating this connection, the merit of the book appears most prominently as a stepping-stone to the goal in the full view of all physical analysis, namely, the resolution of all physical phenomena to the activities of the aether, and of matter in the aether, under the laws of dynamics. During the late 1890s a number of physicists proposed that electricity, as observed in studies of electrical conduction in conductors, electrolytes, and cathode ray tubes, consisted of discrete units, which were given a variety of names, but the reality of these units had not been confirmed in a compelling way.  However, there were also indications that the cathode rays had wavelike properties. Faraday, Weber, Helmholtz, Clifford and others had glimpses of this view; and the experimental works of Zeeman, Goldstein, Crookes, J. J. Thomson and others had greatly strengthened this view.  Weber predicted that electrical phenomena were due to the existence of electrical atoms, the influence of which on one another depended on their position and relative accelerations and velocities.  Helmholtz and others also contended that the existence of electrical atoms followed from Faraday's laws of electrolysis, and Johnstone Stoney, to whom is due the term \"electron\", showed that each chemical ion of the decomposed electrolyte carries a definite and constant quantity of electricity, and inasmuch as these charged ions are separated on the electrodes as neutral substances there must be an instant, however brief, when the charges must be capable of existing separately as electrical atoms; while in 1887, Clifford wrote: \"There is great reason to believe that every material atom carries upon it a small electric current, if it does not wholly consist of this current.\" The Serbian American engineer Nikola Tesla learned of Hertz' experiments at the Exposition Universelle in 1889 and launched into his own experiments in high frequency and high potential current developing \"high-frequency\" alternators (which operated around 15,000 hertz).  .  He concluded from his observations that Maxwell and Hertz were wrong about the existence of airborne electromagnetic waves (which he attributed it to what he called \"electrostatic thrusts\") but saw great potential in Maxwell's idea that electricity and light were part of the same phenomena, seeing it as a way to create a new type of wireless electric lighting.  By 1893 he was giving lectures on \"\", including a demonstration where he would light a Geissler tubes wirelessly.  Tesla worked for many years after that trying to develop a wireless power distribution system. In 1896, J. J. Thomson performed experiments indicating that cathode rays really were particles, found an accurate value for their charge-to-mass ratio e/m, and found that e/m was independent of cathode material.  He made good estimates of both the charge e and the mass m, finding that cathode ray particles, which he called \"corpuscles\", had perhaps one thousandth of the mass of the least massive ion known (hydrogen).  He further showed that the negatively charged particles produced by radioactive materials, by heated materials, and by illuminated materials, were universal.  The nature of the Crookes tube \"cathode ray\" matter was identified by Thomson in 1897. In the late 19th century, the Michelson–Morley experiment was performed by Albert A. Michelson and Edward W. Morley at what is now Case Western Reserve University.  It is generally considered to be the evidence against the theory of a luminiferous aether.  The experiment has also been referred to as \"the kicking-off point for the theoretical aspects of the Second Scientific Revolution.\"  Primarily for this work, Michelson was awarded the Nobel Prize in 1907.  Dayton Miller continued with experiments, conducting thousands of measurements and eventually developing the most accurate interferometer in the world at that time.  Miller and others, such as Morley, continue observations and experiments dealing with the concepts.  A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. By the end of the 19th century electrical engineers had become a distinct profession, separate from physicists and inventors.  They created companies that investigated, developed and perfected the techniques of electricity transmission, and gained support from governments all over the world for starting the first worldwide electrical telecommunication network, the telegraph network.  Pioneers in this field included Werner von Siemens, founder of Siemens AG in 1847, and John Pender, founder of Cable & Wireless. The first public demonstration of a \"alternator system\" took place in 1886.  Large two-phase alternating current generators were built by a British electrician, J. E. H. Gordon, in 1882.  Lord Kelvin and Sebastian Ferranti also developed early alternators, producing frequencies between 100 and 300 hertz.  After 1891, polyphase alternators were introduced to supply currents of multiple differing phases.  Later alternators were designed for varying alternating-current frequencies between sixteen and about one hundred hertz, for use with arc lighting, incandescent lighting and electric motors. The possibility of obtaining the electric current in large quantities, and economically, by means of dynamo electric machines gave impetus to the development of incandescent and arc lighting.  Until these machines had attained a commercial basis voltaic batteries were the only available source of current for electric lighting and power.  The cost of these batteries, however, and the difficulties of maintaining them in reliable operation were prohibitory of their use for practical lighting purposes.  The date of the employment of arc and incandescent lamps may be set at about 1877. Even in 1880, however, but little headway had been made toward the general use of these illuminants; the rapid subsequent growth of this industry is a matter of general knowledge.  The employment of storage batteries, which were originally termed secondary batteries or accumulators, began about 1879.  Such batteries are now utilized on a large scale as auxiliaries to the dynamo machine in electric power-houses and substations, in electric automobiles and in immense numbers in automobile ignition and starting systems, also in fire alarm telegraphy and other signal systems. In 1893, the World's Columbian International Exposition was held in a building which was devoted to electrical exhibits.  General Electric Company (backed by Edison and J. P. Morgan) had proposed to power the electric exhibits with direct current at the cost of one million dollars.  However, Westinghouse proposed to illuminate the Columbian Exposition in Chicago with alternating current for half that price, and Westinghouse won the bid.  It was an historical moment and the beginning of a revolution, as George Westinghouse introduced the public to electrical power by illuminating the Exposition. Between 1885 and 1890 Galileo Ferraris in Italy, Nikola Tesla in the United States, and Mikhail Dolivo-Dobrovolsky in Germany explored poly-phase currents combined with electromagnetic induction leading to the development of practical AC induction motors.  The AC induction motor helped usher in the Second Industrial Revolution.  The rapid advance of electrical technology in the latter 19th and early 20th centuries led to commercial rivalries.  In the War of Currents in the late 1880s, George Westinghouse and Thomas Edison became adversaries due to Edison's promotion of direct current (DC) for electric power distribution over alternating current (AC) advocated by Westinghouse. Several inventors helped develop commercial systems.  Samuel Morse, inventor of a long-range telegraph; Thomas Edison, inventor of the first commercial electrical energy distribution network; George Westinghouse, inventor of the electric locomotive; Alexander Graham Bell, the inventor of the telephone and founder of a successful telephone business. In 1871 the electric telegraph had grown to large proportions and was in use in every civilized country in the world, its lines forming a network in all directions over the surface of the land.  The system most generally in use was the electromagnetic telegraph due to S. F. B. Morse of New York, or modifications of his system.  Submarine cables connecting the Eastern and Western hemispheres were also in successful operation at that time. When, however, in 1918 one views the vast applications of electricity to electric light, electric railways, electric power and other purposes (all it may be repeated made possible and practicable by the perfection of the dynamo machine), it is difficult to believe that no longer ago than 1871 the author of a book published in that year, in referring to the state of the art of applied electricity at that time, could have truthfully written: \"The most important and remarkable of the uses which have been made of electricity consists in its application to telegraph purposes\".  The statement was, however, quite accurate and perhaps the time could have been carried forward to the year 1876 without material modification of the remarks.  In that year the telephone, due to Alexander Graham Bell, was invented, but it was not until several years thereafter that its commercial employment began in earnest.  Since that time also the sister branches of electricity just mentioned have advanced and are advancing with such gigantic strides in every direction that it is difficult to place a limit upon their progress.  Electrical devices account of the use of electricity in the arts and industries. The 1880s saw the spread of large scale commercial electric power systems, first used for lighting and eventually for electro-motive power and heating.  Systems early on used alternating current and direct current.  Large centralized power generation became possible when it was recognized that alternating current electric power lines could use transformers to take advantage of the fact that each doubling of the voltage would allow the same size cable to transmit the same amount of power four times the distance.  Transformer were used to raise voltage at the point of generation (a representative number is a generator voltage in the low kilovolt range) to a much higher voltage (tens of thousands to several hundred thousand volts) for primary transmission, followed to several downward transformations, for commercial and residential domestic use. The International Electro-Technical Exhibition of 1891 featuring the long distance transmission of high-power, three-phase electric current.  It was held between 16 May and 19 October on the disused site of the three former \"Westbahnhöfe\" (Western Railway Stations) in Frankfurt am Main.  The exhibition featured the first long distance transmission of high-power, three-phase electric current, which was generated 175 km away at Lauffen am Neckar.  As a result of this successful field trial, three-phase current became established for electrical transmission networks throughout the world. Much was done in the direction in the improvement of railroad terminal facilities, and it is difficult to find one steam railroad engineer who would have denied that all the important steam railroads of this country were not to be operated electrically.  In other directions the progress of events as to the utilization of electric power was expected to be equally rapid.  In every part of the world the power of falling water, nature's perpetual motion machine, which has been going to waste since the world began, is now being converted into electricity and transmitted by wire hundreds of miles to points where it is usefully and economically employed. The first windmill for electricity production was built in Scotland in July 1887 by the Scottish electrical engineer James Blyth.  Across the Atlantic, in Cleveland, Ohio a larger and heavily engineered machine was designed and constructed in 1887–88 by Charles F. Brush, this was built by his engineering company at his home and operated from 1886 until 1900.  The Brush wind turbine had a rotor 56 ft in diameter and was mounted on a 60-foot (18 m) tower.  Although large by today's standards, the machine was only rated at 12 kW; it turned relatively slowly since it had 144 blades.  The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory.  The machine fell into disuse after 1900 when electricity became available from Cleveland's central stations, and was abandoned in 1908. Various units of electricity and magnetism have been adopted and named by representatives of the electrical engineering institutes of the world, which units and names have been confirmed and legalized by the governments of the United States and other countries.  Thus the volt, from the Italian Volta, has been adopted as the practical unit of electromotive force, the ohm, from the enunciator of Ohm's law, as the practical unit of resistance; the ampere, after the eminent French scientist of that name, as the practical unit of current strength, the henry as the practical unit of inductance, after Joseph Henry and in recognition of his early and important experimental work in mutual induction. Dewar and John Ambrose Fleming predicted that at absolute zero, pure metals would become perfect electromagnetic conductors (though, later, Dewar altered his opinion on the disappearance of resistance believing that there would always be some resistance).  Walther Hermann Nernst developed the third law of thermodynamics and stated that absolute zero was unattainable.  Carl von Linde and William Hampson, both commercial researchers, nearly at the same time filed for patents on the Joule–Thomson effect.  Linde's patent was the climax of 20 years of systematic investigation of established facts, using a regenerative counterflow method.  Hampson's design was also of a regenerative method.  The combined process became known as the Linde–Hampson liquefaction process.  Heike Kamerlingh Onnes purchased a Linde machine for his research.  Zygmunt Florenty Wróblewski conducted research into electrical properties at low temperatures, though his research ended early due to his accidental death.  Around 1864, Karol Olszewski and Wroblewski predicted the electrical phenomena of dropping resistance levels at ultra-cold temperatures.  Olszewski and Wroblewski documented evidence of this in the 1880s.  A milestone was achieved on 10 July 1908 when Onnes at the Leiden University in Leiden produced, for the first time, liquified helium and achieved superconductivity. In 1900, William Du Bois Duddell develops the Singing Arc and produced melodic sounds, from a low to a high-tone, from this arc lamp. Between 1900 and 1910, many scientists like Wilhelm Wien, Max Abraham, Hermann Minkowski, or Gustav Mie believed that all forces of nature are of electromagnetic origin (the so-called \"electromagnetic world view\").  This was connected with the electron theory developed between 1892 and 1904 by Hendrik Lorentz.  Lorentz introduced a strict separation between matter (electrons) and the aether, whereby in his model the ether is completely motionless, and it won't be set in motion in the neighborhood of ponderable matter.  Contrary to other electron models before, the electromagnetic field of the ether appears as a mediator between the electrons, and changes in this field can propagate not faster than the speed of light. In 1896, three years after submitting his thesis on the Kerr effect, Pieter Zeeman disobeyed the direct orders of his supervisor and used laboratory equipment to measure the splitting of spectral lines by a strong magnetic field.  Lorentz theoretically explained the Zeeman effect on the basis of his theory, for which both received the Nobel Prize in Physics in 1902.  A fundamental concept of Lorentz's theory in 1895 was the \"theorem of corresponding states\" for terms of order v/c.  This theorem states that a moving observer (relative to the ether) makes the same observations as a resting observer.  This theorem was extended for terms of all orders by Lorentz in 1904.  Lorentz noticed, that it was necessary to change the space-time variables when changing frames and introduced concepts like physical length contraction (1892) to explain the Michelson–Morley experiment, and the mathematical concept of local time (1895) to explain the aberration of light and the Fizeau experiment.  That resulted in the formulation of the so-called Lorentz transformation by Joseph Larmor (1897, 1900) and Lorentz (1899, 1904).  As Lorentz later noted (1921, 1928), he considered the time indicated by clocks resting in the aether as \"true\" time, while local time was seen by him as a heuristic working hypothesis and a mathematical artifice.  Therefore, Lorentz's theorem is seen by modern historians as being a mathematical transformation from a \"real\" system resting in the aether into a \"fictitious\" system in motion. Continuing the work of Lorentz, Henri Poincaré between 1895 and 1905 formulated on many occasions the principle of relativity and tried to harmonize it with electrodynamics.  He declared simultaneity only a convenient convention which depends on the speed of light, whereby the constancy of the speed of light would be a useful postulate for making the laws of nature as simple as possible.  In 1900 he interpreted Lorentz's local time as the result of clock synchronization by light signals, and introduced the electromagnetic momentum by comparing electromagnetic energy to what he called a \"fictitious fluid\" of mass formula_1.  And finally in June and July 1905 he declared the relativity principle a general law of nature, including gravitation.  He corrected some mistakes of Lorentz and proved the Lorentz covariance of the electromagnetic equations.  Poincaré also suggested that there exist non-electrical forces to stabilize the electron configuration and asserted that gravitation is a non-electrical force as well, contrary to the electromagnetic world view.  However, historians pointed out that he still used the notion of an ether and distinguished between \"apparent\" and \"real\" time and therefore didn't invent special relativity in its modern understanding. In 1905, while he was working in the patent office, Albert Einstein had four papers published in the \"Annalen der Physik\", the leading German physics journal.  These are the papers that history has come to call the \"Annus Mirabilis papers\": All four papers are today recognized as tremendous achievements—and hence 1905 is known as Einstein's \"Wonderful Year\".  At the time, however, they were not noticed by most physicists as being important, and many of those who did notice them rejected them outright.  Some of this work—such as the theory of light quanta—remained controversial for years. The first formulation of a quantum theory describing radiation and matter interaction is due to Paul Dirac, who, during 1920, was first able to compute the coefficient of spontaneous emission of an atom.  Paul Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles.  In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, Werner Heisenberg and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles.  However, further studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer.  At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself.  With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics. In December 1938, the German chemists Otto Hahn and Fritz Strassmann sent a manuscript to \"Naturwissenschaften\" reporting they had detected the element barium after bombarding uranium with neutrons; simultaneously, they communicated these results to Lise Meitner.  Meitner, and her nephew Otto Robert Frisch, correctly interpreted these results as being nuclear fission.  Frisch confirmed this experimentally on 13 January 1939.  In 1944, Hahn received the Nobel Prize in Chemistry for the discovery of nuclear fission.  Some historians who have documented the history of the discovery of nuclear fission believe Meitner should have been awarded the Nobel Prize with Hahn. Difficulties with the Quantum theory increased through the end of 1940.  Improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift and magnetic moment of the electron.  These experiments unequivocally exposed discrepancies which the theory was unable to explain.  With the invention of bubble chambers and spark chambers in the 1950s, experimental particle physics discovered a large and ever-growing number of particles called hadrons.  It seemed that such a large number of particles could not all be fundamental. Shortly after the end of the war in 1945, Bell Labs formed a Solid State Physics Group, led by William Shockley and chemist Stanley Morgan; other personnel including John Bardeen and Walter Brattain, physicist Gerald Pearson, chemist Robert Gibney, electronics expert Hilbert Moore and several technicians.  Their assignment was to seek a solid-state alternative to fragile glass vacuum tube amplifiers.  Their first attempts were based on Shockley's ideas about using an external electrical field on a semiconductor to affect its conductivity.  These experiments failed every time in all sorts of configurations and materials.  The group was at a standstill until Bardeen suggested a theory that invoked surface states that prevented the field from penetrating the semiconductor.  The group changed its focus to study these surface states and they met almost daily to discuss the work.  The rapport of the group was excellent, and ideas were freely exchanged. As to the problems in the electron experiments, a path to a solution was given by Hans Bethe.  In 1947, while he was traveling by train to reach Schenectady from New York, after giving a talk at the conference at Shelter Island on the subject, Bethe completed the first non-relativistic computation of the shift of the lines of the hydrogen atom as measured by Lamb and Retherford.  Despite the limitations of the computation, agreement was excellent.  The idea was simply to attach infinities to corrections at mass and charge that were actually fixed to a finite value by experiments.  In this way, the infinities get absorbed in those constants and yield a finite result in good agreement with experiments.  This procedure was named renormalization. Based on Bethe's intuition and fundamental papers on the subject by Shin'ichirō Tomonaga, Julian Schwinger, Richard Feynman and Freeman Dyson, it was finally possible to get fully covariant formulations that were finite at any order in a perturbation series of quantum electrodynamics.  Shin'ichirō Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel Prize in Physics in 1965 for their work in this area.  Their contributions, and those of Freeman Dyson, were about covariant and gauge-invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory.  Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent.  Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability.  Even though renormalization works very well in practice, Feynman was never entirely comfortable with its mathematical validity, even referring to renormalization as a \"shell game\" and \"hocus pocus\".  QED has served as the model and template for all subsequent quantum field theories.  Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force. Robert Noyce credited Kurt Lehovec for the \"principle of p–n junction isolation\" caused by the action of a biased p-n junction (the diode) as a key concept behind the integrated circuit.  Jack Kilby recorded his initial ideas concerning the integrated circuit in July 1958 and successfully demonstrated the first working integrated circuit on September 12, 1958.  In his patent application of February 6, 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated.\"  Kilby won the 2000 Nobel Prize in Physics for his part of the invention of the integrated circuit.  Robert Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.  Noyce's chip solved many practical problems that Kilby's had not.  Noyce's chip, made at Fairchild Semiconductor, was made of silicon, whereas Kilby's chip was made of germanium. Philo Farnsworth developed the Farnsworth–Hirsch Fusor, or simply fusor, an apparatus designed by Farnsworth to create nuclear fusion.  Unlike most controlled fusion systems, which slowly heat a magnetically confined plasma, the fusor injects high temperature ions directly into a reaction chamber, thereby avoiding a considerable amount of complexity.  When the Farnsworth-Hirsch Fusor was first introduced to the fusion research world in the late 1960s, the Fusor was the first device that could clearly demonstrate it was producing fusion reactions at all.  Hopes at the time were high that it could be quickly developed into a practical power source.  However, as with other fusion experiments, development into a power source has proven difficult.  Nevertheless, the fusor has since become a practical neutron source and is produced commercially for this role. The mirror image of an electromagnet produces a field with the opposite polarity.  Thus the north and south poles of a magnet have the same symmetry as left and right.  Prior to 1956, it was believed that this symmetry was perfect, and that a technician would be unable to distinguish the north and south poles of a magnet except by reference to left and right.  In that year, T. D. Lee and C. N. Yang predicted the nonconservation of parity in the weak interaction.  To the surprise of many physicists, in 1957 C. S. Wu and collaborators at the U.S. National Bureau of Standards demonstrated that under suitable conditions for polarization of nuclei, the beta decay of cobalt-60 preferentially releases electrons toward the south pole of an external magnetic field, and a somewhat higher number of gamma rays toward the north pole.  As a result, the experimental apparatus does not behave comparably with its mirror image. The first step towards the Standard Model was Sheldon Glashow's discovery, in 1960, of a way to combine the electromagnetic and weak interactions.  In 1967, Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form.  The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model.  This includes the masses of the W and Z bosons, and the masses of the fermions - i.e. the quarks and leptons.  After the neutral weak currents caused by boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it.  The W and Z bosons were discovered experimentally in 1981, and their masses were found to be as the Standard Model predicted.  The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74, when experiments confirmed that the hadrons were composed of fractionally charged quarks.  With the establishment of quantum chromodynamics in the 1970s finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravity, and which remains generally accepted within the domain to which it is designed to be applied. The 'standard model' groups the electroweak interaction theory and quantum chromodynamics into a structure denoted by the gauge group \"SU(3)×SU(2)×U(1)\".  The formulation of the unification of the electromagnetic and weak interactions in the standard model is due to Abdus Salam, Steven Weinberg and, subsequently, Sheldon Glashow.  After the discovery, made at CERN, of the existence of neutral weak currents, mediated by the boson foreseen in the standard model, the physicists Salam, Glashow and Weinberg received the 1979 Nobel Prize in Physics for their electroweak theory.  Since then, discoveries of the bottom quark (1977), the top quark (1995) and the tau neutrino (2000) have given credence to the standard model.  Because of its success in explaining a wide variety of experimental results. Before the start of the 21st century, the electrodynamic tether being oriented at an angle to the local vertical between the object and a planet with a magnetic field cut the Earth's magnetic field and generated a current; thereby it converted some of the orbiting body's kinetic energy to electrical energy.  The tether's far end can be left bare, making electrical contact with the ionosphere, creating a generator.  As part of a \"tether propulsion\" system, crafts can use long, strong conductors to change the orbits of spacecraft.  It has the potential to make space travel significantly cheaper.  It is a simplified, very low-budget magnetic sail.  It can be used either to accelerate or brake an orbiting spacecraft.  When direct current is pumped through the tether, it exerts a force against the magnetic field, and the tether accelerates the spacecraft. There are a range of emerging energy technologies.  By 2007, solid state micrometer-scale electric double-layer capacitors based on advanced superionic conductors had been for low-voltage electronics such as deep-sub-voltage nanoelectronics and related technologies (the 22 nm technological node of CMOS and beyond).  Also, the nanowire battery, a lithium-ion battery, was invented by a team led by Dr. Yi Cui in 2007. Reflecting the fundamental importance and applicability of Magnetic resonance imaging in medicine, Paul Lauterbur of the University of Illinois at Urbana-Champaign and Sir Peter Mansfield of the University of Nottingham were awarded the 2003 Nobel Prize in Physiology or Medicine for their \"\"discoveries concerning magnetic resonance imaging\"\".  The Nobel citation acknowledged Lauterbur's insight of using magnetic field gradients to determine spatial localization, a discovery that allowed rapid acquisition of 2D images. Wireless electricity is a form of wireless energy transfer, the ability to provide electrical energy to remote objects without wires.  The term WiTricity was coined in 2005 by Dave Gerding and later used for a project led by Prof. Marin Soljačić in 2007.  The MIT researchers successfully demonstrated the ability to power a 60 watt light bulb wirelessly, using two 5-turn copper coils of 60 cm (24 in) diameter, that were 2 m (7 ft) away, at roughly 45% efficiency.  This technology can potentially be used in a large variety of applications, including consumer, industrial, medical and military.  Its aim is to reduce the dependence on batteries.  Further applications for this technology include transmission of information—it would not interfere with radio waves and thus could be used as a cheap and efficient communication device without requiring a license or a government permit. s of 2017 , there is still no hard evidence that nature is described by a Grand Unified Theory (GUT).  The Higgs particle has been tentatively verified.  The discovery of neutrino oscillations indicates that the Standard Model is incomplete and has led to renewed interest toward certain GUT such as formula_3.  One of the few possible experimental tests of certain GUT is proton decay and also fermion masses.  There are a few more special tests for supersymmetric GUT.  The gauge coupling strengths of QCD, the weak interaction and hypercharge seem to meet at a common length scale called the GUT scale and equal approximately to formula_4 GeV, which is slightly suggestive.  This interesting numerical observation is called the gauge coupling unification, and it works particularly well if one assumes the existence of superpartners of the Standard Model particles.  Still it is possible to achieve the same by postulating, for instance, that ordinary (non supersymmetric) formula_5 models break with an intermediate gauge scale, such as the one of Pati–Salam group. The Theory of Everything (TOE) is a putative theory of theoretical physics that fully explains and links together all known physical phenomena, and, ideally, has predictive power for the outcome of any experiment that could be carried out in principle.  M-Theory is not yet complete, but the underlying structure of the mathematics has been established and is in agreement with not only all the string theories, but with all of our scientific observations of the universe.  Furthermore, it has passed many tests of internal mathematical consistency that many other attempts to combine quantum mechanics and gravity had failed.  Unfortunately, until we can find some way to observe higher dimensions (impossible with our current level of technology) M-Theory has a very difficult time making predictions which can be tested in a laboratory.  Technologically, it may never be possible for it to be \"proven\".  Physicist and author Michio Kaku has remarked that M-Theory may present us with a \"Theory of Everything\" which is so concise that its underlying formula would fit on a T-shirt.  Stephen Hawking originally believed that M-Theory may be the ultimate theory but later suggested that the search for understanding of mathematics and physics will never be complete. The magnetic monopole in the \"quantum\" theory of magnetic charge started with a paper by the physicist Paul A.M. Dirac in 1931.  The detection of magnetic monopoles is an open problem in experimental physics.  In some theoretical models, magnetic monopoles are unlikely to be observed, because they are too massive to be created in particle accelerators, and also too rare in the Universe to enter a particle detector with much probability. After more than twenty years of intensive research, the origin of high-temperature superconductivity is still not clear, but it seems that instead of \"electron-phonon\" attraction mechanisms, as in conventional superconductivity, one is dealing with genuine \"electronic\" mechanisms (e.g. by antiferromagnetic correlations), and instead of s-wave pairing, d-wave pairings are substantial.  One goal of all this research is room-temperature superconductivity.\n\nHistory of gunpowder Gunpowder is the first physical explosive and propellant.  Before its invention, many incendiary and burning devices had been used, including Greek fire.  The invention of gunpowder is usually attributed to experimentation in Chinese alchemy by Taoists in the pursuit of immortality, and is popularly listed as one of the \"Four Great Inventions\" of China.  It was invented during the late Tang dynasty (9th century) but the earliest record of a written formula appeared in the Song dynasty (11th century).  Knowledge of gunpowder spread rapidly throughout the Old World possibly as a result of the Mongol conquests during the 13th century, with the earliest written formula for it outside of China contained within the Opus Majus, a 1267 treatise by the English friar Roger Bacon.  It was employed in warfare to some effect from at least the 12th century in weapons such as fire arrows, bombs, and the fire lance before the appearance of the gun.  While the fire lance was eventually supplanted by the gun, other gunpowder weapons such as rockets continued to see use in China, Korea, India, and eventually Europe.  Bombs too never ceased to develop and continued to progress into the modern day as grenades, mines, and other explosive implements.  Gunpowder has also been used for non-military purposes such as fireworks for entertainment, or in explosives for mining and tunneling. The evolution of guns led to the development of artillery during the 15th century, pioneered by states such as the Duchy of Burgundy.  Firearms came to dominate early modern warfare in Europe by the 17th century.  The gradual improvement of cannons firing heavier rounds for a greater impact against fortifications led to the invention of the star fort and the bastion in the Western world, where traditional city walls and castles were no longer suitable for defense.  The use of gunpowder technology also spread throughout the Islamic world and to India, Korea, and Japan.  The so-called Gunpowder Empires of the early modern period consisted of the Mughal Empire, Safavid Empire, and Ottoman Empire.  The use of gunpowder in warfare during the course of the 19th century diminished due to the invention of smokeless powder.  Gunpowder is often referred to today as \"black powder\" to distinguish it from the propellant used in contemporary firearms. Although it is not known precisely by whom gunpowder was invented, most historians agree that gunpowder's origins were in China by virtue of archaeological evidence and historical documents predating others by centuries, even if some professional military historians know little or nothing else about this early history of gunpowder warfare.  The very earliest possible reference to gunpowder appeared in 142 AD during the Eastern Han dynasty, when the alchemist Wei Boyang wrote about a substance with the properties of gunpowder, and described a mixture of three powders that would \"fly and dance\" violently in his \"Cantong qi\", otherwise known as the \"Book of the Kinship of Three\", a Taoist text on the subject of alchemy.  Although it is impossible to know if he was actually referring to gunpowder, no other explosive known to scientists is composed of three powders.  While it was almost certainly not their intention to create a weapon of war, Taoist alchemists would continue to play a major role in the invention of gunpowder due to their experiments on sulfur and saltpeter in the search for the secrets of eternal life and ways to transmute one material into another.  Historian Peter Lorge notes that despite the early association of gunpowder with Taoism, this may be a quirk of historiography, and a result of the better preservation of texts associated with Taoism rather than being a point of interest limited to only Taoists.  The Taoist quest for the elixir of life attracted many powerful patrons, one of whom was Emperor Wu of Han, and one of the resulting alchemical experiments involved heating sulfur and saltpeter to transform them. The next possible reference to gunpowder appeared in the year 300 during the Jin dynasty (265–420).  A Taoist philosopher by the name of Ge Hong wrote down the ingredients of gunpowder in his surviving works, collectively known as the \"Baopuzi\" (\"The Master Who Embraces Simplicity\").  The \"Inner Chapters\" on Taoism contains records of his experiments with heated saltpeter, pine resin, and charcoal among other carbon materials, resulting in explosion, which most historians acknowledge as an early form of gunpowder.  In 492, Taoist alchemists had noted that saltpeter, one of the most important ingredients in gunpowder, burns with a purple flame, allowing for practical efforts at purifying the substance. The first confirmed reference to what can be considered gunpowder in China occurred more than three hundred years later during the Tang dynasty, first in a formula contained in the \"Taishang Shengzu Jindan Mijue\" (太上聖祖金丹秘訣) in 808, and then about half a century later in a Taoist text known as the \"Zhenyuan miaodao yaolüe\" (真元妙道要略).  The first formula was a combination of six parts sulfur to six parts saltpeter to one part birthwort herb, and the Taoist text warned against an assortment of dangerous formulas, one of which corresponds with gunpowder: \"Some have heated together sulfur, realgar (arsenic disulphide), and saltpeter with honey; smoke [and flames] result, so that their hands and faces have been burnt, and even the whole house burned down.\"  Alchemists called this discovery fire medicine (\"huoyao\" 火藥), and the term has continued to refer to gunpowder in China into the present day, a reminder of its heritage as a side result in the search for longevity increasing drugs. The earliest surviving chemical formula of gunpowder dates to 1044 in the form of the military manual \"Wujing Zongyao\", also known in English as the \"Complete Essentials for the Military Classics\", which contains a collection of factoids on Chinese weaponry.  The \"Wujing Zongyao\" served as a repository of antiquated or fanciful weaponry, and this applied to gunpowder as well, which suggests that it had already been weaponized long before the invention of what would today be considered conventional firearms.  These numerous types of gunpowder weapons styling various names such as \"flying incendiary club for subjugating demons,\" \"caltrop fire ball,\" \"ten-thousand fire flying sand magic bomb,\" \"big bees nest,\" \"burning heaven fierce fire unstoppable bomb,\" \"fire bricks\" which released \"flying swallows,\" \"flying rats,\" \"fire birds,\" and \"fire oxen\" eventually gave way and coalesced into a smaller number of dominant types, notably gunpowder arrows, bombs, and guns.  This was probably because some weapons were deemed too onerous or ineffective to deploy. At this point the formula contained too little saltpeter (about 50%) to be explosive, but the mixture was highly flammable, and contemporary weapons reflected this in their deployment as mainly shock and incendiary weapons.  One of the first, if not the first of these weapons was the fire arrow.  The first possible reference to the use of fire arrows was by the Southern Wu in 904 during the siege of Yuzhang, when an officer under Yang Xingmi by the name of Zheng Fan (鄭璠) ordered his troops to \"shoot off a machine to let fire and burn the Longsha Gate,\" after which he and his troops dashed over the fire into the city and captured it, and he was promoted to Prime Minister Inspectorate for his efforts and the burns his body endured.  A later account of this event corroborated with the report and explained that \"by let fire (飛火) is meant things like firebombs and fire arrows.\"  Arrows carrying gunpowder was possibly the most applicable form of gunpowder weaponry at the time, as early gunpowder may have only produced an effective flame when exposed to oxygen, thus the rush of air around the arrow in flight would have provided a suitable catalyst for the reaction. The first fire arrows were arrows strapped with gunpowder incendiaries, but in 969 two Song generals, Yue Yifang and Feng Jisheng (馮繼升), invented a variant fire arrow which utilizing gunpowder tubes as propellant.  Afterwards fire arrows started transitioning to rocket propelled weapons rather than being fired from a bow.  These fire arrows were shown to the emperor in 970 when the head of a weapons manufacturing bureau sent Feng Jisheng to demonstrate the gunpowder arrow design, for which he was heavily rewarded.  In 1000 a soldier by the name of Tang Fu (唐福) also demonstrated his own designs of gunpowder arrows, gunpowder pots (a proto-bomb which spews fire), and gunpowder caltrops, for which he was richly rewarded as well.  The imperial court took great interest in the progress of gunpowder developments and actively encouraged as well as disseminated military technology, for example in 1002 when a local militia man named Shi Pu (石普) showed his own versions of fireballs and gunpowder arrows to imperial officials, they were so astounded that the emperor and court decreed that a team would be assembled to print the plans and instructions for the new designs to promulgate throughout the realm.  The Song court's policy of rewarding military innovators was reported to have \"brought about a great number of cases of people presenting technology and techniques\" (器械法式) according to the official \"History of Song\".  Production of gunpowder and fire arrows heavily increased in the 11th century as the court centralized the production process, constructing large gunpowder production facilities, hiring artisans, carpenters, and tanners for the military production complex in the capital of Kaifeng.  One surviving source circa 1023 lists all the artisans working in Kaifeng while another notes that in 1083 the imperial court sent 100,000 gunpowder arrows to one garrison and 250,000 to another.  Evidence of gunpowder in the Liao dynasty and Western Xia is much sparser than in Song, but some evidence such as the Song decree of 1073 that all subjects were henceforth forbidden from trading sulfur and saltpeter across the Liao border suggests that the Liao were aware of gunpowder developments to the south and coveted gunpowder ingredients of their own. The Jurchen people of Manchuria united under Wanyan Aguda and established their own Jin dynasty in 1115.  Allying with the Song, they rapidly rose to the forefront of East Asian powers and defeated the Liao dynasty in a surprisingly short span of time, destroying the 150 year balance of power between the Song, Liao, and Western Xia.  Remnants of the Liao fled to the west and became known as the Qara Khitai or Western Liao to the Chinese.  In the east, the fragile Song-Jin alliance broke as the Jin saw that the Song army was unable to even defeat the Liao garrisons and fulfill their end of the alliance in capturing the Liao Southern Capital.  Realizing the weakness of Song, Jin grew tired of waiting and captured all five of the Liao capitals themselves, proceeding to make war on Song, initiating the Jin-Song Wars.  For the first time, two major powers would have access to equally formidable gunpowder weapons, ushering in the Gunpowder Age in earnest.  Initially the Jin expected their campaign in Song to proceed smoothly given how poorly the Song fared against Liao.  However they were met with stout resistance upon besieging Kaifeng in 1126 and faced the usual array of gunpowder arrows and fire bombs, but also a new weapon called the \"thunderclap bomb\" (霹靂炮), which one witness wrote, \"At night the thunderclap bombs were used, hitting the lines of the enemy well, and throwing them into great confusion.  Many fled, screaming in fright.\" Jin troops withdrew with a ransom of Song silk and treasure, but returned several months later, this time with their own gunpowder bombs manufactured by captured Song artisans.  According to historian Wang Zhaochun, the account of this battle provided the \"earliest truly detailed descriptions of the use of gunpowder weapons in warfare.\"  Records show that Jin utilized gunpowder arrows and trebuchets to hurl gunpowder bombs while Song responded with gunpowder arrows, fire bombs, thunderclap bombs, and a new addition called the \"molten metal bomb\" (金汁炮).  As the Jin account describes, when they attacked the city's Xuanhua Gate, their \"fire bombs fell like rain, and their arrows were so numerous as to be uncountable.\"  Jin captured Kaifeng despite the appearance of the molten metal bomb and secured another 20,000 fire arrows for their arsenal. The molten metal bomb appeared again in 1129 when Song general Li Yanxian (李彥仙) clashed with Jin forces while defending a strategic pass.  The Jin assault lasted day and night without respite, using siege carts, fire carts, and sky bridges, but each assault was met with Song soldiers who \"resisted at each occasion, and also used molten metal bombs.  Wherever the gunpowder touched, everything would disintegrate without a trace.\" Song established another capital in modern Hangzhou and the Jin forces followed.  The fighting that ensued would see the first proto-gun, the fire lance, in action.  Despite the low explosive potential of 11th century gunpowder, it's possible that it was already being utilized in early projectile weapons such as the fire lance.  There is some minor evidence of this based on an artistic depiction of it in the form of slightly predating the \"Wujing Zongyao\" showing a demon pointing what appears to be a fire lance at the Buddha, trying to disturb his meditation.  Although most Chinese scholars reject the appearance of the fire lance prior to the Jin-Song wars, a Song text from 1000 and the \"Wujing Zongyao\" do make brief mentions of the fire lance.  The fire lance, as implied by the name, is essentially a long spear or pole affixed with a tube of gunpowder, and as it saw more usage, the tube's length became longer and pellets were added to the composition. The earliest confirmed employment of the fire lance in warfare was by Song dynasty forces against the Jin in 1132 during the siege of De'an (modern Anlu, Hubei Province), where they were used to great effect against wooden siege towers called \"sky bridges\": \"As the sky bridges became stuck fast, more than ten feet from the walls and unable to get any closer, [the defenders] were ready.  From below and above the defensive structures they emerged and attacked with fire lances, striking lances, and hooked sickles, each in turn.  The people [i.e., the porters] at the base of the sky bridges were repulsed.  Pulling their bamboo ropes, they [the porters] ended up drawing the sky bridge back in an anxious and urgent rush, going about fifty paces before stopping.\"  The surviving porters then tried once again to wheel the sky bridges into place but Song soldiers emerged from the walls in force and made a direct attack on the sky bridge soldiers while defenders on the walls threw bricks and shot arrows in conjunction with trebuchets hurling bombs and rocks.  The sky bridges were also set fire to with incendiary bundles of grass and firewood.  Li Heng, the Jin commander, decided to lift the siege and Jin forces were driven back with severe casualties. The siege of De'an marks an important transition and landmark in the history of gunpowder weapons as the fire medicine of the fire lances were described using a new word: \"fire bomb medicine\" (火炮藥), rather than simply \"fire medicine.\"  This could imply the use of a new more potent formula, or simply an acknowledgement of the specialized military application of gunpowder.  Peter Lorge suggests that this \"bomb powder\" may have been corned, making it distinct from normal gunpowder.  Evidence of gunpowder firecrackers also points to their appearance at roughly around the same time fire medicine was making its transition in the literary imagination.  Fire lances continued to be used as anti-personnel weapons into the Ming dynasty, and were even attached to battle carts on one situation in 1163.  Song commander Wei Sheng constructed several hundred of these carts known as \"at-your-desire-war-carts\" (如意戰車), which contained fire lances protruding from protective covering on the sides.  They were used to defend mobile trebuchets that hurled fire bombs. Gunpowder technology also spread to naval warfare and in 1129 Song decreed that all warships were to be fitted with trebuchets for hurling gunpowder bombs.  Older gunpowder weapons such as fire arrows were also utilized.  In 1159 a Song fleet of 120 ships caught a Jin fleet at anchor near Shijiu Island (石臼島) off the shore of Shandong peninsula.  The Song commander \"ordered that gunpowder arrows be shot from all sides, and wherever they struck, flames and smoke rose up in swirls, setting fire to several hundred vessels.\"  Song forces took another victory in 1161 when Song paddle boats ambushed a Jin transport fleet, launching thunderclap bombs, and drowned the Jin force in the Yangtze. According to a minor military official by the name of Zhao Wannian (趙萬年), thunderclap bombs were used again to great effect by the Song during the Jin siege of Xiangyang in 1206-1207.  Both sides had gunpowder weapons, but the Jin troops only used gunpowder arrows for destroying the city's moored vessels.  The Song used fire arrows, fire bombs, and thunderclap bombs.  Fire arrows and bombs were used to destroy Jin trebuchets.  The thunderclap bombs were used on Jin soldiers themselves, causing foot soldiers and horsemen to panic and retreat.  \"We beat our drums and yelled from atop the city wall, and simultaneously fired our thunderclap missiles out from the city walls.  The enemy cavalry was terrified and ran away.\"  The Jin were forced to retreat and make camp by the riverside.  In a rare occurrence, the Song made a successful offensive on Jin forces, and conducted a night assault using boats.  They were loaded with gunpowder arrows, thunderclap bombs, a thousand crossbowmen, five hundred infantry, and a hundred drummers.  Jin troops were surprised in their encampment while asleep by loud drumming, followed by an onslaught of crossbow bolts, and then thunderclap bombs, which caused a panic of such magnitude that they were unable to even saddle themselves and trampled over each other trying to get away.  Two to three thousand Jin troops were slaughtered along with eight to nine hundred horses. Then everything changed when the Jin dynasty attacked, with even better bombs.  The iron bomb.  Traditionally the inspiration for the development of the iron bomb is ascribed to the tale of a fox hunter named Iron Li.  According to the story, around the year 1189 Iron Li developed a new method for hunting foxes which used a ceramic explosive to scare foxes into his nets.  The explosive consisted of a ceramic bottle with a mouth, stuffed with gunpowder, and attached with a fuse.  Explosive and net were placed at strategic points of places such as watering holes frequented by foxes, and when they got near enough, Iron Li would light the fuse, causing the ceramic bottle to explode and scaring the frightened foxes right into his nets.  While a fanciful tale, it's not exactly certain why this would cause the development of the iron bomb, given the explosive was made using ceramics, and other materials such as bamboo or even leather would have done the same job, assuming they made a loud enough noise.  Nonetheless, the iron bomb made its first appearance in 1221 at the siege of Qizhou (in modern Hubei province), and this time it would be the Jin who possessed the technological advantage.  The Song command Zhao Yurong (趙與褣) survived and was able to relay his account for posterity. Qizhou was a major fortress city situated near the Yangtze and a 25 thousand strong Jin army advanced on it in 1221.  News of the approaching army reached Zhao Yurong in Qizhou, and despite being outnumbered nearly eight to one, he decided to hold the city.  Qizhou's arsenal consisted of some three thousand thunderclap bombs, twenty thousand \"great leather bombs\" (皮大炮), and thousands of gunpowder arrows and gunpowder crossbow bolts.  While the formula for gunpowder had become potent enough to consider the Song bombs to be true explosives, they were unable to match the explosive power of the Jin iron bombs.  Yurong describes the uneven exchange thus, \"The barbaric enemy attacked the Northwest Tower with an unceasing flow of catapult projectiles from thirteen catapults.  Each catapult shot was followed by an iron fire bomb [catapult shot], whose sound was like thunder.  That day, the city soldiers in facing the catapult shots showed great courage as they maneuvered [our own] catapults, hindered by injuries from the iron fire bombs.  Their heads, their eyes, their cheeks were exploded to bits, and only one half [of the face] was left.\"  Jin artillerists were able to successfully target the command center itself: \"The enemy fired off catapult stones ... nonstop day and night, and the magistrate's headquarters [帳] at the eastern gate, as well as my own quarters ..., were hit by the most iron fire bombs, to the point that they struck even on top of [my] sleeping quarters and [I] nearly perished!  Some said there was a traitor.  If not, how would they have known the way to strike at both of these places?\"  Zhao was able to examine the new iron bombs himself and described thus, \"In shape they are like gourds, but with a small mouth.  They are made with pig iron, about two inches thick, and they cause the city's walls to shake.\"  Houses were blown apart, towers battered, and defenders blasted from their placements.  Within four weeks all four gates were under heavy bombardment.  Finally the Jin made a frontal assault on the walls and scaled them, after which followed a merciless hunt for soldiers, officers, and officials of every level.  Zhao managed an escape by clambering over the battlement and making a hasty retreat across the river, but his family remained in the city.  Upon returning at a later date to search the ruins, he found that the \"bones and skeletons were so mixed up that there was no way to tell who was who.\" The Mongols and their rise in world history as well as conflicts with both the Jin and Song played a key role in the evolution of gunpowder technology.  Mongol aptitude in incorporating foreign experts extended to the Chinese, who provided artisans that followed Mongol armies willingly and unwillingly far into the west and even east, to Japan.  Unfortunately textual evidence for this is scant as the Mongols left few documents.  This lack of primary source documents has caused some historians and scholars such as Kate Raphel to doubt the Mongol's role in disseminating gunpowder throughout Eurasia.  On the opposite side stand historians such as Tonio Andrade and Stephen Haw, who believe that the Mongol Empire not only used gunpowder weapons but deserves the moniker \"the first gunpowder empire.\" The first concerted Mongol invasion of Jin occurred in 1211 and total conquest was not accomplished until 1234.  In 1232 the Mongols besieged the Jin capital of Kaifeng and deployed gunpowder weapons along with other more conventional siege techniques such as building stockades, watchtowers, trenches, guardhouses, and forcing Chinese captives to haul supplies and fill moats.  Jin scholar Liu Qi (劉祈) recounts in his memoir, \"the attack against the city walls grew increasingly intense, and bombs rained down as [the enemy] advanced.\"  The Jin defenders also deployed gunpowder bombs as well as fire arrows (\"huo jian\" 火箭) launched using a type of early solid-propellant rocket.  Of the bombs, Liu Qi writes, \"From within the walls the defenders responded with a gunpowder bomb called the heaven-shaking-thunder bomb (震天雷).  Whenever the [Mongol] troops encountered one, several men at a time would be turned into ashes.\"  A more fact based and clear description of the bomb exists in the \"History of Jin\": \"The heaven-shaking-thunder bomb is an iron vessel filled with gunpowder.  When lighted with fire and shot off, it goes off like a crash of thunder that can be heard for a hundred li [thirty miles], burning an expanse of land more than half a mu [所爇圍半畝之上, a mu is a sixth of an acre], and the fire can even penetrate iron armor.\"  A Ming official named He Mengchuan would encounter an old cache of these bombs three centuries later in the Xi'an area: \"When I went on official business to Shaanxi Province, I saw on top of Xi'an's city walls an old stockpile of iron bombs.  They were called 'heaven-shaking-thunder' bombs, and they were like an enclosed rice bowl with a hole at the top, just big enough to put your finger in.  The troops said they hadn't been used for a very long time.\"  Furthermore, he wrote, \"When the powder goes off, the bomb rips open, and the iron pieces fly in all directions.  That is how it is able to kill people and horses from far away.\" Heaven-shaking-thunder bombs, also known as thunder crash bombs, were utilized prior to the siege in 1231 when a Jin general made use of them in destroying a Mongol warship, but during the siege the Mongols responded by protecting themselves with elaborate screens of thick cowhide.  This was effective enough for workers to get right up to the walls to undermine their foundations and excavate protective niches.  Jin defenders countered by tying iron cords and attaching them to heaven-shaking-thunder bombs, which were lowered down the walls until they reached the place where the miners worked.  The protective leather screens were unable to withstand the explosion, and were penetrated, killing the excavators.  Another weapon the Jin employed was an improved version of the fire lance called the flying fire lance.  The \"History of Jin\" provides a detailed description: \"To make the lance, use chi-huang paper, sixteen layers of it for the tube, and make it a bit longer than two feet.  Stuff it with willow charcoal, iron fragments, magnet ends, sulfur, white arsenic [probably an error that should mean saltpeter], and other ingredients, and put a fuse to the end.  Each troop has hanging on him a little iron pot to keep fire [probably hot coals], and when it's time to do battle, the flames shoot out the front of the lance more than ten feet, and when the gunpowder is depleted, the tube isn't destroyed.\"  While Mongol soldiers typically held a view of disdain toward most Jin weapons, apparently they greatly feared the flying fire lance and heaven-shaking-thunder bomb.  Kaifeng managed to hold out for a year before the Jin emperor fled and the city capitulated.  In some cases Jin troops still fought with some success, scoring isolated victories such as when a Jin commander led 450 fire lancers against a Mongol encampment, which was \"completely routed, and three thousand five hundred were drowned.\"  Even after the Jin emperor committed suicide in 1234, one loyalist gathered all the metal he could find in the city he was defending, even gold and silver, and made explosives to lob against the Mongols, but the momentum of the Mongol Empire could not be stopped.  By 1234, both the Western Xia and Jin dynasty had been conquered. The Mongol war machine moved south and in 1237 attacked the Song city of Anfeng (modern Shouxian, Anhui Province) \"using gunpowder bombs [huo pao] to burn the [defensive] towers.\"  These bombs were apparently quite large.  \"Several hundred men hurled one bomb, and if it hit the tower it would immediately smash it to pieces.\"  The Song defenders under commander Du Gao (杜杲) rebuilt the towers and retaliated with their own bombs, which they called the \"Elipao,\" after a famous local pear, probably in reference to the shape of the weapon.  Perhaps as another point of military interest, the account of this battle also mentions that the Anfeng defenders were equipped with a type of small arrow to shoot through eye slits of Mongol armor, as normal arrows were too thick to penetrate. By the mid 13th century, gunpowder weapons had become central to the Song war effort.  In 1257 the Song official Li Zengbo was dispatched to inspect frontier city arsenals.  Li considered an ideal city arsenal to include several hundred thousand iron bombshells, and also its own production facility to produce at least a couple thousand a month.  The results of his tour of the border were severely disappointing and in one arsenal he found \"no more than 85 iron bomb-shells, large and small, 95 fire-arrows, and 105 fire-lances.  This is not sufficient for a mere hundred men, let alone a thousand, to use against an attack by the ... barbarians.  The government supposedly wants to make preparations for the defense of its fortified cities, and to furnish them with military supplies against the enemy (yet this is all they give us).  What chilling indifference!\"  Fortunately for the Song, Möngke Khan died in 1259 and the war would not continue until 1269 under the leadership of Kublai Khan, but when it did the Mongols came in full force. Blocking the Mongols' passage south of the Yangtze were the twin fortress cities of Xiangyang and Fancheng.  What resulted was one of the longest sieges the world had ever known, lasting from 1268 to 1273.  For the first three years the Song defenders had been able to receive supplies and reinforcements by water, but in 1271 the Mongols set up a full blockade with a formidable navy of their own, isolating the two cities.  This didn't prevent the Song from running the supply route anyways, and two men by the surname Zhang did exactly that.  The Two Zhangs commanded a hundred paddle wheel boats, travelling by night under the light of lantern fire, but were discovered early on by a Mongol commander.  When the Song fleet arrived near the cities, they found the Mongol fleet to have spread themselves out along the entire width of the Yangtze with \"vessels spread out, filling the entire surface of the river, and there was no gap for them to enter.\"  Another defensive measure the Mongols had taken was the construction of a chain, which stretched across the water.  The two fleets engaged in combat and the Song opened fire with fire-lances, fire-bombs, and crossbows.  A large number of men died trying to cut through chains, pull up stakes, and hurl bombs, while Song marines fought hand to hand using large axes, and according to the Mongol record, \"on their ships they were up to the ankles in blood.\"  With the rise of dawn, the Song vessels made it to the city walls and the citizens \"leapt up a hundred times in joy.\"  In 1273 the Mongols enlisted the expertise of two Muslim engineers, one from Persia and one from Syria, who helped in the construction of counterweight trebuchets.  These new siege weapons had the capability of throwing larger missiles further than the previous traction trebuchets.  One account records, \"when the machinery went off the noise shook heaven and earth; every thing that [the missile] hit was broken and destroyed.\"  The fortress city of Xiangyang fell in 1273. The next major battle to feature gunpowder weapons was during a campaign led by the Mongol general Bayan, who commanded an army of around two hundred thousand, consisting of mostly Chinese soldiers.  It was probably the largest army the Mongols had ever utilized.  Such an army was still unable to successfully storm Song city walls, as seen in the 1274 Siege of Shayang.  Thus Bayan waited for the wind to change to a northerly course before ordering his artillerists to begin bombarding the city with molten metal bombs, which caused such a fire that \"the buildings were burned up and the smoke and flames rose up to heaven.\"  Shayang was captured and its inhabitants massacred. Gunpowder bombs were used again in the 1275 Siege of Changzhou in the latter stages of the Mongol-Song Wars.  Upon arriving at the city, Bayan gave the inhabitants an ultimatum: \"if you ... resist us ... we shall drain your carcasses of blood and use them for pillows.\"  This didn't work and the city resisted anyways, so the Mongol army bombarded them with fire bombs before storming the walls, after which followed an immense slaughter claiming the lives of a quarter million.  The war lasted for only another four years during which some remnants of the Song held up last desperate defenses.  In 1277, 250 defenders under Lou Qianxia conducted a suicide bombing and set off a huge iron bomb when it became clear defeat was imminent.  Of this, the \"History of Song\" writes, \"the noise was like a tremendous thunderclap, shaking the walls and ground, and the smoke filled up the heavens outside.  Many of the troops [outside] were startled to death.  When the fire was extinguished they went in to see.  There were just ashes, not a trace left.\"  So came an end to the Mongol-Song Wars, which saw the deployment of all the gunpowder weapons available to both sides at the time, which for the most part meant gunpowder arrows, bombs, and lances, but in retrospect, another development would overshadow them all, the birth of the gun. In 1280, a large store of gunpowder at Weiyang in Yangzhou accidentally caught fire, producing such a massive explosion that a team of inspectors at the site a week later deduced that some 100 guards had been killed instantly, with wooden beams and pillars blown sky high and landing at a distance of over 10 li (~2 mi.  or ~3 km) away from the explosion, creating a crater more than ten feet deep.  One resident described the noise of the explosion as if it \"was like a volcano erupting, a tsunami crashing.  The entire population was terrified.\"  According to surviving reports, the incident was caused by inexperienced gunpowder makers hired to replace the previous ones, and had been careless while grinding sulfur.  A spark caused by the grinding process came into contact with some fire lances which immediately started spewing flames and jetting around \"like frightened snakes.\"  The gunpowder makers did nothing as they found the sight highly amusing, that is until one fire lance burst into a cache of bombs, causing the entire complex to explode.  The validity of this report is somewhat questionable, assuming everyone within the immediate vicinity was killed. By the time of Jiao Yu and his \"Huolongjing\" (a book that describes military applications of gunpowder in great detail) in the mid 14th century, the explosive potential of gunpowder was perfected, as the level of nitrate in gunpowder formulas had risen from a range of 12% to 91%, with at least 6 different formulas in use that are considered to have maximum explosive potential for gunpowder.  By that time, the Chinese had discovered how to create explosive round shot by packing their hollow shells with this nitrate-enhanced gunpowder. Gunpowder may have been used during the Mongol invasions of Europe.  \"Fire catapults\", \"\"pao\"\", and \"naphtha-shooters\" are mentioned in some sources.  However, according to Timothy May, \"there is no concrete evidence that the Mongols used gunpowder weapons on a regular basis outside of China.\" Shortly after the Mongol invasions of Japan (1274-1281), the Japanese produced a scroll painting depicting a bomb.  Called tetsuhau in Japanese, the bomb is speculated to have been the Chinese thunder crash bomb. To clarify for the purposes of this section and any other references to \"firearms\" or \"guns\" in this article, what is meant is a gunpowder weapon which uses \"the explosive force of the gunpowder to propel a projectile from a tube: cannons, muskets, and pistols are typical examples.\" The early fire lance, considered to be the ancestor of firearms, is not considered a true gun because it did not include projectiles.  Later on, shrapnel such as ceramics and bits of iron were added, but these didn't occlude the barrel, and were only swept along with the discharge rather than make use of windage.  These projectiles were called \"co-viatives.\"  The commonplace nature of the fire lance, if not in quantity, was apparent by the mid 13th century, and in 1257 an arsenal in Jiankang Prefecture reported the manufacture of 333 \"fire emitting tubes\" (突火筒).  In 1259 a type of \"fire-emitting lance\" (突火槍) made an appearance and according to the \"History of Song\": \"It is made from a large bamboo tube, and inside is stuffed a pellet wad (子窠).  Once the fire goes off it completely spews the rear pellet wad forth, and the sound is like a bomb that can be heard for five hundred or more paces.\"  The pellet wad mentioned is possibly the first true bullet in recorded history depending on how bullet is defined, as it did occlude the barrel, unlike previous co-viatives used in the fire lance. Fire lances transformed from the \"bamboo- (or wood- or paper-) barreled firearm to the metal-barreled firearm\" to better withstand the explosive pressure of gunpowder.  From there it branched off into several different gunpowder weapons known as \"eruptors\" in the late 12th and early 13th centuries, with different functions such as the \"filling-the-sky erupting tube\" which spewed out poisonous gas and porcelain shards, the \"orifice-penetrating flying sand magic mist tube\" (鑽穴飛砂神霧筒) which spewed forth sand and poisonous chemicals into orifices, and the more conventional \"phalanx-charging fire gourd\" which shot out lead pellets.  The character for lance, or spear (槍), has continued to refer to both the melee weapon and the firearm into modern China, perhaps as a reminder of its original form as simply a tube of gunpowder tied to a spear. Traditionally the first appearance of the hand cannon is dated to the late 13th century, just after the Mongol conquest of the Song dynasty.  However a sculpture depicting a figure carrying a gourd shaped hand cannon was discovered among the Dazu Rock Carvings in 1985 by Robin Yates.  The sculptures were completed roughly 250 km northwest of Chongqing by 1128, after the fall of Kaifeng to the Jin dynasty.  If the dating is correct this would push back the appearance of the cannon in China by a hundred years more than previously thought.  The bulbous nature of the cannon is congruous with the earliest hand cannons discovered in China and Europe. Archaeological samples of the gun, specifically the hand cannon (huochong), have been dated starting from the 13th century.  The oldest extant gun whose dating is unequivocal is the Xanadu Gun, so called because it was discovered in the ruins of Xanadu, the Mongol summer palace in Inner Mongolia.  The Xanadu Gun is 34.7 cm in length and weighs 6.2 kg.  Its dating is based on archaeological context and a straightforward inscription whose era name and year corresponds with the Gregorian Calendar at 1298.  Not only does the inscription contain the era name and date, it also includes a serial number and manufacturing information which suggests that gun production had already become systematized, or at least become a somewhat standardized affair by the time of its fabrication.  The design of the gun includes axial holes in its rear which some speculate could have been used in a mounting mechanism.  Like most early guns with the possible exception of the Western Xia gun, it is small, weighing just over six kilograms and thirty-five centimeters in length.  Although the Xanadu Gun is the most precisely dated gun from the 13th century, other extant samples with approximate dating likely predate it. One candidate is the Heilongjiang hand cannon, discovered in 1970, and named after the province of its discovery, Heilongjiang, in northeastern China.  It is small and light like the Xanadu gun, weighing only 3.5 kilograms, 34 cm (Needham says 35 cm), and a bore of approximately 2.5 cm.  Based on contextual evidence, historians believe it was used by Yuan forces against a rebellion by Mongol prince Nayan in 1287.  The History of Yuan states that a Jurchen commander known as Li Ting led troops armed with hand cannons into battle against Nayan. Even older, the Ningxia gun was found in Ningxia Hui Autonomous Region by collector Meng Jianmin (孟建民).  This Yuan dynasty firearm is 34.6 cm long, the muzzle 2.6 cm in diameter, and weighs 1.55 kilograms.  The firearm contains a transcription reading, \"Made by bronzesmith Li Liujing in the year Zhiyuan 8 (直元), ningzi number 2565\" (銅匠作頭李六徑，直元捌年造，寧字二仟伍百陸拾伍號).  Similar to the Xanadu Gun, it bears a serial number 2565, which suggests it may have been part of a series of guns manufactured.  While the era name and date corresponds with the Gregorian Calendar at 1271 CE, putting it earlier than both the Heilongjiang Hand Gun as well as the Xanadu Gun, one of the characters used in the era name is irregular, causing some doubt among scholars on the exact date of production. Another specimen, the Wuwei cannon, was discovered in 1980 and may possibly be the oldest as well as largest cannon of the 13th century: a 100 centimeter 108 kilogram bronze cannon discovered in a cellar in Wuwei, Gansu Province containing no inscription, but has been dated by historians to the late Western Xia period between 1214 and 1227.  The gun contained an iron ball about nine centimeters in diameter, which is smaller than the muzzle diameter at twelve centimeters, and 0.1 kilograms of gunpowder in it when discovered, meaning that the projectile might have been another co-viative.  Ben Sinvany and Dang Shoushan believe that the ball used to be much larger prior to its highly corroded state at the time of discovery.  While large in size, the weapon is noticeably more primitive than later Yuan dynasty guns, and is unevenly cast.  A similar weapon was discovered not far from the discovery site in 1997, but much smaller in size at only 1.5 kg.  Chen Bingying disputes this however, and argues there were no guns before 1259, while Dang Shoushan believes the Western Xia guns point to the appearance of guns by 1220, and Stephen Haw goes even further by stating that guns were developed as early as 1200.  Sinologist Joseph Needham and renaissance siege expert Thomas Arnold provide a more conservative estimate of around 1280 for the appearance of the \"true\" cannon.  Whether or not any of these are correct, it seems likely that the gun was born sometime during the 13th century. Although the oldest extant guns appear in the Yuan dynasty, historians have noted the surprising scarcity of reliable evidence of guns in Iran or Central Asia prior to the late 14th century.  In the Middle East no guns are mentioned prior to the 1360s, while Russian records do not contain reliable mentions of firearms until 1382, after the gun's arrival in western Europe, despite their closer proximity and interactions with the Mongol empires. There is however some evidence that does point to the possible appearance of guns in Andalusia as early as the 1330s.  Thomas Allsen says that \"in the Latin West the first uncontestable evidence of firearms is from 1326, surprisingly somewhat earlier than in the lands that lie between China ... and western Europe.  This has caused some doubt among historians on the gun transmission theory, and even whether or not there was a transmission at all.  One dissident opinion comes from Stephen Morillo, Jeremy Black, and Paul Lococo's \"War in World History\" which argues that \"the sources are not entirely clear about Chinese use of gunpowder in guns.  There are references to bamboo and iron cannons, or perhaps proto-cannons, but these seem to have been small, unreliable, handheld weapons in this period.  The Chinese do seem to have invented guns independently of the Europeans, at least in principle; but, in terms of effective cannon, the edge goes to Europe.\"  There is another independent invention theory supporting an Islamic origin of the gun, citing the Mamluk deployment of hand cannons in 1260 and a passage by Ibn Khaldun on the Marinid Siege of Sijilmassa in 1274: \"[ The Sultan] installed siege engines … and gunpowder engines …, which project small balls of iron.  These balls are ejected from a chamber … placed in front of a kindling fire of gunpowder; this happens by a strange property which attributes all actions to the power of the Creator.\"  This passage and its interpretation has been rejected as anachronistic by most historians, who urge caution regarding claims of Islamic firearms use in the 1204-1324 period as late medieval Arabic texts used the same word for gunpowder, naft, as they did for an earlier incendiary, naphtha. Historian Tonio Andrade supports the gun transmission theory, noting that while records of gunpowder weapons and their evolution into the gun exist in China, \"there are no records of any such developments in Europe,\" and that the arrival of the gun in Europe was such that it \"appears fully formed around 1326.\"  This is not strictly true, as Kelly DeVries points out that compilers of early gunpowder recipes in Europe understood that should the instrument carrying gunpowder be enclosed on one end, the gunpowder reaction inside would produce \"flying fire.\"  Andrade goes on to analyze the nature and etymology of gunpowder in Europe and comes to the conclusion that it is intrinsically in favor of the transmission theory rather than an independent invention.  There are the older and more numerous formulas of gunpowder using a variety of different proportions of key ingredients - saltpeter, sulphur, and charcoal - which he believes is proof of its evolution and experimentation in China, where gunpowder was first applied to warfare as an incendiary, then explosive, and finally as a propellant.  In contrast gunpowder formulas in Europe appear both later and offer very little divergence from the already ideal proportions for the purpose of creating an explosive and propellant powder.  Another facet of the gunpowder transmission theory is the appearance of gunpowder in Europe ready made for military usage, and is generally referred to as \"gun\"powder rather than a civilian term such as the Chinese \"fire-drug,\" which suggests an originally non-military usage, whereas in Europe it was almost immediately and exclusively used for its military qualities.  Muslim terms of saltpeter may also point toward a gunpowder transmission, if not the gun itself, as an Andalusian botanist referred to it as \"Chinese snow,\" while in Persia it was called \"Chinese salt.\"  Perhaps even further in the Sinocentric gun transmission camp is Joseph Needham who claims that \"all the long preparations and tentative experiments were made in China, and everything came to Islam and the West fully fledged, whether it was the fire-lance or the explosive bomb, the rocket or the metal-barrel hand-gun and bombard.\"  However, theories of European as well as Islamic origins for the gun still persist today in tandem with the transmission theory. There are problems on both extremes of the gun transmission debate.  Its proponents emphasize the older history of gunpowder evolution as attested by historical records and archaeological samples in China, its less obviously militarily focused name as \"fire medicine,\" the Mongol role as a catalyst in disseminating gunpowder technology, and criticizes the scant or absent evidence of prior experimentation with gunpowder in Europe for non-military purposes before the arrival of the gun.  However, there are still several blanks in the history of a gun transmission theory and the questions they raise which its proponents have been unable to answer.  The rapid spread of guns across Eurasia, only 50 years from China to Europe, with non-existent evidence of its route from one extreme of the continent to the other, remains a mystery.  Other Chinese inventions such as the compass, paper, and printing took centuries to reach Europe, with events such as the Battle of Talas as perhaps a possible takeoff point for discussion.  No such event exists on record for either gunpowder or the gun.  There is simply no clear route of transmission, and while the Mongols are often pointed to as the likeliest vector, Timothy May points out that \"there is no concrete evidence that the Mongols used gunpowder weapons on a regular basis outside of China.\"  A conclusion most military historians in the transmission camp have come to is that the rapid diffusion of gunpowder and the gun is probably best explained by its clear military applications. On the other side of the debate, opponents of the theory criticize the vagueness of Chinese records on the specific usage of gunpowder in weaponry, the existence of gunpowder or possibly lack thereof in incendiary weapons as described by Chinese documents, the weakness of Chinese firearms, the non-existent route of diffusion or evidence of guns between Europe and China before 1326, and emphasize the independent evolution of superior guns in Europe.  This too becomes problematic as already discussed above.  Notably there is an acute dearth of any significant evidence of evolution or experimentation with gunpowder or gunpowder weapons leading up to the gun in 1326, which can be found in China.  Gunpowder appeared in Europe primed for military usage as an explosive and propellant, bypassing a process which took centuries of Chinese experimentation with gunpowder weaponry to reach, making a nearly instantaneous and seamless transition into gun warfare, as its name suggests.  Furthermore, early European gunpowder recipes shared identical defects with Chinese recipes such as the inclusion of the poisons sal ammoniac and arsenic, which provide no benefit to gunpowder.  Bert S. Hall explains this phenomenon in his \"Weapons and Warfare in Renaissance Europe: Gunpowder, Technology, and Tactics\" by drawing upon the gunpowder transmission theory, explaining that \"gunpowder came [to Europe], not as an ancient mystery, but as a well-developed modern technology, in a manner very much like twentieth-century 'technology-transfer' projects.\"  In a similar vein Peter Lorge supposes that the Europeans experienced gunpowder \"free from preconceived notions of what could be done,\" in contrast to China, \"where a wide range of formulas and a broad variety of weapons demonstrated the full range of possibilities and limitations of the technologies involved.\"  There is also the vestige of Chinese influence, and not European, on Muslim terminology of some gunpowder related items such as saltpeter, which has been described as either Chinese snow or salt, fireworks which were called Chinese flowers, and rockets which were called Chinese arrows.  Moreover, Europeans in particular experienced great difficulty in obtaining saltpeter, a primary ingredient of gunpowder which was relatively scarce in Europe compared to China, and had to be obtained from \"distant lands or extracted at high cost from soil rich in dung and urine.\"  Thomas Arnold believes that the similarities between early European cannons and contemporary Chinese models suggests a direct transmission of cannon making knowledge from China rather than a home grown development.  Whatever the truth may be, the first unambiguous references to guns appeared in Europe in the 1320s. The Mongol army incorporated Chinese siege units in 1214 and they constituted part of the body which invaded Khwarezmia in 1219.  Chinese siege units saw action in Transoxania in 1220 and in the north Caucasus during the 1239–1240 Mongol invasions of Georgia and Armenia.  Furthermore, the \"Tarikh-i Jahangushay\" records that Hulagu Khan procured one thousand families of Chinese siege experts in 1253, five years prior to the Siege of Baghdad (1258).  Some historians speculate that the Chinese may have used siege weapons to hurl gunpowder bombs during their time in the Mongol army.  A common theory among historians is that the Mongol invasion brought Chinese gunpowder weapons to Central Asia and other parts of the world. The Muslim world acquired knowledge of gunpowder some time after 1240, but before 1280, by which time Hasan al-Rammah had written, in Arabic, recipes for gunpowder, instructions for the purification of saltpeter, and descriptions of gunpowder incendiaries.  Gunpowder arrived in the Middle East, possibly through India, from China.  This is implied by al-Rammah's usage of \"terms that suggested he derived his knowledge from Chinese sources\" and his references to saltpeter as \"Chinese snow\" Arabic: ثلج الصين‎ ‎ thalj al-ṣīn , fireworks as \"Chinese flowers\" and rockets as \"Chinese arrows\".  However, because al-Rammah attributes his material to \"his father and forefathers\", al-Hassan argues that gunpowder became prevalent in Syria and Egypt by \"the end of the twelfth century or the beginning of the thirteenth\".  Persians called saltpeter \"Chinese salt\" or \"salt from Chinese salt marshes\" (namak shūra chīnī Persian: نمک شوره چيني‎ ‎ ).  Al-Baytar, an Arab from Spain who had immigrated to Egypt, wrote in Arabic that \"snow of China\" (Arabic: ثلج الصين‎ ‎ thalj al-ṣīn ) was the name used to describe saltpeter.  Al-Baytar died in 1248. The earliest surviving documentary evidence for the use of the hand cannon in the Islamic world are from several Arabic manuscripts dated to the 14th century.  Khan claims that it was invading Mongols who introduced gunpowder to the Islamic world and cites Mamluk antagonism towards early riflemen in their infantry as an example of how gunpowder weapons were not always met with open acceptance in the Middle East.  Similarly, the refusal of their Qizilbash forces to use firearms contributed to the Safavid rout at Chaldiran in 1514. Hasan al-Rammah also describes the purifying of saltpeter using the chemical processes of solution and crystallization.  This was the first clear method for the purification of saltpeter.  The earliest torpedo was also first described in 1270 by Hasan al-Rammah in \"The Book of Military Horsemanship and Ingenious War Devices\", which illustrated a torpedo running with a rocket system filled with explosive materials and having three firing points. A common theory of how gunpowder came to Europe is that it made its way along the Silk Road through the Middle East.  Another is that it was brought to Europe during the Mongol invasion in the first half of the 13th century.  Some sources claim that Chinese firearms and gunpowder weapons may have been deployed by Mongols against European forces at the Battle of Mohi in 1241.  It may also have been due to subsequent diplomatic and military contacts (see Franco-Mongol alliance).  Professor Kenneth Warren Chase credits the Mongols for introducing into Europe gunpowder and its associated weaponry. The earliest European references to gunpowder are found in Roger Bacon's \"Opus Majus\" from 1267.  Coincidentally Roger Bacon was known to have been a close friend of William of Rubruck, who served as an ambassador to the Mongols from 1253-1255.  William returned to Europe in 1257 and accounts of his journey were generally received with lackluster response and saw little circulation.  However his friend Roger Bacon took particular interest in his experience.  Historians such as Joseph Needham and Kenneth Chase have noted this and pointed out William of Rubruck as a possible intermediary in the transmission of gunpowder.  Roger Bacon's description of gunpowder usage also suggests it was commonly found in various parts of the world prior to its appearance in Europe: The oldest written recipes for gunpowder in Europe were recorded under the name Marcus Graecus or Mark the Greek between 1280 and 1300. Historian Kenneth Warren Chase has suggested that a medieval Armenian manuscript from 1307 which mentions powerful weapons had been built by the Chinese is a reference to the Chinese invention of gunpowder.  The Armenian monk Hetoum says this about China: There is a record of a gun in Europe dating to 1322 being discovered in the nineteenth century but the artifact has since been lost.  The earliest known European depiction of a gun appeared in 1326 in a manuscript by Walter de Milemete, although not necessarily drawn by him, known as \"De Nobilitatibus, sapientii et prudentiis regum\" (Concerning the Majesty, Wisdom, and Prudence of Kings), which displays a gun with a large arrow emerging from it and its user lowering a long stick to ignite the gun through the touchole In the same year, another similar illustration showed a darker gun being set off by a group of knights, which also featured in another work of de Milemete's, \"De secretis secretorum Aristotelis\".  On 11 February of that same year, the Signoria of Florence appointed two officers to obtain \"canones de mettallo\" and ammunition for the town's defense.  In the following year a document from the Turin area recorded a certain amount was paid \"for the making of a certain instrument or device made by Friar Marcello for the projection of pellets of lead.\"  A reference from 1331 describes an attack mounted by two Germanic knights on Cividale del Friuli, using gunpowder weapons of some sort.  The 1320s seem to have been the takeoff point for guns in Europe according to most modern military historians.  Scholars suggest that the lack of gunpowder weapons in a well-traveled Venetian's catalogue for a new crusade in 1321 implies that guns were unknown in Europe up until this point, further solidifying the 1320 mark, however more evidence in this area may be forthcoming in the future. From the 1320s guns spread rapidly across Europe.  The French raiding party that sacked and burned Southampton in 1338 brought with them a ribaudequin and 48 bolts (but only 3 pounds of gunpowder).  By 1341 the town of Lille had a \"tonnoire master,\" and a tonnoire was an arrow-hurling gun.  In 1345, two iron cannons were present in Toulouse.  In 1346 Aix-la-Chapelle too possessed iron cannons which shot arrows (busa ferrea ad sagittandum tonitrum).  The Battle of Crécy in 1346 was one of the first in Europe where cannons were used.  Florentine chronicler Giovanni Villani describes the deployment of firearms thus, \"The English king arranged his archers, of whom he had many, on the carts, and some below and with guns [bombarde] that threw out small iron pellets [pallottole] with fire, to frighten the French horsemen and cause them to desert.\"  Another incident in the summer of 1346 involved a man by the name of Peter de Bruges who demonstrated to the consuls of Tournai the power of an arrow shooting gun, which went off making \"a terrible and tremendous noise,\" and the arrow flew over the town, landing in a monastery plaza, unceremoniously killing a man by headshot.  De Bruges ran in fear of prosecution but the consuls judged the incident an accident of no fault of his own.  Two years later the town of Deventer had in its possession three cannons, or \"dunrebussen\", and Frankfurt had cannons that shot arrows (\"büzenpzle\"), while the city of Rouen reported a \"pot that shot iron arrows with fire\" (pot de fer à traire garros à feu).  In 1350 Petrarch wrote that the presence of cannons on the battlefield was 'as common and familiar as other kinds of arms'. Along with the transmission and growing use of guns in Europe also came an infamous reputation.  At least three Europeans in the 14th century described guns as either abnormal or inhuman.  For instance, in 1344 Petrarch writes, \"I wonder that thou hast not also brazen globes, which are cast forth by the force of flame with a horrible sound of thunder.  Was not the wrath of an immortal god thundering from heaven sufficient, that the small being man— oh cruelty joined to pride— must even thunder on earth?  Human rage has endeavored to imitate the thunder which cannot be imitated … and that which is wont to be sent from the clouds is now thrown from an infernal instrument.\"  furthermore he compares guns to the plague, \"This plague was only recently so rare as to be looked on as a miracle; now … it has become as common as any other kind of weapon.\"  Englishman John Mirfield describes the gun as \"that devilish instrument of war colloquially termed gunne,\" and Francesco di Giorgio Martini thought likewise of the discovery of guns and gunpowder, attributing it \"not to human but to devilish agency.\"  In the history of early European guns, Christian authorities made vehement remarks against the use of gunpowder weapons, calling them blasphemous and part of the 'Black Arts'.  By the mid-14th century, however, even the army of the Papal States would be armed with artillery and gunpowder weapons. Among the oldest archaeological examples of guns in Europe, there are two pieces from Sweden, the Loshult gun, and lesser known Morko gun.  Possibly Europe's oldest extant firearm, the Loshult gun is a cast-bronze gun in the shape of a vase discovered in 1861 and named after the Swedish parish where the farmer dug it up.  It is generally dated to around the mid 14th century and measures 31 cm overall, at a weight of 9 kilograms; the bore at the muzzle is 36 mm narrowing down to 31 mm on its way to the rear.  A replica made by scholars has performed well shooting both arrows and other projectiles such as lead balls, grapeshot, and pieces of flint.  The arrows and lead balls proved capable of penetrating late medieval plate armor as well as hitting a stationary target 200 meters away.  However scholars believe the gun was primarily used for short range engagements because of the deep ruts and scratches in the barrel, which suggests that it carried shrapnel, an inaccurate but deadly projectile.  It is now preserved in the Statens Historiska Museet Stockholm, inv.-no.  2891.  The Morko gun was found by a fisherman in the Baltic Sea at the coast of Södermansland near Nynäs before 1828 and dates to around the same period as the Loshult gun, ca. 1390.  In Germany the oldest gun is the Tannenberg handgonne discovered in 1849 at the bottom of a water well of the 1399 destroyed Tannenberg castle in Hessen.  It was found with a lead ball still loaded, and its wooden staff nearby, which started decaying after being exposed to air.  The gun was aimed using the wooden staff, which fit into the back socket of the gun.  References to \"gunnis cum telar\" (guns with handles) were recorded in 1350 and by 1411 it was recorded that John the Good, Duke of Burgundy, had 4000 handguns stored in his armory. Around the late 14th century European and Ottoman guns began to deviate in purpose and design from guns in China, changing from small anti-personnel and incendiary devices to the larger artillery pieces most people imagine today when using the word \"cannon.\"  If the 1320s can be considered the arrival of the gun on the European scene, then the end of the 14th century may very well be the departure point from the trajectory of gun development in China.  In the last quarter of the 14th century, European guns grew larger and began to blast down fortifications. Gunpowder technology is believed to have arrived in India by the mid-14th century, but could have been introduced much earlier by the Mongols, who had conquered both China and some borderlands of India, perhaps as early as the mid-13th century.  The unification of a large single Mongol Empire resulted in the free transmission of Chinese technology into Mongol conquered parts of India.  Regardless, it is believed that the Mongols used Chinese gunpowder weapons during their invasions of India.  It was written in the \"Tarikh-i Firishta\" (1606–1607) that the envoy of the Mongol ruler Hulegu Khan was presented with a dazzling pyrotechnics display upon his arrival in Delhi in 1258 AD.  The first gunpowder device, as opposed to naphtha-based pyrotechnics, introduced to India from China in the second half of the 13th century, was a rocket called the \"hawai\" (also called \"ban\").  The rocket was used as an instrument of war from the second half of the 14th century onward, and the Delhi sultanate as well as Bahmani kingdom made good use of them.  As a part of an embassy to India by Timurid leader Shah Rukh (1405–1447), 'Abd al-Razzaq mentioned naphtha-throwers mounted on elephants and a variety of pyrotechnics put on display.  Roger Pauly has written that \"while gunpowder was primarily a Chinese innovation,\" the saltpeter that led to the invention of gunpowder may have arrived from India, although it is also likely that it originates indigenously in China. Firearms known as \"top-o-tufak\" also existed in the Vijayanagara Empire of India by as early as 1366 AD.  By 1442 guns had a clearly felt presence in India as attested to by historical records.  From then on the employment of gunpowder warfare in India was prevalent, with events such as the siege of Belgaum in 1473 AD by the Sultan Muhammad Shah Bahmani. Korea began production of gunpowder during the years 1374–76.  In the 14th century a Korean scholar named Choe Museon discovered a way to produce it after visiting China and bribing a merchant by the name of Li Yuan for the gunpowder formula.  In 1377 he figured out how to extract potassium nitrate from the soil and subsequently invented the juhwa, Korea's first rocket, and further developments led to the birth of singijeons, Korean arrow rockets.  The multiple rocket launcher known as hwacha (\"fire cart\" 火車) was developed from the juhwa and singijeon in Korea by 1409 during the Joseon Dynasty.  Its inventors include Yi Do (이도, not to be mistaken for Sejong the Great) and Choi Hae-san (최해산, son of Choe Museon).  However the first hwachas did not fire rockets, but utilized mounted bronze guns that shot iron-fletched darts.  Rocket launching hwachas were developed in 1451 under the decree of King Munjong and his younger brother Pe.  ImYung (Yi Gu, 임영대군 이구).  This \"Munjong Hwacha\" is the well-known type today, and could fire 100 rocket arrows or 200 small Chongtong bullets at one time with changeable modules.  At the time, 50 units were deployed in Hanseong (present-day Seoul), and another 80 on the northern border.  By the end of 1451, hundreds of hwachas were deployed throughout Korea. Naval gunpowder weapons also appeared and were rapidly adopted by Korean ships for conflicts against Japanese pirates in 1380 and 1383.  By 1410, 160 Korean ships were reported to have equipped artillery of some sort.  Mortars firing thunder-crash bombs are known to have been used, and four types of cannons are mentioned: chonja (heaven), chija (earth), hyonja (black), and hwangja (yellow), but their specifications are unknown.  These cannons typically shot wooden arrows tipped with iron, the longest of which were nine feet long, but stone and iron balls were sometimes used as well. Firearms seem to have been known in Japan around 1270 as proto-cannons invented in China, which the Japanese called teppō (鉄砲 lit.  \"iron cannon\").  Gunpowder weaponry exchange between China and Japan was slow and only a small number of hand guns ever reached Japan.  The first recorded appearance of the cannon in Japan was in 1510 when a Buddhist priest presented Hōjō Ujitsuna with a teppō iron cannon he acquired during his travels in China.  However gunpowder weapons saw very little use in Japan until Portuguese firearms were introduced in 1543.  During the Japanese invasions of Korea (1592-1598), the forces of Toyotomi Hideyoshi effectively utilized matchlock firearms against the Korean forces of Joseon, although they would ultimately be defeated and forced to withdraw from the Korean peninsula. The 15th through 18th centuries saw widespread improvement in gunpowder technology.  Military developments in Europe and the aggressive application of technological advances in this period is usually referred to as early modern warfare, when Europe gained a global advantage in deep-water warfare and fortress architecture.  Eventually military science in Europe led to a marked technological departure from the rest of the world known as the Great Divergence, a term coined by Samuel P. Huntington.  Gunpowder remained of central importance until it was replaced by more advanced explosives beginning in the 1860s. Gun development and proliferation in China continued under the Ming dynasty.  The success of its founder Zhu Yuanzhang, who declared his reign to be the era of Hongwu, or \"Great Martiality,\" has often been attributed to his effective use of guns.  Early Ming military codes stipulated that ideally 10 percent of all soldiers should be gunners.  By 1380 the Ming dynasty boasted around 130,000 gunners out of its 1.3 to 1.8 million strong army.  Under the Hongwu Emperor's successors, the percentage climbed higher and by the 1440s it reached 20 percent.  In 1466 the ideal composition was 30 percent.  The Hongwu Emperor created a Bureau of Armaments (軍器局) which was tasked with producing every three years 3000 handheld bronze guns, 3000 signal cannons, and ammunition as well as accoutrements such as ramrods.  His Armory Bureau (兵仗局) was responsible for producing types of guns known as \"great generals,\" \"secondary generals,\" \"tertiary generals,\" and \"gate-seizing generals.\"  Other firearms such as \"miraculous [fire] lances,\" \"miraculous guns,\" and \"horse-beheading guns\" were also produced.  It is unclear what proportion or how many of each type were actually manufactured.  However most early Ming guns weighed only two to three kilograms while guns considered \"large\" at the time weighed around only seventy-five kilograms.  A gun known as the \"Great Bowl-Mouth Tube\" (大碗口筒) dated to 1372 weighs only 15.75 kilograms and was 36.5 centimeters long, its muzzle 11 centimeters in diameter.  Other excavated guns of this type range from 8.35 to 26.5 kilograms.  While they were usually mounted on ships or gates they were also relatively small compared to the cannons of the 16th to 17th centuries associated with naval warfare.  Ming sources suggest guns such as these shot stones and iron balls, but were primarily used against men rather than for causing structural damage to ships or walls.  Accuracy was low and they were limited to a range of only 50 paces or so. Despite the relatively small size of Ming guns, some elements of gunpowder weapon design followed world trends.  The growing length to muzzle bore ratio matched the rate at which European guns were developing up until the 1450s.  The practice of corning gunpowder had been developed by 1370 for the purpose of increasing explosive power in land mines, and was arguably used in guns as well according to one record of a fire-tube shooting a projectile 457 meters, which was probably only possible at the time with the usage of corned powder.  Around the same year Ming guns transitioned from using stone shots to iron ammunition, which has greater density and increased firearm power. Guns as well as other gunpowder weapons were used in abundance during the famous Battle of Lake Poyang which lasted from 30 August to 4 October 1363.  There were at least a hundred vessels and approximately 500,000 combatants altogether involved, although traditional numbers have gone even higher.  Poyang Lake was situated on a strategic location connecting the Yangtze with other river basins and in the early 1360s Zhu Yuanzhang held key garrisons on the lake, which he administered from Nanjing 560 kilometers downriver.  Upstream was the state of Han (大漢; Pinyin: \"Dahan\", \"Great Han\") under Chen Youliang, who set out to wrest control of the lake from Zhu Yuanzhang.  Chen's force consisted of \"tower ships\" designed for depositing soldiers on riverside city walls rather than waterborne combat, and the purpose of these ships was to capture the city of Nanchang, which guarded Lake Poyang from the south.  This proved futile as the city defenders simply moved the walls back, and Chen was forced to personally lead an assault on the city gates, which was met with a counterattack with huochong (early Ming guns), and they were driven back.  After this failure, Chen set up a blockade, determined to starve out the defenders, but a small fishing boat managed to slip out and reached Nanjing in time to warn Zhu Yuanzhang. Zhu's fleet arrived at Poyang Lake on 29 August faced with Chen's larger force and was outnumbered three to two.  According to one Ming source, Zhu's force arrived armed with \"fire bombs, fire guns, fire arrows, fire seeds [probably grenades], large and small fire lances, large and small 'commander' fire-tubes, large and small iron bombs, rockets.\"  This shows that older gunpowder weapons co-existed alongside guns, and proto-guns such as fire lances were not supplanted until after early Ming.  A new weapon called the \"No Alternative\" was also mentioned.  The No Alternative was \"made from a circular reed mat about five inches around and seven feet long that was pasted over with red paper and bound together with silk and hemp— stuffed inside it was gunpowder twisted in with bullets and all kinds of [subsidiary] gunpowder weapons.\"  It was hung from a pole on the foremast, and when an enemy ship came into close range, the fuse was lit, and the weapon would supposedly fall onto the enemy ship, at which point things inside shot out \"and burned everything to bits, with no hope of salvation.\"  Zhu's fleet engaged Chen's under orders to \"get close to the enemy's ships and first set off gunpowder weapons (發火器), then bows and crossbows, and finally attack their ships with short range weapons.\"  Fire bombs were hurled using naval trebuchets and the Ming succeeded in \"burning twenty or more enemy vessels and killing or drowning many enemy troops.\"  Ming eventually came out victorious by ramming and burning the enemy fleet with fire ships.  While guns were used during the battle, ultimately they were not pivotal to success, and the battle was won using incendiary weapons. In 1358 at the Siege of Shaoxing the defenders \"used ... fire tubes to attack the enemy's advance guard\" against Ming.  The siege was won by the defenders, whose \"fire tubes went off all at once, and the [attacker's] great army could not stand against them and had to withdraw.\"  Ming began fielding cannons in greater proportions during the Siege of Suzhou in 1366, but they were relatively small and were used primarily for killing human beings rather than creating a breech in the city's defensive fortifications.  Suzhou's fortifications were previously rebuilt in 1352 and contemporary documents of the city wall record a width of eleven meters at the base, five meters at the top, seven meters high, and seventeen kilometers all around.  The walls were constructed using tamped earth covered with brick and sloped from bottom to top.  Ming commander Xu Da ordered the army to set up a blockade.  Watchtowers 13 meters high were constructed, which the \"Ming Veritable Records\" notes \"were placed bows and guns.\"  Each of the 48 Ming divisions (衛) were equipped with 50 large and small \"general tubes\" (大小將軍筒) for a total of 2400 guns, and besieged the city.  Large ones weighed only 80 kilograms or less and the majority were small ones which weighed a couple kilograms at most.  The largest possibly a meter long with a muzzle diameter of 21 centimeters, but these would have been rare exceptions in the early Ming arsenal.  The lack of larger siege weapons in China unlike the rest of the world where cannons grew larger and more potent has been attributed to the immense thickness of traditional Chinese walls, which Tonio Andrade suggests provided no incentive for creating larger cannons, since even industrial artillery had trouble overcoming them.  Chinese walls during the Ming period continued the traditional building practices and nearly every prefectural and provincial capital was fortified with walls between 10 and 20 meters thick at the base and 5 to 10 meters wide at the top. Each division was also equipped with five Xiangyang Trebuchets and five \"Seven-Component Trebuchets\" (七梢炮), which were used to destroy wooden structure on the walls, but were unable to breech the walls themselves despite the fact that \"the noise of the guns and the paos went day and night and didn't stop.\"  In a rare occurrence of Chinese history, the city defenses were eventually breached, but this happened through the gates, which were covered by an outwork, outer gate, and finally consisted of an inner gate.  Rather than bombarding the entrance, the breach was made by traditional manual mining and battering.  This was only possible because the defenders were already starving by mid-autumn of 1367 and unable to put up a proper resistance, succumbing to the onslaught of a frontal assault.  The battle continued inside the city where Zhang Shicheng's remaining 20 thousand troops engaged in street to street combat until he fled from his palace.  His wives and concubines burned themselves to death.  Zhang then tried to commit suicide, but was captured and taken back to Nanjing as \"a guest.\" Despite the increasing quantity of guns fielded in battles and sieges, Ming cannons remained relatively small and limited to anti-personnel purposes.  The peak of Chinese cannon development is exemplified by the muzzle loading wrought iron \"great general cannon\" (大將軍炮) which weighed up to 360 kilograms and could fire a 4.8 kilogram lead ball.  Its heavier variant, the \"great divine cannon\" (大神銃), could weigh up to 600 kilograms and was capable of firing several iron balls and upward of a hundred iron shots at once.  The great general and divine cannons were the last indigenous Chinese cannon designs prior to the incorporation of European models in the 16th century. The development of large artillery pieces and larger guns began with Burgundy.  Originally a minor power, it grew to become one of the most powerful states in 14th-century Europe, and a great innovator in siege warfare.  The Duke of Burgundy, Philip the Bold (1363-1404), based his power on the effective use of big guns and promoted research and development in all aspects of gunpowder weaponry technology.  Philip established manufacturers and employed more cannon casters than any European power before him, however he wasn't the first to build large guns.  The first appeared in 1375 when a dozen or so smiths in Caen developed a 900 kilogram cannon which took six weeks to create.  Whereas most European guns before 1370 weighed about 20 to 40 lbs (9–14 kg), the French siege of Château de Saint-Sauveur-le-Vicomte in 1375 during the Hundred Years War saw the use of guns weighing over a ton (900 kg), firing stone balls weighing over 100 lbs (45 kg).  Philip used large guns to help the French capture the fortress of Odruik in 1377, which was held by the English at the time.  These guns fired projectiles far larger than any that had been used before, and among the 140 cannons Philip brought to Odruik, 109 were large, with seven guns that could shoot projectiles as heavy as 90 kilograms.  The cannons smashed the city walls, some going right through them, and Odruik surrendered, inaugurating a new era of artillery warfare.  Burgundy's territories rapidly expanded using their new gun designs. Europe entered an arms race to build ever larger artillery pieces.  Burgundy's guns continued to develop, and in the same year that Odruik fell, Philip's smith constructed a gun capable of shooting balls weighing 200 kilograms.  In 1382 an \"incredibly huge bombard\" was aimed at Oudenaarde, its muzzle 60 inches around and 20 inches in diameter.  One observer described it thus, \"One could hear it for five leagues in the daytime, and ten leagues at night, and it made such a huge noise that … it seemed as though all the demons of hell were present.\"  By the early 15th century both French and English armies were equipped with larger pieces known as bombards, weighing up to 5 tons (4,535 kg) and firing balls weighing up to 300 lbs (136 kg).  In 1404 a cannon was created with the inscription, \"I am named Katrin.  Beware of what I hold.  I punish injustice.\"  In 1411 the cast iron cannon Faule Mette was created, and it was capable of shooting stone balls weighing 400 kilograms, but didn't very often.  The artillery trains used by Henry V of England in the 1415 Siege of Harfleur and 1419 Siege of Rouen proved effective in breaching French fortifications, while artillery contributed to the victories of French forces under Joan of Arc in the Loire Campaign (1429).  It wasn't just the English blasting their way through France either.  Burgundy started by battering down the walls of Vellexon in 1409 and in 1411 they took Ham with only three shots from their bombard Griette.  France eventually came around under the reforms of King Charles V (1422-1461), whose leadership made France the premier artillery force in Europe, and won the Hundred Years' War.  Gun developments in France would prove extremely effective during the Italian wars of the 1490s.  In 1431 a wrought-iron cannon was forged, weighing over 12,000 kilograms and could fire projectiles of 300 kilograms.  It was called the Dulle Griet.  These weapons were transformational for European warfare.  A hundred years earlier the Frenchman Pierre Dubois wrote that a \"castle can hardly be taken within a year, and even if it does fall, it means more expenses for the king's purse and for his subjects than the conquest is worth,\" but by the 15th century European walls fell with the utmost regularity. The Ottoman Empire was also developing their own artillery pieces.  Mehmed the Conqueror (1432-1481) was determined to procure large cannons for the purpose of conquering Constantinople and paid handsomely as well as allowed creative autonomy for cannon makers.  One of the cannon makers that responded to his call was a Hungarian named Urban, who left Byzantium after being denied a raise by the Byzantine emperor.  Urban labored for four months to create a six meter long cannon, which required hundreds of pounds of gunpowder to fire, and its stone projectiles weighed between 550 and 800 kilograms.  The gun's projectiles were reported to have flown for a mile before landing, and shook the entire ground when fired, the roar blasting four miles away.  It had to be transported by 30 wagons pulled by 60 oxen, with the assistance of 200 handlers.  An additional 50 carpenters and 200 laborers helped in the transport by leveling terrain and building bridges.  During the actual siege of Constantinople the gun proved to be somewhat underwhelming.  The aiming process was laborious and after each shot it required hot oil ointment for cooling.  Its rate of fire was once every three hours, and may have even suffered damage from cracks early on, never to be repaired. Fortunately for the Ottomans it wasn't Mehmed's only cannon.  Dozens of other large cannons alongside 500 smaller cannons bombarded Constantinople's walls in their weakest sections for 55 days.  A Greek contemporary, Kritoboulos, describes the scene thus, \"The stone, borne with tremendous force and velocity, hit the wall, which it immediately shook and knocked down, and was itself broken into many fragments and scattered, hurling the pieces everywhere and killing those who happened by be near by.  Sometimes it demolished a whole section, and sometimes a half-section, and sometimes a larger or smaller section of a tower or turret or battlement.  And there was no part of the wall strong enough or resistant enough or thick enough to be able to withstand it, or to wholly resist such force and such a blow of the stone cannon-ball.\"  Mehmed's smaller artillery pieces also proved effective.  Constantinople's defenders wielded their own formidable guns and \"fired … five or ten bullets at a time, each about the size of a … walnut, and having a great power of penetration.  If one of these hit an armed man it would go right through his shield and his body and go on to hit anyone else who happened to be in his way, and even a third, until the force of the powder diminished; so one shot might hit two or three men.\"  Despite the fierce defense, the city's fortifications were ultimately overwhelmed in a final assault and the sultan won the siege. While China was the birthplace of gunpowder the guns there remained relatively small and light, weighing less than 80 kilograms or less for the large ones, and only a couple kilograms at most for the small ones during the early Ming era.  Guns themselves had proliferated throughout China and become a common sight during sieges, so the question has arisen then why large guns were not first developed in China.  According to Tonio Andrade, this wasn't a matter of metallurgy, which was sophisticated in China, and the Ming dynasty did construct large guns in the 1370s, but never followed up afterwards.  Nor was it the lack of warfare, which other historians have suggested to be the case, but does not stand up to scrutiny as walls were a constant factor of war which stood in the way of many Chinese armies since time immemorial into the twentieth century.  The answer Andrade provides is simply that Chinese walls were much less vulnerable to bombardment.  Andrade argues that traditional Chinese walls were built differently from medieval European walls in ways which made them more suitable for resisting cannon fire. Chinese walls were bigger than medieval European walls.  In the mid-twentieth century a European expert in fortification commented on their immensity: \"in China … the principal towns are surrounded to the present day by walls so substantial, lofty, and formidable that the medieval fortifications of Europe are puny in comparison.\"  Chinese walls were thick.  Ming prefectural and provincial capital walls were ten to twenty meters thick at the base and five to ten meters at the top. In Europe the height of wall construction was reached under the Roman Empire, whose walls often reached ten meters in height, the same as many Chinese city walls, but were only 1.5 to 2.5 meters thick.  Rome's Servian Walls reached 3.6 and 4 meters in thickness and 6 to 10 meters in height.  Other fortifications also reached these specifications across the empire, but all these paled in comparison to contemporary Chinese walls, which could reach a thickness of 20 meters at the base in extreme cases.  Even the walls of Constantinople which have been described as \"the most famous and complicated system of defence in the civilized world,\" could not match up to a major Chinese city wall.  Had both the outer and inner walls of Constantinople been combined, they would have only reached roughly a bit more than a third the width of a major wall in China.  European walls of the 1200s and 1300s could reach the Roman equivalents but rarely exceeded them in length, width, and height, remaining around two meters thick.  It is apt to note that when referring to a very thick wall in medieval Europe, what is usually meant is a wall of 2.5 meters in width, which would have been considered thin in a Chinese context.  There are some exceptions such as the Hillfort of Otzenhausen, a Celtic ringfort with a thickness of forty meters in some parts, but Celtic fort-building practices died out in the early medieval period.  Andrade goes on to note that the walls of the \"marketplace\" of Chang'an were thicker than the walls of major European capitals. Aside from their immense size, Chinese walls were also structurally different from the ones built in medieval Europe.  Whereas European walls were for the most part constructed of stone with gravel or rubble filling and limestone mortar, Chinese walls had tamped earthen cores which absorbed the energy of artillery shots.  Walls were built using constructed wooden frameworks which were then filled with layers of earth tamped down to a highly compact state, and once that was completed the frameworks were removed for use in the next wall section.  Starting from the Song dynasty these walls were improved with an outer layer of bricks or stone to prevent corrosion, and during the Ming, earthworks were interspersed with stone and rubble.  Most Chinese walls were also sloped rather than vertical to better deflect projectile energy. The Chinese Wall Theory essentially rests on a cost benefit hypothesis, where the Ming recognized the highly resistant nature of their walls to structural damage, and could not imagine any affordable development of the guns available to them at the time to be capable of breaching said walls.  Even as late as the 1490s a Florentine diplomat considered the French claim that \"their artillery is capable of creating a breach in a wall of eight feet in thickness\" to be ridiculous and the French \"braggarts by nature.\"  In fact \"twentieth\" century explosive shells had some difficulty creating a breach in tamped earthen walls. Andrade goes on to question whether or not Europeans would have developed large artillery pieces in the first place had they faced Chinese style walls so resistant to projectiles, coming to the conclusion that such exorbitant investments in weapons unable to serve their primary purpose would not have been ideal.  Yet Chinese walls do not fully explain the divergence in gun development as European guns grew not only bigger, but more effective as well.  By 1490 the European gun had achieved the basic form it would take for the next three centuries, during which it would dominate the fields of warfare.  The Classic Gun had arrived, and in the 1510s and 1520s when the Chinese encountered European firearms, they fully recognized they were superior to their own. Asianist Kenneth Chase offers another theory as to why gun development stagnated during the Ming dynasty.  Chase argues that guns were not particularly useful against China's traditional enemies: horse nomads.  Guns were supposedly problematic to deploy against nomads because of their size and slow speed, drawing out supply chains, and creating logistical challenges.  Theoretically, the more mobile nomads took the initiative in sallying, retreating, and engaging at will.  Chinese armies therefore relied less heavily on guns in warfare than Europeans, who fought large infantry battles and sieges which favored guns, or so Chase argues. However, the Chase hypothesis has been refuted based on observations made by Tonio Andrade, who notes that the Asian nomad argument becomes problematic when viewed through a Chinese perspective.  For the Chinese themselves considered guns to be highly valuable against nomads.  During the Ming-Mongol wars of the 1300s and early 1400s guns were utilized, notably in defending against a Mongol invasion in 1449, and guns were in high demand along the northern borders where gun emplacements were common.  Several sources make it clear that Chinese military leaders found guns to be highly effective against nomads throughout the 15th and 16th centuries.  The military scholar Weng Wanda goes as far as to claim that only with firearms could one hope to succeed against the fast moving Mongols, and purchased special guns for both the Great Wall defenses and offensive troops who fought in the steppes.  Andrade also points out that Chase downplays the amount of warfare going on outside of the northern frontier, specifically southern China where huge infantry battles and sieges such as in Europe were the norm.  In 1368 the Ming army invaded Sichuan, Mongolia, and Yunnan.  Although little mention of firearm is used in accounts of the Mongolia campaign, Ming cannons readily routed the war elephants of Yunnan, and in the case of Sichuan, both sides employed equally potent firearms.  When the Ming founder Zhu Yuanzhang died, the civil war which ensued had Chinese armies fighting one another, again with equally powerful firearms.  After the usurper Yongle's victory, the Ming embarked upon yet another five expeditions into Mongolia alongside an invasion of Dai Viet, many of their troops wielding firearms.  In 1414 the Ming army clashed with an Oirat force near the Tula River and frightened them so much with their guns that the Oirats fled without their spare horses, only to be ambushed by concealed Chinese guns.  According to a Chinese observer the Oirats avoided battle several days later, \"fearing that the guns had arrived again.\"  In Vietnam, as with Yunnan, the war elephants fared poorly and were defeated with a combination of arrows and firearms.  Chinese firearms were used in defensive fortifications to some effect, but were ineffective at targeting individual targets such as Vietnamese guerillas presented themselves. Chase's argument has also been criticized by Stephen Morillo for its reliance on a simple cause and effect analysis.  Alternatively Morillo suggests that the major difference between Chinese and European weapon development was economic.  Within Morillo's framework, European weapons were more competitive due to private manufacturing whereas Chinese weapons were manufactured according to government specifications.  Although generally true, Peter Lorge points out that gun specifications were widespread in China, and ironically the true gun was first developed during the Song dynasty, when guns were the exclusive enterprise of the government, suggesting that the economics of production were less influential on gun development than assumed.  In contrast, less innovation occurred during the Ming dynasty when most of production was shifted to the domain of private artisans.  Andrade concludes that although the Chase hypothesis should not be discarded outright, it does not offer a full explanation for the stagnation of firearm development in China. The star fort, also known as the bastion fort, \"trace italienne\", or renaissance fortress, was a style of fortification that became popular in Europe during the 16th century.  As a response to gunpowder artillery, European fortifications began displaying architectural principles such as lower and thicker walls in the mid-1400s.  Cannon towers were built with artillery rooms where cannons could discharge fire from slits in the walls.  However this proved problematic as the slow rate of fire, reverberating concussions, and noxious fumes produced greatly hindered defenders.  Gun towers also limited the size and number of cannon placements because the rooms could only be built so big.  Notable surviving artillery towers include a seven layer defensive structure built in 1480 at Fougères in Brittany, and a four layer tower built in 1479 at Querfurth in Saxony.  Eventually defensive artillery towers gave way to the star fort's angle bastions.  The bastion and star fort was developed in Italy, where the Florentine engineer Giuliano da Sangallo (1445-1516) compiled a comprehensive defensive plan using the geometric bastion and full \"trace italienne\" that became widespread in Europe.  The main distinguishing features of the star fort were its angle bastions, each placed to support their neighbor with lethal crossfire, covering all angles, making them extremely difficult to engage with and attack.  Angle bastions consisted of two faces and two flanks.  Artillery positions positioned at the flanks could fire parallel into the opposite bastion's line of fire, thus providing two lines of cover fire against an armed assault on the wall, and preventing mining parties from finding refuge.  Meanwhile, artillery positioned on the bastion platform could fire frontally from the two faces, also providing overlapping fire with the opposite bastion.  Overlapping mutually supporting defensive fire was the greatest advantage enjoyed by the star fort.  As a result, sieges lasted longer and became more difficult affairs.  By the 1530s the bastion fort had become the dominant defensive structure in Italy.  Outside Europe, the star fort became an \"engine of European expansion,\" and acted as a force multiplier so that small European garrisons could hold out against numerically superior forces.  Wherever star forts were erected the natives experienced great, but not insurmountable difficulty, in uprooting European invaders. Gun development and design in Europe reached its \"classic\" form in the 1480s.  The so-called Classic Gun is referred to thus for its longer, lighter, more efficient, and more accurate design compared to its predecessors only three decades prior.  Indeed, cannons of the 1480s show little difference and surprising similarity with cannons three centuries later in the 1750s.  This 300-year period during which the classic cannon dominated gives it its moniker.  The early classical European guns are exemplified by two cannons from 1488 now preserved in a plaza in Neuchâtel, Switzerland.  The Neuchâtel guns are 224 centimeters long, with a bore of 6.2 centimeters and the other is slightly longer, 252 centimeters, with the same bore size.  Classical guns such as these are differentiated from older firearms by an assortment of improvements.  Their longer length-to-bore ratio imparts more energy into the shot, enabling the projectile to shoot further.  Not only longer, they were also lighter as the barrel walls were made thinner to allow for faster dissipation of heat.  Classical guns also no longer needed the help of a wooden plug to load since they offered a tighter fit between projectile and barrel, further increasing the accuracy of gunpowder warfare.  Classical guns were deadlier due to the combination of new weapon developments such as gunpowder corning and the iron shot, which had been gradually adopted in Europe during the 1400s.  When these guns reached China in the 1510s, the Chinese were highly impressed by them, primarily for their longer and thinner barrels. The two primary theories for the appearance of the classic gun involve the development of gunpowder corning and a new method for casting guns.  The corning hypothesis stipulates that the longer barrels came about as a reaction to the development of corned gunpowder.  Around the late 14th century European powdermakers began adding liquid to the constituents of gunpowder, which reduced dust, and with it the risk of explosion during manufacture.  The powder makers would then shape the resulting paste of moistened gunpowder—known as mill cake—into \"corns\" or granules to allow it to dry.  Not only did \"corned\" powder keep better, because of its reduced surface area, but gunners also found that it was more powerful and easier to load into guns.  The main advantage of corning is that the flame lights all the granules when the gunpowder is lit, spreading between them before significant gas expansion has occurred.  Without corning, much of the powder away from the initial flame would be blown out of the barrel before it burnt.  The size of the granules varied for different types of gun.  Prior to corning, gunpowder would frequently demix into its constitutive components and was an unreliable substance for use in guns.  The same granulation process is used nowadays in the pharmaceutical industry to ensure that each tablet contains the same proportion of active ingredient.  Before long, powdermakers standardized the process by forcing mill cake through sieves instead of corning powder by hand.  The faster gunpowder reaction was suitable for smaller guns, since large ones had a tendency to crack, and the more controlled reaction allowed large guns to have longer, thinner walls. Although the logical follow through is sound, the corning hypothesis has been refuted for two reasons.  One, the powder makers were probably more worried about spoilage than the effect of corned gunpowder on guns, and two, corning as a practice had existed in China since the 1370s.  In Europe corning arose as a response to the difficulty of obtaining pure saltpeter, specifically saltpeter consisting of potassium nitrate, rather than sodium nitrate or calcium nitrate, the latter two of which absorb water vapor more readily than potassium nitrate.  Thus corning was an imperative to producing reliable weaponized gunpowder.  Supposedly this was much less problematic in Asia, where pure saltpeter was more readily available and easily manufactured.  In China gunpowder corning had been practiced since the 1370s for the purpose of strengthening the explosive power of mines, since gunpowder corns provided more space required for a rapid combustion.  Corned powder might also have been used in guns based on a record of a fire-tube shooting a projectile 457 meters, a feet probably only possible with the aid of corned powder at the time. The second theory comes from Kay Smith who points out that key to developing the classic gun may be a new method of gun casting, muzzle side up.  Smith observes: \"The surviving pieces of ordnance from earlier in the 15th century are big pieces with large bore sizes.  They do not look like the long thin gun. … Essentially they are parallel-sided tubes with flat ends.  The explanation is, probably, that they were cast muzzle down in the traditional bell-founding method whereas the long thin guns were cast muzzle up. … Perhaps this marks the real 'revolution' in artillery.  Once the technique of casting muzzle up with the attendant advantages, and it is not clear what those are at present, had been mastered by cannon founders, the way was open for the development of the 'classic' form of artillery.\"  However, Smith himself states that it is not clear what advantages this technique would have conferred, despite its widespread adoption. Across the 15th and 16th centuries there were primarily two different types of manufactured cannons.  The wrought iron cannon and the cast-bronze cannon.  Wrought iron guns were structurally composed of two layers: an inner tube of iron staves held together in a tight fit by an outer case of iron hoops.  Bronze cannons on the other hand were cast in one piece similar to bells.  The technique used in casting bronze cannons was so similar to the bell that the two were often looked upon as a connected enterprise.  Church bells in the age of Catholic authority were viewed as idolatrous and met their end as raw metal used for casting cannons. Both iron and bronze cannons had their advantages and disadvantages.  Forged iron cannons were far cheaper, up to ten times so, than bronze cannons, but more unstable due to their piece built nature.  Iron cannons tended to burst apart, sometimes killing or severely maiming their operators.  Successive firings took their toll on iron cannons as individual pieces were shook loose.  Bronze cannons simply cracked.  Even without use, iron cannons were liable to rust away.  Bronze cannons did not rust.  Another less practical reason for the dominance of bronze cannons was their aesthetic appeal.  Because cannons were so important as displays of power and prestige, rulers liked to commission bronze cannons, which could be sculpted into fanciful designs such as flute columns, lions' and serpents' mouths, containing artistic motifs or symbols.  It was for these reasons that the cast-bronze cannon became the preferred type by the late 1400s.  By the late 1500s only small breech-loading pieces were made of wrought iron.  Iron cannons would regain ground in the late 1500s as improved casting techniques allowed cast-iron cannons to become more reliable, although still inferior to the safer bronze cannons. Although few in number there were some exceptions to the bronze and iron paradigm.  Composite metal cannons were produced in the Ming and Qing dynasties during the 17th century.  In China, the Ming dynasty started producing smoothbore breech loading cast-bronze cannons after the European fashion no later than 1620.  However, in addition to European expertise and design, the Chinese had an advantage in their long history of metal casting practices.  Two decades after the Ming started manufacturing Dutch style cannons known to the Chinese as \"red barbarian cannons\" (紅夷炮 hongyipao), Ming foundries merged native casting technology with improved cannon designs to create a distinctive cannon known as the \"Dingliao grand general.\"  Through combining the advanced cast-iron technique of southern China and the iron-bronze composite barrels invented in northern China, the Dingliao grand general cannons exemplified the best of both iron and bronze cannon designs.  Unlike traditional iron and bronze cannons, the Dingliao grand general'rs inner barrel was made of iron, while the exterior of brass.  Scholar Huang Yi-long describes the process: The resulting bronze-iron composite cannons were superior to iron or bronze cannons in many respects.  They were lighter, stronger, longer lasting, and able to withstand more intensive explosive pressure.  Chinese artisans also experimented with other variants such as cannons featuring wrought iron cores with cast iron exteriors.  While inferior to their bronze-iron counterparts, these were considerably cheaper and more durable than standard iron cannons.  Both types met with success and were considered \"among the best in the world\" during the 17th century.  The Chinese composite metal casting technique was effective enough that Portuguese imperial officials sought to employ Chinese gunsmiths for their cannon foundries in Goa, so that they could impart their methods for Portuguese weapons manufacturing. Soon after the Ming started producing the composite metal Dingliao grand generals in 1642, Beijing was captured by the Manchu Qing dynasty and along with it all of northern China.  The Manchu elite did not concern themselves directly with guns and their production, preferring to delegate the task to Chinese craftsmen, who produced for the Qing a similar composite metal cannon known as the \"Shenwei grand general.\"  However, after the Qing gained hegemony over East Asia in the mid-1700s, the practice of casting composite metal cannons fell into disuse until the dynasty faced external threats once again in the Opium War of 1840, at which point smoothbore cannons were already starting to become obsolete as a result of rifled barrels.  Today there are currently 44 known extant composite metal cannons from the Ming-Qing period. The arquebus, derived from the German \"Hackenbüchse\", was a form of handgun that appeared in Germany during the late 15th century.  Although the term \"arquebus\" was applied to many different forms of firearms from the 15th to 17th centuries, it was originally used to describe \"a hand-gun with a hook-like projection or lug on its under surface, useful for steadying it against battlements or other objects when firing.\"  These \"hook guns\" were in their earliest forms defensive weapons mounted on German city walls in the early 1400s, but by the late 1400s had transitioned into handheld firearms, with heavier variants known as \"muskets\" that were fired from resting Y-shaped supports appearing by the early 1500s.  The musket was able to penetrate all forms of armor available at the time, making armor obsolete, and as a consequence the heavy musket as well.  Although there is relatively little to no difference in design between arquebus and musket except in size and strength, it was the term \"musket\" which remained in use up into the 1800s.  It may not be completely inaccurate to suggest that the musket was in its fabrication simply a larger arquebus.  At least on one occasion the musket and arquebus have been used interchangeably to refer to the same weapon, and even referred to as an \"arquebus musket.\"  A Habsburg commander in the mid-1560s once referred to muskets as \"double arquebuses.\"  The definition of arquebus and similar firearms is therefore quite convoluted as the term has been applied to different sorts of firearms as well as acquiring several names like \"hackbut,\" \"harquebus\", \"schiopo,\" \"sclopus,\" \"tüfenk\", \"tofak\", \"matchlock\", and \"firelock\".  Some say that the hackbut was a forerunner of the arquebus. The \"matchlock\" firing mechanism became a common term for the arquebus after it was added to the firearm.  Prior to the appearance of the matchlock, handguns were fired from the chest, tucked under one arm, while the other arm maneuvered a hot pricker to the touch hole to ignite the gunpowder.  The matchlock changed this by adding a firing mechanism consisting of two parts, the match, and the lock.  The lock mechanism held within a clamp a two to three feet long length of smoldering rope soaked in saltpeter, which was the match.  Connected to the lock lever was a trigger, which lowered the match into a priming pan when pulled, igniting the priming powder, causing a flash to travel through the touch hole, also igniting the gunpowder within the barrel, and propelling the bullet out the muzzle.  While matchlocks provided a crucial advantage by allowing the user to aim the firearm using both hands, it was also awkward to utilize.  To avoid accidentally igniting the gunpowder the match had to be detached while loading the gun.  In some instances the match would also go out, so both ends of the match were kept lit.  This proved cumbersome to maneuver as both hands were required to hold the match during removal, one end in each hand.  The procedure was so complex that a 1607 drill manual published by Jacob de Gheyn in the Netherlands listed 28 steps just to fire and load the gun.  In 1584 the Ming general Qi Jiguang composed an 11 step song to practice the procedure in rhythm: \"One, clean the gun.  Two pour the powder.  Three tamp the powder down.  Four drop the pellet.  Five drive the pellet down.  Six put in paper (stopper).  Seven drive the paper down.  Eight open the flashpan cover.  Nine pour in the flash powder.  Ten close the flashpan, and clamp the fuse.  Eleven, listen for the signal, then open the flashpan cover.  Aiming at the enemy, raise your gun and fire.\"  Reloading a gun during the 16th century took anywhere from between 20 seconds to a minute under the most ideal conditions. The arquebus is considered to be the first portable \"shoulder\" arms firearm.  Arquebuses were used as early as 1472 by the Spanish and Portuguese at Zamora.  Likewise, the Castilians used arquebuses as well in 1476.  In 1496 Philip Monch of the Palatinate composed an illustrated \"Buch der Strynt un(d) Buchsse(n)\" on guns and \"harquebuses.\"  The Mamluks in particular were conservatively against the incorporation of gunpowder weapons.  When faced with cannons and arquebuses wielded by the Ottomans they criticized them thus, \"God curse the man who invented them, and God curse the man who fires on Muslims with them.\"  Insults were also levied against the Ottomans for having \"brought with you this contrivance artfully devised by the Christians of Europe when they were incapable of meeting the Muslim armies on the battlefield.\"  Similarly, musketeers and musket-wielding infantrymen were despised in society by the feudal knights, even until the time of Cervantes (1547–1616 AD).  Eventually the Mamluks under Qaitbay were ordered in 1489 to train in the use of al-bunduq al-rasas (arquebuses).  However, in 1514 an Ottoman army of 12,000 soldiers wielding arquebuses still managed to rout a much larger Mamluk force.  The arquebus had become a common infantry weapon by the 16th century due to its relative cheapness - a helmet, breastplate and pike cost about three and a quarter ducats while an arquebus only a little over one ducat.  Another advantage of arquebuses over other equipment and weapons was its short training period.  While a bow potentially took years to master, an effective arquebusier could be trained in just two weeks.  According to a 1571 report by Vincentio d'Alessandri, Persian arms including arquebuses \"were superior and better tempered than those of any other nation.\" In the early 1500s a larger arquebus known as the musket appeared.  The heavy musket, while being rather awkward to handle, requiring a fork rest to fire properly, had the advantage of being able to penetrate the best armor within a range of 180 meters, regular armor at 365 meters, and an unarmed man at 548 meters.  However, both the musket and arquebus were effectively limited to a range of only 90 to 185 meters regardless of armor since they were incredibly inaccurate.  According to some sources, a smoothbore musket was completely incapable of hitting a man sized target past the 73 meter mark.  While rifled guns did exist at this time in the form of grooves cut into the interior of a barrel, these were considered specialist weapons and limited in number.  In some aspects this made the smoothbore musket an inferior weapon compared to the bow.  The average Mamluk archer for example was capable of hitting targets only 68 meters far away but could keep up a pace of six to eight shots per minute.  In comparison, sixteenth century matchlocks fired off one shot every several minutes, and much less when taking into consideration misfires and malfunctions which occurred up to half the time.  This is not to say that firearms of the 16th century were inferior to the bow and arrow, for it could better penetrate armor and required less training, but the disadvantages of the musket were very real, and it would not be until the 1590s that archers were for the most part phased out of \"European\" warfare.  This was possibly a consequence of the increased effectiveness of musket warfare due to the rise of volley fire in Europe as first applied by the Dutch.  At this time gunners in European armies reached as high as 40 percent of infantry forces. As the virtues of the musket became apparent it was quickly adopted throughout Eurasia so that by 1560 even in China generals were giving praise to the new weapon.  Qi Jiguang, a noted partisan of the musket, gave a eulogy on the effectiveness of the gun in 1560: Other East Asian powers such as Đại Việt also adopted the matchlock musket in quick order.  Đại Việt in particular was considered by the Ming to have produced the most advanced matchlocks in the world during the 17th century, surpassing even Ottoman, Japanese, and European firearms.  European observers of the Trịnh–Nguyễn War also corroborated with the Ming in the proficiency of matchlock making by the Vietnamese.  The Vietnamese matchlock was said to have been able to pierce several layers of iron armour, kill two to five men in one shot, yet also fire quietly for a weapon of its caliber. The volley fire, specifically the musketry volley technique, also known as the countermarch, is in its simplest form the idea of having soldiers shoot in turns.  Although volley fire is most often associated with firearms, the concept of continuous and concerted rotating fire was practiced using crossbows since at least the Tang dynasty.  Both the encyclopedic text written during the Tang dynasty known as the \"Tongdian\" and the official \"History of Song\" contain mentions of the volley fire technique and elaborate on its usage.  They contained two widely known illustrations of the volley fire formation from both the Tang and Song dynasties respectively.  In 1131, crossbowmen employed the volley fire tactic to great effect under Song generals Wu Jie (吳玠) and his younger brother Wu Lin (吳璘) during the Jin-Song Wars.  In Europe volley fire was also used by archers, for example at the Battle of Agincourt in 1415. The earliest possible employment of volley fire for firearms occurred in late 14th century China during a military conflict between Ming and Mong Mao forces.  Volley fire was also possibly implemented with firearms in 1414 during the Yongle Emperor's campaigns against the Mongols, and possibly again in another expedition in 1422.  However the language used in these sources is unclear as to whether or not \"repeating\" fire was part of the technique implemented.  For example, during the 1388 anti-insurrection war waged against the Mong Mao by the Ming general Mu Ying, the Ming troops equipped with guns and fire arrows were arrayed in three lines.  The general Mu Ying explained this was so that \"when the elephants advance, the front line of guns and arrows will shoot all at once.  If they do not retreat, the next line will continue this.  If they still do not retreat, then the third line will continue this.\"  When the armored war elephants broke into a run, charging the Ming lines, the Ming forces stood their ground, \"shooting arrows and stones, the noise shaking the mountains and vallies.  The elephants shook with fear and ran.\"  According to the \"Ming Shilu\", half the elephants were killed while 37 were captured, and of the 100,000 strong insurrection force, at least 30,000 were killed, and 10,000 were captured.  Andrade and other historians have interpreted this passage as evidence of volley fire, however he admits that it is ambiguous as to whether or not the Ming lines practiced repeated fire and reloading, so at best it can only be considered a limited form of volley fire. The \"Ming Shilu\" goes on to mention another possible instance of volley fire, this time during the Yongle Emperor's campaigns against the Mongols.  In 1414 \"the commander-in-chief (都督) Zhu Chong led Lü Guang and others directly to the fore, where they assaulted the enemy by firing firearms and guns continuously and in succession.  Countless enemies were killed.\"  In this case the source makes no mention of taking turns or forming lines, but Andrade believes that since the Ming were facing horseback Mongol forces, it would have been impossible to keep continuous fire in the face of a cavalry charge had ordered ranks of gunners not been implemented.  The same rationality is applied to another passage on the 1422 expedition, where \"the emperor ordered that all the generals train their troops outside each encampment by arranging their formations so that the gunnery units (神機銃) occupied the foremost positions and the cavalry units occupied the rear.  He ordered officers to exercise and drill in their free time (暇閑操習).  He admonished them as follows: \"A formation that is dense is solid, while an advance force is sparse, and when they arrive at the gates of war and it's time to fight, then first use the guns to destroy their advance guard and then use cavalry to rush their solidity.  In this way there is nothing to fear.\"\"  Some historians have extrapolated from this that the Ming forces were using volley fire with firearms since their opponents were cavalry units, and hence impossible to stop with slow firing hand cannons unless it was through continuous volley fire, much less with a thin advance guard of gunnery units.  According to Wang Zhaochun, \"the meaning of this is that when fighting, the gun troops line up in front of the entire formation, and between them there must be a certain amount of space, so that they can load bullets and powder and employ shooting by turns and in concert to destroy the enemy advance guard.  Once the enemy has been thrown into chaos, the rear densely arrayed cavalry troops together come forth in great vigor, striking forth with irresistible force.\"  Even if Wang is correct, the evidence is still inconclusive. The volley tactic next appeared in early 16th century Europe when the Ottoman Janissaries clashed with European forces at the Battle of Mohács on 29 August 1526.  The Janissaries equipped with 2000 \"tüfenks\" (usually translated as musket) \"formed nine consecutive rows and they fired their weapons row by row,\" in a \"kneeling or standing position without the need for additional support or rest.\"  Contrary to the popular belief that the Ottomans' success at Mohács was due to their artillery, a view which many later historians have supported, contemporary European and Ottoman sources on the battle attribute their success to the Janissaries' successful deployment of firearms.  However the Janissaries' prowess declined early in the 17th century as troop standards dropped and the drill was abandoned.  According to the author of \"The Laws of the Janissaries (Kavanin-i Yenigeriyan)\", by 1606 members of the Janissaries were faced with supply issues so that they \"were no longer given powder for the drills and that the soldiers used the wick for their candles and not for their muskets.\" By 1548 the Ming had started fielding arquebuses after procuring knowledge of the weapon from the pirate network at Shuangyu.  The military leader Qi Jiguang, who was at first ambivalent towards matchlocks, became one of the primary advocates for their incorporation into the Ming army later on in his life.  After having suffered his first defeats at the hands of the wokou, he realized the vital role of this new weapon in combating piracy, for it out ranged their heaviest arrows.  By 1560 he had invented a style of musket warfare similar to the Tang crossbow volley technique which he described the same year in his magnum opus, the \"Jixiao Xinshu\": Qi Jiguang further elaborates on the five layered musket volley formation: If melee weapons could not be brought into combat, such as during long range defense, Qi recommended waiting \"until the face-the-enemy signal [is given], and then, whether from behind wooden stockades, or from moat banks, or from below abatis (拒馬), [they] open up on the enemy, firing by turns (更番射賊).  Those who are empty reload; those who are full fire again.  While the ones who have fired are loading, those who are full then fire again.  In this way, all day long, the firing of guns will not be lacking, and there must be no firing to the point of exhaustion [of ammo] and no slipups with guns.\"  In 1571 Qi prescribed an ideal infantry regiment of 1080 arquebusiers out of 2700 men, or 40 percent of the infantry force.  However it is not known how well this was actually implemented, and there is evidence that Qi was met with stiff resistance to the incorporation of newer gunpowder weapons in northern China while he was stationed there.  He writes that \"in the north soldiers are stupid and impatient, to the point that they cannot see the strength of the musket, and they insist on holding tight to their fast lances (a type of fire lance), and although when comparing and vying on the practice field the musket can hit the bullseye ten times better than the fast-lance and five times better than the bow and arrow, they refuse to be convinced.\" The musket volley fire technique may have been used in Japan as well as early as 1575 at the Battle of Nagashino by Oda Nobunaga's arquebusiers.  But this has been called into dispute in recent years by J.S.A. Elisonas and J.P. Lamers in their translation of \"The Chronicle of Oda Nobunaga\" by Ota Gyuichi.  In Lamers' \"Japonius\" he says that \"whether or not Nobunaga actually operated with three rotating ranks cannot be determined on the basis of reliable evidence.\"  They claim that the version of events describing volley fire was written several years after the battle, and an earlier account says to the contrary that guns were fired en masse.  However both Korean and Chinese sources note that Japanese gunners were making use of volley fire during the Japanese invasions of Korea from 1592–98. Frederick Lewis Taylor claims that a kneeling volley fire may have been employed by Prospero Colonna's arquebusiers as early as the Battle of Bicocca (1522).  However this has been called into question by Tonio Andrade who believes this is an over interpretation as well as mis-citation of a passage by Charles Oman suggesting that the Spanish arquebusiers kneeled to reload, when in fact Oman never made such a claim.  European gunners might have implemented the volley fire to some extent since at least 1579 when the Englishman Thomas Digges suggested that musketeers should, \"after the old Romane manner make three or four several fronts, with convenient spaces for the first to retire and unite himselfe with the second, and both these if occasion so require, with the third; the shot [musketeers] having their convenient lanes continually during the fight to discharge their peces.\"  The Spanish too displayed some awareness of the volley technique and described it in a military manual dating to 1586: \"Start with three files of five soldiers each, separated one from the other by fifteen paces, and they should comport themselves not with fury but with calm skillfulness [con reposo diestramente] such that when the first file has finished shooting they make space for the next (which is coming up to shoot) without turning face, countermarching [contrapassando] to the left but showing the enemy only the side of their bodies, which is the narrowest of the body, and [taking their place at the rear] about one to three steps behind, with five or six pellets in their mouths, and two lighted matchlock fuses … and they load [their pieces] promptly … and return to shoot when it's their turn again.\" Regardless, it is clear that the concept of volley fire had existed in Europe for quite some time during the 16th century, but it was in the Netherlands during the 1590s that the musketry volley really took off.  The key to this development was William Louis, Count of Nassau-Dillenburg who in 1594 described the technique in a letter to his cousin: For many Europeans this new way of conducting warfare seemed ridiculous, so that in the beginning they were openly mocked.  But the Dutch army continued to drill the volley under both Louis and his cousin Maurice, Prince of Orange, so that it became second nature.  One Dutch historian recounts the exercises in which regiments marched \"man by man bringing the rearmost to the front and the frontmost to the rear. … The beginnings were very difficult, and many people felt, because it was all so unusual, that it was odd and ridiculous [lacherlich].  They were mocked by the enemy, but with time the great advantages of the practices became clear … and eventually they were copied by other nations.\"  Soon the reorganized Dutch army displayed the virtues of the countermarch volley and the practice spread across Europe.  An important component to the successful deployment of volley fire was the drill, which according to Geoffrey Parker, \"only two civilisation have invented drill for their infantry: China and Europe.  Moreover, both of them did so twice: in the fifth century BC in North China and in Greece, and again in the late sixteenth century.  Exponents of the second phase— Qi Jiguang in Imperial China and Maurice of Nassau in the Dutch Republic—explicitly sought to revive classical precedents, and in the West, marching in step and standing on parade became a permanent part of military life.\"  Drill was difficult and the manner in which the volley fire should be executed had not been perfected in Louis' time.  It is clear from Holland's historical sources that it took many trials and experiments for the process to be refined. Indeed, just using the musket itself was considered unorthodox and in some parts of the world there was even a push back against the incorporation of muskets.  According to Qi Jiguang, this was because: In Korea the Joseon dynasty underwent a devastating war with newly unified Japan that lasted from 1592 to 1598.  The shock of this encounter spurred the court to undergo a process of military strengthening.  One of the core elements of military strengthening was to adopt the musket.  According to reformers, \"In recent times in China they did not have muskets; they first learned about them from the Wokou pirates in Zhejiang Province.  Qi Jiguang trained troops in their use for several years until they [muskets] became one of the skills of the Chinese, who subsequently used them to defeat the Japanese.\"  By 1607 Korean musketeers had been trained in the fashion which Qi Jiguang prescribed, and a drill manual had been produced based on the Chinese leader's \"Jixiao Xinshu\".  Of the volley fire, the manual says that \"every musketeer squad should either divide into two musketeers per layer or one and deliver fire in five volleys or in ten.\"  Another Korean manual produced in 1649 describes a similar process: \"When the enemy approaches to within a hundred paces, a signal gun is fired and a conch is blown, at which the soldiers stand.  Then a gong is sounded, the conch stops blowing, and the heavenly swan [a double-reed horn] is sounded, at which the musketeers fire in concert, either all at once or in five volleys (齊放一次盡擧或分五擧).\"  This training method proved to be quite formidable in the 1619 Battle of Sarhu when 10,000 Korean musketeers managed to kill many Manchus before their allies surrendered.  While Korea went on to lose both wars against the Manchu invasions of 1627 and 1636, their musketeers were well respected by Manchu leaders.  It was the first Qing emperor Hong Taiji who wrote: \"The Koreans are incapable on horseback but do not transgress the principles of the military arts.  They excel at infantry fighting, especially in musketeer tactics.\" The Gunpowder Empires generally refer to the Islamic Ottoman, Safavid and Mughal empires.  The phrase was first coined by Marshall Hodgson in the title of Book 5 (\"The Second Flowering: The Empires of Gunpowder Times\") of his highly influential three-volume work, \"The Venture of Islam\" (1974).  Hogdson applied the term \"gunpowder empire\" to three Islamic political entities he identified as separate from the unstable, geographically limited confederations of Turkic clans that prevailed in post-Mongol times.  He called them \"military patronage states of the Later Middle Period,\" which possessed three defining characteristics: first, a legitimization of independent dynastic law; second, the conception of the whole state as a single military force; third, the attempt to explain all economic and high cultural resources as appanages of the chief military families.  Connecting these empires were their traditions which grew \"out of Mongol notions of greatness,\" but \"[s]uch notions could fully mature and create stable bureaucratic empires only after gunpowder weapons and their specialized technology attained a primary place in military life.\" William H. McNeill further expanded on the concept of gunpowder empires by arguing that such states \"were able to monopolize the new artillery, central authorities were able to unite larger territories into new, or newly consolidated, empires.\"  In the McNeill political framework, gunpowder empires were notable for their monopolization of firearms, whereas although Europe pioneered the development of the new artillery in the fifteenth century, no state monopolized it.  Gun-casting know-how had been concentrated in the Low Countries near the mouths of the Scheldt and Rhine rivers, which France and the Habsburgs divided up in territory among themselves, resulting in an arms standoff.  By contrast, such monopolies allowed states to create militarized empires in the Near East, Russia and India, and \"in a considerably modified fashion\" in China and Japan. In 2011 Douglas E. Streusand criticized the Hodgson-McNeill Gunpowder-Empire hypothesis, calling it into disfavor as a neither \"adequate [n]or accurate\" explanation, although the term remains in use.  The main problem with the Hodgson-McNeill theory is that the acquisition of firearms does not seem to have preceded the initial acquisition of territory constituting the imperial critical mass of any of the three early modern Islamic empires, except in the case of the Mughals.  Moreover, it seems that the commitment to military autocratic rule pre-dated the acquisition of gunpowder weapons in all three cases.  Reasons other than (or in addition to) military technology have been offered for the nearly simultaneous rise of three centralized military empires in contiguous areas dominated by decentralized Turkic tribes.  One explanation, called \"Confessionalization\" by historians of fifteenth century Europe, invokes examination of how the relation of church and state \"mediated through confessional statements and church ordinances\" lead to the origins of absolutist polities.  Douglas Streusand uses the Safavids as an example: The Safavids from the beginning imposed a new religious identity on their general population; they did not seek to develop a national or linguistic identity, but their policy had that effect.  Nor does it seem to be the case that the acquisition of gunpowder weapons and their integration into the military was influenced by considerations of whichever variety of Islam the particular empire promoted.  Whether or not gunpowder was inherently linked to the existence of any of these three empires, it cannot be questioned that each of the three acquired artillery and firearms early in their history and made such weapons an integral part of their military tactics. In India, guns made of bronze were recovered from Calicut (1504) and Diu (1533).  By the 17th century, Indians were manufacturing a diverse variety of firearms; large guns in particular, became visible in Tanjore, Dacca, Bijapur and Murshidabad.  Gujarāt supplied Europe saltpeter for use in gunpowder warfare during the 17th century.  Bengal and Mālwa participated in saltpeter production.  The Dutch, French, Portuguese, and English used Chāpra as a center of saltpeter refining. Fathullah Shirazi (c. 1582), who worked for Akbar the Great as a mechanical engineer, developed an early multi gun shot.  As opposed to the polybolos and repeating crossbows used earlier in ancient Greece and China, respectively, Shirazi's rapid-firing gun had multiple gun barrels that fired hand cannons loaded with gunpowder. In the \"Encyclopædia Britannica (2008)\", Stephen Oliver Fought & John F. Guilmartin, Jr. describe the gunpowder technology in 18th century India, with reference to the rocket artillery used by the Kingdom of Mysore (as described in the \"Fathul Mujahidin\") and its influence on the Congreve rocket: Hyder Ali, prince of Mysore, developed war bombs with an important change: the use of metal pipes to contain the combustion powder.  Although the hammered soft iron he used was crude, the bursting strength of the container of black powder was much higher than the earlier paper construction.  Thus a greater internal pressure was possible, with a resultant greater thrust of the propulsive jet.  The rocket body was lashed with leather thongs to a long bamboo stick.  Range was perhaps up to three-quarters of a mile (more than a kilometer).  Although individually these rockets were not accurate, dispersion error became less important when large numbers were fired rapidly in mass attacks.  They were particularly effective against cavalry and were hurled into the air, after lighting, or skimmed along the hard dry ground.  Hyder Ali's son, Tippu Sultan, continued to develop and expand the use of rocket weapons, reportedly increasing the number of rocket troops from 1,200 to a corps of 5,000.  In battles at Seringapatam in 1792 and 1799 these rockets were used with considerable effect against the British. The news of the successful use of rockets spread through Europe.  In England Sir William Congreve began to experiment privately.  First, he experimented with a number of black-powder formulas and set down standard specifications of composition.  He also standardized construction details and used improved production techniques.  Also, his designs made it possible to choose either an explosive (ball charge) or incendiary warhead. In \"A History of Greek Fire and Gunpowder\", James Riddick Partington describes Indian rockets, mines and other means of gunpowder warfare: The Indian war rockets were formidable weapons before such rockets were used in Europe.  They had bam-boo rods, a rocket-body lashed to the rod, and iron points.  They were directed at the target and fired by lighting the fuse, but the trajectory was rather erratic.  The use of mines and counter-mines with explosive charges of gunpowder is mentioned for the times of Akbar and Jahāngir. Gunpowder was used for hydraulic engineering in China by 1541.  Gunpowder blasting followed by dredging of the detritus was a technique which Chen Mu employed to improve the Grand Canal at the waterway where it crossed the Yellow River.  In Europe gunpowder was utilized in the construction of the Canal du Midi in Southern France.  It was completed in 1681 and linked the Mediterranean sea with the Atlantic with 240 km of canal and 100 locks.  Another noteworthy consumer of black powder was the Erie canal in New York, which was 585 km long and took eight years to complete, starting in 1817. Before gunpowder was applied to civil engineering, there were two ways to break up large rocks, by hard labor or by heating with large fires followed by rapid quenching.  The earliest record for the use of gunpowder in mines comes from Hungary in 1627.  It was introduced to Britain in 1638 by German miners, after which records are numerous.  Until the invention of the safety fuse by William Bickford in 1831, the practice was extremely dangerous.  Another reason for danger were the dense fumes given off and the risk of igniting flammable gas when used in coal mines. Black powder was also extensively used in railway construction.  At first railways followed the contours of the land, or crossed low ground by means of bridges and viaducts, but later railways made extensive use of cuttings and tunnels.  One 2400-ft stretch of the 5.4 mi Box Tunnel on the Great Western Railway line between London and Bristol consumed a ton of gunpowder per week for over two years.  The 12.9 km long Mont Cenis Tunnel was completed in 13 years starting in 1857 but, even with black powder, progress was only 25 cm a day until the invention of pneumatic drills sped up the work. The latter half of the 19th century saw the invention of nitroglycerin, nitrocellulose and smokeless powders which soon replaced black powder in many applications. Shot and gunpowder for military purposes were made by skilled military tradesmen, who were later called \"firemakers\", who were also required to make fireworks for celebrations of victory or peace.  During the Renaissance two European schools of pyrotechnic thought emerged, one in Italy and the other at Nürnberg, Germany.  The Italian school of pyrotechnics emphasized elaborate fireworks, the German school stressed scientific advancement.  Both schools added significantly to the further development of pyrotechnics and, by the mid-17th century, fireworks were used for entertainment on an unprecedented scale in Europe—being popular even at resorts and public gardens.  At the same time some military men were disguising gray in their beards by dusting them with gunpowder, the antiquary John Aubrey noted in his memoranda. Gunpowder as a gun propellant suffered from several weaknesses inherent to its composition.  It was subject to passivation by moisture and was awkward to load and ignite in a weapon.  Sulfur oxides and moisture produced corrosion on metal gun components and the smoke gave away the position of the shooter.  The development of cartridges and explosive primers were a major improvement in the use of gunpowders in the field. From the earliest period, gunpowder has been composed of a nitrate salt, sulfur, and carbonaceous matter.  The Nitrate component is the Oxidizing agent, Sulfur is a low melting Reducing agent and serves to aid in the transfer of heat through the gunpowder mass, and Carbon is a reducing component producing hot, high pressure gas.  The utility of gunpowder lies in its ability to accelerate a projectile by the explosive expansion of gas.  The development of new explosive materials in the 19th century naturally led to improvements in the rate and magnitude of pressure rise as well as new means for ignition of the charge. Nitroester compositions nitrocellulose and nitroglycerin were developed by Henri Braconnot in 1832 and Ascanio Sobrero in 1846, respectively.  The development of smokeless powder stemmed from efforts by numerous workers to produce an improved gun propellant providing better resistance to moisture, greater muzzle velocity, and generally greater reliability.  By the end of the 19th century, nitroester compositions Poudre B, Ballistite, and Cordite were the major smokeless propellants.  Significantly, many nitroesters were capable of detonation without confinement.  The classic gunpowder composition was only capable of Deflagration in the open. The process of aromatic nitration to afford picric acid from the trinitration of phenol was performed in the early 19th century, though it is possible that Johann Rudolf Glauber may have reported it much earlier.  Nitroesters would find use in gun propellant formulation.  Nitroaromatics like picric acid eventually found use in explosive cannon shells owing to their ability to withstand the severe shock of firing. During the American Revolutionary War, a number of caves were mined for saltpeter to make gunpowder when supplies from Europe were embargoed.  Abigail Adams, reputedly also made gunpowder at her family farm in Massachusetts. The New York Committee of Safety (American Revolution) produced some essays on making gunpowder that were printed in 1776. During the American Civil War, British India was the main source for saltpeter for the manufacture of gunpowder for the Union armies.  This supply was threatened by the British government during the Trent Affair, when Union naval forces stopped a British ship, the RMS Trent, and removed two Confederate diplomats.  The British government responded in part by halting all exports of saltpeter to the United States, threatening their gunpowder manufacturing resources.  Shortly thereafter, the situation was resolved and the Confederate diplomats were released. The Union Navy blockaded the southern Confederate States, which reduced the amount of gunpowder that could be imported from overseas.  The Confederate Nitre and Mining Bureau was formed to produce gunpowder for the army and the navy from domestic resources.  Nitre is the English spelling of \"Niter\".  While carbon and sulfur were readily available throughout the south, potassium nitrate was often produced from the Calcium nitrate found in cave dirt, tobacco barn floors and barn stalls other places.  A number of caves were mined, and the men and boys who worked in the caves were called \"peter monkey\", somewhat in imitation of the naval term \"powder monkey\" that was used for the boys who brought up charges of gunpowder on gunboats. </div>\n\nCriticism of Facebook Criticism of Facebook relates to how Facebook's market dominance have led to international media coverage and significant reporting of its shortcomings.  Notable issues include Internet privacy, such as its use of a widespread \"like\" button on third-party websites tracking users, possible indefinite records of user information, automatic facial recognition software, and its role in the workplace, including employer-employee account disclosure. The use of Facebook can have psychological effects, including feelings of jealousy and stress, a lack of attention, and social media addiction, in some cases comparable to drug addiction. Facebook's company tactics have also received prominent coverage, including electricity usage, tax avoidance, real-name user requirement policies, censorship, and its involvement in the United States PRISM surveillance program. Due to allowing users to publish material by themselves, Facebook has come under scrutiny for the amount of freedom it gives users, including copyright and intellectual property infringement, hate speech, incitement of rape and terrorism, fake news, Facebook murder, crimes and violent incidents live-streamed through its Facebook Live functionality. Facebook has been banned by several governments, including Syria, China, and Iran. The company has also been subject to multiple litigation cases over the years, with its most prominent case concerning allegations that CEO Mark Zuckerberg broke an oral contract with Cameron Winklevoss, Tyler Winklevoss, and Divya Narendra to build the then-named \"HarvardConnection\" social network in 2004, instead allegedly opting to steal the idea and code to launch Facebook months before HarvardConnection began.  The original lawsuit was eventually settled in 2009, with Facebook paying approximately $20 million in cash and 1.25 million shares.  A new lawsuit in 2011 was dismissed. In 2015, it was reported that a growing number of Facebook users are being wrongfully and inexplicably being suspended from their accounts by Facebook to give up copies of their private identification information, such as copies of their driver's license, state-issued ID cards, passports, military cards, etc, with users being permanently locked out of their accounts if this information isn't given up.  This has created great displeasure for users who practice discretion with such information.  Facebook does not require the release of such information when individuals sign up for the site.  Although facebook is defending it as account security, because such information can seriously harm individuals, this method has been widely described by users as a presumptuous, dictatorial move and an offensive invasion of privacy by Facebook.  Other popular websites have only asked for verification of identities through an e-mail confirmation link, or in some cases, a cellular phone text message confirmation. In 2010, the Electronic Frontier Foundation identified two personal information aggregation techniques called \"connections\" and \"instant personalization\".  They demonstrated that anyone could get access to information saved to a Facebook profile, even if the information was not intended to be made public.  A \"connection\" is created when a user clicks a \"Like\" button for a product or service, either on Facebook itself or an external site.  Facebook treats such relationships as public information, and the user's identity may be displayed on the Facebook page of the product or service. Instant Personalization was a pilot program which shared Facebook account information with affiliated sites, such as sharing a user's list of \"liked\" bands with a music website, so that when the user visits the site, their preferred music plays automatically.  The EFF noted that \"For users that have not opted out, Instant Personalization is instant data leakage.  As soon as you visit the sites in the pilot program (Yelp, Pandora, and Microsoft Docs) the sites can access your name, your picture, your gender, your current location, your list of friends, all the Pages you have Liked—everything Facebook classifies as public information.  Even if you opt out of Instant Personalization, there's still data leakage if your friends use Instant Personalization websites—their activities can give away information about you, unless you block those applications individually.\" On December 27, 2012, CBS News reported that Randi Zuckerberg, sister of Facebook founder Mark Zuckerberg, criticized a friend for being \"way uncool\" in sharing a private Facebook photo of her on Twitter, only to be told that the image had appeared on a friend-of-a-friend's Facebook news feed.  Commenting on this misunderstanding of Facebook's privacy settings, Eva Galperin of the EFF said \"Even Randi Zuckerberg can get it wrong.  That's an illustration of how confusing they can be.\" In August 2007, the code used to generate Facebook's home and search page as visitors browse the site was accidentally made public.  A configuration problem on a Facebook server caused the PHP code to be displayed instead of the web page the code should have created, raising concerns about how secure private data on the site was.  A visitor to the site copied, published and later removed the code from his web forum, claiming he had been served and threatened with legal notice by Facebook.  Facebook's response was quoted by the site that broke the story: In November, Facebook launched Beacon, a system (discontinued in September 2009) where third-party websites could include a script by Facebook on their sites, and use it to send information about the actions of Facebook users on their site to Facebook, prompting serious privacy concerns.  Information such as purchases made and games played were published in the user's news feed.  An informative notice about this action appeared on the third party site and gave the user the opportunity to cancel it, and the user could also cancel it on Facebook.  Originally if no action was taken, the information was automatically published.  On November 29 this was changed to require confirmation from the user before publishing each story gathered by Beacon. On December 1, Facebook's credibility in regard to the Beacon program was further tested when it was reported that the \"New York Times\" \"essentially accuses\" Mark Zuckerberg of lying to the paper and leaving Coca-Cola, which is reversing course on the program, a similar impression.  A security engineer at CA, Inc. also claimed in a November 29, 2007 blog post that Facebook collected data from affiliate sites even when the consumer opted out and even when not logged into the Facebook site.  On November 30, 2007, the CA security blog posted a Facebook clarification statement addressing the use of data collected in the Beacon program: The Beacon service ended in September 2009 along with the settlement of a class-action lawsuit against Facebook resulting from the service. On September 5, 2006, Facebook introduced two new features called \"News Feed\" and \"Mini-Feed\".  The first of the new features, News Feed, appears on every Facebook member's home page, displaying recent Facebook activities of the member's friends.  The second feature, Mini-Feed, keeps a log of similar events on each member's profile page.  Members can manually delete items from their Mini-Feeds if they wish to do so, and through privacy settings can control what is actually published in their respective Mini-Feeds. Some Facebook members still feel that the ability to opt out of the entire News Feed and Mini-Feed system is necessary, as evidenced by a statement from the \"Students Against Facebook News Feed\" group, which peaked at over 740,000 members in 2006.  Reacting to users' concerns, Facebook developed new privacy features to give users some control over information about them that was broadcast by the News Feed.  According to subsequent news articles, members have widely regarded the additional privacy options as an acceptable compromise. In May 2010, Facebook added privacy controls and streamlined its privacy settings, giving users more ways to manage status updates and other information that is broadcast to the public News Feed.  Among the new privacy settings is the ability to control who sees each new status update a user posts: Everyone, Friends of Friends, or Friends Only.  Users can now hide each status update from specific people as well.  However, a user who presses \"like\" or comments on the photo or status update of a friend cannot prevent that action from appearing in the news feeds of all the user's friends, even non-mutual ones.  The \"View As\" option, used to show a user how privacy controls filter out what a specific given friend can see, only displays the user's timeline and gives no indication that items missing from the timeline may still be showing up in the friend's own news feed. Government and local authorities rely on Facebook and other social networks to investigate crimes and obtain evidence to help establish a crime, provide location information, establish motives, prove and disprove alibis, and reveal communications.  Federal, state, and local investigations have not been restricted to profiles that are publicly available or willingly provided to the government; Facebook has willingly provided information in response to government subpoenas or requests, except with regard to private, unopened inbox messages less than 181 days old, which require a warrant and a finding of probable cause under federal law.  An article by Junichi Semitsu published in the \"Pace Law Review\"' reports that \"even when the government lacks reasonable suspicion of criminal activity and the user opts for the strictest privacy controls, Facebook users still cannot expect federal law to stop their 'private' content and communications from being used against them. \"  Facebook's privacy policy states that \"We may also share information when we have a good faith belief it is necessary to prevent fraud or other illegal activity, to prevent imminent bodily harm, or to protect ourselves and you from people violating our Statement of Rights and Responsibilities.  This may include sharing information with other companies, lawyers, courts or other government entities.\"  Since Congress has failed to meaningfully amend the Electronic Communications Privacy Act to protect most communications on social-networking sites such as Facebook, and since the Supreme Court has largely refused to recognize a Fourth Amendment privacy right to information shared with a third party, there is no federal statutory or constitutional right that prevents the government from issuing requests that amount to fishing expeditions and there is no Facebook privacy policy that forbids the company from handing over private user information that suggests any illegal activity. The 2013 mass surveillance disclosures identified Facebook as a participant in the U.S. National Security Administration's PRISM program.  Facebook now reports the number of requests it receives for user information from governments around the world. On May 31, 2008 the Canadian Internet Policy and Public Interest Clinic (CIPPIC), per Director Phillipa Lawson, filed a 35-page complaint with the Office of the Privacy Commissioner against Facebook based on 22 breaches of the Canadian Personal Information Protection and Electronic Documents Act (PIPEDA).  University of Ottawa law students Lisa Feinberg, Harley Finkelstein, and Jordan Eric Plener, initiated the \"minefield of privacy invasion\" suit.  Facebook's Chris Kelly contradicted the claims, saying that: \"We've reviewed the complaint and found it has serious factual errors—most notably its neglect of the fact that almost all Facebook data is willingly shared by users.\"  Assistant Privacy Commissioner Elizabeth Denham released a report of her findings on July 16, 2009.  In it, she found that several of CIPPIC's complaints were well-founded.  Facebook agreed to comply with some, but not all, of her recommendations.  The Assistant Commissioner found that Facebook did not do enough to ensure users granted meaningful consent for the disclosure of personal information to third parties and did not place adequate safeguards to ensure unauthorized access by third party developers to personal information. There have been some concerns expressed regarding the use of Facebook as a means of surveillance and data mining. Two Massachusetts Institute of Technology (MIT) students were able to use an automated script to download the publicly posted information of over 70,000 Facebook profiles from four schools (MIT, NYU, the University of Oklahoma, and Harvard University) as part of a research project on Facebook privacy published on December 14, 2005.  Since then, Facebook has bolstered security protection for users, responding: \"We've built numerous defenses to combat phishing and malware, including complex automated systems that work behind the scenes to detect and flag Facebook accounts that are likely to be compromised (based on anomalous activity like lots of messages sent in a short period of time, or messages with links that are known to be bad).\" A second clause that brought criticism from some users allowed Facebook the right to sell users' data to private companies, stating \"We may share your information with third parties, including responsible companies with which we have a relationship.\"  This concern was addressed by spokesman Chris Hughes, who said \"Simply put, we have never provided our users' information to third party companies, nor do we intend to.\"  Facebook eventually removed this clause from its privacy policy. In the United Kingdom, the Trades Union Congress (TUC) has encouraged employers to allow their staff to access Facebook and other social-networking sites from work, provided they proceed with caution. In September 2007, Facebook drew criticism after it began allowing search engines to index profile pages, though Facebook's privacy settings allow users to turn this off. Concerns were also raised on the BBC's \"Watchdog\" program in October 2007 when Facebook was shown to be an easy way in which to collect an individual's personal information in order to facilitate identity theft.  However, there is barely any personal information presented to non-friends - if users leave the privacy controls on their default settings, the only personal information visible to a non-friend is the user's name, gender, profile picture, networks, and user name. A \"New York Times\" article in February 2008 pointed out that Facebook does not actually provide a mechanism for users to close their accounts, and raised the concern that private user data would remain indefinitely on Facebook's servers.  , Facebook gives users the options to deactivate or delete their accounts. Deactivating an account allows it to be restored later, while deleting it will remove the account \"permanently\", although some data submitted by that account (\"like posting to a group or sending someone a message\") will remain. Facebook had allowed users to deactivate their accounts but not actually remove account content from its servers.  A Facebook representative explained to a student from the University of British Columbia that users had to clear their own accounts by manually deleting all of the content including wall posts, friends, and groups.  A New York Times article noted the issue, and raised a concern that emails and other private user data remain indefinitely on Facebook's servers.  Facebook subsequently began allowing users to permanently delete their accounts in 2010.  Facebook's Privacy Policy now states: \"When you delete an account, it is permanently deleted from Facebook.\" A notable ancillary effect of social-networking websites, is the ability for participants to mourn publicly for a deceased individual.  On Facebook, friends often leave messages of sadness, grief, or hope on the individual's page, transforming it into a sort of public book of condolences.  This particular phenomenon has been documented at a number of schools.  Facebook originally held a policy that profiles of people known to be deceased would be removed after 30 days due to privacy concerns.  Due to user response, Facebook changed its policy to place deceased members' profiles in a \"memorialization state\".  Facebook's Privacy Policy regarding memorialization says, \"If we are notified that a user is deceased, we may memorialize the user's account.  In such cases we restrict profile access to confirmed friends, and allow friends and family to write on the user's Wall in remembrance.  We may close an account if we receive a formal request from the user's next of kin or other proper legal request to do so.\" Some of these memorial groups have also caused legal issues.  Notably, on January 1, 2008, one such memorial group posted the identity of murdered Toronto teenager Stefanie Rengel, whose family had not yet given the Toronto Police Service their consent to release her name to the media, and the identities of her accused killers, in defiance of Canada's Youth Criminal Justice Act which prohibits publishing the names of the under-age accused.  While police and Facebook staff attempted to comply with the privacy regulations by deleting such posts, they noted difficulty in effectively policing the individual users who repeatedly republished the deleted information. In July 2007, Adrienne Felt, an undergraduate student at the University of Virginia, discovered a cross-site scripting (XSS) hole in the Facebook Platform that could inject JavaScript into profiles.  She used the hole to import custom CSS and demonstrate how the platform could be used to violate privacy rules or create a worm. \"Quit Facebook Day\" was an online event which took place on May 31, 2010 (coinciding with Memorial Day), in which Facebook users stated that they would quit the social network, due to privacy concerns. It was estimated that 2% of Facebook users coming from the United States would delete their accounts. However, only 33,000 (roughly 0.0066% of its roughly 500 million members at the time) users quit the site.  The number one reason for users to quit Facebook was privacy concerns (48%), being followed by a general dissatisfaction with Facebook (14%), negative aspects regarding Facebook friends (13%) and the feeling of getting addicted to Facebook (6%).  Facebook quitters were found to be more concerned about privacy, more addicted to the Internet and more conscientious. Facebook enabled an automatic facial recognition feature in June 2011, called \"Tag Suggestions\", a product of a research project named \"DeepFace\".  The feature compares newly uploaded photographs to those of the uploader's Facebook friends, in order to suggest photo tags. \"National Journal Daily\" claims \"Facebook is facing new scrutiny over its decision to automatically turn on a new facial recognition feature aimed at helping users identify their friends in photos\".  Facebook has defended the feature, saying users can disable it.  Facebook introduced the feature in an opt-out basis.  European Union data-protection regulators said they would investigate the feature to see if it violated privacy rules. Naomi Lachance stated in web blog for NPR:\"All Tech Considered\" that Facebook's facial recognition is 98% of the time compared to the FBI's 85% out of 50 people.  It's also noted, however, that the accuracies of Facebook searches being from a larger, more diverse photo selection compared to the FBI's closed database. Mark Zuckerberg showed no worries when speaking about Facebook's AIs saying, \"Unsupervised learning is a long term focus of our AI research team at Facebook, and it remains an important challenge for the whole AI research community\" and \"It will saves lives by diagnosing diseases and driving us around more safely.  It will enable breakthroughs by helping us find new planets and understand Earth's climate.  It will help in areas we haven't even thought of today\". In August 2011, the Irish Data Protection Commissioner (DPC) started an investigation after receiving 22 complaints by europe-v-facebook. org, which was founded by a group of Austrian students.  The DPC stated in first reactions that the Irish DPC is legally responsible for privacy on Facebook for all users within the European Union and that he will \"investigate the complaints using his full legal powers if necessary\".  The complaints were filed in Ireland because all users who are not residents of the United States or Canada have a contract with \"Facebook Ireland Ltd\", located in Dublin, Ireland.  Under European law Facebook Ireland is the \"data controller\" for facebook.com, and therefore, facebook.com is governed by European data protection laws.  Facebook Ireland Ltd. was established by Facebook Inc. to avoid US taxes (see Double Irish arrangement). The group 'europe-v-facebook. org' made access requests at Facebook Ireland and received up to 1,222 pages of data per person in 57 data categories that Facebook was holding about them, including data that was previously removed by the users.  Despite the amount of information given, the group claimed that Facebook did not give them all of its data.  Some of the information not included was \"likes\", data about the new face recognition function, data about third party websites that use \"social plugins\" visited by users and information about uploaded videos.  Currently the group claims that Facebook holds at least 84 data categories about every user. The first 16 complaints target different problems, from undeleted old \"pokes\" all the way to the question if sharing and new functions on Facebook should be opt-in or opt-out.  The second wave of 6 more complaints was targeting more issues including one against the \"Like\" button.  The most severe could be a complaint that claims that the privacy policy, and the consent to the privacy policy is void under European laws. In an interview with the Irish Independent a spokesperson said that the DPC will \"go and audit Facebook, go into the premises and go through in great detail every aspect of security\".  He continued by saying: \"It's a very significant, detailed and intense undertaking that will stretch over four or five days.\"  In December 2011 the DPC has published a first report on Facebook.  This report was not legally binding but suggested changes that Facebook should undertake until July 2012.  The DPC is planning to do a review about Facebook's progress in July 2012. In spring 2012, Facebook had to undertake many changes, e.g. having an extended download tool that should allow users to exercise the European right to access to all stored information or an update of the worldwide privacy policy.  These changes were seen as not sufficient to comply with European law by europe-v-facebook. org.  The download tool does not allow, for example, access to all data.  The group has launched our-policy. org to suggest improvements to the new policy, which they saw as a backdrop for privacy on Facebook.  Since the group managed to get more than 7.000 comments on Facebook's pages, Facebook had to do a worldwide vote on the proposed changes.  Such a vote would have only been binding if 30% of all users would have taken part.  Facebook did not promote the vote, resulting in only 0.038% participation with about 87% voting against Facebook's new policy.  The new privacy policy took effect on the same day. An article published by \"USA Today\" in November 2011 claimed that Facebook creates logs of pages visited both by its members and by non-members.  Relying on tracking cookies to keep track of pages visited, the United States Congress and the World Wide Web Consortium are attempting to set new guidelines to deal with Internet privacy concerns, potentially giving users the ability to limit or stop technology companies from tracking their activities. In early November 2015, Facebook was ordered by the Belgian Privacy Commissioner to cease tracking non-users, citing European laws, or else risk fines of up to £250,000 per day.  As a result, instead of removing tracking cookies, Facebook prevents non-users from seeing any material on Facebook, including publicly-posted content.  Arguing that the cookies provided better security, Facebook said in a statement: \"We're disappointed we were unable to reach an agreement and now people will be required to log in or register for an account to see publicly available content on Facebook.\" Social networks, like Facebook, can have a detrimental effect on marriages, with users becoming worried about their spouse's contacts and relations with other people online, leading to marital breakdown and divorce.  According to a 2009 survey in the UK, around 20 percent of divorce petitions included some kind of reference to Facebook. By statistics, 63% of Facebook profiles are automatically set \"visible to the public\" meaning anyone can access the profiles that users have updated.  Facebook also has its own built in messaging system that people can send message to any other user, unless they have disabled the feature to \"from friends only\".  Stalking is not only limited to SNS stalking, but can lead to further 'in person' stalking, because nearly 25% of real life stalking victims reported it started with online instant messaging (e.g. Facebook chat). The notion that people are very much aware that they are being surveiled on websites, like Facebook, and use the surveillance as an opportunity to portray themselves in a way that connotes a certain lifestyle—of which, that individual may, or may not, distort how they are perceived in reality. In 2010, \"The Wall Street Journal\" found that many of Facebook's top-rated apps were transmitting identifying information to \"dozens of advertising and Internet tracking companies\".  The apps used an HTTP referer that exposed the user's identity and sometimes their friends' identities.  Facebook said that \"While knowledge of user ID does not permit access to anyone’s private information on Facebook, we plan to introduce new technical systems that will dramatically limit the sharing of User ID’s\".  A blog post by a member of Facebook's team further stated that \"press reports have exaggerated the implications of sharing a user ID\", though still acknowledging that some of the apps were passing the ID in a manner that violated Facebook's policies. In an effort to surveil the personal lives of current, or prospective employees, some employers have asked employees to disclose their Facebook log-in information.  This has resulted in the passing of a bill in New Jersey making it illegal for employers to ask potential or current employees for access to their Facebook accounts.  Although, the U.S government has yet to pass a national law protecting prospective employees and their social networking sites, from employers, the fourth amendment of the US constitution can protect prospective employees in specific situations. A 2011 study in the online journal \"First Monday\", examines how parents consistently enable children as young as 10 years old to sign up for accounts, directly violating Facebook's policy banning young visitors.  This policy is in compliance with a United States law, the 1998 Children's Online Privacy Protection Act, which requires minors aged 13 or younger to gain explicit parental consent to access commercial websites.  In other jurisdictions where a similar law sets a lower minimum age, Facebook enforces the lower age.  Of the 1,007 households surveyed for the study, 76% of parents reported that their child joined Facebook at an age younger than 13, the minimum age in the site's terms of service.  The study also reported that Facebook removes roughly 20,000 users each day for violating its minimum age policy.  The study's authors also note, \"Indeed, Facebook takes various measures both to restrict access to children and delete their accounts if they join.\"  The findings of the study raise questions primarily about the shortcomings of United States federal law, but also implicitly continue to raise questions about whether or not Facebook does enough to publicize its terms of service with respect to minors.  Only 53% of parents said they were aware that Facebook has a minimum signup age; 35% of these parents believe that the minimum age is merely a recommendation, or thought the signup age was 16 or 18, and not 13. Students who post illegal or otherwise inappropriate material have faced disciplinary action from their universities, colleges, and schools including expulsion.  Others posting libelous content relating to faculty have also faced disciplinary action.  The \"Journal of Education for Business\" states that \"a recent study of 200 Facebook profiles found that 42% had comments regarding alcohol, 53% had photos involving alcohol use, 20% had comments regarding sexual activities, 25% had seminude or sexually provocative photos, and 50% included the use of profanity.\"  It is inferred that negative or incriminating Facebook posts can effect alumnis' and potential employers' perception of them.  This perception can greatly impact the students' relationships, ability to gain employment, and maintain school enrollment.  The desire for social acceptance leads individuals to want to share the most intimate details of their personal lives along with illicit drug use and binge drinking.  Too often, these portrayals of their daily lives are exaggerated and/or embellished to attract others like minded to them. On January 23, 2006, \"The Chronicle of Higher Education\" continued an ongoing national debate on social networks with an opinion piece written by Michael Bugeja, director of the Journalism School at Iowa State University, entitled \"Facing the Facebook\".  Bugeja, author of the Oxford University Press text \"Interpersonal Divide\" (2005), quoted representatives of the American Association of University Professors and colleagues in higher education to document the distraction of students using Facebook and other social networks during class and at other venues in the wireless campus.  Bugeja followed up on January 26, 2007 in \"The Chronicle\" with an article titled \"Distractions in the Wireless Classroom\", quoting several educators across the country who were banning laptops in the classroom.  Similarly, organizations such as the National Association for Campus Activities, the Association for Education in Journalism and Mass Communication, and others have hosted seminars and presentations to discuss ramifications of students' use of Facebook and other social-networking sites. The EDUCAUSE Learning Initiative has also released a brief pamphlet entitled \"7 Things You Should Know About Facebook\" aimed at higher education professionals that \"describes what <nowiki>[Facebook]</nowiki> is, where it is going, and why it matters to teaching and learning\". Some research on Facebook in higher education suggests that there may be some small educational benefits associated with student Facebook use, including improving engagement which is related to student retention.  2012 research has found that time spent on Facebook is related to involvement in campus activities.  This same study found that certain Facebook activities like commenting and creating or RSVPing to events were positively related to student engagement while playing games and checking up on friends was negatively related.  Furthermore, using technologies such as Facebook to connect with others can help college students be less depressed and cope with feelings of loneliness and homesickness. As of February 2012, only four published peer-reviewed studies have examined the relationship between Facebook use and grades.  There is considerable variance in the findings.  Pasek et al. (2009) found there was no relationship between Facebook use and grades.  Kolek and Saunders (2008) found that there were no differences in overall grade point average (GPA) between users and non-users of Facebook.  Kirschner and Karpinski (2010) found that Facebook users reported a lower mean GPA than non-users.  Junco's (2012) study clarifies the discrepancies in these findings.  While Junco (2012) found a negative relationship between time spent on Facebook and student GPA in his large sample of college students, the real-world impact of the relationship was negligible.  Furthermore, Junco (2012) found that sharing links and checking up on friends were positively related to GPA while posting status updates was negatively related.  In addition to noting the differences in how Facebook use was measured among the four studies, Junco (2012) concludes that the ways in which students use Facebook are more important in predicting academic outcomes. Facebook has been criticized for making people envious and unhappy due to the constant exposure to positive yet unrepresentative highlights of their peers.  Such highlights include, but are not limited to, wall posts, videos, and photos that depict or reference such positive or otherwise outstanding activities, experiences, and facts.  This effect is caused mainly by the fact that most users of Facebook usually only display the positive aspects of their lives while excluding the negative, though it is also strongly connected to inequality and the disparities between social groups as Facebook is open to users from all classes of society.  Sites such as AddictionInfo.org state that this kind of envy has profound effects on other aspects of life and can lead to severe depression, self-loathing, rage and hatred, resentment, feelings of inferiority and insecurity, pessimism, suicidal tendencies and desires, social isolation, and other issues that can prove very serious.  This condition has often been called \"Facebook Envy\" or \"Facebook Depression\" by the media. A joint study conducted by two German universities demonstrated Facebook envy and found that as many as one out of three people actually feel worse and less satisfied with their lives after visiting the site.  Vacation photos were found to be the most common source of feelings of resentment and jealousy.  After that, social interaction was the second biggest cause of envy, as Facebook users compare the number of birthday greetings, likes, and comments to those of their friends.  Visitors who contributed the least tended to feel the worst.  \"According to our findings, passive following triggers invidious emotions, with users mainly envying happiness of others, the way others spend their vacations; and socialize,\" the study states. A 2013 study by researchers at the University of Michigan found that the more people used Facebook, the worse they felt afterwards. Research performed by psychologists from Edinburgh Napier University indicated that Facebook adds stress to users' lives.  Causes of stress included fear of missing important social information, fear of offending contacts, discomfort or guilt from rejecting user requests or deleting unwanted contacts or being unfriended or blocked by Facebook friends or other users, the displeasure of having friend requests rejected or ignored, the pressure to be entertaining, criticism or intimidation from other Facebook users, and having to use appropriate etiquette for different types of friends.  Many people who started using Facebook for positive purposes or with positive expectations have found that the website has negatively impacted their lives. Next to that, the increasing number of messages and social relationships embedded in SNS also increases the amount of social information demanding a reaction from SNS users.  Consequently SNS users perceive they are giving too much social support to other SNS friends.  This dark side of SNS usage is called ‘social overload’.  It is caused by the extent of usage, number of friends, subjective social support norms, and type of relationship (online-only vs offline friends) while age has only an indirect effect.  The psychological and behavioral consequences of social overload include perceptions of SNS exhaustion, low user satisfaction, and high intentions to reduce or stop using SNS. The \"World Unplugged\" study, which was conducted in 2011, claims that for some users quitting social networking sites is comparable to quitting smoking or giving up alcohol.  Another study conducted in 2012 by researchers from the University of Chicago Booth School of Business in the United States found that drugs like alcohol and tobacco could not keep up with social networking sites regarding their level of addictiveness.  A 2013 study in the journal \"CyberPsychology, Behavior, and Social Networking\" found that some users decided to quit social networking sites because they felt they were addicted.  In 2014, the site went down for about 30 minutes, prompting several users to call 9-1-1. It has been admitted by many students that they have experienced bullying on the site, which leads to psychological harm.  Students of high schools face a possibility of bullying and other adverse behaviors over Facebook every day.  Many studies have attempted to discover whether Facebook has a positive or negative effect on children’s and teenagers’ social lives, and many of them have come to the conclusion that there are distinct social problems that arise with Facebook usage.  British neuroscientist Susan Greenfield stuck up for the issues that children encounter on social media sites.  She said that they can rewire the brain, which caused some hysteria over whether or not social networking sites are safe.  She did not back up her claims with research, but did cause quite a few studies to be done on the subject.  When that self is then broken down by others by badmouthing, criticism, harassment, criminalization or vilification, intimidation, demonization, demoralization, belittlement, or attacking someone over the site it can cause much of the envy, anger, or depression. Sherry Turkle, in her book \"Alone Together: Why We Expect More from Technology and Less from Each Other\", argues that social media bring people closer and further apart at the same time.  One of the main points she makes is that there is a high risk in treating persons online with dispatch like objects.  Although people are networked on Facebook, their expectations of each other tend to be lessened.  According to Turkle, this could cause a feeling of loneliness in spite of being together. A 2014 study titled \"Experimental evidence of massive-scale emotional contagion through social networks\" manipulated the balance of positive and negative messages seen by 689,000 Facebook users.  The paper details the experiment running from January 11 to 18, 2012, in an attempt to identify emotional contagion effects by altering the amount of emotional content in the targeted users' news feed.  The researchers concluded that they had found \"some of the first experimental evidence to support the controversial claims that emotions can spread throughout a network, [though] the effect sizes from the manipulations are small\". The study was criticized for both its ethics and methods/claims.  As controversy about the study grew, Adam Kramer, a lead author of both studies and member of the Facebook data team, defended the work in a Facebook update.  A few days later, Sheryl Sandburg, Facebook's COO, made a statement while travelling abroad.  While at an Indian Chambers of Commerce event in New Delhi she stated that: \"This was part of ongoing research companies do to test different products, and that was what it was.  It was poorly communicated and for that communication we apologize.  We never meant to upset you.\" Shortly thereafter, on July 3, 2014, \"USA Today\" reported that the privacy watchdog group Electronic Privacy Information Center (EPIC) had filed a formal complaint with the Federal Trade claiming that Facebook had broken the law when it conducted the study on the emotions of its users without their knowledge or consent.  In its complaint the EPIC alleged that Facebook had deceived it users by secretly conducting a psychological experiment on their emotions: \"At the time of the experiment, Facebook did not state in the Data Use Policy that user data would be used for research purposes.  Facebook also failed to inform users that their personal information would be shared with researchers.\" In the UK, the study was also criticised by the British Psychological Society, which said, in a letter to \"The Guardian\", \"There has undoubtedly been some degree of harm caused, with many individuals affected by increased levels of negative emotion, with consequent potential economic costs, increase in possible mental health problems and burden on health services.  The so-called 'positive' manipulation is also potentially harmful.\" The consequences of the controversy are pending (be it FTC or court proceedings) but it did prompt an \"Editorial Expression of Concern\" from its publisher, the Proceedings of the National Academy of Sciences, as well as an blog posting from OkCupid that \"We experiment on human beings!\"  In September 2014, law professor James Grimmelmann argued that the actions of both companies were \"illegal, immoral, and mood-altering\" and filed notices with the Maryland Attorney General and Cornell Institutional Review Board. Facebook uses a complicated series of shell companies in tax havens to avoid paying billions of dollars in corporate tax.  For example, in 2011, Facebook paid £2.9m tax on £840m profits, no tax in 2012, no tax in 2013, and £4,327 in 2014 on hundreds of millions of pounds in UK revenues which were transferred to tax havens.  Facebook routes billions of dollars in profits using the Double Irish and Dutch Sandwich tax avoidance schemes to bank accounts in the Cayman Islands. On July 6, 2016, the U.S. Department of Justice filed a petition in the U.S. District Court in San Francisco, asking for a court order to enforce an administrative summons issued to Facebook, Inc., under Internal Revenue Code section 7602, in connection with an Internal Revenue Service examination of Facebook's year 2010 U.S. Federal income tax return. In May 2011, emails were sent to journalists and bloggers making critical allegations about Google's privacy policies; however, it was later discovered that the anti-Google campaign, conducted by PR giant Burson-Marsteller, was paid for by Facebook in what CNN referred to as \"a new level skullduggery\" and which \"Daily Beast\" called a \"clumsy smear\".  While taking responsibility for the campaign, Burson-Marsteller said it should not have agreed to keep its client's (Facebook's) identity a secret.  \"Whatever the rationale, this was not at all standard operating procedure and is against our policies, and the assignment on those terms should have been declined\", it said in a statement. Facebook has also been criticized for having lax enforcement of third-party copyrights for videos uploaded to the service.  In 2015, some Facebook pages were accused of plagiarizing videos from YouTube users and re-posting them as their own content using Facebook's video platform, and in some cases, achieving higher levels of engagement and views than the original YouTube post.  Videos hosted by Facebook are given a higher priority and prominence within the platform and its user experience (including direct embedding within the News Feed and pages), giving a disadvantage to posting it as a link to the original external source.  In August 2015, Facebook announced a video-matching technology aiming to identify reposted videos, and also stated its intention to improve its procedures to remove infringing content faster.  In April 2016, Facebook implemented a feature known as \"Rights Manager\", which allows rightsholders to manage and restrict the upload of their content onto the service by third-parties. In 2013, Facebook was criticized for allowing users to upload and share videos depicting violent content, including clips of people being decapitated.  Having previously refused to delete such clips under the guideline that users have the right to depict the \"world in which we live\", Facebook changed its stance in May, announcing that it would remove reported videos while evaluating its policy.  The following October, Facebook stated that it would allow graphic videos on the platform, as long as the intention of the video was to \"condemn, not glorify, the acts depicted\", further stating that \"Sometimes, those experiences and issues involve graphic content that is of public interest or concern, such as human rights abuses, acts of terrorism, and other violence.  When people share this type of graphic content, it is often to condemn it.  If it is being shared for sadistic pleasure or to celebrate violence, Facebook removes it.\"  However, Facebook once again received criticism, with the Family Online Safety Institute saying that such videos \"crossed a line\" and can potentially cause psychological damage among young Facebook users, and then-Prime Minister of the United Kingdom David Cameron calling the decision \"irresponsible\", citing the same concerns regarding young users.  Two days later, Facebook removed a video of a beheading following \"worldwide outrage\", and while acknowledging its commitment to allowing people to upload gory material for the purpose of condemnation, it also stated that it would be further strengthening its enforcement to prevent glorification.  The company's policies were also criticized as part of these developments, with some drawing particular attention to Facebook's permission of graphic content but potential removal of breastfeeding images.  In January 2015, Facebook announced that new warnings would be displayed on graphic content, requiring users to explicitly confirm that they wish to see the material. Facebook Live, introduced in August 2015 for celebrities and gradually rolled out for regular users starting in January 2016, lets users broadcast live videos, with Facebook's intention for the feature to be presenting public events or private celebrations.  However, the feature has been used to record multiple crimes, deaths, and violent incidents, causing significant media attention. Facebook has received criticism for not removing videos faster, and Facebook Live has been described as a \"monster [Facebook] cannot tame\" and \"a gruesome crime scene for murders\".  In response, CEO Mark Zuckerberg announced in May 2017 that the company would hire 3,000 people to review content and invest in tools to remove videos faster. In 2008, Facebook was criticized for hosting groups dedicated to promoting anorexia.  The groups promoted dramatic weight loss programs, shared extreme diet tips, and posted pictures of emaciated girls under \"Thinspiration\" headlines.  Members reported having switched to Facebook from Myspace, another social networking service, due to a perceived higher level of safety and intimacy at Facebook.  In a statement to \"BBC News\", a Facebook spokesperson stated that \"Many Facebook groups relate to controversial topics; this alone is not a reason to disable a group.  In cases where content is reported and found to violate the site's terms of use, Facebook will remove it.\" In Italy in 2009, the discovery of pro-mafia groups, one of them claiming Bernardo Provenzano's sainthood, caused an alert in the country and brought the government to rapidly issue a law that would force Internet service providers to deny access to entire websites in case of refused removal of illegal contents.  The amendment was passed by the Italian Senate and now needs to be passed unchanged by the Chamber of Deputies to become effective. Facebook criticized the government's efforts, telling \"Bloomberg\" that it \"would be like closing an entire railway network just because of offensive graffiti at one station\", and that \"Facebook would always remove any content promoting violence and already had a takedown procedure in place.\" On March 31, 2010, \"The Today Show\" ran a segment detailing the deaths of three separate adolescent girls and trolls' subsequent reactions to their deaths.  Shortly after the suicide of high school student Alexis Pilkington, anonymous posters began trolling for reactions across various message boards, referring to Pilkington as a \"suicidal CUSS\", and posting graphic images on her Facebook memorial page.  The segment also included an exposé of a 2006 accident, in which an eighteen-year-old student out for a drive fatally crashed her father's car into a highway pylon; trolls e-mailed her grieving family the leaked pictures of her mutilated corpse. There have been cases where Facebook \"trolls\" were jailed for their communications on Facebook, particularly memorial pages.  In Autumn 2010, Colm Coss of Ardwick, Britain, was sentenced to 26 weeks in jail under s127 of the Communications Act 2003 of Great Britain, for \"malicious communications\" for leaving messages deemed obscene and hurtful on Facebook memorial pages. In April 2011, Bradley Paul Hampson was sentenced to three years in jail after pleading guilty to two counts of using a carriage service, the Internet, to cause offense, for posts on Facebook memorial pages, and one count each of distributing and possessing child pornography when he posted images on the memorial pages of the deceased with phalluses superimposed alongside phrases such as \"Woot I'm dead\". A series of pro-rape and 'rape joke' content on Facebook drew attention from the media and women's groups.  Rape Is No Joke (RINJ), a group opposing the pages, argued that removing \"pro-rape\" pages from Facebook and other social media was not a violation of free speech in the context of Article 19 of the Universal Declaration of Human Rights and the concepts recognized in international human rights law in the International Covenant on Civil and Political Rights. RINJ repeatedly challenged Facebook to remove the rape pages.  RINJ then turned to advertisers on Facebook telling them not to let their advertising be posted on Facebook's 'rape pages'. Following a campaign that involved the participation of Women, Action and the Media, the Everyday Sexism Project and the activist Soraya Chemaly, who were among 100 advocacy groups, Facebook agreed to update its policy on hate speech.  The campaign highlighted content that promoted domestic and sexual violence against women, and used over 57,000 tweets and more than 4,900 emails to create outcomes such as the withdrawal of advertising from Facebook by 15 companies, including Nissan UK, House of Burlesque and Nationwide UK.  The social media website initially responded by stating that \"While it may be vulgar and offensive, distasteful content on its own does not violate our policies\", but then agreed to take action on May 29, 2013 after it had \"become clear that our systems to identify and remove hate speech have failed to work as effectively as we would like, particularly around issues of gender-based hate\". In June 2015, the U.K. National Society for the Prevention of Cruelty to Children raised concerns about Facebook's apparent refusal when asked to remove controversial video material which allegedly showed a baby in emotional distress. In March 2017, BBC News reported in an investigation that Facebook only removed 18 of the 100 groups and posts it had reported for containing child exploitation images.  The BBC had been granted an interview with Facebook policy director Simon Milner under the condition that they provide evidence of the activity.  However, when presented with the images, Facebook cancelled the interview, and told the BBC that it had been reported to the National Crime Agency for illegally distributing child exploitation images (the NCA could not confirm whether the BBC was actually being investigated).  Milner later stated to the BBC that the investigation had exposed flaws in its image moderation process that have since been addressed, and that all of the reported content was removed from the service. In July 2017, \"GMA News\" reported that \"a number\" of secret Facebook groups that had been engaging in illegal activity of sharing \"obscene\" photos of women had been exposed, with the Philippine National Bureau of Investigation warning group members of the possibility of being liable for violating child pornography and anti-voyeurism laws.  Facebook stated that it would remove the groups as violations of its community guidelines.  A few days later, \"GMA News\" had an interview with one of the female victims targeted by one of the groups, who stated that she received friend requests from strangers and inappropriate messages.  After reporting to authorities, the Philippine National Police's anti-cybercrime unit promised to take action in finding the accounts responsible.  Senator Risa Hontiveros responded to the incidents with the proposal of a law that would impose \"stiff penalties\" on such group members, stating that \"These people have no right to enjoy our internet freedom only to abuse our women and children.  We will not allow them to shame our young women, suppress their right to express themselves through social media and contribute to a culture of misogyny and hate\". Facebook has been suspected of having a double-standard when it came to pages and posts regarding the Arab-Israeli conflict.  When it comes to so-called incitement, Facebook has been accused of being unfair, removing only posts and pages that attack Palestinians, while turning a blind eye to similar posts that are violently anti-Semitic.  The NGO Shurat Hadin-Israel Law Center conducted an experiment over the incitement issue, which sought to expose what it viewed as double standards regarding anti-Israel sentiment vis-a-vis the simultaneous launch of two Facebook pages: \"Stop Palestinians\" and \"Stop Israel\".  Following the launch of the two nearly-identical pages, the NGO posted hateful content simultaneously on both pages.  Next, Shurat Hadin's reported both faux-incitement pages to Facebook to see which, if either, would be removed.  According to them, despite featuring nearly identical content, only one was removed from the online platform.  They said the page inciting against Palestinians was closed by Facebook (on the same day that it was reported) for \"containing credible threat of violence\" which \"violated our [Facebook's] community standards\", but not the page inciting against Israelis.  Shurat Hadin said that Facebook claimed that this page was \"not in violation of Facebook's rules\".  Shurat Hadin's staged anti-Israel group \"Stop Israel\" still remains active on Facebook.  ProPublica stated in September 2017 that a website was able to target ads at Facebook users who were interested in “how to burn Jew“ and and \"Jew hater”.  Facebook removed the categories and said it would try to stop them from appearing to potential advertisers. Facebook has been accused of being a public platform used to incite terrorism.  In October 2015, 20,000 Israelis claimed that Facebook was ignoring Palestinian incitement on its platform and filed a class-action suit demanding that Facebook remove all posts \"containing incitement to murder Jews\". Israeli politicians have complained that Facebook does not comply or assist with requests from the police for tracking and reporting individuals when they share their intent to kill or commit any other act of terrorism on their Facebook pages.  In June 2016, following the murder of Hallel Ariel, 13, by a terrorist who posted on Facebook, Israeli Minister of Public Security Gilad Erdan charged that \"Facebook, which has brought a positive revolution to the world, has become a monster...The dialogue, the incitement, the lies of the young Palestinian generation are happening on the Facebook platform.\"  Erdan accused Facebook of \"sabotaging the work of Israeli police\" and \"refusing to cooperate\" when Israel Police turns to the site for assistance.  It also \"sets a very high bar\" for removing inciteful content. In July 2016, a civil action for $1 billion in damages was filed in the United States District Court for the Southern District of New York on behalf of the victims and family members of four Israeli-Americans and one US citizen killed by Hamas terrorists since June 2014.  The victims and plaintiffs in the case are the families of Yaakov Naftali Fraenkel, a 16-year-old who was kidnapped and murdered by Hamas operatives in 2014; Taylor Force, a 29-year-old American MBA student and US Army veteran killed in a stabbing spree in Jaffa in 2016; Chaya Braun, a three-month-old thrown from her stroller and slammed into the pavement when a Hamas attacker drove his car into a light rail station in Jerusalem in an October 2014; 76-year-old Richard Lakin who was killed in an October 2015 shooting and stabbing attack on a Jerusalem bus; and Menachem Mendel Rivkin, who was seriously wounded in a January 2016 stabbing attack in Jerusalem.  The plaintiffs claimed that Facebook knowingly provided its social media platform and communication services to Hamas in violation of provisions of US Anti-Terrorism laws which prohibits US businesses from providing any material support, including services, to designated terrorist groups and their leaders.  The government of the United States has designated Hamas as a \"Foreign Terrorist Organization\" as defined by US law.  The suit claims that Hamas \"used and relied on Facebook's online social network platform and communications services to facilitate and carry out its terrorist activity, including the terrorist attacks in which Hamas murdered and injured the victims and their families in this case\". In August 2016, Israel's security service, the Shin Bet, reported that it had arrested nine Palestinians who had been recruited by the Lebanon-based Hezbollah terrorist organization.  Operatives of Hezbollah in Lebanon and Gaza Strip recruited residents of the West Bank, Gaza and Israel through Facebook and other social media sites.  After recruiting cell leaders on Facebook, Hezbollah and the recruits used encrypted communications to avoid detection, and the leaders continued to recruit other members.  The terror cells received Hezbollah funding and planned to conduct suicide bombings and ambushes and had begun preparing explosive devices for attacks, said the security service, which claimed credit for preventing the attacks.  The Shin Bet said it also detected multiple attempts by Hezbollah to recruit Israeli Arabs through a Facebook profile. Currently, legislation is being prepared in Israel, allowing fines of 300,000 shekels for Facebook and other social media like Twitter and YouTube for every post inciting or praising terrorism that isn't removed within 48 hours, and could possibly lead to further acts of terrorism. In June 2017, Facebook published a blog post, offering insights into how it detects and combats terrorism content.  The company claimed that the majority of the terrorism accounts that are found are discovered by Facebook itself, while it reviews reports of terrorism content \"urgently\", and, in cases of imminent harm, \"promptly inform authorities\".  It also develops new tools to aid in its efforts, including the use of artificial intelligence to match terrorist images and videos, detecting when content is shared across related accounts, and developing technologies to stop repeat offenders.  The company stated that it has 150 people dedicated to terrorism countermeasures, and works with governments and industries in an effort to curb terrorist propaganda.  Its blog post stated that \"We want Facebook to be a hostile place for terrorists.\" In June 2017, \"The Guardian\" reported that a software bug had exposed the personal details of 1,000 Facebook workers involved in reviewing and removing terrorism content, by displaying their profiles in the \"Activity\" logs of Facebook groups related to terrorism efforts.  In Facebook's Dublin, Ireland headquarters, six individuals were determined to be \"high priority\" victims of the error, after the company concluded that their profiles were likely viewed by potential terrorists in groups such as ISIS, Hezbollah and the Kurdistan Workers' Party.  The bug itself, discovered in November 2016 and fixed two weeks later, was active for one month, and had also been retroactively exposing censored personal accounts from August 2016.  One affected worker had fled Ireland, gone into hiding, and only returned to Ireland after five months due to a lack of money.  Suffering from psychological distress, he filed a legal claim against Facebook and CPL Resources, an outsourcing company, seeking compensation.  A Facebook spokesperson stated that \"Our investigation found that only a small fraction of the names were likely viewed, and we never had evidence of any threat to the people impacted or their families as a result of this matter\", and Craig D’Souza, Facebook's head of global investigations, said: \"Keep in mind that when the person sees your name on the list, it was in their activity log, which contains a lot of information [...] there is a good chance that they associate you with another admin of the group or a hacker\".  Facebook offered to install a home-alarm monitoring system, provide transport to and from work, and counseling through its employee assistance program.  As a result of the data leak, Facebook is reportedly testing the use of alternative, administrative accounts for workers reviewing content, rather than requiring workers to sign in with their personal profiles. Facebook has been criticized for not doing enough to limit the spread of fake news stories on their site, especially after the 2016 United States presidential election, which some have claimed Donald Trump would not have won if Facebook had not helped spread what they claim to have been fake stories that were biased in his favor.  Mark Zuckerberg has begun to take steps to eliminate the prevalence of fake news on Facebook as a result criticisms of Facebook's influence on the presidential election.  At a conference called \"Techonomy\" Mark Zuckerberg stated in regards to Donald Trump, \"There's a profound lack of empathy in asserting that the only reason why someone could have voted the way that they did is because they saw some fake news\".  Zuckerberg affirms the idea that people do not stray from their own ideals and political leanings.  He stated, \"I don't know what to do about that\" and, \"When we started, the north star for us was: We're building a safe community\". Mark Zuckerberg has also been quoted in his own Facebook post, \"Of all the content on Facebook, more than 99 percent of what people see is authentic\".  In addition, \"The Pew Research Center\", stated that \"62% of Americans obtain some, or all, of their news on social media-the bulk of it from Facebook\".  The former editor at Facebook leaked inflammatory information about the websites' algorithm's pointing to certain falsehoods and bias by the news created within Facebook.  Although Facebook initially denied claims of issues with fake new stories and their algorithims, they fired the entire trending team involved with a fake news story about Megyn Kelly being a \"closeted liberal\". Other fake news such as the story headlined as \"(Shocking Video Footage) 18 dead in horrific roller coaster accident\" back in 2014 are meant to entice users into sharing information with them.  They accomplish this by using an authorization app that allows them to see personal information such as your friends list and in some cases post to your wall on your behalf.  Instructions for removing these permissions have been posted by companies like Dominant Domains Web Design and SEO. Facebook has a real-name system policy for user profiles.  The real-name policy stems from the position \"that way, you always know who you're connecting with.  This helps keep our community safe.\"  The real-name system does not allow adopted names or pseudonyms, and in its enforcement has suspended accounts of legitimate users, until the user provides identification indicating the name.  Facebook representatives have described these incidents as very rare.  A user claimed responsibility via the anonymous Android and iOS app Secret for reporting \"fake names\" which caused user profiles to be suspended, specifically targeting the stage names of drag queens. On October 1, 2014, Chris Cox, Chief Product Officer at Facebook, offered an apology: \"In the two weeks since the real-name policy issues surfaced, we've had the chance to hear from many of you in these communities and understand the policy more clearly as you experience it.  We've also come to understand how painful this has been.  We owe you a better service and a better experience using Facebook, and we're going to fix the way this policy gets handled so everyone affected here can go back to using Facebook as you were.\" On December 15, 2015, Facebook announced in a press release that it would be providing a compromise to its real name policy after protests from groups such as the gay/lesbian community and abuse-victims.  The site is developing a protocol that will allow members to provide specifics as to their \"special circumstance\" or \"unique situation\" with a request to use pseudonyms, subject to verification of their true identities.  At that time, this was already being tested in the U.S. Product manager Todd Gage and vice president of global operations Justin Osofsky also promised a new method for reducing the number of members who must go through ID verification while ensuring the safety of others on Facebook.  The fake name reporting procedure will also be modified, forcing anyone who makes such an allegation to provide specifics that would be investigated and giving the accused individual time to dispute the allegation. There have been complaints of user statuses being mistakenly or intentionally deleted for alleged violations of Facebook's posting guidelines.  Especially for non-English speaking writers, Facebook does not have a proper support system to genuinely read the content and make decisions.  Sometimes the content of a status did not have any \"abusive\" or defaming language, but it nevertheless got deleted on the basis that it had been secretly reported by a group of people as \"offensive\".  For other languages than English, Facebook till now is not able to identify the group approach that is used to vilify humanitarian activism.  In another incident, Facebook had to apologize after it deleted a free speech group's post about the abuse of human rights in Syria.  In that case, a spokesman for Facebook said the post was \"mistakenly\" removed by a member of its moderation team, which receives a high volume of take-down requests. Facebook instituted a policy by which it is now self-policed by the community of Facebook users.  Some users have complained that this policy allows Facebook to empower abusive users to harass them by allowing them to submit reports on even benign comments and photos as being \"offensive\" or \"in violation of Facebook Rights and Responsibilities\" and that enough of these reports result in the user who is being harassed in this way getting their account blocked for a predetermined number of days or weeks, or even deactivated entirely. Facebook UK policy director Simon Milner told Wired Magazine that \"Once the piece of content has been seen, assessed and deemed OK, (Facebook) will ignore further reports about it.\" Facebook lacks live support, making it difficult to resolve issues that require the services of an administrator or are not covered in the FAQs, such as the enabling of a disabled account.  The automated emailing system used when filling out a support form often refers users back to the help center or to pages that are outdated and cannot be accessed, leaving users at a dead end with no further support available. Facebook has had a number of outages and downtime large enough to draw some media attention.  A 2007 outage resulted in a security hole that enabled some users to read other users' personal mail.  In 2008, the site was inaccessible for about a day, from many locations in many countries.  In spite of these occurrences, a report issued by Pingdom found that Facebook had less downtime in 2008 than most social-networking websites.  On September 16, 2009, Facebook started having major problems with loading when people signed in.  On September 18, 2009, Facebook went down for the second time in 2009, the first time being when a group of hackers were deliberately trying to drown out a political speaker who had social networking problems from continuously speaking against the Iranian election results. In October 2009, an unspecified number of Facebook users were unable to access their accounts for over three weeks. Facebook has been criticized heavily for 'tracking' users, even when logged out of the site.  Australian technologist Nik Cubrilovic discovered that when a user logs out of Facebook, the cookies from that login are still kept in the browser, allowing Facebook to track users on websites that include \"social widgets\" distributed by the social network.  Facebook has denied the claims, saying they have 'no interest' in tracking users or their activity.  They also promised after the discovery of the cookies that they would remove them, saying they will no longer have them on the site.  A group of users in the United States have sued Facebook for breaching privacy laws. As of December 2015, to comply with a court order citing violations of the European Union Directive on Privacy and Electronic Communications—which requires users to consent to tracking and storage of data by websites, Facebook no longer allows users in Belgium to view any content on the service, even public pages, without being registered and logged in. In June 2012, Facebook removed all existing email addresses from user profiles, and added a new @facebook. com email address.  Facebook claimed this was part of adding a \"new setting that gives people the choice to decide which addresses they want to show on their timelines\".  However, this setting was redundant to the existing \"Only Me\" privacy setting which was already available to hide addresses from timelines.  Users complained the change was unnecessary, they did not want an @facebook. com email address, and they did not receive adequate notification their profiles had been changed.  The change in email address was synchronised to phones due to a software bug, causing existing email addresses details to be deleted.  The facebook.com email service was retired in February 2014. On March 27, 2016, following a bombing in Lahore, Pakistan, Facebook activated its \"Safety Check\" feature, which allows people to let friends and loved ones know they are okay following a crisis or natural disaster, to people who were never in danger, or even close to the Pakistan explosion.  Some users as far as the US, UK and Egypt received notifications asking if they were okay. In 2016, Facebook banned and also removed content regarding the Kashmir dispute, triggering a response from \"The Guardian\", BBC and other media group on the Facebook's policies on censorship.  Facebook censorship policies have been criticised especially after the company banned the posts about the Indian army's attack on protesters, including children, with pellet gun.  A human rights group superimposed pellet injuries similar to those inflicted on Kashmiri people on the faces of popular Indian actors, famous people including Facebook founder Mark Zuckerberg and even Prime Minister Narendra Modi as a response, which went viral. Facebook has a policy to censor anything related to Kurdish opposition against Turkey, such as maps of Kurdistan, flags of opposing parties (such as PKK and YPG), and criticism of Mustafa Kemal Atatürk, the founder of Turkey. Facebook's search function has been accused of preventing users from searching for certain terms.  Michael Arrington of TechCrunch has written about Facebook's possible censorship of \"Ron Paul\" as a search term.  MoveOn.org's Facebook group for organizing protests against privacy violations could for a time not be found by searching.  The very word \"privacy\" was also restricted. On February 4, 2010, a number of Facebook groups against the Democratic Alliance for the Betterment and Progress of Hong Kong (DAB) were removed without any reason given.  The DAB is one of the largest pro-Beijing political parties in Hong Kong.  The affected groups have since been restored. In May 2011, Facebook announced that in the coming months it will be \"archiving\" all groups in the old format, part of the consequence of which is losing all the existing members of a group, which would effectively destroy many groups, forcing them to re-acquire members from scratch.  A few groups have been given an option to \"upgrade\" to the new groups format, which keeps the members, but the criteria for determining whether a group is offered this \"upgrade\" are unknown.  Some groups have had success in getting this upgrade by having activity in their group, while others have not.  One article has claimed an empirical observation that disproportionately more \"liberal\" groups have been able to upgrade than \"conservative\" groups, leading to accusations of potential political bias, or of politically motivated censorship of conservative groups. Around July 1, 2015 Facebook started to automatically ban accounts that use the word \"moskal\", which is a widely used historical slang term for people of Russia (formerly Moskovia until 1721), which may be seen offensive by some individuals.  However, use of similar words such as \"khokhol\", which are widely used by Russian nationalists against Ukrainians, as well as insulting uses of \"ukrop\" (literally dill), were not prosecuted.  In an experiment, journalist Max Kononenko has posted the poem \"Моя родословная\" by Alexander Pushkin for this account to be banned automatically within minutes.  Posts of vice minister of Roskomnadzor, Max Ksenzov, were similarly automatically deleted.  Ksenzov has accused Facebook of censorship and double standards and has removed his account in protest. Facebook has a policy of removing photos which they believe violate the terms and conditions of the website.  Images have been removed from user pages on topics such as breastfeeding, nudes in art, apparent breasts, naked mannequins, kisses between persons of the same sex and family photos. In September 2016, a Norwegian newspaper has published an open letter to Zuckerberg after banning \"Napalm Girl\", a Pulitzer Prize-winning documentary photograph from the Vietnam War made by Nick Ut.  Half of the ministers in the Norwegian government shared the famous Nick Ut photo on their Facebook pages, among them prime minister Erna Solberg from the Conservative Party (Høyre).  But after only a few hours, several of the Facebook posts, including the Prime Minister's post, were deleted by Facebook. As a reaction to the letter, Facebook reconsidered its opinion on this picture and republished it, recognizing \"the history and global importance of this image in documenting a particular moment in time\". Facebook has been repeatedly criticized for removing photos uploaded by mothers breastfeeding their babies and canceling their Facebook accounts.  Although photos that show an exposed breast violate Facebook's decency code, even when the baby covered the nipple, Facebook took several days to respond to criticism and deactivate a paid advertisement for a dating service that used a photo of a topless model. The breastfeeding photo controversy continued following public protests and the growth in membership of a Facebook group titled \"Hey, Facebook, breastfeeding is not obscene!  (Official petition to Facebook).\"  In December 2011, Facebook removed photos of mothers breastfeeding and after public criticism, restored the photos.  The company said it removed the photos they believed violated the pornographic rules in the company's terms and conditions.  During February 2012, the company renewed its policy of removing photos of mothers breastfeeding.  Founders of a Facebook group \"Respect the Breast\" reported that \"women say they are tired of people lashing out at what is natural and what they believe is healthy for their children.\" Facebook has worked with Pakistani government to censor 'blasphemous' pages and speech inside Pakistan. In Germany, Facebook actively censors anti-refugee political expressions. In May 2016, Facebook and other technology companies agreed to a new \"code of conduct\" by the European Commission to review hateful online content within 24 hours of being notified, and subsequently remove such content if necessary.  A year later, \"Reuters\" reported that the European Union had approved proposals to make Facebook and other technology companies tackle hate speech content on their platforms, but that a final agreement in the European Parliament is needed to make the proposals into law.  In June 2017, the European Commission praised Facebook's efforts in fighting hateful content, having reviewed \"nearly 58 percent of flagged content within 24 hours\". In May 2016, Facebook was accused by a former employee for leaving out conservative topics from the trending bar.  Although Facebook denied these allegations, the site planned to improve the trending bar.  Facebook was also in the news regarding this issue, on June 13, after anti-jihad activist Pamela Geller said that two of her pages were deleted (one was a page and the other a group, both named \"Stop Islamization of America\") and in the aftermath of the June 12 Orlando massacre, in which a pro-Islamic State gunman murdered 49 people, and injured 53 others, at a popular gay nightclub before being killed by a police tactical unit. Several countries have banned access to Facebook, including Syria, China, and Iran. In 2010, the Office of the Data Protection Supervisor, a branch of the government of the Isle of Man, received so many complaints about Facebook that they deemed it necessary to provide a \"Facebook Guidance\" booklet (available online as a PDF file), which cited (amongst other things) Facebook policies and guidelines and included an elusive Facebook telephone number.  This number when called, however, proved to provide no telephone support for Facebook users, and only played back a recorded message advising callers to review Facebook's online help information. In 2010, Facebook reportedly allowed an objectionable page, deemed by the Islamic Lawyers Forum (ILF), to be anti-Muslim.  The ILF filed a petition with Pakistan's Lahore High Court.  On May 18, 2010, Justice Ijaz Ahmad Chaudhry ordered Pakistan's Telecommunication Authority to block access to Facebook until May 31.  The offensive page had provoked street demonstrations in Muslim countries due to visual depictions of Prophet Mohammed, which are regarded as blasphemous by Muslims.  A spokesman said Pakistan Telecommunication Authority would move to implement the ban once the order has been issued by the Ministry of Information and Technology.  \"We will implement the order as soon as we get the instructions\", Khurram Mehran told AFP.  \"We have already blocked the URL link and issued instruction to Internet service providers yesterday\", he added.  Rai Bashir told AFP that \"We moved the petition in the wake of widespread resentment in the Muslim community against the Facebook contents\".  The petition called on the government of Pakistan to lodge a strong protest with the owners of Facebook, he added.  Bashir said a PTA official told the judge his organization had blocked the page, but the court ordered a total ban on the site.  People demonstrated outside court in the eastern city of Lahore, Pakistan, carrying banners condemning Facebook.  Protests in Pakistan on a larger scale took place after the ban and widespread news of that objectionable page.  The ban was lifted on May 31 after Facebook reportedly assured the Lahore High Court that it would remedy the issues in dispute. In 2011, a court in Pakistan was petitioned to place a permanent ban on Facebook for hosting a page called \"2nd Annual Draw Muhammad Day May 20th 2011\". Ontario government employees, Federal public servants, MPPs, and cabinet ministers were blocked from access to Facebook on government computers in May 2007.  When the employees tried to access Facebook, a warning message \"The Internet website that you have requested has been deemed unacceptable for use for government business purposes\".  This warning also appears when employees try to access YouTube, MySpace, gambling or pornographic websites.  However, innovative employees have found ways around such protocols, and many claim to use the site for political or work-related purposes. A number of local governments including those in the UK and Finland imposed restrictions on the use of Facebook in the workplace due to the technical strain incurred.  Other government-related agencies, such as the US Marine Corps have imposed similar restrictions. A number of hospitals in Finland have also restricted Facebook use citing privacy concerns. The University of New Mexico (UNM) in October 2005 blocked access to Facebook from UNM campus computers and networks, citing unsolicited e-mails and a similar site called UNM Facebook.  After a UNM user signed into Facebook from off campus, a message from Facebook said, \"We are working with the UNM administration to lift the block and have explained that it was instituted based on erroneous information, but they have not yet committed to restore your access.\"  UNM, in a message to students who tried to access the site from the UNM network, wrote, \"This site is temporarily unavailable while UNM and the site owners work out procedural issues.  The site is in violation of UNM's Acceptable Computer Use Policy for abusing computing resources (e.g., spamming, trademark infringement, etc.).  The site forces use of UNM credentials (e.g., NetID or email address) for non-UNM business.\"  However, after Facebook created an encrypted login and displayed a precautionary message not to use university passwords for access, UNM unblocked access the following spring semester. The \"Columbus Dispatch\" reported on June 22, 2006, that Kent State University's athletic director had planned to ban the use of Facebook by athletes and gave them until August 1 to delete their accounts.  On July 5, 2006, the \"Daily Kent Stater\" reported that the director reversed the decision after reviewing the privacy settings of Facebook. Several web sites concerned with social networking, such as Plugtodo.com and Salesforce.com have criticized the lack of information that users get when they share data.  Advanced users cannot limit the amount of information anyone can access in their profiles, but Facebook promotes the sharing of personal information for marketing purposes, leading to the promotion of the service using personal data from users who are not fully aware of this.  Facebook exposes personal data, without supporting open standards for data interchange.  According to several communities and authors closed social networking, on the other hand, promotes data retrieval from other people while not exposing one's personal information. Openbook was established in early 2010 both as a parody of Facebook and a critique of its changing privacy management protocols. Divya Narendra, Cameron Winklevoss, and Tyler Winklevoss, founders of the social network ConnectU, filed a lawsuit against Facebook in September 2004.  The lawsuit alleged that Zuckerberg had broken an oral contract to build the social-networking site, copied the idea, and used source code that they provided to Zuckerberg to create competing site Facebook.  Facebook countersued in regards to Social Butterfly, a project put out by The Winklevoss Chang Group, an alleged partnership between ConnectU and i2hub.  It named among the defendants ConnectU, Cameron Winklevoss, Tyler Winklevoss, Divya Narendra, and Wayne Chang, founder of i2hub.  The parties reached a settlement agreement in February 2008, for $20 million in cash and 1,253,326 Facebook shares.  On August 26, 2010, \"The New York Times\" reported that Facebook shares were trading at $76 per share in the secondary market, putting the total settlement value now at close to $120 million. ConnectU filed another lawsuit against Facebook on March 11, 2008, attempting to rescind the settlement, claiming that Facebook, in settlement negotiations, had overstated the value of stock it was granting the ConnectU founders as part of the settlement.  ConnectU argued that Facebook represented itself as being worth $15 billion at the time, due to the post-money valuation arising from Microsoft's purchase in 2007 of a 1.6% stake in Facebook for US $246 million.  Facebook announced that valuation in a press release.  However, Facebook subsequently performed an internal valuation that estimated a company value of $3.75 billion.  ConnectU then fired the law firm Quinn Emanuel that had represented it in settlement discussions.  Quinn Emanuel filed a $13 million lien against the settlement proceeds and ConnectU sued for malpractice.  On August 25, 2010, an arbitration panel ruled that Quinn Emanuel had \"earned its full contingency fee\".  It also found that Quinn Emanuel committed no malpractice.  ConnectU's lawsuit against Facebook to quadruple its settlement remains ongoing. In January 2010, it was reported that i2hub founder Wayne Chang and The i2hub Organization launched a lawsuit against ConnectU and its founders, Cameron Winklevoss, Tyler Winklevoss, and Divya Narendra, seeking 50% of the settlement.  The complaint states \"Through this litigation, Chang asserts his ownership interest in The Winklevoss Chang Group and ConnectU, including the settlement proceeds.\"  Lee Gesmer (of Gesmer Updegrove, LLP) posted the detailed 33-page complaint online.  On April 12, 2011, a three-judge panel of a federal appeals court in San Francisco ruled that the Winklevoss brothers, whose fight over Facebook's origins was a major narrative arc of the film \"The Social Network\", cannot back out of a settlement they signed with the company in 2008. Aaron Greenspan created a web portal as a Harvard undergraduate called houseSYSTEM that launched in August 2003.  In February 2004, when thefacebook.com launched, Greenspan recognized aspects of his own work in the site, and later came to believe that Zuckerberg was copying his work one feature at a time—a claim that Zuckerberg denied.  Regarding Greenspan's allegations, Zuckerberg was described in \"The New York Times\" as \"saying through a spokeswoman that he was not sure how to respond\".  Greenspan's company filed a Petition to Cancel the \"Facebook\" trademark, which included claims of prior use and fraud by Facebook, Inc. against the USPTO.  Facebook, Inc. agreed to a formal settlement with Greenspan in late May 2009 and issued a press release, but the terms were not disclosed. On November 17, 2009, Rebecca Swift, on behalf of herself and all others similarly situated, filed a class action lawsuit against Zynga Game Network Inc. and Facebook, Inc. in the United States District Court for the Northern District of California for violation of the Unfair competition law and the Consumers Legal Remedies Act, and for unjust enrichment. On June 30, 2010, Paul Ceglia, the owner of a wood pellet fuel company in Allegany County, New York, filed a lawsuit against Zuckerberg, claiming 84% ownership of Facebook as well as additional monetary damages.  According to Ceglia, he and Zuckerberg signed a contract on April 28, 2003, that for an initial fee of $1,000, entitles Ceglia to 50% of the website's revenue, as well as additional 1% interest per each day after January 1, 2004, until website completion.  Zuckerberg was developing other projects at the time, among which was \"Facemash\", the predecessor of Facebook, but did not register the domain name \"thefacebook.com\" until January 1, 2004.  Facebook management has dismissed the lawsuit as \"completely frivolous\".  Facebook spokesman Barry Schnitt issued a statement indicating that the counsel for Ceglia had unsuccessfully attempted to seek an out-of-court settlement.  In an interview to \"ABC World News\", Zuckerberg stated he is confident of never signing such an agreement.  At the time, Zuckerberg worked for Ceglia as a code developer on a project named \"StreetFax\".  Judge Thomas Brown of Allegany Court issued a restraining order on all financial transfers concerning ownership of Facebook until further notice; in response, Facebook management successfully filed for the case to be moved to federal court.  According to Facebook, the order does not affect their business but lacks legal basis. In \"Young v. Facebook, Inc.\", plaintiff Karen Beth Young alleged violations of the Americans with Disabilities Act and related state laws on disability as well as breach of contract and negligence.  A District Court judge dismissed the complaint, ruling that Facebook is a website, not a physical place, so the Americans with Disabilities Act does not apply. In March 2010, Judge Richard Seeborg issued an order approving the class settlement in \"Lane v. Facebook, Inc.\" This lawsuit charged that user's private information was being posted on Facebook without consent using Facebook's Beacon program. \"Fraley v. Facebook, Inc.\" was a class-action case that alleged that Facebook had misappropriated users' names and likenesses in advertisements.  The case settled in 2013, with checks to class members mailed in November 2016. While Facebook originally made changes to its terms of use or, terms of service, on February 4, 2009, the changes went unnoticed until Chris Walters, a blogger for the consumer-oriented blog, \"The Consumerist\", noticed the change on February 15, 2009.  Walters complained the change gave Facebook the right to \"Do anything they want with your content.  Forever.\"  The section under the most controversy is the \"User Content Posted on the Site\" clause.  Before the changes, the clause read:You may remove your User Content from the Site at any time.  If you choose to remove your User Content, the license granted above will automatically expire, however you acknowledge that the Company may retain archived copies of your User Content.The \"license granted\" refers to the license that Facebook has to one's \"name, likeness, and image\" to use in promotions and external advertising.  The new terms of use deleted the phrase that states the license would \"automatically expire\" if a user chose to remove content.  By omitting this line, Facebook license extends to adopt users' content perpetually and irrevocably years after the content has been deleted. Many users of Facebook voiced opinions against the changes to the Facebook Terms of Use, leading to an Internet-wide debate over the ownership of content.  The Electronic Privacy Information Center (EPIC) prepared a formal complaint with the Federal Trade Commission.  Many individuals were frustrated with the removal of the controversial clause.  Facebook users, numbering more than 38,000, joined a user group against the changes, and a number of blogs and news sites have written about this issue. After the change was brought to light in Walters's blog entry, in his blog on February 16, 2009, Zuckerberg addressed the issues concerning the recently made changes to Facebook's terms of use.  Zuckerberg wrote \"Our philosophy is that people own their information and control who they share it with.\"  In addition to this statement Zuckerberg explained the paradox created when people want to share their information (phone number, pictures, email address, etc.) with the public, but at the same time desire to remain in complete control of who has access to this info. In order to calm criticism, Facebook returned to its original terms of use.  However, on February 17, 2009, Zuckerberg wrote in his blog, that although Facebook reverted to its original terms of use, it is in the process of developing new terms in order to address the paradox.  Zuckerberg stated that these new terms will allow Facebook users to \"share and control their information, and it will be written clearly in language everyone can understand.\"  Zuckerberg invited users to join a group entitled \"Facebook Bill of Rights and Responsibilities\" to give their input and help shape the new terms. On February 26, 2009, Zuckerberg posted a blog, updating users on the progress of the new Terms of Use.  He wrote, \"We decided we needed to do things differently and so we're going to develop new policies that will govern our system from the ground up in an open and transparent way.\"  Zuckerberg introduces the two new additions to Facebook: the Facebook Principles and the Statement of Rights and Responsibilities.  Both additions allow users to vote on changes to the terms of use before they are officially released.  Because \"Facebook is still in the business of introducing new and therefore potentially disruptive technologies\", Zuckerberg explains, users need to adjust and familiarize themselves with the products before they can adequately show their support. This new voting system was initially applauded as Facebook's step to a more democratized social network system.  However, the new terms were harshly criticized in a report by computer scientists from the University of Cambridge, who stated that the democratic process surrounding the new terms is disingenuous and significant problems remain in the new terms.  The report was endorsed by the Open Rights Group. In December 2009, EPIC and a number of other U.S. privacy organizations filed another complaint with the Federal Trade Commission (FTC) regarding Facebook's Terms of Service.  In January 2011 EPIC filed a subsequent complaint claiming that Facebook's new policy of sharing users' home address and mobile phone information with third-party developers were \"misleading and fail[ed] to provide users clear and privacy protections\", particularly for children under age 18.  Facebook temporarily suspended implementation of its policy in February 2011, but the following month announced it was \"actively considering\" reinstating the third-party policy. Facebook has been criticized for failing to offer users a feature to export their friends' information, such as contact information, for use with other services or software.  The inability of users to export their social graph in an open standard format contributes to vendor lock-in and contravenes the principles of data portability.  Automated collection of user information without Facebook's consent violates its Statement of Rights and Responsibilities, and third-party attempts to do so (e.g., Web scraping) have resulted in litigation, Power.com. Facebook Connect has been criticized for its lack of interoperability with OpenID. Facebook’s strategy of making revenue through advertising has created a lot of controversy for its users as some argue that it is \"a bit creepy… but it is also brilliant.\"  Some Facebook users have raised privacy concerns because they do not like that Facebook sells user’s information to third parties.  In 2012, users sued Facebook for using their pictures and information on a Facebook advertisement.  Facebook gathers user information by keeping track of pages users have \"Liked\" and through the interactions users have with their connections.  They then create value of the gathered data by selling it.  In 2009 users also filed a lawsuit for Facebook’s privacy invasion through Facebook Beacon system.  Facebook’s team believed that through the Beacon system people could inspire their friends to buy similar products, however, users did not like the idea of sharing certain online purchases with their Facebook friends.  Users were against Facebook’s invasion of privacy and sharing that privacy with the world.  Facebook users became more aware of Facebook’s behavior with user information in 2009 as Facebook launched their new Terms of Service.  In Facebook’s terms of service, Facebook admits that user information may be used for some of Facebook’s own purposes such as sharing a link to your posted images or for their own commercials and advertisements. As Dijck argues in his book that, \"the more users know about what happens to their personal data, the more inclined they are to raise objections.\"  This created a battle between Facebook and Facebook users described as the \"battle for information control.\"  Facebook users have become aware of Facebook’s intentions and people now see Facebook \"as serving the interests of companies rather than its users.\"  In response to Facebook selling user information to third parties, concerned users have resorted to the method of \"Obfuscation.\"  Through obfuscation users can purposely hide their real identity and provide Facebook with false information that will make their collected data less accurate.  By obfuscating information through sites such as \"FaceCloak,\" Facebook users have regained control of their personal information.  In other cases, Facebook’s revenue strategy of collecting user data and selling it to third parties is described as a \"good business.\"  This is because there have been cases where Facebook users that are \"engaged\" have been targeted with advertisements that are wedding correlated.  This is regarded as a \"good business\" because the strategy is beneficial for Facebook and the user that will be planning their wedding.  Marketing brands via Facebook can help create an organic reach as opposed to billboards because fans are directly targeted with information and ads of companies that they \"like\" or may benefit from.  Although some users or marketers may appreciate the direct marketing that Facebook has produced, users that have filed lawsuits win the case.  That is because in the end \"Facebook’s response to lawsuits and other allegations over its privacy policies has been to listen to user complaints.\" s of December 2010 , the Better Business Bureau gave Facebook an \"A\" rating. s of December 2010 , the 36-month running count of complaints about Facebook logged with the Better Business Bureau is 1136, including 101 (\"Making a full refund, as the consumer requested\"), 868 (\"Agreeing to perform according to their contract\"), 1 (\"Refuse [sic] to adjust, relying on terms of agreement\"), 20 (\"Unassigned\"), 0 (\"Unanswered\") and 136 (\"Refusing to make an adjustment\"). Facebook's software has proven vulnerable to likejacking.  On July 28, 2010, the BBC reported that security consultant Ron Bowes used a piece of code to scan Facebook profiles to collect data of 100 million profiles.  The data collected was not hidden by the user's privacy settings.  Bowes then published the list online.  This list, which has been shared as a downloadable file, contains the URL of every searchable Facebook user's profile, their name and unique ID.  Bowes said he published the data to highlight privacy issues, but Facebook claimed it was already public information. In early June 2013, the \"New York Times\" reported that increase in malicious links related to the Trojan horse malware program Zeus were identified by Eric Feinberg, founder of the advocacy group Fans Against Kounterfeit Enterprise (FAKE).  Feinberg said that the links were present on popular NFL Facebook fan pages and, following contact with Facebook, was dissatisfied with the corporation's \"after-the-fact approach\".  Feinberg called for oversight, stating, \"If you really want to hack someone, the easiest place to start is a fake Facebook profile—it's so simple, it's stupid.\" On August 19, 2013, it was reported that a Facebook user from Palestinian Autonomy, Khalil Shreateh, found a bug that allowed him to post material to other users' Facebook Walls.  Users are not supposed to have the ability to post material to the Facebook Walls of other users unless they are approved friends of those users that they have posted material to.  To prove that he was telling the truth, Shreateh posted material to Sarah Goodin's wall, a friend of Facebook CEO Mark Zuckerberg.  Following this, Shreateh contacted Facebook's security team with the proof that his bug was real, explaining in detail what was going on.  Facebook has a bounty program in which it compensates people a $500+ fee for reporting bugs instead of using them to their advantage or selling them on the black market.  However, it was reported that instead of fixing the bug and paying Shreateh the fee, Facebook originally told him that \"this was not a bug\" and dismissed him.  Shreateh then tried a second time to inform Facebook, but they dismissed him yet again.  On the third try, Shreateh used the bug to post a message to Mark Zuckerberg's Wall, stating \"Sorry for breaking your privacy ... but a couple of days ago, I found a serious Facebook exploit\" and that Facebook's security team was not taking him seriously.  Within minutes, a security engineer contacted Shreateh, questioned him on how he performed the move and ultimately acknowledged that it was a bug in the system.  Facebook temporarily suspended Shreateh's account and fixed the bug after several days.  However, in a move that was met with much public criticism and disapproval, Facebook refused to pay out the 500+ fee to Shreateh; instead, Facebook responded that by posting to Zuckerberg's account, Shreateh had violated one of their terms of service policies and therefore \"could not be paid.\"  Included with this, the Facebook team strongly censured Shreateh over his manner of resolving the matter.  In closing, they asked that Shreateh continue to help them find bugs. On August 22, 2013, Yahoo News reported that Marc Maiffret, a chief technology officer of the cybersecurity firm BeyondTrust, is prompting hackers to help raise a $10,000 reward for Khalil Shreateh.  On August 20, Maiffret stated that he had already raised $9,000 in his efforts, including the $2,000 he himself contributed.  He and other hackers alike have denounced Facebook for refusing Shreateh compensation.  Maiffret said: \"He is sitting there in Palestine doing this research on a five-year-old laptop that looks like it is half broken.  It's something that might help him out in a big way.\"  Facebook representatives have since responded, \"We will not change our practice of refusing to pay rewards to researchers who have tested vulnerabilities against real users.\"  Facebook representatives also claimed they'd paid out over $1 million to individuals who have discovered bugs in the past. In 2010, Prineville, Oregon was chosen as the site for a new Facebook data center.  However, the center has been met with criticism from environmental groups such as Greenpeace because the power utility company contracted for the center, PacifiCorp, generates 60% of its electricity from coal.  In September 2010, Facebook received a letter from Greenpeace containing half a million signatures asking the company to cut its ties to coal-based electricity. On April 21, 2011, Greenpeace released a report showing that of the top ten big brands in cloud computing, Facebook relied the most on coal for electricity for its data centers.  At the time, data centers consumed up to 2% of all global electricity and this amount was projected to increase.  Phil Radford of Greenpeace said \"we are concerned that this new explosion in electricity use could lock us into old, polluting energy sources instead of the clean energy available today.\" On Thursday, December 15, 2011, Greenpeace and Facebook announced together that Facebook would shift to use clean and renewable energy to power its own operations.  Marcy Scott Lynn, of Facebook's sustainability program, said it looked forward \"to a day when our primary energy sources are clean and renewable\" and that the company is \"working with Greenpeace and others to help bring that day closer.\" In July 2012, startup Limited Run claimed that 80% of its Facebook clicks came from bots.  Limited Run co-founder Tom Mango told \"TechCrunch\" that they \"spent roughly a month testing this\" with six web analytics services including Google Analytics and in-house software.  Click fraud (Allege reason) Limited Run said it came to the conclusion that the clicks were fraudulent after running its own analysis.  It determined that most of the clicks for which Facebook was charging it came from computers that weren’t loading Javascript, a programming language that allows Web pages to be interactive.  Almost all Web browsers load Javascript by default, so the assumption is that if a click comes from one that isn’t, it’s probably not a real person but a bot.] Facebook offers an advertising tool for pages to get more \"likes\".  According to Business Insider, this advertising tool is called \"Suggested Posts\" or \"Suggested Pages\", allowing companies to market their page to thousands of new users for as little as US$50. Global Fortune 100 firms are increasingly using social media marketing tools as the number of \"likes\" per Facebook page has risen by 115% globally.  Biotechnology company Comprendia investigated Facebook’s \"likes\" through advertising by analyzing the life science pages with the most likes.  They concluded that at as much as 40% of \"likes\" from company pages are suspected to be fake.  to Facebook’s annual report, an estimated 0.4% and 1.2% of active users are undesirable accounts that create fake likes. Small companies such as PubChase have publicly testified against Facebook’s advertising tool, claiming legitimate advertising on Facebook creates fraudulent Facebook \"likes\".  In May 2013, PubChase decided to build up their Facebook following through Facebook’s advertising, which promises to \"connect with more of the people who matter to you.\"  After the first day, the company grew suspicious of the increased likes as they ended up with 900 likes from India.  According to PubChase, none of the users behind the \"likes\" seemed to be scientists.  The statistics from Google Analytics indicate that India is not in the company’s main user base.  PubChase continues by stating that Facebook has no interface to delete the fake likes; rather, the company must manually delete each follower themselves. In February 2014, Derek Muller used his YouTube account \"Veritasium\" to upload a video titled \"Facebook Fraud\".  Within 3 days, the video had gone viral with more than a million views (it has reached 2,521,614 views as of June 10, 2014).  In the video, Muller illustrates how after paying US$50 to Facebook advertising, the \"likes\" to his fan page have tripled in a few days and soon reached 70,000 \"likes\", compared to his original 2,115 likes before the advertising.  Despite the significant increase in likes, Muller noticed his page has actually decreased in engagement – there were fewer people commenting, sharing, and liking his posts and updates despite the significant increase in \"likes\".  Muller also noticed that the users that \"liked\" his page were users that liked hundreds of other pages, including competing pages such as AT&T and T-Mobile.  He theorizes that users are purposely clicking \"like\" on any and every page in order to deter attention away from the pages they were paid to \"like\".  Muller claims, \"I never bought fake likes, I used Facebook legitimate advertising, but the results are as if I paid for fake likes from a click farm\" In response to the fake \"likes\" complaints, Facebook told Business Insider: On August 3, 2007, British companies including First Direct, Vodafone, Virgin Media, The Automobile Association, Halifax and the Prudential removed their advertisements from Facebook.  A Virgin Media spokeswoman said \"We want to advertise on social networks but we have to protect our brand\".  The companies found that their services were being advertised on pages of the British National Party, a far-right political party in the UK. \" New Media Age\" magazine was first to alert the companies that their ads were coming up on BNP's Facebook page. In August 2012, Facebook revealed that more than 83 million Facebook accounts (8.7% of total users) are fake accounts.  These fake profiles consist of duplicate profiles, accounts for spamming purposes and personal profiles for business, organization or non-human entities such as pets.  As a result of this revelation, the share price of Facebook dropped below $20.  Furthermore, there are lots of work which try to detect fake profile using automated means, in one such work machine learning techniques are used to detect fake users. In September 2008, Facebook permanently moved its users to what they termed the \"New Facebook\" or Facebook 3.0.  This version contained several different features and a complete layout redesign.  Between July and September, users had been given the option to use the new Facebook in place of the original design, or to return to the old design. Facebook's decision to migrate their users was met with some controversy in their community.  Several groups started opposing the decision, some with over a million users. In October 2009, Facebook redesigned the news feed so that the user could view all types of things that their friends were involved with.  In a statement, they said, ... your applications [stories] generate can show up in both views.  The best way for your stories to appear in the News Feed filter is to create stories that are highly engaging, as high quality, interesting stories are most likely to garner likes and comments by the user's friends. This redesign was explained as: News Feed will focus on popular content, determined by an algorithm based on interest in that story, including the number of times an item is liked or commented on.  Live Feed will display all recent stories from a large number of a user's friends. The redesign was met immediately with criticism with users, many who did not like the amount of information that was coming at them.  This was also compounded by the fact that people couldn't select what they saw. In November 2009, Facebook issued a proposed new privacy policy, and adopted it unaltered in December 2009.  They combined this with a rollout of new privacy settings.  This new policy declared certain information, including \"lists of friends\", to be \"publicly available\", with no privacy settings; it was previously possible to keep access to this information restricted.  Due to this change, the users who had set their \"list of friends\" as private were forced to make it public without even being informed, and the option to make it private again was removed.  This was protested by many people and privacy organizations such as the EFF. The change was described by Ryan Tate as \"Facebook's Great Betrayal\", forcing user profile photos and friends lists to be visible in users' public listing, even for users who had explicitly chosen to hide this information previously, and making photos and personal information public unless users were proactive about limiting access.  For example, a user whose \"Family and Relationships\" information was set to be viewable by \"Friends Only\" would default to being viewable by \"Everyone\" (publicly viewable).  That is, information such as the gender of partner the user is interested in, relationship status, and family relations became viewable to those even without a Facebook account.  Facebook was heavily criticized for both reducing its users' privacy and pushing users to remove privacy protections.  Groups criticizing the changes include the Electronic Frontier Foundation and American Civil Liberties Union.  Mark Zuckerberg, CEO, had hundreds of personal photos and his events calendar exposed in the transition.  Facebook has since re-included an option to hide friends lists from being viewable; however, this preference is no longer listed with other privacy settings, and the former ability to hide the friends list from selected people among one's own friends is no longer possible.  Journalist Dan Gillmor deleted his Facebook account over the changes, stating he \"can't entirely trust Facebook\" and Heidi Moore at Slate's Big Money temporarily deactivated her account as a \"conscientious objection\".  Other journalists have been similarly disappointed and outraged by the changes.  Defending the changes, founder Mark Zuckerberg said \"we decided that these would be the social norms now and we just went for it\".  The Office of the Privacy Commissioner of Canada launched another investigation into Facebook's privacy policies after complaints following the change. In February 2016, TRAI ruled against differential data pricing for limited services from mobile phone operators effectively ending zero-rating platforms in India.  Zero rating provides access to limited number of websites for no charge to the end user.  Net-neutrality supporters from India (SaveTheInternet.in) brought out the negative implications of Facebook Free Basic program and spread awareness to the public.  Facebook's Free Basics program was a collaboration with Reliance Communications to launch Free Basics in India.  The TRAI ruling against differential pricing marked the end of Free Basics in India. Earlier, Facebook had spent $44 million USD in advertising and it implored all of its Indian users to send an email to the Telecom Regulatory Authority to support its program.  TRAI later asked Facebook to provide specific responses from the supporters of Free Basics.\n\nList of ecoregions in North America (CEC) This list of ecoregions of North America provides an overview of North American ecoregions designated by the Commission for Environmental Cooperation (CEC) in its North American Environmental Atlas.  It should not be confused with Wikipedia articles based on the classification system developed by the World Wildlife Fund, such as List of ecoregions (WWF) and Lists of ecoregions by country. The commission was established in 1994 by the member states of Canada, Mexico, and the United States to address regional environmental concerns under the North American Agreement on Environmental Cooperation (NAAEC), the environmental side accord to the North American Free Trade Agreement (NAFTA).  The Commission's 1997 report, \"Ecological Regions of North America\", provides a framework that may be used by government agencies, non-governmental organizations, and academic researchers as a basis for risk analysis, resource management, and environmental study of the continent's ecosystems. Ecoregions may be identified by similarities in geology, physiography, vegetation, climate soils, land use, wildlife distributions, and hydrology. The classification system has four levels.  Only the first three levels are shown on this list.  \"Level I\" divides North America into 15 broad ecoregions.  \"Level II\" subdivides the continent into 52 smaller ecoregions.  \"Level III\" subdivides those regions again into 182 ecoregions.  \"Level IV\" is a further subdivision of Level III ecoregions.  Level IV mapping is still underway but is complete across most of the United States.  For an example of Level IV data, see List of ecoregions in Oregon and the associated articles. The Arctic Cordillera is one of the world's fifteen diverse ecoregions, characterized by a vast mountain chain spanning the spine of the range.  The geographic range is composed along the provinces of Labrador: including Eastern Baffin, The Devon Islands, Ellesmere, Bylot Islands, the Thorngat Mountains, and some parts of the Northeastern fringe.  The landscape is dominated by massive polar icefields, alpine glaciers, inland fjords, and large bordering bodies of water, distinctive of many similar arctic regions in the world.  Although the terrain is infamous for its unforgiving conditions, humans maintained an established population of 1000 people – 80% of which were Inuit.  In addition, the landscape is 75% covered by ice or exposed bedrock, with a continuous permafrost that persists throughout the year, making plant and animal life somewhat scarce.  The temperature of the Arctic Cordillera ranges from 6 °C in summer, down to –16 °C in winter.Vegetation is largely absent in this area due to permanent ice and snow. The Arctic Cordillera is a cold, harsh environment making plant life and animal-life sparse; even soil is rare in this ecoregion.  Moss, cottongrass, and Arctic heather are examples of plant life that can be found in valleys.  Meanwhile, polar bears, seals, and walruses roam the shores and survive off the thriving marine ecosystem.  Fish, clams, and shrimp are just a few of the resources the local Inuit communities of Nunavut use in the highly productive waters to support their economy.  Nunavut’s government is also investing in exploration of mineral resources; Breakwater Resources, for example, is a zinc-lead mine in Arctic Bay that just reopened in April 2003 after closing the year before due to declining resources.  Climate change is the strongest human influence in the Arctic Cordillera.  Rising temperatures in the Arctic are causing ice shelves, and the habitats they provide, to shrink from year to year.  Researchers of global warming also express concern for the economic, political, and social consequences of the resulting decline in fisheries stocks expected because of the changing climate. The Arctic Cordillera is one of Canada’s most inhospitable climates.  The northern part is covered by the ice caps and glaciers cover a large part of the south.  It was not always as cold as it is today.  40 million-year-old tree stumps found in 1985 on Axel Heiberg Island suggest that the area used to be warmer and wetter with much more biodiversity.  Today the weather is generally very cold and dry with a few weeks of sun and rain in the summer.  Snow is the most common form of precipitation in the Cordillera.  The region only gets 20−60 centimeters of precipitation annually.  The temperature in this ecoregion averages around 4 degrees Celsius during the summer.  In the winter the temperature is −35 degrees Celsius on average.  A polar cell is a system of winds that influence the climate of the Cordillera.  It is made up of the Westerlies, which are winds that blow warm air east to west from 30 to 60 degrees latitude up to the poles, and the Polar Easterlies, which blow cold air back south where it will repeat the process. This region can be divided up into three major areas Ellesmere Island, Baffin Island, and the coastline of the most northern part of Labrador.  Nearly 75% of the land within this ecoregion is exposed bedrock or ice.  The majority of the water in this ecoregion is locked up in frozen ice and snow, therefore there are very few named rivers or other bodies of water within this region.  The annual amount precipitation is about 200 mm, which usually falls down as snow or ice.  Huge ice caps dominate the landscape, and they spawn large glaciers that are pushed down steep fjords and into the sea.  When the temperature gets above freezing for an extended period time a little amount of runoff is created, which is generally under 200 mm annually. The Arctic Cordillera is dominated by vast mountain ranges stretching for thousands of miles, virtually untouched by man.  These mountains were formed millions of years ago during the mid-Mesozoic when the North American Plate moved northward, pushing earth and rock upwards.  The mountains of the north contain metamorphic and igneous rock, and are predominantly sedimentary rock.  On the other hand, the southern mountains are greater, composed of granite gneiss and magmatic volcanic rock.  These mountains are characterized as being highly erodible with very steep and jagged cliffs with narrow ledges.  The highest peak in the Arctic Cordillera mountain range is Barbeau Peak – standing almost nine thousand feet tall.  In general, the Arctic Cordillera Mountain Range is most similar (in composition and age) to the Appalachian Mountain Range of the United States.  However, as the Appalachian Mountains are slightly older, their cliffs have been eroded, and are less jagged than those of the Arctic Cordillera.  This ecoregion is also home to very limited amounts of exposed soil.  Only in extremely sheltered places – such as that of caves – is surface soil present.  The remaining soil is hidden beneath deep snow and ice, and is kept in a constant state of permafrost. The Arctic Cordillera is a very high stress environment for plants to try and grow and regenerate.  Vegetation is largely absent due to permanent ice and snow.  Due to the extremely cold, dry climate, along with the ice-fields and lack of soil materials, the high and mid-elevations are largely devoid of significant populations of plants.  In the warmer valleys at low elevations and along coastal margins, the plant cover is more extensive, consisting of herbaceous and shrub-type communities.  Stream-banks and coastlines are the most biologically productive areas here.  The plants in this region have a history of being survivors and stress tolerant to high winds, low temperatures, few available macronutrients like nitrogen and phosphorus.  Plants have adaptations such as fluffy seed masses, staying low to the ground, and use of other plant masses for extra insulation. Due to the harsh environments and extremely low temperatures that encompass the Arctic Cordillera, there is not a large variety of plants and animals that are able to survive and exist as a population.  However, some animal species, both herbivores and carnivores, are able to survive the extreme weather and terrain.  Among these animals are wolves, polar bears, Arctic foxes, musk-oxen, and caribou.  For the most part, the large carnivores are the dominant species in the ecoregion, mainly the polar bear.  It is the keystone species for the area due to many of its habits, including its diet and hunting strategies.  In addition, the life history of the 22,000 polar bears in the Arctic clearly defines its current existence in the Arctic Cordillera. The large carnivorous species defines the ecoregion due to its intimate relationship with the ice as well as its extremely intelligent hunting tactics.  No other predatory animal defines the Arctic Cordillera as well as the large white polar bear and that is why when people think about arctic animals, they think about the polar bear.  As long as the polar bear remains existent, it will be the keystone species of the Arctic Cordillera.  However, this existence relies solely on the degree of ice melt that is encountered in the future. The polar bear is one of the most notably affected species in the Arctic Cordillera, mainly due to their heavy reliance on arctic ice for hunting and bedding grounds.  Habitat loss, caused by global warming, has led to many dangerous behavioral changes including a new behavior called long swims.  These are swims lasting as long as ten days performed by mother bears to attempt to find food for their cubs, which generally lead to the death of the cub.  Because of their stature and aggressiveness, direct conservation practices are not very useful to the polar bear.  Instead, scientific observation to better understand these animals is the largest form of traditional conservation. The Arctic black spruce is an example of a plant native to the Arctic Cordillera that is considered to be in ecological decline.  The black spruce is a species of least concern because of habitat loss and deforestation from the spruce budworm moth.  In the Arctic Cordillera however, the black spruce population is in good health, and is slowly gaining habitat through the retreat of polar ice. Another species that is of great importance to this ecoregion is the endangered Bowhead whale (\"Balaena mysticetus\").  Five total stocks of this species exist in the region within the arctic oceans and adjacent seas: the Spitsbergen stock, Baffin Bay/Davis Strait, stock and Hudson Bay/Foxe Basin Stock, Sea of Okhotsk Stock, and the Bering/Chukchi/Beaufort Stock.  Historically, these whales have served as a cultural icon, and an important source of food and fuel to the Inuit people.  At this point in time, their populations were estimated between 30,000 and 50,000 individuals. However, with the expansion of commercial whaling in the 16th and 17th century, this species was exploited to dangerously low numbers.  Commercial hunting of bowheads was officially ended in 1921, when moratoria were established to protect the remaining 3,000 individuals left in the wild. Today, those same moratoria are still in effect, but the Bowhead population has been reinstated to a manageable population of between 7,000 and 10,000 individuals.  Nonetheless, these whales have been (and remain) on the IUCN Red List since 1984.  One of the most important conservation efforts for this species is “legal” protection by the International Convention for the Regulation of Whaling, which came into force in 1935.  This convention was further strengthened and ratified by Canada in 1977 to support the International Whaling Commission’s (IWC) recommendation for full protection of the bowhead whale.  Further conservation efforts have involved more physically demanding solutions, including the recommended funding of specialized technical machines that have the capability to remove debris that commonly kills these whales due to entanglement and accidental indigestion. One of the planet's most recent biomes, a result of the last ice age only 10,000 years ago, the tundra contains unique flora and fauna formed during the last glaciation in areas unrestricted by permanent ice.  The tundra region is found in high latitudes, primarily in Alaska, Canada, Russia, Greenland, Iceland, Scandinavia, as well as the Antarctic Islands.  Consisting of the arctic, alpine and Antarctic regions, and stemming from the Samer language, tundra literally means a \"high and dry place\". The adversity of soil and climatic conditions proves for low production levels, as well as little biomass accumulation due to slow rates of nutrient release in cold and wet soils, specifically as a result of limited nitrogen and phosphorus (Nadelhoffer et al. 1996) Additionally, there are low temperatures and strong winds in the tundra causing most vegetation to be dominated by woody plants that hug the soil.  Within the tundra, some dominant plant species include lichen, cotton grass, and Arctic willow. Lichens dominate the tundra as the regions major primary producer.  A symbiotic combination of algae and fungi, a lichen is able to survive in the harsh conditions of the tundra (Biodiversity Institute of Ontario et al. 2010).  Their unique structure and survivability makes lichen a prominent and keystone plant species in the tundra ecosystem. Cotton grass is another dominant plant species in the tundra producing much of its growth early in the summer.  Being a member of the sedge family, it forms a large part of the vegetation in the tundra because it is able to deal with harsh and cold temperatures.  This perennial plant contains flowering heads with dense brittles that are spread during heavy winds, enabling pollination (Wein and Bliss 1974).  Additionally, its survivability in the tundra can be attributed to cotton grass’s ability to photosynthesize in low temperatures and low light. The Arctic willow, commonly named rock willow, is found in the North American tundra.  Most uniquely, the Arctic willow often has long trailing branches that root where they intersect with the surface of the ground, and the roots are shallow as to thrive in the frozen ground of the tundra (Wielgolaski 1972). In addition to species such as lichens, cotton grass, and Arctic willows, shrubs, sedges, lichens, mosses, and vascular plants dominate the tundra plant community (Folch and Camarasa 2000).  Despite the tundra eco-region’s reputation of being a cold and desolate ‘polar desert’, it is actually a varying landscape supporting a diverse amount of plant and animal species. Since the tundra has such a harsh environment, the animals who live here have adapted in a way to call the tundra their home.  The keystone species of the tundra can be as small as a lemming to as large as a musk ox.  The low biodiversity means that fluctuation in individual animals can substantially affect the entire ecosystem.  The main predators of the tundra are the polar bear, the Arctic wolf and the Arctic fox.  They all have thick white coats that help them blend into their environment and stalk prey.  The polar bear spends majority of its time out on the ice hunting seals and sometimes when small rodents are scarce on land the Arctic fox will follow the bears and eat their scraps.  Wolves use teamwork to attack herds of caribou or musk ox for food.  Lemming are small rodents that fluctuate every three to four years and with their fluctuations also comes the fluctuation of their predators such as the Arctic fox and the snowy owl.  The keystone herbivores are the musk ox and the caribou.  They have thick shaggy coats that they shed during the warmer months.  Caribou use their nimble legs to escape quickly from predators while the musk ox use each other to make a fierce wall of horns.  These animals help keep each other alive as well as the ecosystem around them. The tundra is an extremely harsh, cold, windy and unique ecosystem found on the extreme north and south latitudes of our Earth.  The soil consists mostly of frozen permafrost, which makes it difficult for extended root systems to grow, water to drain and support of a wide variety of plant life.  This permafrost is also responsible for creating an extremely unusual topography.  The land of the tundra is constantly changing as permafrost and snow melts and refreezes through the changing seasons.  Land slumps and depressions occur as a result of melting permafrost that takes up less space when the soil was frozen.  Depressions that occur as a result of melting permafrost are known as thermokarst, and are often in the form of pits, funnel-shaped sinkholes, valleys, ravines and sometimes caves.  Pingos are another feature of the tundra, and can be defined as a cone shaped hill or mound of soil with a core of ice.  Lastly, polygons make up a crucial part of the tundra and are created when two large cracks create a large ice wedge and slowly slumps into itself filling with water as heat from sunlight melts the permafrost.  Often small lakes are formed from polygons on the surface of the tundra. The flora and fauna must adapt to extremely harsh conditions, however has been able to do so successfully through evolutionary change.  Many threats exist today to the tundra biome including mining, oil drilling, increased habitat loss, human habitations moving farther north and global warming which is melting more and more permafrost and changing the delicate balance of the soils.  It is imperative that we fully understand how our ecosystems function in order to monitor their stability through our changing climate. The tundra is characterized by a harsh, frost-laden landscape with negative temperatures, a lack of precipitation and nutrients, and extremely short seasons.  In the winter it is cold and dark, and in the summer when the snow and the top layer of permafrost melt, it is very soggy and the tundra is covered with marshes, lakes, bogs and streams.  Spring and fall are only short periods between winter and summer.  In the peak of winter, average temperatures can reach −30 °F.  In arctic regions, there generally is not a great difference between daytime highs and nighttime lows, as the sun generally never rises or simply hangs briefly on the horizon.  Summers in the tundra, on the other hand, are very short, in some locations only lasting a few weeks.  Daily temperatures can reach up to 60 °F but overnight lows go down into the 30s, 20s or lower, depending on the region.  This results in daily average temperatures to come out to around 50 °F.  It may rain or snow, and frost still occurs.  The average annual temperature is −18 °F.  Nights can last for weeks, and when the sun barely rises during some months in the winter, the temperature can drop to −94 °F.  During the summer the sun shines almost 24 hours a day.  Temperatures can get up to 54 °F but it can get as cold as 37 °F.  Average summer temperatures range from 37 °F to 60 °F.  The tundra is very much like a desert in terms of precipitation.  Yearly average precipitation varies by region, but generally there is only about 6 – of precipitation per year and in some regions it can have up to 20 in .  This precipitation usually falls in the form of light, fluffy snow. Due to its vulnerable state, the powerful forces of climate change, ozone depletion, air pollution, and construction threaten the tundra's survival.  The melting of permafrost increases as a result of global warming, which could drastically alter both the landscape and the biodiversity of the region.  The ozone depletion at both the North and South Poles increase the strength of ultraviolet rays that harm the tundra.  Air pollution around the world creates smog clouds that contaminate the lichen in the ecosystem, which is a major food source in the region.  The construction of pipelines and roads to obtain oil, gas, and minerals cause physical disturbances and habitat fragmentation.  There are a number of possible solutions, according to National Geographic, including switching to alternative energy, establishing protected areas and park reserves to restrict human influence, limit road construction, mining activities, and the building of pipelines in tundra habitat, and limiting tourism and respecting local cultures.  The creation of the Arctic National Refuge is an example of a measure being enacted to protect the North American tundra.  The Arctic Refuge was originally created in 1960 by the Public Land Order 2214, which was created “for the purpose of preserving unique wildlife, wilderness and recreational values” and “withdrawn from all forms of appropriation under the public land laws, including the mining but not the mineral leasing laws, nor disposals of materials”.  In 1980, the Alaska National Interest Lands Conservation Act (ANILCA) re-designated the Range as a part of the larger Arctic National Wildlife Refuge, and declared “that the ‘production of oil and gas from the Arctic National Wildlife Refuge is prohibited and no leasing or other development leading to production of oil and gas from the [Refuge] shall be undertaken until authorized by an act of Congress’”. Though species have adapted to the harsh climate of the tundra, several species have become endangered due to changing environmental factors.  Both plant species and animal species have become endangered.  The Aleutian shield fern is a plant species that has been endangered due to caribou tramping and grazing, slumping from growing substrate, and human foot traffic.  Animal species that are endangered in the tundra include the Arctic fox, caribou, and polar bears.  These animals have been endangered due to overhunting, infestation of disease, loss of diet and habitat due to climate change, and human destructive activities, such as searches for natural gas and oil, mining, and road building.  In an effort to conserve these endangered species, many regulations and standards are being put into action along with establishing prohibition of unauthorized plant collecting.  Standards are being set in regards to mining and mineral explorations.  This will help in not disturbing the habitats as much.  In addition to this, protection of caribou grounds has been established along with regulations in regards to removal of gravel roads for airstrips and road fill, which takes away from many of the animals’ critical territories. The tundra is one of the first places on Earth we have noted the effects of climate change.  As an indicator biome, the tundra is a crucial part of the whole global climate system and can help predict the changes the rest of the world will face.  The Earth depends on regulating mechanisms and air circulation patterns the tundra provides to keep climates steady worldwide.  Human-induced climate change is devastating the tundra because intense complications are present in remote areas, free from human interference.  Changes in climate, permafrost, ice pack, and glacier formations pose a serious threat to the stability of global climate because these conditions are influenced and reinforced by positive feedback loops.  Temperatures in the tundra are rising to the highest temperatures recorded in four centuries and are rising more rapidly than anywhere worldwide The land surfaces in the tundra are no longer reflecting radiation from the sun out of the atmosphere.  Soils and open water are absorbing heat from the sun and leading to more warming.  Changes in the tundra influence climate change in lower latitudes because air pressure changes are shifting global air and ocean circulation patterns.  Sea ice extent in the tundra has reached lowest recorded levels in centuries and this will dramatically affect people and wildlife worldwide.  Changes in climate will be noticed first and seen most intensely in the northern regions of the planet.  The tundra will show effects from climate change the soonest and will hopefully serve as a catalyst for action for people all over the world. According to the US Energy Information Administration, the arctic tundra holds an estimated 13% or 90 billion barrels of the world's undiscovered conventional oil sources.  However, there are a number of challenges to oil exploration, drilling, and transportation in an arctic tundra environment that limits the profitability of the venture.  Oil and gas fields in the arctic need to be large, with lots of proven reserves, because oil companies need that money to make the investment profitable.  Natural gas is a more recoverable resource than oil in tundra eco-regions.  It is estimated that there are 221.4 million undiscovered, technically recoverable cubic feet of natural gas in the Arctic.  Oil sands, often pejoratively referred to as tar sands, are a phenomenon unique to the tundra environment and are profitable and plentiful in the Athabasca region of the Alberta sands.  Oil sands consist of bitumen, which contains petroleum, found in a natural state combined with clays, sands, and water.  Oil sands must be heavily processed and refined to yield synthetic crude oil, similar to conventional crude oil.  Arctic tundra may contain minerals such as coal, copper, gold, iron, nickel, diamonds and the base feedstock for uranium oxide called pitchblende. The arctic tundra has an exceptionally short growing period, minimal sunlight and limited resources, creating a brutal environment for plants and animals.  By adapting to these harsh conditions, animals and plants represent iconic characteristics of the tundra.  Plants grow in aggregated formations which provide shelter from wind, ice and also improves seed success.  Animals have adapted with specialized organs, such as a rete mirabile, an organ that efficiently transfers heat.  Frogs and amphibians use “anti-freeze” to prevent organ damage while hibernating.  Polar bears, foxes and owls use insulated fur and feathers to protect for the cold conditions.  These complex interactions between plants, animals and abiotic factors in the tundra are held together by the permafrost layer, located 450 m under the soil.  However climate change is causing this crucial layer of frozen soil to melt.  As a result, tundra communities are becoming unstable and basic processes are breaking down.  Other factors such as oil development and drilling in tundra ecosystems has completely disheveled the wildlife and vegetation populations.  Tundra exploration vehicles used for oil development and polar bear tours (“an eco-friendly” industry) leave traces of tire marks for 20-plus years after disturbance occurs.  Other factors such as high CO2 emissions from tourism and from warming tundra soil, creates a positive feedback loop, acceleration changes to the tundra. The taiga ecoregion includes much of the interior Alaska as well as the Yukon forested area, and extends on the west from the Bering Sea to the Richardson Mountains in on the east, with the Brooks Range on the north and the Alaska Range on the south end.  It is a region with a vast mosaic of habitats and a fragile yet extensive patchwork of ecological characteristics.  All aspects of the region such as soils and plant species, hydrology, and climate interact, and are affected by climate change, new emerging natural resources, and other environmental threats such as deforestation.  These threats alter the biotic and abiotic components of the region, which lead to further degradation and to various endangered species. The main type of soil in the taiga is a Spodosol.  These soils contain a Spodic horizon, a sandy layer of soil that has high accumulations of iron and aluminum oxides, which lays underneath a leached A horizon.  The color contrast between the Spodic horizon and the overlying horizon is very easy to identify.  The color change is the result of the migration of iron and aluminum oxides from small, but consistent amounts of rainfall from the top horizon to the lower horizon of soil. The decomposition of organic matter is very slow in the taiga because of the cold climate and low moisture.  With slow decomposition of organic matter nutrient cycling is very slow and the nutrient level of the soil is also very low.  The soils in the taiga are quite acidic as well.  A relatively small amount of rainfall coupled with slow decomposition of organic material allows the acidic plant debris to sit and saturate the top horizons of the soil profile. As a result of the infertile soil only a few plant species can really thrive in taiga.  The common plant species in the taiga are coniferous trees.  Not only do conifer trees thrive in acidic soils, they actually make the soil more acidic.  Acidic leaflitter (or needles) from conifers falls to the forest floor and the precipitation leaches the acids down into the soil.  Other species that can tolerate the acidic soils of the taiga are lichens and mosses, yellow nutsedge and water horsetail.  The depth to bedrock has an effect on the plants that grow well in the taiga as well.  A shallow depth to bedrock forces the plants to have shallow roots, limiting overall stability and water uptake. Beaver, Canadian lynx, bobcat, wolverine, and snowshoe hare are all keystone species in the taiga area.  These species are keystone because they have learned to adapt to the cold climate of the area and are able to survive year round. These species survive year round in taiga by changing fur color and growing extra fur.  They have adapted to use each other to survive too.  All of the predators depend on the snowshoe hare at some point during the year.  All of the species also depend on forests in the area for shelter. Watersheds characterize much of the taiga ecoregion as interconnecting rivers, streams, lakes and coastline.  Due to a cool climate, low evaporation levels keeps moisture levels high and enables water to have serious influences for ecosystems.  The vast majority of water in the taiga is freshwater, occupying lakes and rivers. Many watersheds are dominated by large rivers that dump huge amounts of freshwater into the ocean such as the Lena river in Central Siberia .  This exportation of freshwater helps control the thermohaline circulation and the global climate.  Flow rates of taiga rivers are variable and \"flashy\" due to the presence of a permafrost that keeps water from percolating deep into the soil.  Due to global warming, flow rates have increased as more of the permafrost melts every year.  In addition to \"flashy\" flow levels, the permafrost in the taiga allows dissolved inorganic nitrogen and organic carbon levels in the water to be higher while calcium, magnesium, sulfate, and hydrogen bicarbonate levels are shown to be much lower.  As a dominant characteristic in the soil, the permafrost also influences the degree to which water percolates into the soil.  Where there is a year-long permafrost, the water table is located much deeper in the soil and is less available to organisms, while a discontinuous permafrost provides much shallower access. Lakes that cover the taiga are characteristically formed by receding glaciers, and therefore have many unique features.  The vast majority of lakes and ponds in the taiga ecoregion are oligotrophic, and have much higher levels of allochthonous versus autochthonous matter.  This is due to glacier formation and has implications in how trophic levels interact with limiting nutrients.  These oligotrophic lakes show organic nitrogen and carbon as more limiting nutrients for trophic growth over phosphorus.  This contrasts sharply with mesotrophic or eutrophic lakes from similar climates. When we look at the climate of the taiga, we are looking at average temperatures, abiotic factors such as precipitation, and circulatory patterns.  According to the study in Global Change Biology, the average yearly temperatures across the Alaskan and Canadian taiga ranged from −26.6 °C to 4.8 °C.  This indicates the extreme cold weather the taiga has for the majority of the year.  As for precipitation, the majority of it is snow, but rain is also an important factor.  According to \"The International Journal of Climatology\", precipitation in the form of rain ranged from 40 mm average in August, to 15 mm average in April over a multi-year study.  Rain is not the only kind of precipitation that affects the taiga; the main factor in precipitation is usually snow.  According to CEC Ecological Regions of North America, snow and freshwater ice can occupy the taiga for half to three quarters of the year.  A CEC Ecological Regions of North America document states that the lowest average precipitation is on the western side of taiga; can be as little as 200 mm and on the east coast it can be as high as exceeding 1,000 mm.  As for circulatory patterns, we're finding that the temperature increases have led to a seasons shift.  Global Change Biology also has noted with the change in temperature over time, as well as the overall climate change, the growing season has lengthened.  Their findings illustrate that the growing season has grown 2.66 days per ten years.  This growing season change as a result of global warming is having an extreme effect on the taiga. Climate change has played its role in threatening the taiga ecoregion.  Equally as harmful are the human effects like deforestation, however many associations and regulations are working to protect the taiga and reverse the damage.  Climate change is resulting in rising temperatures, and decreases in moisture, which cause parasites and other insects to be more active thus causing tree stress and death.  Thawing permafrost has led to many forests experiencing less stability and they become “drunken forests” (the decrease in soil stability causes the trees to lean or fall over).  Increased tree death then leads to a carbon dioxide out flux, thus further propagating the increases in global warming.  It is essential for climate change to be combated with global action, which is what the Kyoto Protocol in 1997 was created to do.  Other measures to protect the taiga would be to prohibit unsustainable deforestation, switch to renewable energy, and protect old growth forests, (they sequester the most carbon dioxide).  The taiga also suffers from more direct human effects such as logging and mining sites.  Logging has been a very profitable business in the region, however fragmentation of forests leads to loss of habitats, relocation of keystone species, increases in erosion, increases in magnitude and frequency of flooding, and altered soil composition.  Regions in which permafrost has thawed and trees have fallen take centuries to recover.  Canadian and Russian governments enacted a Protection Belt, which covers 21.1 million ha, and initiatives like the Far East Association for the use of non-timber forest products, gives economic significance to the forests while avoiding logging.  In addition to logging, studies have measured over 99,300 tones of airborne pollutants from just one metal extracting plant over a 50-year span.  These pollutants are 90% sulfur dioxide, which is a precursor to acid rain.  Other emissions include nitrogen oxides, sulfurous anhydrides, and inorganic dust.  Forests in a 50 km radius of these sites can serve little to no biological services once affected, and there has been little appearance of protection measures to regulate mining plants. The taiga is inhabited by many species, some of which are endangered, and include the Canadian lynx, gray wolf, and grizzly bear.  The Canadian lynx is one well-known animal to inhabit the North American taiga region and is listed as threatened in the U.S.  The mother lynx will have a litter of about 4 kittens in the spring.  Following the birth, the female is the sole caretaker, not letting them out of her sight until 12 months when they begin to learn to hunt.  According to the USDS Forest Service, protection for the lynx has increased since 2000, which marks the date it became protected under the Endangered Species Act.  Since much of the lynx’s habitat is land managed by the agency, efforts to maintain and increase the habitat for the Canadian lynx using forest management plans are underway. The taiga region is also interspersed with various plant species.  The endangered or threatened species include Labrador tea, lady’s slipper orchid, helleborine orchid, long leaf pine, ligonberry plant, Newfoundland pine marten, Methuselahs beard, lodgepole pine, and Scots pine.  The life history of the long leaf pine is a tree species that has been around for quite sometime, and can reach more than 250 years in age.  To begin the tree’s life, a seed falls from the parent in October to late November awaiting water to begin germination in a few weeks.  For those individuals that make it, they will enter what is known as the grass stage.  During this stage the roots are being established, and the bud of the tree is protected from fire.  Years later, the long leaf will reach about 6 – in height and the diameter will increase with time.  Somewhere around 30 years after the trees will begin to produce cones with fertile seeds and average about 110 ft at maturity.  One recent study discusses the effects of logging in the 1950s on pine species.  Since then, conservation efforts have increased the number of pine (and other) tree species.  The Nature Conservancy is prioritizing its protection efforts to rebuild long leaf pine forests through land purchases, conservation easements, and management of land sites.  Restoration is also a large part of efforts to ensure the long leaf pine remains extant.  By planting seedlings, controlling competitive vegetation, and controlled burning methods, scientists and volunteers are working to increase the number of the long leaf pine. Over the next 100 years, global annual mean temperatures are expected to rise by 1.4−5.8 °C, but changes in high latitudes where the boreal biome exists will be much more extreme (perhaps as much as a 10 °C rise).  Warming observed at high latitudes over the past 50 years exceeds the global average by as much as a factor of 5 (2–3 °C in Alaska versus the 0.53° global mean). The effects of increased temperature on boreal forest growth has varied, often depending on tree species, site type and region, as well as whether or not the warming is accompanied by increases or decreases in precipitation.  However, studies of tree rings from all parts of the boreal zone have indicated an inverse growth response to temperature, likely as a result of direct temperature and drought stress.  As global warming increases, negative effects on growth are likely to become more widespread as ecosystems and species will be unable to adapt to increasingly extreme environmental conditions. Perhaps the most significant effect of climate change on the boreal region is the increase in severity of disturbance regimes, particularly fire and insect outbreaks.  Fire is the dominant type of disturbance in boreal North America, but the past 30 plus years have seen a gradual increase in fire frequency and severity as a result of warmer and drier conditions.  From the 1960s to the 1990s, the annual area burned increased from an average of 1.4 to 3.1 million hectares per year.  Insect outbreaks also represent an increasingly significant threat.  Historically, temperatures have been low enough in the wintertime to control insect populations, but under global warming, many insects are surviving and reproducing during the winter months, causing severe damage to forests across the North American boreal.  The main culprits are the mountain pine beetle in the western provinces of British Columbia and Alberta, and the spruce bark beetle in Alaska. Taiga (boreal forests) have amazing natural resources that are being exploited by humans.  Human activities have a huge effect on the taiga ecoregions mainly through extensive logging, natural gas extraction and mine-fracking.  This results in loss of habitat and increases the rate of deforestation.  It is important to use the natural resources but its key to use natural resources sustainably and not over exploit them.  In recent years rules and regulations have been set in place to conserve the forests in order to reduce the amount of trees that are cut.  There has been an increase in oil extraction and mining throughout the United States and Canada.  Exploitation of tar sands oil reserves has increased mining.  This is a large operation that started in Alberta Canada.  Oil extraction has a direct effect on the taiga forests because the most valuable and abundant oil resources come from taiga forests.  Tar sands have affected over 75% of the habitat in Alberta taiga forest due to the clearing of the forests and the oil ponds that come from the extraction.  These tar sands also create awful toxic oil ponds that affect the wildlife and surrounding vegetation.  Oil extraction also affects the forest soil, which harms tree and plant growth. Today, the world population has an increasingly high ecological footprint and a large part of that has to do with the populations carbon footprint.  As a result of that, oil supplies have increased, which has spread across the U.S. and into other countries.  This is detrimental to natural ecosystems.  Taiga being the largest region is seeing major consequences of our actions on extracting oil and natural gas.  This is also causing climate change temperatures to increase at a rapid rate, which is affecting wildlife and forests.  However, even though Human activities are responsible for the exploitation of these natural resources humans are the solution and have the tools to fix this issue.  It is crucial that humans reduce the consumption rate of these natural resources in order to increase environmental conditions. Most of the water in this ecoregion is fresh water and contained in rivers, lakes, and ground water.  Washington, Oregon, and Idaho are mainly drained by the Columbia River, its tributaries, and other streams that flow to the Pacific Ocean.  The Columbia River Basin is the fourth largest watershed in North America.  According to a 2004 GIS inventory by the Environmental Protection Agency, there are approximately 10,535 lakes and reservoirs in the Pacific Northwest.  The largest lakes in the Pacific Northwest include Lake Washington, Lake Roosevelt, Lake Chelan, Upper Klamath Lake, Lake Pend Oreille, Priest Lake, and Lake Coeur d’Alene. In British Columbia the Fraser River watershed covers one-fourth of the land and extends from Mount Robson to the Georgia Strait and Gulf Islands.  This basin is the fifth largest drainage basin in Canada and contains thirteen main sub-watersheds, each consisting of small rivers, streams, creeks, marshes, bogs, and swamps.  The largest lake in British Columbia is Williston Lake which covers 680 square miles. Alaska contains abundant natural resources which include ground and surface water.  The southwestern part of Alaska is drained by the Yukon River and its tributaries that include the Porcupine, Tanana, and Koyukuk Rivers.  The Yukon River is the third longest river and fourth largest drainage basin in North America with a drainage area of 832,700 square kilometers.  Alaska contains over three million lakes and the largest is Lake Iliamna which covers an area of 1,000 square miles. Vegetative cover is extremely diverse within the northwestern forested mountain ecological region as the region can be broken down into different zones based on elevation, temperature and mean annual rainfall.  Alpine communities; areas of high elevation (> 8,200 feet) can support the growth of herbs, grasses, lichen, and shrubs well adapted for these harsh conditions.  Common plants here include mountain sorrel, capitate sedge, mat muhly, Newberry knotweed, and red huckleberry.  Lichens such as the witch’s hair lichen and cup lichen also persist here.  Subalpine communities; located below the alpine communities (6,500-8,200 feet) support the presence of lodgepole pine, subalpine fir, pacific silver fir, grand fir, and Engelmann spruce.  The Engelmann spruce–subalpine fir forest association occupies the greatest water-yielding areas in the Rocky Mountains and the natural adaptations of these trees are important in maintaining stable vegetation.  The mountainous slopes and rolling plains slope from about 5,500 feet at the foot of the Rocky Mountains to about 2,000 feet in the lowest elevations.  The dominant trees present in the region consist of; ponderosa pine, Rocky Mountain Douglas fir, lodgepole pine, and quaking aspen the drier southeast and central portions.  Western hemlock, western red cedar, Douglas fir, and western white pine make up the majority of the moist west and southwest portions.  White spruce is also found at this elevation and is a keystone tree species found in the Alaskan interior.  The dry southern interior grasslands and forests generally occur at low elevations (under 4000 feet) and usually have a lower canopy closure than forests at higher elevations that receive more precipitation They are characterized by very warm to hot, dry summers, and moderately cool winters with little snowfall.  Frequent low-severity, ‘‘stand-maintaining’’ fires are thought to have played a key historic role in shaping these ecosystems.  Much of this area consists of small scrub like ponderosa pine with bluebunch wheatgrass, blue grass, June-grass, and big sagebrush dominating the understory. This ecoregion is abundant with varying types of mammals, fish, and birds.  Many dominant animal species, such as the bighorn sheep and hoary marmot, have adapted to the terrain of the region.  The talus slopes provide burrowing shelters for the hoary marmot, and the bighorn sheep have adapted to climb the steep slopes in order to find shelter from predators (National Park Service).  Top carnivorous predators include coyotes, wolves, and cougars.  The grizzly bear is a keystone species found in this region.  As an “ecosystem engineer”, they regulate the species they prey on, disperse plant seeds, aerate the soil as they dig, and bring salmon carcasses into the forest (Suzuki).  The dominant fish species of the region, in which the grizzly bear preys on, is pacific salmon.  The typical bird species that can be found here include blue grouse, Steller’s jay, and black-billed magpie (Commission for Environmental Cooperation, 2008). The northern spotted owl (\"Strix occidentalis caurina\") is considered a species of utmost concern in the Northwestern Forested Mountains region.  This small raptor was listed as threatened under the Endangered Species Act of 1973.  The current population is 15,000 birds, all of which are located in North America.  Over 70% of the species’ habitat was destroyed in the 19th and 20th centuries, and the timber industry is causing that number to increase.  Both northern spotted owls and the timber industry prefer old-growth forests, so as demand for timber products increases, the spotted owl’s habitat decreases.  Forest management plans that stress limits on timber harvest and suggest alternative options are being formed, along with plans to prevent habitat fragmentation. The barred owl is also causing a decrease in the population numbers of the northern spotted owl, as they are a larger, more competitive species that have begun to use the same habitat, however, no major plans have been formed to manage this situation. Malheur wire-lettuce (\"Stephanomeria malheurensis\") is also an endangered species in the region.  Only one population of this plant survives in the wild, located in Harney, Oregon.  The self-pollinating shrub is found at high elevations in volcanic soils.  Because the range is so small, any disturbance in the habitat could be detrimental.  One of the main threats is Cheatgrass, which can expand to completely cover the ground and use up resources also needed by Malheur wire-lettuce.  It is generally agreed that in order to protect the species, efforts must be focused on forming new populations, and more importantly, maintaining the condition of the current site in Oregon. The Northwestern Forested Mountain ecoregion is rich in natural resources.  Historically the most sought after resources were the minerals found here.  The presence of gold drove much of the early settlers to this ecoregion.  These early settlers extracted gold from the streams, and timber for building, flora, and fauna.  Today many more resources are utilized by the economies of this area.  Large scale mining operations have become less common throughout the entirety of the region.  There are a few prospective industrial mines lobbying for permitting to dig in both Canada and Alaska.  Canada is the 6th-largest petroleum producer in the world.  The largest point of extraction within this ecoregion is in Alberta, Canada.  This area is abundant in tar sands, a crude form of petroleum.  In order to begin this operation large tracts of boreal forest are removed.  After the large pits are dug there is a constant risk of further environmental degradation through oil spillage.  Logging in the past was often conducted through large clear cuts.  The environmental effects of large clearcuts became apparent and are now less common.  There are logging techniques that can benefit the ecological integrity of a system.  Group selection can mimic natural processes and increase both horizontal and vertical structure to a forest.  As well as increase biotic diversity of both flora and fauna.  Tourism generates a considerable amount of revenue for the different economies of this area.  Tourists come to these areas for a multitude of outdoor activities.  In the winter tourists travel from all across the globe to ski the Rocky Mountains, British Columbia, and Alaska ranges.  In the summer the national parks draw in millions.  Other summer activities include but not limited to hunting, fishing, mountain biking, backpacking, rafting, kayaking, and wildlife viewing/ photography.  Resource use and extraction is sustainable when a system can replenish resources faster than they are being used.  A practice is unsustainable when usage exceeds this threshold thereby damaging the ecological integrity of the ecoregion. Extending from the lower Yukon of Canada all the way into northern California and Nevada, the northwestern-forested mountains range in different about three climate zones; moist maritime, arid dry, and sub arctic. The moist maritime climate of the Northwestern Forested Mountains is found along a narrow strip of coastal Oregon, Washington, British Columbia, and southern Alaska in North America.  It is formed by westerly winds coming off of the Pacific Ocean, which hit the mountains and rise to a cooler atmosphere.  This causes rainy, cloudy, and moist atmospheric conditions where up to 100 inches of rain per year can be seen, and is a temperate zone ranging from about 15 °F in the winter to about 65 °F in the winter. The arid dry zone is west of the mountain ranges and doesn't receive much rain due to the north to south orientation of the mountains, which block clouds and precipitation.  It can range from the upper 80s (°F) in the summer to single digits in the winter.  It generally only receives about 20 in of rain per year. The sub arctic region ranges from Fairbanks, Alaska to the Yukon of Canada and averages a mean of 50 °F.  in the summer and is often negative 13 in the winter.  On the mountain tops it can receive up to 100 in of precipitation per year, and often considered the snowiest place on earth. The Northwestern Forested Mountains experience phenomena called decadal oscillations, the La Niña and El Niño.  This is a shift in temperatures from warmer (La Niña) to colder (El Niño) and each phase generally last about a decade.  These phases are caused by many factors including, jet streams, trade winds, precipitation, land surface, temperature, ocean surface temperature, and sea level pressure. The biggest threats to this region are fires and invasive pests.  As fires occur, they alter the forest composition dramatically.  Fire scars create entry for heart rot and other fatal conditions.  Burned soils repel water and the runoff creates sediment and ash polluting rivers and streams, harming fish and wildlife that depend on these water sources.  An especially troubling aspect of fires’ aftermath is the increased vulnerability of trees to non-native invasive pests.  Burned stands create a perfect habitat for pests who will find shelter in the regrowth.  These pests create tunneling galleries that further weaken a tree’s ability to fend off pathogens that lead to mortality. Preventing forest fires and controlling pest populations go hand-in-hand, which leaves room for any combination of treatment plans.  Especially helpful is the use of prescribed burns, which consists of randomly dropping a match on a grid that has been divided and planted at scattered time periods.  After the fire, workers must go in to peel bark off felled logs, and, if possible, remove dead, dying, and severely damaged/stressed trees as soon as possible. The effects of fossil fuels emissions, the largest contributor to climate change, cause rising CO2 levels in the earth’s atmosphere.  This raises atmospheric temperatures and levels of precipitation in the Northwestern Forested Mountains.  Being a very mountainous region, weather patterns contribute higher levels of precipitation.  This can cause landslides, channel erosion and floods.  The warmer air temperatures also create more rain and less snow, something dangerous for many animal and tree species; with less snow pack comes more vulnerability for trees and insects. A large contributor to fire susceptible forests is past land use; the higher air temperatures make wildfires more common.  Wildfires are extremely detrimental for species inhabiting the landscape; they destroy habitats and it takes many years to restore the land to how it used to be. These effects caused by climate change can destroy animal habitats and species diversity.  Not only will these climate catastrophes directly reduce animal populations, but it will indirectly disrupt trophic levels by reducing food sources for many keystone species.  Climate change contributes to a worsening economy in this region as well by taking away valuable resources for recreational uses, like snow for skiing and fish for fishing. The region is strongly influenced by the large mountain ranges stretching throughout most of the coast.  Changes in elevation cause changes in plant/animal diversity, this can be exemplified through observing the alpine tundra's vegetation which consists of shrubs, herbs, mosses, and lichens; while lower elevations, the temperate coastal forest hold magnificently large trees such as western hemlock, California redwood, and the red alder.  These differences are in direct correlation with the availability of oxygen, and other nutrients at higher elevations.  The mountains also create rain-shadow areas due to the clouds having to release their precipitation in order to get over the mountains, or be blocked all together.  Trees, which perform better under stress, grow in these areas such as the Douglas fir (www.countriesquest.com).  As for the soil, the region generally has a thin podzol soil, causing it to be extremely acidic.  Farmers must compensate by applying fertilizers and lime to lower the acidic levels for agricultural viability.  Digging even deeper the then soil within the region will reveal mostly igneous and sedimentary rock.  Colluvium and morainal deposits make up most of the surface materials.  Mountains, which so intensely affect the region, are massive formations resulting from upheaval caused by continental collisions The climate of the marine west coast forests is humid.  According to the Köppen climate classification System, this climate is very damp throughout most of the year, receiving a great amount of rainfall along with heavy cloud cover.  The marine climate can also be defined with its narrow range of temperatures throughout the year.  Precipitation is ample and consistent in the marine west coast, with many days of rainfall and a large annual accumulation.  Many areas in the marine west coast climate have more than 150 days of rainfall per a year, along with averaging around 50 to 250 centimeters per a year of total rainfall (Britannica, 2013).  The average temperatures of areas within the marine west coast forests usually range from 10 °C to 15 °C (Britannica, 2013). These mild temperatures are in collaboration with the moderating effect of ocean bodies on air temperatures due to the constant influx of oceanic air influencing the marine west coast throughout the year (Ritter, 2009).  The marine west coast is located in the path of westerly winds from the ocean that contribute to its cloudy skies, significant amount of precipitation, and mild temperatures (Hollow, 2001).  The rainfall, seasons, and temperature are all dependent on each other and are all affected by the global circulatory pattern. The main watersheds in the region are the Puget Sound and Columbia River Watershed.  Due to the region’s proximity to the Pacific Ocean, this ecoregion experiences large amounts of precipitation annually, creating a very humid and wet climate.  The majority of river and stream activity is directly influenced by the annual precipitation patterns.  In the rainy season from October to May, most of the low elevation rivers and streams experience peak run off levels.  Rivers and streams at higher elevation are more influenced by snow melt and therefore experience peak run off from late spring into early summer due to the snowmelt.  The permeability levels of bedrock in the area of interest dictate surface water in the region.  Volcanic parent material, as found in Oregon, tends to result in lower levels of ground water due to the low permeability of the rock.  Although areas with volcanic parent material may have fewer ground water aquifers, these areas tend to have better developed stream networks and higher stream drainage levels (Moore, 765).  Areas with newer volcanic bedrock have higher levels of permeability, and are therefore more likely to have ground water aquifers.  These areas will experience lower stream drainage densities and less developed stream networks due to the greater rate of ground water recharge (Moore, 765). The plants in this region are responsible for holding the geography and geology of the area intact.  The North-South orientation of the mountain ranges combines with the moist polar air masses and mild westerlies coming eastward off the Pacific Ocean to form a weather pattern that dominates the area.  This pattern consists of a temperate moist zone on the west side of the mountains and a drier moderate climate on the east side.  The moist conditions along with glacial valleys cut by the glaciers allow for a variety of plant life to thrive. The softwood stands of the highlands are keystone species in maintaining land integrity.  The ability of the firs and spruces to populate the high altitude and shallow soil works like glue to hold the soil in place.  As you drop in altitude pines and cedars do the same for the lower slopes.  Erosion control is key to keeping the glacial valleys and their rivers free from silt build up, which has the ability to devastate the salmon population, as well as holding the integrity of the mountain ranges. Marine West Coast Forests combine aquatic ecosystems with temperate rainforests to provide habitat for an abundance of wildlife.  The sea otter is considered a keystone species because of the critical role it plays in maintaining the structure of the ecosystem.  Sea otters feed on sea urchins, which are herbivores of kelps.  A large mass of kelp can become an underwater kelp forest, which is considered by many to be one of the most productive and dynamic ecosystems on Earth.  Two more dominant species found in the Marine West Coast Forest are the gray wolf and the grizzly bear.  Both predators regulate elk populations, which tend to over-browse many shrub species in riparian zones.  With less elk browsing, the riparian zones can provide habitats for birds and help maintain a healthier marine ecosystem.  In addition, grizzly bears provide a connection between the marine coast and the forests when they eat nitrogen-rich salmon and transfer the nutrients to the forests.  The Pacific salmon provide strong sources of nitrogen for the aquatic ecosystems.  Due to the high precipitation in this Eco region, the nitrogen levels can be very low.  The Pacific salmon helps to normalize the nitrogen levels.  Without anyone of these species, the ecosystem would fall apart.  The Marine West Coast Forests are a unique habitat for a diverse group of species. Several species struggle to survive in the ever disappearing and degrading ecosystems of the northwest.  These species face a high risk of extinction; some iconic examples of those listed as threatened or endangered in this ecoregion include the giant sequoia, coast redwood, and marbled murrelet. The giant sequoia and coast redwood are listed as a vulnerable under the IUCN Red List standards (Conifer Specialist Group 1998).  Large-scale logging, felling 90 to 95 percent of the old-growth forest between 1856 and 1955, is primarily to blame for these species’ now limited range.  The remainder of most populations of giant sequoias and coast redwoods is now almost entirely in parks and reserves (Farjon & Page 1999).  Fire prevention policy, however, is most to blame for the continued declining of populations, as the build-up of undergrowth hampers the regeneration of both species (Vankat 1977).  Luckily, plans to improve management and plant trees on cleared land are in place (Farjon & Page 1999). Though the marbled murrelet is still considered abundant, its population has undergone a rapid decline, principally because the old-growth forests in which they breed are subject to logging (Piatt et al. 2006).  Current estimates are nearly half of historic numbers, suggesting just 350,000 to 420,000 remain (Piatt et al. 2007).  The IUCN has listed the species as endangered (BirdLife International 2012).  Hard forest edges resulting from forest fragmentation greatly subject murrelet nests to corvid predation and other associated disturbances (Peery et al. 2004).  Declines in areas where logging is not an issue can be explained by the overexploitation and subsequent collapse of the pacific sardine fishery.  Nylon gill-nets in shallow waters and oil spills have cause considerable mortality, as well (Piatt & Naslund 1995).  In response, conservation measures have been implemented to slow the species’ decline, including: the prevention of logging within identified breeding areas (Nelson 1997), the development of detailed research and recovery plans (Kaiser et al. 1994, CMMRT 2003, Escene 2007), and the protection of 179 square kilometers on Afognak Island by the Exxon Valdex Trustee Council (EVOSTC 1995). The Marine West Coast Forest's primary environmental threats are human development and population growth, logging, spruce bark beetle populations, and invasive species.  This ecological region is home to large cities like Vancouver, Portland, Anchorage, and Seattle.  As these cities continue to grow in population, greater tracts of land are being developed, and more resources are needed to accommodate these higher populations.  Logging is another large human induced environmental threat to the ecoregion.  Logging causes habitat fragmentation and adversely affects important species such as spotted owl, grizzly bear, and Kermode \"spirit\" bears, who all require large tracts of land to survive (Demarchi, Nelson, Kavanagh, Sims, Mann, 2013).  The spruce-bark beetle is an insect that destroys spruce trees by tunneling into the bark of the trees.  These beetles are widespread in the northern part of the ecoregion in states such as Alaska (Alaska Department of Fish and Game, 2013).  The beetle’s distribution and survival rate has increased in the last decade due to climate change.  Invasive species are also rampant in the ecoregion.  These foreign plants and animals disrupt naturally occurring species in the ecoregion.  Several solutions have been enacted to solve the environmental threats of the Marine West Coast Forest.  Public land ownership is positively correlated with environmental preservation, as seen by the parts of the ecoregion located in Alaska (Alaska Department of Fish and Game, 2013).  When land is privately owned, the most effective measures are education of the beautiful natural areas, smart land use, and planned efficient growth (Oregon Department of Fish and Wildlife, 2006). The Marine West Coast Forests are located along the coast and some islands of northern California up to Alaska.  The rise of the sea level will increase soil erosion of these marine areas (Coastal Areas Impacts and Adaptation).  Depending on to what degree the sea level will rise, the introduction of salt water to the soil in the marine forest can slow and or destroy the growth of marine forest plants as well as the habitat of forest animals (Oberrecht).  Freshwater flow will greatly disrupt the ecology of the Marine West Coast Forest.  The trend seems to be that wet regions are getting wetter and the dry regions are getting drier (Song).  The Marine West Coastal Region is a wet region that will most likely see these increases in precipitation levels. The precipitation level increasing will change the stream chemistry of vital spawning areas for salmon.  Spawning salmon are most successful when the water is cold and with a steady flow (Coastal Areas Impacts & Adaptation).  The rising temperature of the streams from rainfall instead of snowfall will be more likely to also develop and spread disease through salmon (Coastal Areas Impacts & Adaptation).  The estuaries, where the ocean and river water meet is a very vulnerable area.  The rising sea level will bring more salt water into the estuaries (Oberrecht).  The salinity of the water will increase further up rivers and this can alter the mixing and flushing rates of the estuary, increasing pollution dramatically (Oberrecht).  The change of balance in an estuary will also decrease the buffer effect that estuaries have against storms (Oberrecht). Very few places in the world have the Mediterranean climate of California.  It is one of the more rare in the world, with only five locations: the Mediterranean Basin, Southwest Australia, the Cape Province—Western Cape of South Africa, the Chilean Matorral, and the California chaparral and woodlands ecoregion of California and the Baja California Peninsula.  The region is typified by warm dry summers and mild wet winters.  This is unusual as most climates have more precipitation in the summer.  There are three variations to the Mediterranean climate in California, a cool summer/cool winter variation, a cool summer/cool winter with summer fog variation, and a hot summer/cool winter variation.  The average temperatures for the cool summer variations are below 71 °F in the summer and between 64 and 27 degrees Fahrenheit in the winter.  Average summer temperatures for the hot summer variation are above 71 degrees Fahrenheit.  Average annual precipitation for this climate is 25 – per year. Defined by the Pacific Coast on the west, the Sierra Nevada (mountains) and the deserts of California on the east, and the Northern California Coast Ranges on the north, the Mediterranean California ecoregion has unique physical characteristics that play a large role in the natural systems of the region, including hydrology. The unusual precipitation pattern of the Mediterranean climate is due to subtropical high-pressure systems in the summer and the polar jet stream in the winter.  Rainfall in the summer is uncommon because the marine layer becomes capped with dry sinking air.  The marine layer is an air mass over a large body of water brought about by a temperature inversion from the cooling effect of the water on the warmer air.  The marine layer is often accompanied by fog.  The polar jet stream in the winter brings with it rain and snow.  The jet stream is an extremely powerful air current flowing west to east often at over 100 miles per hour. The precipitation in the region is closely associated with winter frontal storms from the Pacific Ocean, which bring cool air and rain to the area.  The annual rainfall varies in different elevations, but the average range is between 400 – annually.  Much of the rain in Central and Northern California flows out the Sacramento and San Joaquin Rivers, which with numerous tributaries run through an upper part of the ecoregion. Fog is also an important aspect of the hydrologic cycle in this ecoregion; the cooling of air over the warm seawater create an dense fog that covers large areas of the coast.  This fog affects the vegetation and overall environment on the coast.  On the contrary, fire also influences this region.  The fire-flood sequence that occurs post-fire can greatly effect populations of species in the region.  The combination of the geophysical characteristics, little rainfall, and the bodies of water in the region make it a unique, distinct environment. Mediterranean climate California's geology is characterized by the meeting of the North American Plate and Pacific Plate, with much of its region near or influenced by the San Andreas Fault along the junction.  When the two plates collided the Pacific Plate was pushed under the North American Plate, and the California Coast Ranges and Sierra Nevada were uplifted.  The Coast Ranges are largely metamorphic rock formed from the submergence of the Pacific Plate, and the Sierra are uplifted granite batholiths. Not along the San Andreas Fault, the granitic Peninsular Ranges system also uplifted with the collision, and runs from Southern California, down the Baja California Peninsula, into Baja California Sur state, northwest Mexico.  The Transverse Ranges are another major Southern California mountain system primarily in the Mediterranean climate zone.  Large earthquakes can do considerable damage to populated areas, and to the state's water, transportation, and energy infrastructure. The Central Valley of California is a significant feature of Mediterranean climate California.  It was an ancient oceanic inlet that eventually sediment filled in, the deposition supplied by erosion of the surrounding mountain ranges.  The soil is composed of both the metamorphic, oceanic crust-like Coastal Range sediment and the mineral-rich granitic Sierra sediment.  The combination creates very fertile soil.  The flatness and fertility of the soil, along with the almost year-round sunshine has attracted much agriculture to the area.  As a result, native species no longer dominate the landscape.  The southern portion, named the San Joaquin Valley, also produces two-thirds of California’s oil from underground reserves.  Fossils are found where adjacent tar pits occur. The Mediterranean California ecoregion, is well known for its large variety and abundance of animals.  One of these important animals is the American golden eagle, which plays a massive role in maintaining the ecoregion’s ecosystem through its top-down predation on smaller, more abundant animals.  The golden eagle is considered to be the apex predator of this community, and there are no other species bigger than them on the food chain.  Their lifespan can be up to around 30 years in the wild and even longer in captivity.  Native to mountain areas and grasslands, California is a great region for this bird of prey to thrive in.  The main reason for the golden eagle being a keystone species of this ecoregion is their ability to keep small herbivorous mammal populations in line.  “Prairie Dogs, ground squirrels, other rodents, hares, and rabbits, all of which eat grass and seeds, constitute 77.9% of the golden eagles diet.\"  They also are known to prey on animals such as, cranes, black-tailed jack rabbits, swans, deer, coyotes, badgers, mountain goats, bobcats, and various fish species. Another less popular species, but yet still keystone to this region, is the kangaroo rat.  Studies have shown that kangaroo rats play very large roles in maintaining the population sizes and animal diversity throughout the region.  Although they are small and on the verge of extinction, these animals play a large role in maintaining plant diversity, which helps the various herbivores with food supply, and also protection for other small animals seeking shelter.  kangaroo rats occupy many land habitats ranging from desserts, and grasslands, to chaparral areas making them present in all areas of the Mediterranean California ecoregion.  Kangaroo rats like to feed on many various grass seeds, as well as mesquite beans and thus is the reason that plants tend to not grow as well when sharing the same community with these rats.  On occasions though, these animals like to feed on green vegetation, and insects.  Unfortunately for the rat though, it is preyed upon by many predators.  These predators include, owls, snakes, bobcats, foxes, badgers, coyotes, cats and dogs, and many more.  Other dominant species in the region include, mountain lions, coyotes, sea otters, brown bears, and various large birds of prey. The vegetation in the Mediterranean California ecoregion is a mixture of grasses and shrubs called chaparral with some oak forests as well.  This area is very highly populated and agriculture is prevalent in the valleys (Comm.  of Env.  Coop.  2011).  Evergreen trees and shrubs—such as heaths—mainly dominate Mediterranean vegetation with a shrubby to herbaceous understory.  Mediterranean vegetation embodies less than 5% of terrestrial ecosystems around the world.  A very important aspect of this ecosystem is its frequent wildfires leading to most of its vegetation adapting fire response mechanisms (Vilà and Sardans 1999).  Common shrubs within this region are chamise or greasewood (\"Adenostoma fasciculatum\"), manzanita (\"Arctostaphylos\" spp.) , coast sagebrush (\"Artemisia californica\"), and California-lilacs (\"Ceanothus \"spp.)  (Conrad 1987). Because the climate is so dry and experiences frequent fires, competition is high among plants within this ecoregion.  The Mediterranean community found in southern California is said to have a successional stage after wildfires.  The fire leaves patches of bare ground which then are quickly filled with newly germinated seeds.  Native and introduced herbs persist for the first year following a fire.  Shrubs and subshrubs slowly fill in and hit their peak at four to eight years after the fire.  Extinctions, unlike many other communities are frequently the cause of environmental extremes rather than competitive invasive species (Zedler et al. 1983).  Human disturbance can increase wildfires with the introduction of grasses such as Bromus rubens which can be readily established in the newly burned, cleared patches.  These grasses are more densely compacted and create more fuel for fires.  Agricultural grazing can also greatly decrease the chaparral (tangled shrubby brush habitat), which is the home of many native endemic species (Fleming et al. 2009, Zedler et al. 1983). An endangered species is a species of organisms, either flora or fauna, which face a very high risk of extinction in a proximate time frame, much sooner than the long-term horizon in which species typically persist.  There are many species of birds, mammals, reptiles, amphibians and plants that live in the Mediterranean California chaparral and woodlands ecoregion.  Yet due to a variety of factors including habitat loss due to the 30 million humans who share the land, some species are endangered. Endangered, threatened, and vulnerable species of the Mediterranean California chaparral and woodlands ecoregion include: The California condor (\"Gymnogyps californianus\") is one of the most iconic species in the state.  With over a 9 ft wingspan, condors are the largest flying land bird in North America.  They are opportunistic scavengers that prey on large dead mammals.  The main factors that led to the species endangered status were settlement of the west, shooting, poisoning from lead and DDT, egg collecting, and general habitat degradation.  Serious conservation efforts have been made since the 1960s and this severely endangered species has begun a recovery path.  A condor recovery program has been started and a wild population is steadily growing. Another species is the tiny and secretive San Joaquin kit fox (\"Vulpes macrotis\" subsp.  \"mutica\") is one of the most endangered animals in California.  The kit fox is the size of a cat, with big ears, a long bushy tail and furry toes that help to keep it cool in its hot and dry Californian Mediterranean environment.  Biologists state that there are fewer than 7,000 San Joaquin kit foxes.  San Joaquin kit fox populations rise and fall with the amount of annual rainfall: more rain means more kit foxes.  Changes in precipitation patterns, including reduced rainfall and increase changes of drought, all caused by climate change, would affect San Joaquin kit fox populations.  The change in the Central Valley from open grasslands to farms, orchards, houses and roads has most affected San Joaquin kit foxes, causing death, illness, injury, difficulty in finding a mate and difficulty in finding food.  These kit foxes also are killed and out competed for resources by coyotes and red foxes.  Another threat is poison used to kill rats and mice.  A recent decision by the federal government to limit to use of these poisons outdoors may keep kit foxes safe. Humans have used resources of this ecoregion for many years, dating all the way back to early Native Americans.  Some traditional resources that are still used today are in danger of being overharvested.  These include the Pacific Ocean fisheries, the dwindling timber industry, the rivers flowing from the mountains and the grasslands.  All of these resources are either being over harvested or destroyed through agricultural and industrial development.  Grasslands hold many native oak trees that are being lost due to overgrazing or forest fires.  The overgrazing is attributed to the increasing number of cattle farms while the forest fires come from the use of natural water for human and agricultural use.  As more water is used, oak trees lose out without this key component and fires increase due to drying out of the grasslands and forests.  The government has tried to install conservation programs to halter the increased use of the land and waterways, but more must be done to create a truly sustainable environment. Emerging resources from the region are mainly high value agricultural crops.  These include stone fruits, sugar beets, rice, nuts, grapes, cotton and specialized cattle systems.  Many of these cannot be grown in other parts of the country and thrive in this type of climate.  However, because of the dry seasons, these products require large amounts of water as well as varied chemicals and fertilizers to increase production.  Many of these farming enterprises are enormous and not sustainable.  They leach out chemicals, bring in mass amounts of inputs, and degrade a lot of the land.  As with the traditional resources, the government has implemented conservation programs, but only a limited amount. Climate change in the Mediterranean California ecoregion is expected to ultimately have negative effects on the ecosystem and the region's biodiversity.  The coast of California is expected to warm by as much as 2 °C in the next 50 years.  This is going to cause hotter and drier seasons; the normally wet winters (when a majority of the ecosystem's rain in received) will be drier, and the summers will be especially hotter as well.  Increased wildfires will result from the region's warming – mainly in the summer.  The shrubbery and trees characteristic of the California chaparral will not fare well in the warmer (and increased fire) region; grasses that are able to regrow asexually or from special off chutes will fare the best.  Ultimately the soil quality is going to degrade due to the increased burnings and increased temperatures.  Overall, climate change does not bode well for the Mediterranean California ecosystem. There are several large threats to this region.  Many of California’s large population centers are located within it which causes stress on the surrounding environment because people have a desire to move to California so new homes and industry have to be established in order to accommodate all of the people moving into the region and this requires expansion.  Research shows that this eco region is already 20% urban environments and 15% agricultural lands.  The research also concluded that population density and urban area has increased by 13% between 1990 and 2000 while agricultural lands in the region have only expanded by 1%.  The study conducted also showed direct relationships between the growth of the population and the number of species that were threatened in the area.  Expansion will break up the contiguous landscape and move humans closer to the native flora and fauna which will over pressure species that need large open tracts of land to thrive and harm the species diversity of the region.  Prevailing winds coming from the west off of the Pacific Ocean all of the pollution created gets carried up to these higher inland sites and causes the species there to suffer with the pollution generated. The region is also plagued by wildfires.  The area is becoming arid species diversity will drop as organisms adapted for dryer climates thrive.  No current management plans are in place, a Species refugia to save struggling species that inhabit this region has been proposed by some.  Forests similar to these are more resilient to such events due to the spatial arrangement, it would be possible to replicate this in the current forest and make it resilient to the fires that will increase in the near future. 25 Feb. 2013. The Eastern Temperate Forests of North America are a vast and diverse region.  Stretching inland from the Atlantic coast about 385 miles (620 km), they reach from Michigan in the north and Texas in the south; they cover the land of New England to Florida, Alabama to Michigan, and Missouri to the Appalachian Mountains.  This ecoregion enjoys a mild and moist climate, though it is generally warmer as latitude decreases and drier as longitude increases.  Warm summers and mild to cool winters have provided favorable growing conditions for a number of plant species, the dominant being large, broadleaf, deciduous trees and (to a lesser extent) needle-leaf, coniferous, evergreen trees.  Indeed, before the arrival of Europeans, this area was almost completely forested.  After their arrival a few centuries ago, much of the eastern forests had been cleared for timber and to make way for cropland.  In more recent time, however, these open areas have been abandoned and are slowly returning to forest.  Although heavily influenced by people, the Eastern Temperate Forests have proven to be a very resilient region; these great forests still provide habitat for many birds, animals, reptiles, amphibians, and insects, as well as recreational and economic benefits for the people of the region. The Eastern Temperate Forest region has a wide range of fluctuating temperatures dependent on time of year.  In this region, there are four distinct seasons- winter, spring, summer, and fall.  This seasonal variation is caused by exposure to both warm and cold air masses due to the biomes mid-latitude positioning between the polar regions and the tropics and is reflected in both the seasonal temperatures and precipitation levels.  The highest temperatures, averaging 21 °C, occur during the summer months of July and August, and the lowest temperatures, averaging 0 °C, occur during the winter months of December, January, and February.  The year-round average temperature within the region is 10 °C.  Levels of precipitation vary with the seasons as well, with the highest levels of precipitation, averaging 95 mm/month, occurring in May and August, and the lowest, averaging 60 mm/month, occurring in June and the winter months of January, February, March, and December.  The Eastern Temperate Forest region can thus be described as “warm, humid, and temperate” with abundant levels of precipitation year-round. There are many global patterns that affect and contribute to the climate of the Eastern Temperate Forest region, such as global ocean currents, El Nino, La Nina, the Gulf Stream current, and global air circulation patterns.  El Niño, caused by warmer sea-surface temperatures in the Pacific Ocean, can lead to “wet winters” and warm episodes occurring between the months of December and February in the southeastern region of the United States Eastern Temperate Forest.  La Niña is caused by cooler than normal sea-surface temperatures in the central and eastern tropical Pacific Ocean, it leads to drier than normal conditions in the winter months in the Southeast region of the Eastern Temperate Forest.  The global ocean current that effects the Eastern Temperate Forest most is the Gulf Stream current which brings a warm flow of water from South to North along the eastern coast of North America in the Atlantic Ocean, it keeps temperatures in this region relatively warm.  The winds that have the greatest effect on the climate of the region are the prevailing westerlies and the tropical easterlies.  The prevailing westerlies, caused by the Coriolis Effect, explain why most major events that occur in North America come from the west and proceed east, which is where the majority of the Eastern Temperate Forest is located. The Eastern Temperate Forest Ecoregion has favorable growing conditions for a number of plant species, the dominant being large, broadleaf, deciduous trees.  Before the arrival of Europeans, this area was almost completely forested.  After their arrival a few centuries ago, much of these forests had been cleared for timber and to make way for cropland.  In more recent time, however, these open areas have been abandoned and are slowly returning to forest.  Of the many plant species that inhabit the Eastern Temperate Forests today, those of the oak (Quercus), beech (Fagus), maple (Acer), basswood (Tilia), and pine (Pinus) genera are the most characteristic and defining of this ecoregion.  These plants can be broken down into several main communities: northern hardwood, beech-maple, maple-basswood, mixed mesophytic, oak-hickory, and southern mixed hardwood forests.  With the exception of Pinus, all of these species are angiosperms, meaning that they produce flowers and fruits, an important food source to many animals who inhabit the region.  The flowers of angiosperms provide nectar, their leaves are important vegetable matter for herbivores, and their seeds are rich in fat and protein rich that allow many animals to fatten up for their winter hibernation.  The trees of the Eastern Temperate Forests provide food, shelter, and a suitable habitat for countless species of both flora and fauna; they yield lumber, fuel, recreation, and aesthetic enjoyment to not only the people who live in this region, but also those who visit and enjoy products produced from the resources gleaned from these vast forests. Arboreal species are widely found in the region due to the high density of tree cover, providing a suitable habitat and food source for the animals; this includes birds and many ground squirrels.  Migratory songbirds are common in the eastern temperate forests once the canopy opens up in the spring.  Mammals that are native to the eastern forests are white-tailed deer, black bears, ground squirrels (gray squirrels and chipmunks), as well as red and grey foxes.  Bird species include, the black-throated warbler, piping plover, and the yellow- breasted chat.  Amphibious species that are common to the region are the American toad and the box turtle. White-tailed deer populations are very large across the eastern US, making it both a dominant and defining species.  The white-tailed deer competes with other herbivores for limited food resources directly affecting the ecosystem, as well as indirectly affecting the area by altering habitats for small vertebrates and mammals.  According to the Virginia Journal of Science’s research on white-tailed deer, deer are grazers primarily, feeding on the leaves of shrubs and such; however in the winter months they are found browsing the woody stems of shrubs and saplings.  White-tailed deer have four stomachs, each with their own specific digestive action.  The complex breaking down of food allows the deer to each woody plants and other things that most animals cannot digest.  Areas with high deer populations, will see a dramatic shift in forest cover because small saplings and shrubs growth with be retarded on hindered due to their browsing habits.  White tailed deer are polygamous; in the northern parts of the region they will mate in November and for more southern dwelling populations mating occurs in January.  A female will give birth to one to three fawns, after a 6-month gestation period.  After about 3 months, the young will leave their parents.  White tailed deer typically live about three years but can live up to 15 years.  White-tailed deer exemplify a “k-selection” species.  They have long gestation periods, can reproduce more than once in a lifetime and are only a few offspring are produced at once. The United States has more endangered species than all of the other continents combined, the Eastern Temperate Forest’s endangered and threatened species make up a little less than a quarter of that number.  Endangered and threatened mammals (but not limited to) include, the Louisiana black bear, the red wolf, the Key deer, the eastern puma (cougar) the West Indian manatee, the North Atlantic right whale, the Mississippi sandhill crane, the piping plover, and the leatherback sea turtle.  Endangered and threatened flowering/non-flowering plants include, the Virginia round-leaf birch, the Tennessee yellow-eyed grass, the Michaux's sumac, the Florida torreya and the Louisiana quillwort, among many others.  The region is also home to the only two endangered lichen species, rock gnome lichen and Florida perforate reindeer lichen. The piping plover is a bird that has been on the endangered species list since 1985 in the Great Lakes watershed (including: NY, PA, IL, MI, and WI.)  This species nearly became extinct after over hunting in the 19th and early 20th century due to use of feathers for fashion hats.  Current potential sources of endangerment include, the development of coastlines for recreation, and detrimental material washing up to shore.  The management of the habitat sites, closing off sections of the beach where birds are nesting, creation of a mimic habitat, predation management, restriction of beach vehicles, and vegetation control are current conservation efforts being enforced. The Louisiana quillwort has been on the list of endangered species since 1992; contrary to its name it is only now found in MI and AL.  Threats to this species include, pollution (herbicides and chemicals), construction in proximity to stream, vehicle traffic on or near stream, changes in flow rate and erosion (these two factors most likely caused from climate change.)  Conservation efforts being enforces are, updates to where the population status is, permanently protecting existing habitats (through local and federal levels), look for potential populations that are not accounted for, preserve the genetic stock of the species remaining, and more in-depth habitat studies leading to population fluctuation. The Appalachian Mountains are a main topic of research, regarding the geology of the surrounding area.  They formed when the ancestral continents of North America and Africa collided together and are about 480 million years old.  The folded and thrust faulted igneous rocks, marine sedimentary rock and rocks that look like that of the ancient ocean floor, reveal that they got pushed up during plate collisions. Ice ages, during the Pleistocene epoch (after the Appalachians formed), contributed a great deal to the current appearance of the surrounding area.  Surfaces that were once covered by ice were eroded and smoothed out during glacier movement.  Therefore, the Appalachians used to be much taller when they formed, than they are today.  Glaciers also deposited parent materials of the underlying bedrocks, which contribute to the formation of soils later on. There are very clear soil horizons, when looking at a cross section of this land.  These are labeled and described (see Figure 2) as: O: organic matter, A: fine particles of organic matter and mineral material, B: material layer where most nutrients accumulate, C: parent material, and R: bedrock1.  The U.S. Soil Taxonomy classifies Inceptisols, Mollisols, and Spodosols as good soils that can support temperate forests that like mature soils that can support deep root systems1.  Different levels of nitrogen also have a big effect on a soils capability of supporting life.  The presence of too much nitrogen can cause declines in species richness and abundance.  The types of vegetation that exist in the Appalachian area heavily rely on the existing soil types and amount of nutrients available. The Eastern Temperate region has a vast wealth of natural resources that are utilized by people.  The two most common traditional resources include timber and coal.  Timber specifically hardwoods, which make up the majority of timber from this region, are utilized widely for furniture production.  In 1997 there was about 6 billion dollars worth of solid wood exports with 36% coming from the eastern United States.  Coal is the other major traditional resource of the region.  Coal is found on the western slopes of the Appalachian mountain range as well as in parts of Illinois and Indiana.  In 2003 U.S. coal production was about 1.07 billion short tons and while not all of this comes from the eastern region a large portion of it does as 6 of the top 10 coal producing states are from within this region as of 2012. Natural gas and oil from hydraulic fracturing is an interesting relatively new emerging resource from the region.  “Fracking” as it is commonly known involves sending pressurized water or sand into shale deposits into order to open up more cracks for which natural gas and oil can flow through, into the pipes and out of the ground.  There were 8.982 drills as of 2011 in Pennsylvania alone that operated under hydraulic fracturing.  Though this is an intriguing emerging resource for the region it also is extremely controversial as oil and gas from the “fracking” process can sometimes seep into ground water and contaminate it. There are three major current threats to the Eastern Temperate Forest.  These include agriculture, invasive species and overpopulation/urbanization.  A major use of land in the eastern temperate forest is for agricultural purposes due to the rich soils which are easily converted to farmland.  Pesticides in particular threaten the health of the eastern temperate forest region because they are used in massive quantities for agricultural production but are also widely popular in homes, businesses, schools, hospitals, and parks to maintain lawns or fields. Another problem with no easy solution that faces the eastern temperate forest are non-native invasive species like the emerald ash borer.  The emerald ash borer is thought to have been introduced to Michigan from China about 15 years ago.  The adult beetles target ash trees as places to lay their eggs, when the larvae hatch they bore through the bark and kill the tree.  The health of the ash population is of major concern because they provide habitat for many wildlife species and edible seeds for birds, mammals, and insects. The biggest threat besides climate change to the eastern temperate forest is its high density of human inhabitants.  According to the Commission for Environmental Cooperation approximately 160 million people or over 40 percent of North America’s population, lives within the ecological region of the eastern temperate forest12.  Such population density can be attributed to the concentration of the continents economic, political, and industrial power in this region.  Major cities and sprawling suburban communities between them have drastically changed the regions landscape and fragmented local habitat.  Roads and highways divide habitat and limit migration while urbanization and deforestation completely eliminate suitable habitat and food sources.  Studies conducted by Kansas State University have shown that fragmentation can decrease population productivity by isolating populations, crowding species, and causing edge effect. As the planet faces more intense and severe effects of climate changes, each element of the eastern temperate region will be affected, from flora and fauna, to soil and water.  Vegetation mortality, soil content, species existence, water levels, and overall functionality of the Eco region will continue to change and be altered as global warming and the concentration of greenhouse gases increases.  Climate change correlates with disturbances such as insect outbreaks, harsh weather, and susceptibility of forests to invasive species, all of which can affect the functions of a forest.  Insect breakouts can completely destroy an entire habitat within one season.  With increased drought and higher temperatures, the weakened forest can suffer from multiple tree species loss, along with the loss of animals and creatures that serve vital predatory roles within the ecosystem.  Plants that are considered to be moist-forest herbs, such as Cohoosh and Clintonia, are threatened by the lack of available water that is vital to their survival.  As climate change more rapidly progresses, temperature increases will affect the length of the growing season.  Tree species growing range will shift to adapt to the new climates, typically moving to higher altitudes or more northern regions.  For example, mountaintop tree species like the red spruce will potentially die out because there is no higher altitude that is available for relocation.  In addition to the northern migration, southern species such as the red oak have expanded their territories.  Therefore, as species that thrive in the lower areas of the region are expanding into a greater space, they are beginning to compete for resources and nutrients with pre-existing native species.  This can be said for many bird species as well.  A study conducted by the USDA Forest Service confirms that 27 out of 38 bird species that inhabit eastern temperate forests, have expanded their territory further north.  The water cycle is also incredibly susceptible to the effects of climate change.  The water quality and ecosystems within lakes, streams, and rivers are all greatly affected by the alterations of precipitation patterns.  Increases in runoff potentially increase the chemical contents within the water, such as nitrate and acid pulses.  Aquatic species are stressed by not only the warmer temperatures themselves, but also the low flows and timing of ice-outs and thaws.  Such factors affect oxygenation cycles, productive cycles, and reproductive cycles.  Seeing as though the Eastern Temperate Forest region is considered to be a significant evolutionary zone for fauna, the effects of climate change can substantially alter the balances and chains of not only the Ecoregion, but the planet as well. The Eastern Temperate Forest ecoregion is divided into five Level II ecoregions: Mixed Wood plains, Central USA plains, Southeastern USA plains, Ozark and Ouachita- Appalachian Forests, and Mississippi Alluvial and Southeastern Coastal Plains. The land formation of the 490590 km2 area of the Mixed Wood plains is predominantly plains, with some hills, and the bodies of water are many small lakes.  The surface materials of the region are moraines and lacustrine and the soil composition includes forest soils and fine textured soils.  The mean annual precipitation of the area ranges from 720 – and the mean annual temperature generally varies between 4–10 °C.  In this area, human activity includes fruit and dairy agriculture, major urban areas, and some forestry and tourism attractions.  The most prominent wildlife observed are white tailed deer, moose, and the grey squirrel, and vegetation includes a wide range of trees such as oak, hickory, maple, beech, and some pine and basswood species. The second sub-ecoregion is the Central USA Plains, anarea of 253665 km2 , that has a landform of smooth plains.  The majority of this region’s surface material is moraine with some lacustrine, and the soil consists of calcium enriched prairie soils and forest soils on moraine.  The climate consists of a mean annual precipitation of 760–1,100 mm and average temperatures varying from 7–13 °C.  Human activities largely include corn and soybean agriculture, major urban areas, and local dairy operations.  Vegetation is mostly prairie type in the west, but also includes oak, hickory, elm, ash, beech, and maple.  White tailed deer, cottontail rabbits, and grey squirrels are the most commonly represented wildlife. The Southeastern USA plains are the third Level II ecoregion and have a land area of 946770 km2 .  The majority of this land consists of irregular plains with low hills, which is made up of predominantly residuum and some loess on weakly developed soils.  The climate of this region is an annual precipitation of 1000 – and average temperatures of 13−19 °C.  Human activities include predominantly forestry with tobacco, hog, and cotton agriculture, along with major urban areas.  There is a wide array of wildlife which can include white-tailed deer, grey squirrels, armadillos, wild turkeys, northern cardinals, and mockingbirds.  The vegetation of the area is less diverse and includes oak, hickory, loblolly, and shortleaf pines. The Ozark and Ouachita-Appalachian Forests region is an area mostly consisting of hills and low mountains, with some wild valleys that make up the 518690 km2 of land.  This land is primarily residuum and colluvium matter on weakly developed soils and is put to use by humans through forestry, coal mining, some local agriculture, and tourism operations.  The temperature averages around 17–18 °C annually and precipitation can be anywhere from 1000 – , which provides a suitable environment for mixed oaks and hickory, white pine, birch, beech, maple, and hemlock trees.  In this environment, black bears, white tailed deer, chipmunks, and wild turkeys are commonly found The final of the five Level II ecoregions in the Eastern Temperate Forest is Mississippi Alluvial and Southeastern Coastal Plains.  The 368720 km2 of land in this region is home to a very vast amount of organisms including animals such as white-tailed deer, opossums, armadillos, American alligators, mockingbirds, and egrets, along with varying vegetation from bottomland forests (ash, oak, tupelo, bald cypress) and southern mixed forests (beech, sweet gum, magnolias, oaks, pine, saw palmetto).  The climate of 13−27 °C and precipitation varying between 1100 – annually provides adequate conditions for forestry, citrus, soybean, and cotton agriculture, fishing, and tourism. The Tropical Wet Forests ecoregion in North America includes the southern tip of the Florida Peninsula in the United States; within Mexico, the Gulf Coastal Plain, the western and southern part of the Pacific Coastal Plain, most of the Yucatán Peninsula and the lowlands of the Chiapas Sierra Madre, which continue south to Central and South America. The tropical wet forests of North America have an average year round temperatures between 68−78.8 °F.  Thus, frost does not occur under these conditions.  The temperatures remain fairly uniform throughout the year; therefore there is not a change of seasons.  There is also no dry season, as all months experience precipitation.  The average annual precipitation ranges from eight to fourteen feet per year.  The high levels of precipitation usually cause poor soil quality because soluble nutrients are lost due to the nutrient leaching process.  The average humidity is between 77−88%.  Nine out of twelve months of the year are considered “wet” months.  The overall climate of the tropical wet forests ecoregion can best be described as humid, warm, and wet.  George Hadley, a scientist who researched during the 18th century suggested that warm tropical air rises and moves north.  Colder high latitude air flows south nearer to the Earth’s surface where it displaces the former air.  Hadley’s explanation is highly accepted and still expanded upon today.  The warm, moist air in tropical wet forests is unstable; meaning as soon as the air rises it becomes saturated.  In addition, there are large amounts of heat, or convection occurring at the same time.  The vast bulk of vertical movement of air occurs in the Hadley cell and thus provides an explanation for the global circulation patterns. The direction of the wind at various levels of the atmosphere determines local climate and can result in severe weather patterns.  For example, in an El Nino winter the presence of warm water in the eastern Pacific Ocean can shift the position of a subtropical jet stream.  This results in heavy rainfall in the tropical wet forest ecoregion.  Also, in a warming climate the Hadley cell could increase the severity of climate.  As a result, the ecoregion may become hotter and wetter for longer periods of time. Hydrology in Tropical Wet Rainforests keeps the flow of water throughout the ecosystem, which nourishes plants, keeps local aquifers and rivers active and the environment in the area functioning.  The watershed and basin pattern have three major contexts; first, low-gradient drainage, second, typically high ground water table, and third, extensive drainage canal network.  This idea applies to all areas, but have unique outcomes in Tropical Wet Rain Forests in North America specifically.  Tropical Wet Rainforests have an excess of vegetation, compared to many other ecoregion types such as savannahs, and therefore have a much slower drainage rate than other ecosystems.  When an ecosystem has a high ground water table it separates the time between drainage and absorption of water in an area.  It helps organisms to absorb nutrients, while also slowly filling up aquifers in the ecosystem.  So primarily the down time between rainfall and drainage is slowed due to vegetation and climate, but now due to the vastness of the ecosystem, the drainage canal network is large and water can fall in one place, and end up in many other places at the end of the draining process. Wet tropical forests in North America span from sea level to an altitude of 1000 m .  They have particular geologic, topographic and soil conditions that characterize them.  These characteristics influence biotic structures and relationships and have contributed to the high biodiversity of the ecoregion. The geology of these forests is primarily composed of folded and metamorphic hills, which are covered by a thin layer of alluvium (loose sediments and soil).  The bedrock is sedimentary and rich in silica and dates back to the Precenozoic periods when much of the region was underwater. The topography of wet tropical forests includes valleys, hills, ridges and low mountains.  Depending on elevation and the location of such features, areas as referred to as either lowland or highland.  These elevation and topographical changes allow for a higher variety of specialized conditions, which increases habitat.  The inclination changes (or slope) of the forest floor greatly affects water drainage and the leaching of nutrients, and valleys can have an accumulation of sediments and nutrients versus plateaus and ridges.  But the most important topographic characteristic is the extensive network of rivers that weave across the landscape, acting as a drainage system to the forest that can receive upwards of 250 inches of rain a year. The soils in wet tropical forests are some of the most diverse of any region, and they are the cause for many biological adaptations.  There is a combination of highly weathered and leached soils as well as less weathered alluvial soils, categorized as “oxisols” and “ultisols”.  Their pH can vary immensely, sometimes being as acidic as 4.0.  The soils are generally shallow, often only a few inches deep. The soil is produced from decomposing organic matter and the breakdown of bedrock, but is generally poor in nutrients; most nutrients are found as superficial detritus and within the living components of the ecosystem.  There are multiple reasons for why the soil is generally very poor in nutrients.  Firstly, the warm and humid climate allows for a rapid decomposition rate, meaning that nutrients do not stay present in or on top of the soil for long before being absorbed by the biota.  Secondly, the acidity of the soil, caused by the few cation exchange sites to be occupied by hydrogen ions, increases the loss of minerals such as iron, aluminium oxides and phosphorus.  Thirdly, leaching, which is the continuous downward movement and loss of solutes and minerals from the soil, happens regularly due to the heavy rainfall.  You wouldn’t be able to tell that the soil is poor from the lush, dense vegetation in these wet tropical forests; but shortly after an area of forest is cleared for agriculture (usually through slash-and-burn) the small amount of nutrients wash away and the soil becomes infertile. The ecosystems have developed highly specialized ways of mitigating effects such as leaching, but these functions are fragile, and need to be protected.  This includes tree adaptations such as buttress roots and thick root mats that grow laterally along the forest floor.  These adaptations mitigate nutrient loss by capturing the nutrients in falling detritus, before the nutrients are absorbed and decomposed into the soil, and lost from leaching by the heavy rains.  The geologic, topographic and soil changes across wet tropical forest ecosystems has contributed to the astonishing biodiversity in biota we see today. The plant communities of the tropical wet forest are the most diverse, abundant, and lush plant life in the world.  The plants define the tropical wet forest by contributing to ecosystem functions, such as producing nourished rainfall and storing atmospheric carbon.  Tropical wet forests are characterized by the complex, physical structure of the ecosystem.  There are many layers of plant communities, though they are rarely visible from the ground.  Shrubs and creepers fill the forest floor with saplings dispersed throughout.  Large trees hold their full crowns in the canopy, prohibiting sunlight to plants below.  Beneath the canopy of trees lies a network of stout branches, thick limbs, and climbers.  Sometimes even above these trees, the largest of canopies fill the sky like individual islands. Large trees, such as the pacque, allspice, and breadnut tree, provide habitat for most animal species and other plant species.  The leaves are usually oval, thick, and waxy with pointed drip-tips to alleviate water collection.  Roots are often buttressed (flaring from above ground), radiated across the forest floor, or stilted as prop roots.  Lichens, orchids, and mosses cover the trunks of trees, retaining moisture and hosting small invertebrates.  Most tropical trees have large, colorful, fragrant blossoms and plump fruits, perfect feeding for animals and insects.  Climbers, hemiepiphytes, and epiphytes are the major groups of non-tree species, although they tend to inhabit trees.  Climbers provide a road system in canopies for motile animals.  Vines are large in biomass and are an essential food source to many fauna.  Hemiepiphytes have the most unusual growth forms and are parasitic to larger trees.  Epiphytes claim space on a branch and set roots, trap minimal soil, and photosynthesize.  They adhere tightly to the bark of trees but, are not internally parasitic.  As rain forests become drier and more disturbed, these native species become more rare.  The loss of these plant communities severely affects the world, in regard to increase of carbon dioxide, high floods, and impure water. The two main keystone species of the Tropical Wet Forest ecoregion are the American crocodile and the Mexican jaguar.  They are both top predators and influence the population of their pray.  American crocodiles create habitat for many creatures through their water holes and the paths they create.  Their diet consists of fish, snails, birds, frogs, and mammals that come to the water’s edge.  Males can grow up to 15 feet long and weigh up to 2,000 pounds while females range from 8–13 feet.  Their average life span is around 45 years.  Females lay a clutch between 20−60 eggs which hatch after an average of 85 days.  The mother leaves the young to fend for themselves after a few days.  The jaguar is the third largest cat in the world and the largest in North America.  It is between 5 and 8 feet, nose to tail, and weighs between 140 and 300 pounds.  Their average lifespan in the wild is 12–16 years while in captivity it ranges from 20–27 years.  They have been observed to prey on around 85 different species, the most common of which are terrestrial mammals, they prefer giant anteaters, capybaras.  Females become sexually mature around 2–3 years while males become sexually mature around 3–4 years.  They have a gestation period about 100 days and give birth to an average litter of 2 cubs.  The cubs are able to open their eyes after about 8 days and are able to walk 10 days after that.  They stay with their mother for a year and half. Tropical wet forests are known for their wide diversity of natural resources.  Historically, the primary harvestable products they produce are from plants including exotic lumber such as mahogany, red cedar, and also gum tree for rubber.  Other plants that can be utilized from this region include common food items such as bananas, cacao, oranges, coffee, sesame, alfalfa, cotton, and a variety of peppers. Following Spanish and English colonization of the area there was a shift towards larger scaled agriculture plantations.  With these plantations came increased production of sugar cane, beans, pineapples, and chiles as well as an increase in harvesting of precious lumbers.  This trend continued largely up into the 1960s when large swaths of land were cleared to make room for cattle ranches. Consecutively came the influx from the petrochemical industry to extract the vast reservoirs of oil that exist underground.  This new development led to even larger portions of land being cleared for oil drilling sites and roads compounding the existing problem of deforestation in the region. One ray of hope for the future of natural resource procurement in tropical wet forests is the search for medicinally valuable plant secondary compounds.  Plants that contain compounds that can treat ailments ranging from analgesics, antibiotics, heart drugs, enzymes, hormones, diuretics, anti-parasitics, denitifrices, laxatives, dysentery treatments, anti-coagulants and hundreds more exist and could prove to be a valuable economically viable as well as sustainable alternative to current resources being utilized in the area. Deforestation is the main threat to the North Americans tropical wet forests and has devastating consequences.  Deforestation causes habitat loss and habitat fragmentation which have drastic effects on biodiversity.  Deforestation of tropical wet forests has caused many native species to become endangered or extinct at an alarming rate.  The Tropical Wet Forests around the global are being deforested at an alarming rate.  For example, some counties like Florida have lost 50% of their tropical wet forest habitat and Costa Rica has lost about 90%. Protection of the tropical wet forests we have left is very important for its continued existence.  Many Reserves have been created in an attempt to protect the little we have left of these forests.  Some examples of this in the United States are Florida's Everglades National Park and the Big Cypress National Preserve. Another important tool for the continued existence of tropical wet forests is Restoration.  There have been successful restoration projects of a tropical wet forest with native species in Costa Rica.  These restoration projects have been shown to significantly improve the native animal and plant species survival.  It is necessary for good management plans to be developed if we are to use tropical wet forests sustainably. The IUCN Red List has 65,521 species listed as threatened in the tropical wet forests.  The \"Harpia harpyja\", harpy eagle is one threatened species in the tropical wet forests, they are the largest neotropical bird of prey, nest in the tallest trees, prey mostly on animals that live in trees, lay between 1−2 eggs but only allowing 1 egg to hatch, reproduce every 2–4 years, and reaches sexual maturity between the ages of 4 and 5.  The harpy eagle is suffering because of slow reproductive rates, hunting, food competition, fragmentation, and habitat destruction.  There are many orchid species that are threatened in the tropical wet forests.  Orchids are a smart plant that manipulate other species into pollinating them, and once pollinated they produce seeds that are eventually released in hopes to be carried to a specific type of fungi (depending on the orchid) where it will attach for mycorrhizal symbioses, and then bloom after a few years or decades depending on the environment and species.  Many orchid species are suffering because of overharvesting, burning, clearing, and development.  Many efforts are being done to help save both species.  Spreading knowledge (educating), creating reserves, and coming up with alternatives are the top three actions being done to conserve both species. Over the last 100 years the Earth's temperature has increased 0.6 degrees Celsius and it is predicted to increase an additional 3.5 degrees over the next century.  Tropical wet forests account for only 6% of earth's land surface yet are responsible for 40% of earth’s oxygen production.  Any type of change to this system can prove to have significant detrimental effects in terms of global oxygen availability.  In addition, due to the sensitivity and fragile interactions between organisms and the atmosphere, ecosystem services such as carbon sequestration rates, will experience even larger adverse effects. Amounts of precipitation and moisture availability are also areas of concern.  Global precipitation is expected to rise two-fold in tropical areas.  This will cause shifts in vegetation as moist forest species expand into new areas of moisture.  Increasing atmospheric emissions also plays an integral role in precipitation patterns.  Annual rainfall is projected to decrease across the Everglades National Park causing a hydrologic change across the entire region.  Dry vegetative communities will outnumber hydric vegetative communities in this particular area. Furthermore, a one degree increase in atmospheric temperature is the result from a doubling of atmospheric CO2.  Effects of this increase on forest soil temperature include reduced tree growth and higher decomposition rates of deep soil organic matter.  Ultimately, as the forests become a larger carbon source to the atmosphere, ecosystem services cease to function, and the delicate balance found in the tropics is disrupted, the climate warming cycle intensifies. An iconic ecosystem of this region is the complex interaction and the variety of biota along with fairly consistent abiotic factors; even though this eco region covers roughly seven percent of the earth's surface, its tree community is the most diverse on the planet.  It would not be unusual to have 100 different tree species coexisting within a one-hectare plot.  The tree community contains many broad-leafed evergreen trees, which form a high canopy (30–40 meters) above the ground.  The understory contains a variety of more shade tolerant plants, which is a necessity for survival due to the thick canopy above.  The vegetation is \"spatially heterogeneous\".  This plant community survives in nutrient-poor soils conditions making disturbances (such as deforestation) to have greater effects because regeneration of the forest takes much longer.  Tributaries and river systems have formed from the large amount of rainfall and typically carry a lot of sediments, but increase water demands and the construction of dams can further alter and strain these ecosystems. Tropical rain forests.  (n.d.).  Retrieved from http://www.marietta.edu/~biol/biomes/troprain.htm The North American Deserts include both cold and hot deserts, which supply a variety of climates.  Due to this fact, they are often used for agricultural, business, or petroleum purposes.  These factors have been taking a toll on the desert climate, organisms, and landscape.  These deserts are the Mojave, Sonoran, Chihuahuan and the Great Basin. The North American Deserts are home to a variety of plant species.  These plants are categorized as either xerophytes, adapted to the arid conditions of the desert, or phreatophytes, which are plants with very deep roots that are dependent on a permanent water supply and survive by tapping groundwater. These species have come to possess several adaptations that allow them to survive and thrive in these dry and harsh conditions.  One of the most common of these species is the barrel cactus (Echinocactus and Ferocactus).  This plant was important to Native Americans and served a number of purposes, including use for food and water and creating fish hooks from the spines.  Another common species is the Shin Digger (Agave lechuguilla). With its shallow roots, it is able to take in a large quantity of water and store it in its pedals for extended periods of time.  The Ocotillo (Fouquieria splendens) is another plant frequently found in this area, which is a very unusually shaped plant.  Because of this, it is often referred to as a “vine cactus.”  This plant has an adaptive ability to photosynthesize during very dry conditions and gather large quantities of water when it is available.  The Great Basin is also home to the oldest species in the world, the bristlecone pine (\"Pinus longaeva\").  Its needles allow it to retain water and use very little of it during its lifetime.  It is able to grow on exposed rocky surfaces in higher elevations about forested areas.  With these advantages come some drawbacks, including its very slow growth rate, which leaves it vulnerable to being out-competed by faster growing trees. There are a variety of mammals that define the North American Deserts such as the bighorn sheep, mule deer, white-tailed deer, ground squirrel, coyote, prairie dog, cottontail rabbit, desert packrat, and mountain lion.  There are a number of birds and reptiles that thrive in these ecosystems as well.  The cactus wren, Gambel's quail, burrowing owl, red-tailed hawk, hummingbird, desert tortoise, and vulture to name a few. An example of a keystone species in the North American deserts would be the coyote or mountain lion.  These two predators can control the population and distribution of a large number of prey species.  A single mountain lion can roam an area of hundreds of kilometers, in which deer, rabbits, and bird species are partly controlled by a predator of this caliber.  They will change the feeding behavior or where they decide to nest or burrow is largely a reaction to the mountain lions activity.  Another example, such as the hummingbird, new plants or animals could also come into the habitat and push out native species.  In the Sonoran Desert, the hummingbird pollinates many native species of cactus and other plants.  The hummingbirds in this region, such as the Costa's hummingbird, have evolved to have very long beaks and tongues that wrap around the skull in order to reach the nectar for that sweet sugar staple. The Great Basin Desert is the only Cold desert, bordered by the Rocky Mountain range to the east, and the Sierra Nevada – Cascade to the west.  The northernmost part of the desert lies 2000 m above sea level, and due to high summer temperatures, not all of the fallen precipitation is fully absorbed into the soil, resulting in a high sodium concentration.  In other areas, mountain erosion has caused deep soils of fine particles, which allows for standing lakes. The Mojave lies between the Sonoran (south) and the Great Basin (north).  Here, soil is shallow, rocky, and dry.  The average elevation is between 3000 – above sea level.  The Mojave has several mountain rage boundaries, the Garlock and the San Andres.  They are made up of the two largest faults in the state of California. The Sonoran is referred to as the Base and Range geologic province.  Here, the Mogollon rim exists of sandstone and limestone piled over millions of years.  The basin and valley were made from volcanic eruption 40 million years ago, and the underlying rock is made primarily of cretaceous (aged granites). The Chihuahuan desert is made up of calcareous soils that have a high pH and calcium concentration.  The soil is thin, sandy, and gravel like, and rests atop deep layers of limestone.  Higher elevations allow water to sink deeper into soils that are made of finer particles, and deep sedimentary fans exist.  Limestone beds show that this desert was at one point fully submerged beneath the sea.  This desert features elevations ranging from 1200 m above sea level, to 350 m below. There are common patterns of hydrological cycles throughout the North American Deserts, but specifics of times and source of water range.  All four deserts rely on rivers, precipitation, and underground aquifers to replenish their water supply.  The water in the North American desert is mainly freshwater.  There is an ephemeral flow of underground water during the wet seasons that slows during each sub-desert’s dry season.  Oases form in all four deserts when the groundwater reaches the surface and pools in the hollows of the desert basins.  Being surrounded by mountains provides a rain shadow effect that contributes to the dry climate and creates the desert ecosystem.  All four deserts experience times of drought and times of intense precipitation.  The Colorado River flows through the Mojave, Great Basin, and Sonoran desert . But, differences in seasonal rain create the differing hydrological cycles.  The Great Basin receives most its rainfall in the winter.  This leads to creation of playa lakes in the spring, as the snowfall melts and flows down surrounding mountains.  The Sonoran Desert has a bimodal precipitation pattern that includes winter storms and summer monsoons, which help sustain flora.  The Chihuahuan Desert relies primarily on its intense summer monsoon for water.  During the summer is when the area sees the accumulation of playa lakes.  They may all have similar characteristics, but the difference in location and evaluation attribute to the diversity of their hydrological sources and cycles.  Although the Northern American Deserts are characteristically dry, they still contain the water necessary to fuel their ecosystem and sustain the life of humans, animals, and plants alike. North American deserts can be categorized by climate categories of hot and cold deserts.  The cold deserts include the Thompson Okanagan Plateau, Columbian Plateau, Northern and Central Basins, Colorado Plateaus, and the Snake River Plane. All of these North American Deserts are included in the cold category, which indicates that they have a dry mid-latitude steppe or desert climate.  These areas are affected by their interior position within the continent leading to broader temperature ranges and considerable rainfall.  More specifically, these areas are affected by the rain shadow created by neighboring mountain ranges, acting as a barrier to westerly flowing air carrying moisture.  All of these cold deserts experience about 100–300 mm of precipitation in a year indicating a semi-arid climate. The warm deserts of North America include The Mojave Basin and Range, the Sonoran desert, and the Chihuahuan desert.  These areas have a tropical desert climate, and are known as the hottest and driest place on the continent.  This is due to the continental interior location on the leeward side of mountains, with constant subtropical high pressures.  The high temperatures throughout the year are due to the high percentage of sunshine caused by high sun angles.  Increased distance from a body of water leads to a lack of clouds, which is associated with much cooler nighttime temperatures because all the heat of the day is lost.  The only source of water in the warm deserts is an oasis; this creates an arid climate in the area distinguishable by the lack of moisture in the soil due to annual precipitation being less than half of the annual potential evapotranspiration. The North American Desert biome is facing a variety of ecological threats.  Human disturbance poses the number one concern to this fragile ecosystem.  The Sonoran desert contains the two large cities of Tucson and Phoenix, Arizona, which contain over 3 million people.  These dense human populations deplete the water table of the entire desert and are sending the desert towards desertification.  Also, the Chihuahuan desert is seeing the effects of agricultural expansions, invasive species, illegal poaching, and extractions of resources such as salt, lime, and sand.  These activities in the desert lead to eventual desertification and a loss of overall biodiversity.  A number of organizations such as the United States Nature Conservancy and the World Wildlife Fund have begun working together to conserve the threatened desert ecosystem.  The less heavily populated areas of the desert are being sought out and conserved in order to prevent future human habitation and disturbance.  Also, several organizations are now monitoring the use and health of the Rio Grande system located in the Chihuahuan desert, while also building new low tech water treatment facilities that will help to prevent overall water table depletion.  The World Wildlife Fund is replanting disturbed, upland vegetation in order to retain species habitat and biodiversity.  These measures are helping to protect and preserve the four North American Desert ecosystems. The giant kangaroo rat is one of the most peculiar looking rodents around.  The Dipodomys ingens can grow up to 34.7 centimeters in length and have a tail of up to 19.8 centimeters long.  They can weigh up to 180 grams.  It is mainly found in the San Joaquin Valley in California.  The giant kangaroo rat forages for food from sunset to sunrise.  Its diet consists mainly of seeds, that are sun dried and some greenery.  They store food in their cheeks until they bring it back to their burrow systems, where they store food that could last them up to 2 years of drought.  The giant kangaroo rats develop rather quickly.  Depending on the environmental conditions, they can reproduce after about 5 months.  Their litter size varies but averages about 3.75 offspring.These rodents are rather resilient when it comes to surviving under natural conditions, such as drought and low plant productivity.  However, when the human factor is introduced, they have a much less successful survival rate.  Aqueducts and other water projects started crisscrossing the giant kangaroo rat habitat.  Agriculture moved in because of the new water routes and suddenly the habitat of many species became agricultural land.  Kangaroo rats became a pest for farmers and rodenticide-treated grain became common practice which took out another chunk of their population. Nichol's Turk's head cactus (\"Echinocactus horizonthalonius\" var.  \"nicholii\") is one of multiple species of \"Echinocactus horizonthalonius\".  The Nichol’s Turk’s head cactus ranges from blue-green to yellow-green.  It tends to be around 46 centimeters tall and has about a 20 centimeter diameter.  It has 8 ribs that are lined with spines.  The cactus blooms from April to May with a purple flower and white, hairy fruit.  Like many cacti, it is rather slow growing at a rate of just 2 inches in 10 years, due to minimal nutrient input.  Its habitat is located mainly in the Vekol and Waterman Mountains in Arizona and it has a population in the Sierra del Viejo Mountains of northwestern Sonora.  The cactus is particularly fond of Horquilla limestone outcrops.  The biggest threats to these cacti are habitat loss to new development, vehicle/off-roading damage, mining, and human collection.  Among other threats, erosion from foot traffic from drug and human trafficking in the area. North American Deserts, as in most arid systems, experience water and temperature change as the most limiting factors in this ecoregion.  Climate change's major effects thus far have been an increase in average annual temperature as well as an increase in average annual rainfall. The most prevalent factor is the increase in rainfall events and the severity of the events.  Between 1931 and 2000, there have been measurable increases in seasonal rainfall during the summertime monsoon in the southern United States and northern Mexico.  Because of this increase in rainfall, changes in the vegetative cover have caused native species to disappear and invasive species populations to rise.  The kangaroo rat, which also supported Mojave rattlesnake and burying owl populations, has essentially disappeared from the Chihuahan Desert, while the non-native Bailey’s pocket mouse has colonized the area.  Increased rainfall has also led to decrease in soil quality and less vegetative cover, which leads to increasingly higher temperatures.  In the Sonoran Desert, anthropogenic land degradation as well as natural erosion from increased rainfall has caused a 4–5 degree increase in average afternoon temperatures, which means for many species less available water and nutrients they need to survive.  These effects will lead to less biodiversity in the area, which is one of the main combatant factors that biota have against climate change. As the effects of climate change continue to develop, North American Deserts will be increasingly affected, leading worsening biodiversity loss and decreases in the ecoregion productivity.  Deserts are one of the most delicate ecosystems, relying on limited water and nutrient sources to survive.  When these careful relationships are disturbed by the unpredictable and worsening effects of climate change, it will be very hard for these ecosystems to recover or endure. In the North American Deserts there are emerging natural resources within the ecosystem.  A few natural resources within the desert consist of oil, sunlight, copper, zinc, and water.  Some of these resources are renewable and some are non-renewable.  Most of these resources are being exploited by humans and most actions are not sustainable. Sunlight is one of the deserts most important resource as it is renewable and has sustainable exploitations.  Deserts within North America tend to have fields of solar panels, so they can reuse the sun as energy.  Areas such as New Mexico, Texas, Arizona, and the Great Basin area, put up fields for green energy.  We monitored how the sun provides energy for resources such as plants and animals; we decided to make solar panels to produce energy for us.  Water is also a resource found in the desert that can be reused and has sustainable exploitations. Oil is the most exploited resource within the deserts.  In the North American desert most of the oil is found within the Great Basin region and this resource is non-renewable.  Oil is mined out of rocks and creates massive holes that disrupt the ecosystem.  The process with taking oil is not sustainable and this resource is scarce. Another resource that is mined is copper.  Along with oil, this resource is also scarce as it is non-renewable and also has the same mining affects as oil does.  This resource can be used for things such as computers, TVs, cell phones, and other electronics.  Copper is mainly found in California.  Other mined resources consist of zinc, uranium, rocks, jade, crystals, gold, and quartz.\n\nPlug-in electric vehicle A plug-in electric vehicle (PEV) is any motor vehicle that can be recharged from an external source of electricity, such as wall sockets, and the electricity stored in the rechargeable battery packs drives or contributes to drive the wheels.  PEV is a subset of electric vehicles that includes all-electric or battery electric vehicles (BEVs), plug-in hybrid vehicles (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles.  In China, plug-in electric vehicles are called new energy vehicles (NEVs). Plug-in cars have several benefits compared to conventional internal combustion engine vehicles.  They have lower operating and maintenance costs, and produce little or no local air pollution.  They reduce dependence on petroleum and may reduce greenhouse gas emissions from the onboard source of power, depending on the fuel and technology used for electricity generation to charge the batteries.  Plug-in hybrids capture most of these benefits when they are operating in all-electric mode.  Despite their potential benefits, market penetration of plug-in electric vehicles has been slower than expected as adoption faces several hurdles and limitations.  The global market share of the light-duty plug-in vehicle segment achieved 0.86% of total new car sales in 2016, up from 0.62% in 2015 and 0.38% in 2014.  However, the stock of plug-in electric cars represented just 0.15% of the 1.4 billion motor vehicles on the world's roads by the end of 2016. s of 2016 , plug-in electric vehicles are more expensive than conventional vehicles and hybrid electric vehicles due to the additional cost of their lithium-ion battery packs.  Other factors discouraging the adoption of electric cars are the lack of public and private recharging infrastructure and, in the case of all-electric vehicles, drivers' fear of the batteries running out of energy before reaching their destination due to the limited range of existing electric cars.  Plug-in hybrids eliminate the problem of range anxiety associated to all-electric vehicles, because the combustion engine works as a backup when the batteries are depleted, giving PHEVs driving range comparable to other vehicles with gasoline tanks.  Several national and local governments have established tax credits, subsidies, and other incentives to promote the introduction and adoption in the mass market of plug-in electric vehicles depending on their battery size and all-electric range. s of 2016 , there are over 60 models of highway capable plug-in electric passenger cars and light-duty utility vans available for retail sales in the world.  Cumulative global sales of highway legal plug-in electric passenger cars and light utility vehicles achieved the 2 million unit milestone in December 2016.  The global ratio between the stock of all-electrics (BEVs) and plug-in hybrids (PHEVs) was 61:39 at the end of 2016.  Global sales of pure electric cars and vans achieved the 1 million milestone in September 2016.  s of 2016 , the Nissan Leaf is the world's top selling highway-capable all-electric car in history, with global sales of over 250,000 units, followed by the Tesla Model S with more than 158,000 units sold worldwide.  Ranking next are the Chevrolet Volt plug-in hybrid, which together with its sibling the Opel/Vauxhall Ampera has combined global sales of 134,500 units, and the Mitsubishi Outlander P-HEV with about 119,500 units delivered, both through December 2016.  The Tesla Model S has been the world's top selling plug-in car for two years in a row, 2015 and 2016. s of 2016 , China has the world's largest stock of highway legal light-duty plug-in electric vehicles with cumulative sales of more than 645,000 plug-in electric passenger cars.  Among country markets, the United States ranks second with more than 570,000 plug-in electric cars sold through December 2016.  Japan is the world's third largest plug-in car country market with about 147,500 plug-ins sold through December 2016.  More than 637,000 light-duty plug-in electric passenger cars have been registered in Europe up until December 2016, representing 31.4% of global sales.  s of 2016 , sales in the European light-duty plug-in electric segment are led by Norway with over 135,000 units registered, followed by the Netherlands with more than 113,000 units registered at the end of December 2016, and France with over 108,000 units.  China is the world's leader in the plug-in heavy-duty segment, including electric all-electric buses, and plug-in commercial and sanitation trucks.  The stock of new energy vehicles sold in China totaled more than 950,000 units through December 2016.  s of 2015 , China was the world's largest plug-in electric bus market with a stock of almost 173,000 vehicles. A plug-in electric vehicle (PEV) is any motor vehicle with rechargeable battery packs that can be charged from the electric grid, and the electricity stored on board drives or contributes to drive the wheels for propulsion.  Plug-in electric vehicles are also sometimes referred to as grid-enabled vehicles (GEV) and also as electrically chargeable vehicles. PEV is a subcategory of electric vehicles that includes battery electric vehicles (BEVs), plug-in hybrid vehicles, (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles.  Even though conventional hybrid electric vehicles (HEVs) have a battery that is continually recharged with power from the internal combustion engine and regenerative braking, they can not be recharged from an off-vehicle electric energy source, and therefore, they do not belong to the category of plug-in electric vehicles. \"Plug-in electric drive vehicle\" is the legal term used in U.S. federal legislation to designate the category of motor vehicles eligible for federal tax credits depending on battery size and their all-electric range.  In some European countries, particularly in France, \"electrically chargeable vehicle\" is the formal term used to designate the vehicles eligible for these incentives. While the term \"plug-in electric vehicle\" most often refers to automobiles or \"plug-in cars\", there are several other types of plug-in electric vehicle, including scooters, motorcycles, neighborhood electric vehicles or microcars, city cars, vans, light trucks or light commercial vehicles, buses, trucks or lorries, and military vehicles. A battery electric vehicle (BEV) uses chemical energy stored in rechargeable battery packs as its only source for propulsion.  BEVs use electric motors and motor controllers instead of internal combustion engines (ICEs) for propulsion. A plug-in hybrid operates as an all-electric vehicle or BEV when operating in charge-depleting mode, but it switches to charge-sustaining mode after the battery has reached its minimum state of charge (SOC) threshold, exhausting the vehicle's all-electric range (AER). A plug-in hybrid electric vehicle (PHEV or PHV), also known as a plug-in hybrid, is a hybrid electric vehicle with rechargeable batteries that can be restored to full charge by connecting a plug to an external electric power source.  A plug-in hybrid shares the characteristics of both a conventional hybrid electric vehicle and an all-electric vehicle: it uses a gasoline engine and an electric motor for propulsion, but a PHEV has a larger battery pack that can be recharged, allowing operation in all-electric mode until the battery is depleted. An aftermarket electric vehicle conversion is the modification of a conventional internal combustion engine vehicle (ICEV) or hybrid electric vehicle (HEV) to electric propulsion, creating an all-electric or plug-in hybrid electric vehicle. There are several companies in the U.S. offering conversions.  The most common conversions have been from hybrid electric cars to plug-in hybrid, but due to the different technology used in hybrids by each carmaker, the easiest conversions are for 2004–2009 Toyota Prius and for the Ford Escape/Mercury Mariner Hybrid. In China the term new energy vehicles (NEVs) refers to vehicles that are partially or fully powered by electricity, such as battery electric vehicles (BEVs) and plug-in hybrids (PHEVs).  The Chinese government began implementation of its NEV program in 2009 to foster the development and introduction of new energy vehicles. Internal combustion engines are relatively inefficient at converting on-board fuel energy to propulsion as most of the energy is wasted as heat, and the rest while the engine is idling.  Electric motors, on the other hand, are more efficient at converting stored energy into driving a vehicle.  Electric drive vehicles do not consume energy while at rest or coasting, and modern plug-in cars can capture and reuse as much as one fifth of the energy normally lost during braking through regenerative braking.  Typically, conventional gasoline engines effectively use only 15% of the fuel energy content to move the vehicle or to power accessories, and diesel engines can reach on-board efficiencies of 20%, while electric drive vehicles typically have on-board efficiencies of around 80%. In the United States, as of early 2010 with a national average electricity rate of per kWh, the cost per mile for a plug-in electric vehicle operating in all-electric mode is estimated between $0.02 to $0.04, while the cost per mile of a standard automobile varies between $0.08 to $0.20, considering a gasoline price of $3.00 per gallon.  As petroleum price is expected to increase in the future due to oil production decline and increases in global demand, the cost difference in favor of PEVs is expected to become even more advantageous. According to Consumer Reports, as of December 2011 the Nissan Leaf has a cost of 3.5 cents per mile and the Chevrolet Volt has a cost in electric mode of 3.8 cents per mile.  The Volt cost per mile is higher because it is heavier than the Leaf.  These estimates are based on the fuel economy and energy consumption measured on their tests and using a U.S. national average rate of 11 cents per kWh of electricity.  When the Volt runs in range-extended mode using its premium gasoline-powered engine, the plug-in hybrid has a cost of 12.5 cents per mile.  The out-of-pocket cost per mile of the three most fuel efficient gasoline-powered cars as tested by the magazine are the Toyota Prius, with a cost of 8.6 cents per miles, the Honda Civic Hybrid with 9.5 cents per mile, the Toyota Corolla with 11.9 cents per mile, and the Hyundai Elantra 13.1 cents per mile.  The analysis also found that on trips up to 100 mi , the Volt is cheaper to drive than the Prius and the other three cars due to the Volt's 35 mi driving range on electricity.  The previous operating costs do not include maintenance, depreciation or other costs. All-electric and plug-in hybrid vehicles also have lower maintenance costs as compared to internal combustion vehicles, since electronic systems break down much less often than the mechanical systems in conventional vehicles, and the fewer mechanical systems on board last longer due to the better use of the electric engine.  PEVs do not require oil changes and other routine maintenance checks. The Edison Electric Institute (EEI) conducted an analysis that demonstrated that between January 1976 and February 2012 the real price for gasoline has been much more volatile than the real price of electricity in the United States.  The analysis is based on a plug-in electric vehicle with an efficiency of 3.4 miles per kW-hr (like the Mitsubishi i MiEV) and a gasoline-powered vehicle with a fuel economy rated at 30 mpgUS (like the 2012 Fiat 500).  The EEI estimated that operating a plug-in would have had an equivalent cost of around a gallon in the late 1970s and early 1980s, and around a gallon since the late 1990s.  In contrast, the price to operate an internal combustion engine vehicle has had much ample variations, costing more than per gallon during the 1979 energy crisis, then had a couple of lows with prices at less than during 1999 and 2001, only to climb and reach a maximum of more than before the beginning of the 2007–2009 financial crisis, by early 2012 has fluctuated around .  The analysis found that the cost of an equivalent electric-gallon of gasoline would have been not only cheaper to operate during the entire analysis period but also that equivalent electricity prices are more stable and have been declining in terms of equivalent dollars per gallon. Electric cars, as well as plug-in hybrids operating in all-electric mode, emit no harmful tailpipe pollutants from the onboard source of power, such as particulates (soot), volatile organic compounds, hydrocarbons, carbon monoxide, ozone, lead, and various oxides of nitrogen.  The clean air benefit is usually local because, depending on the source of the electricity used to recharge the batteries, air pollutant emissions are shifted to the location of the generation plants.  In a similar manner, plug-in electric vehicles operating in all-electric mode do not emit greenhouse gases from the onboard source of power, but from the point of view of a well-to-wheel assessment, the extent of the benefit also depends on the fuel and technology used for electricity generation.  This fact has been referred to as the long tailpipe of plug-in electric vehicles.  From the perspective of a full life cycle analysis, the electricity used to recharge the batteries must be generated from renewable or clean sources such as wind, solar, hydroelectric, or nuclear power for PEVs to have almost none or zero well-to-wheel emissions.  On the other hand, when PEVs are recharged from coal-fired plants, they usually produce slightly more greenhouse gas emissions than internal combustion engine vehicles and higher than hybrid electric vehicles.  In the case of plug-in hybrid electric vehicles operating in hybrid mode with assistance of the internal combustion engine, tailpipe and greenhouse emissions are lower in comparison to conventional cars because of their higher fuel economy. The magnitude of the potential advantage depends on the mix of generation sources and therefore varies by country and by region.  For example, France can obtain significant emission benefits from electric and plug-in hybrids because most of its electricity is generated by nuclear power plants; California, where most energy comes from natural gas, hydroelectric and nuclear plants can also secure substantial emission benefits.  The U.K. also has a significant potential to benefit from PEVs as natural gas plants dominate the generation mix.  On the other hand, emission benefits in Germany, China, India, and the central regions of the United States are limited or non-existent because most electricity is generated from coal.  However these countries and regions might still obtain some air quality benefits by reducing local air pollution in urban areas.  Cities with chronic air pollution problems, such as Los Angeles, México City, Santiago, Chile, São Paulo, Beijing, Bangkok and Kathmandu may also gain local clean air benefits by shifting the harmful emission to electric generation plants located outside the cities.  Nevertheless, the location of the plants is not relevant when considering greenhouse gas emission because their effect is global. A report published in June 2011, prepared by Ricardo in collaboration with experts from the UK's Low Carbon Vehicle Partnership, found that hybrid electric cars, plug-in hybrids and all-electric cars generate more carbon emissions during their production than current conventional vehicles, but still have a lower overall carbon footprint over the full life cycle.  The higher carbon footprint during production of electric drive vehicles is due mainly to the production of batteries.  As an example, 43 percent of production emissions for a mid-size electric car are generated from the battery production, while for standard mid-sized gasolineinternal combustion engine vehicle, around 75% of the embedded carbon emissions during production comes from the steel used in the vehicle glider.  The following table summarizes key results of this study for four powertrain technologies: The Ricardo study also found that the lifecycle carbon emissions for mid-sized gasoline and diesel vehicles are almost identical, and that the greater fuel efficiency of the diesel engine is offset by higher production emissions. In 2014 Volkswagen published the results of life-cycle assessment of its electric vehicles certified by TÜV NORD, and independent inspection agency.  The study found that CO2 emissions during the use phase of its all-electric VW e-Golf are 99% lower than those of the Golf 1.2 TSI when powers comes from exclusively hydroelectricity generated in Germany, Austria and Switzerland.  Accounting for the full lifecycle, the e-Golf reduces emissions by 61%, offsetting higher production emissions.  When the actual EU-27 electricity mix is considered, the e-Golf emissions are still 26% lower than those of the conventional Golf 1.2 TSI.  Similar results were found when comparing the e-Golf with the Golf 1.6 TDI.  The analysis considered recycling of the three vehicles at the end of their lifetime. Uniti The Swedish automotive startup Uniti Sweden AB has been working on an electric city car entirely designed to be sustainable.  The CEO Lewis Horne said “\"When people sell electric cars, the motivation is sustainability.  So, why are they so heavy, and why are they the same cars?  Where you just take out petrol and you put in the huge battery and you call it sustainable.  But it’s not.\"” .  The company has been designing a car that uses sustainable and recycled material such as bio-composite to reduce the footprint of the car during production.  In addition, the fully automated factory provided in partnership with Siemens will be able to work with the lights turned off during 22h per day to save energy. The following table compares tailpipe and upstream CO emissions estimated by the U.S. Environmental Protection Agency for all series production model year 2014 plug-in electric vehicles available in the U.S. market.  Total emissions include the emissions associated with the production and distribution of electricity used to charge the vehicle, and for plug-in hybrid electric vehicles, it also includes emissions associated with tailpipe emissions produced from the internal combustion engine.  These figures were published by the EPA in October 2014 in its annual report \"\"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends: 1975 Through 2014\".\"  All emissions are estimated considering average real world city and highway operation based on the EPA 5-cycle label methodology, using a weighted 55% city and 45% highway driving.  For the first time, the \"2014 Trends \"report presents an analysis of the impact of alternative fuel vehicles, with emphasis in plug-in electric vehicles because as their market share is approaching 1%, the EPA concluded that PEVs began to have a measurable impact on the U.S. overall new vehicle fuel economy and CO emissions. For purposes of an accurate estimation of emissions, the analysis took into consideration the differences in operation between plug-in hybrids.  Some, like the Chevrolet Volt, can operate in all-electric mode without using gasoline, and others operate in a blended mode like the Toyota Prius PHV, which uses both energy stored in the battery and energy from the gasoline tank to propel the vehicle, but that can deliver substantial all-electric driving in blended mode.  In addition, since the all-electric range of plug-in hybrids depends on the size of the battery pack, the analysis introduced a utility factor as a projection of the share of miles that will be driven using electricity by an average driver, for both, electric only and blended EV modes.  Since all-electric cars do not produce tailpipe emissions, the utility factor applies only to plug-in hybrids.  The following table shows the overall fuel economy expressed in terms of miles per gallon gasoline equivalent (mpg-e) and the utility factor for the ten MY2014 plug-in hybrids available in the U.S. market, and EPA's best estimate of the CO tailpipe emissions produced by these PHEVs. In order to account for the upstream CO emissions associated with the production and distribution of electricity, and since electricity production in the United States varies significantly from region to region, the EPA considered three scenarios/ranges with the low end scenario corresponding to the California powerplant emissions factor, the middle of the range represented by the national average powerplant emissions factor, and the upper end of the range corresponding to the powerplant emissions factor for the Rocky Mountains.  The EPA estimates that the electricity GHG emission factors for various regions of the country vary from 346 g CO /kWh in California to 986 g CO /kWh in the Rockies, with a national average of 648 g CO /kWh. The Union of Concerned Scientists (UCS) published a study in 2012 that assessed average greenhouse gas emissions in the U.S. resulting from charging plug-in car batteries from the perspective of the full life-cycle (well-to-wheel analysis) and according to fuel and technology used to generate electric power by region.  The study used the model year 2011 Nissan Leaf all-electric car to establish the analysis baseline, and electric-utility emissions are based on EPA's 2009 estimates.  The UCS study expressed the results in terms of miles per gallon instead of the conventional unit of grams of greenhouse gases or carbon dioxide equivalent emissions per year in order to make the results more friendly for consumers.  The study found that in areas where electricity is generated from natural gas, nuclear, hydroelectric or renewable sources, the potential of plug-in electric cars to reduce greenhouse emissions is significant.  On the other hand, in regions where a high proportion of power is generated from coal, hybrid electric cars produce less CO equivalent emissions than plug-in electric cars, and the best fuel efficient gasoline-powered subcompact car produces slightly less emissions than a PEV.  In the worst-case scenario, the study estimated that for a region where all energy is generated from coal, a plug-in electric car would emit greenhouse gas emissions equivalent to a gasoline car rated at a combined city/highway driving fuel economy of 30 mpgUS .  In contrast, in a region that is completely reliant on natural gas, the PEV would be equivalent to a gasoline-powered car rated at 50 mpgUS . The study concluded that for 45% of the U.S. population, a plug-in electric car will generate lower CO equivalent emissions than a gasoline-powered car capable of combined 50 mpgUS , such as the Toyota Prius and the Prius c.  The UCS also found that for 37% of the population, the electric car emissions will fall in the range of a gasoline-powered car rated at a combined fuel economy of 41 to , such as the Honda Civic Hybrid and the Lexus CT200h.  Only 18% of the population lives in areas where the power-supply is more dependent on burning carbon, and the greenhouse gas emissions will be equivalent to a car rated at a combined fuel economy of 31 to , such as the Chevrolet Cruze and Ford Focus.  The study found that there are no regions in the U.S. where plug-in electric cars will have higher greenhouse gas emissions than the average new compact gasoline engine automobile, and the area with the dirtiest power supply produces CO emissions equivalent to a gasoline-powered car rated at 33 mpgUS . In September 2014 the UCS published an updated analysis of its 2012 report.  The 2014 analysis found that 60% of Americans, up from 45% in 2009, live in regions where an all-electric car produce fewer CO equivalent emissions per mile than the most efficient hybrid.  The UCS study found several reasons for the improvement.  First, electric utilities have adopted cleaner sources of electricity to their mix between the two analysis.  The 2014 study used electric-utility emissions based on EPA's 2010 estimates, but since coal use nationwide is down by about 5% from 2010 to 2014, actual efficiency in 2014 is better than estimated in the UCS study.  Second, electric vehicles have become more efficient, as the average 2013 all-electric vehicle used 0.33 kWh per mile, representing a 5% improvement over 2011 models.  Also, some new models are cleaner than the average, such as the BMW i3, which is rated at 0.27 kWh by the EPA.  An i3 charged with power from the Midwest grid would be as clean as a gasoline-powered car with about 50 mpgUS , up from 39 mpgUS for the average electric car in the 2012 study.  In states with a cleaner mix generation, the gains were larger.  The average all-electric car in California went up to 95 mpgUS equivalent from 78 mpgUS in the 2012 study.  States with dirtier generation that rely heavily on coal still lag, such as Colorado, where the average BEV only achieves the same emissions as a 34 mpgUS gasoline-powered car.  The author of the 2014 analysis noted that the benefits are not distributed evenly across the U.S. because electric car adoptions is concentrated in the states with cleaner power. In November 2015 the Union of Concerned Scientists published a new report comparing two battery electric vehicles (BEVs) with similar gasoline vehicles by examining their global warming emissions over their full life-cycle, cradle-to-grave analysis.  The two BEVs modeled, midsize and full-size, are based on the two most popular BEV models sold in the United States in 2015, the Nissan Leaf and the Tesla Model S.  The study found that all-electric cars representative of those sold today, on average produce less than half the global warming emissions of comparable gasoline-powered vehicles, despite taken into account the higher emissions associated with BEV manufacturing.  Considering the regions where the two most popular electric cars are being sold, excess manufacturing emissions are offset within 6 to 16 months of average driving.  The study also concluded that driving an average EV results in lower global warming emissions than driving a gasoline car that gets 50 mpgUS in regions covering two-thirds of the U.S. population, up from 45% in 2009.  Based on where EVs are being sold in the United States in 2015, the average EV produces global warming emissions equal to a gasoline vehicle with a 68 mpgUS fuel economy rating.  The authors identified two main reason for the fact that EV-related emissions have become even lower in many parts of the country since the first study was conducted in 2012.  Electricity generation has been getting cleaner, as coal-fired generation has declined while lower-carbon alternatives have increased.  In addition, electric cars are becoming more efficient.  For example, the Nissan Leaf and the Chevrolet Volt, have undergone improvements to increase their efficiencies compared to the original models launched in 2010, and other even more efficient BEV models, such as the most lightweight and efficient BMW i3, have entered the market. One criticism to the UCS study is that the analysis was made using average emissions rates across regions instead of marginal generation at different times of the day.  The former approach does not take into account the generation mix within interconnected electricity markets and shifting load profiles throughout the day.  An analysis by three economist affiliated with the National Bureau of Economic Research (NBER), published in November 2014, developed a methodology to estimate marginal emissions of electricity demand that vary by location and time of day across the United States.  The marginal analysis, applied to plug-in electric vehicles, found that the emissions of charging PEVs vary by region and hours of the day.  In some regions, such as the Western U.S. and Texas, CO emissions per mile from driving PEVs are less than those from driving a hybrid car.  However, in other regions, such as the Upper Midwest, charging during the recommended hours of midnight to 4 a.m. implies that PEVs generate more emissions per mile than the average car currently on the road.  The results show a fundamental tension between electricity load management and environmental goals as the hours when electricity is the least expensive to produce tend to be the hours with the greatest emissions.  This occurs because coal-fired units, which have higher emission rates, are most commonly used to meet base-level and off-peak electricity demand; while natural gas units, which have relatively low emissions rates, are often brought online to meet peak demand. A study published in the UK in April 2013 assessed the carbon footprint of plug-in electric vehicles in 20 countries.  As a baseline the analysis established that manufacturing emissions account for 70 g CO/km for an electric car and 40 g CO/km for a petrol car.  The study found that in countries with coal-intensive generation, PEVs are no different from conventional petrol-powered vehicles.  Among these countries are China, Indonesia, Australia, South Africa and India.  A pure electric car in India generates emissions comparable to a 20 mpgUS petrol car. The country ranking was led by Paraguay, where all electricity is produced from hydropower, and Iceland, where electricity production relies on renewable power, mainly hydro and geothermal power.  Resulting carbon emissions from an electric car in both countries are 70 g CO/km, which is equivalent to a 220 mpgUS petrol car, and correspond to manufacturing emissions.  Next in the ranking are other countries with low carbon electricity generation, including Sweden (mostly hydro and nuclear power ), Brazil (mainly hydropower) and France (predominantly nuclear power).  Countries ranking in the middle include Japan, Germany, the UK and the United States. The following table shows the emissions intensity estimated in the study for those countries where electric vehicle are available, and the corresponding emissions equivalent in miles per US gallon of a petrol-powered car: For many net oil importing countries the 2000s energy crisis brought back concerns first raised during the 1973 oil crisis.  For the United States, the other developed countries and emerging countries their dependence on foreign oil has revived concerns about their vulnerability to price shocks and supply disruption.  Also, there have been concerns about the uncertainty surrounding peak oil production and the higher cost of extracting unconventional oil.  A third issue that has been raised is the threat to national security because most proven oil reserves are concentrated in relatively few geographic locations, including some countries with strong resource nationalism, unstable governments or hostile to U.S. interests.  In addition, for many developing countries, and particularly for the poorest African countries, high oil prices have an adverse impact on the government budget and deteriorate their terms of trade thus jeopardizing their balance of payments, all leading to lower economic growth. Through the gradual replacement of internal combustion engine vehicles for electric cars and plug-in hybrids, electric drive vehicles can contribute significantly to lessen the dependence of the transport sector on imported oil as well as contributing to the development of a more resilient energy supply. Plug-in electric vehicles offer users the opportunity to sell electricity stored in their batteries back to the power grid, thereby helping utilities to operate more efficiently in the management of their demand peaks.  A vehicle-to-grid (V2G) system would take advantage of the fact that most vehicles are parked an average of 95 percent of the time.  During such idle times the electricity stored in the batteries could be transferred from the PEV to the power lines and back to the grid.  In the U.S this transfer back to the grid have an estimated value to the utilities of up to $4,000 per year per car.  In a V2G system it would also be expected that battery electric (BEVs) and plug-in hybrids (PHEVs) would have the capability to communicate automatically with the power grid to sell demand response services by either delivering electricity into the grid or by throttling their charging rate. s of 2015 , plug-in electric vehicles are significantly more expensive as compared to conventional internal combustion engine vehicles and hybrid electric vehicles due to the additional cost of their lithium-ion battery pack.  According to a 2010 study by the National Research Council, the cost of a lithium-ion battery pack was about /kWh of usable energy, and considering that a PHEV-10 requires about 2.0 kWh and a PHEV-40 about 8 kWh, the manufacturer cost of the battery pack for a PHEV-10 is around and it goes up to for a PHEV-40.  s of 2012 , and based on the three battery size options offered for the Tesla Model S, the New York Times estimated the cost of automotive battery packs between to per kilowatt-hour.  A 2013 study by the American Council for an Energy-Efficient Economy reported that battery costs came down from per kWh in 2007 to per kWh in 2012.  The U.S. Department of Energy has set cost targets for its sponsored battery research of per kWh in 2015 and per kWh by 2022.  Cost reductions through advances in battery technology and higher production volumes will allow plug-in electric vehicles to be more competitive with conventional internal combustion engine vehicles. According to a study published in February 2016 by Bloomberg New Energy Finance (BNEF), battery prices fell 65% since 2010, and 35% just in 2015, reaching per kWh.  The study concludes that battery costs are on a trajectory to make electric vehicles without government subsidies as affordable as internal combustion engine cars in most countries by 2022.  BNEF projects that by 2040, long-range electric cars will cost less than expressed in 2016 dollars.  BNEF expects electric car battery costs to be well below per kWh by 2030, and to fall further thereafter as new chemistries become available. A study published in 2011 by the Belfer Center, Harvard University, found that the gasoline costs savings of plug-in electric cars do not offset their higher purchase prices when comparing their lifetime net present value of purchase and operating costs for the U.S. market at 2010 prices, and assuming no government subidies.  According to the study estimates, a PHEV-40 is more expensive than a conventional internal combustion engine, while a battery electric vehicles is more expensive.  These findings assumed a battery cost of per kWh, which means that the Chevrolet Volt battery pack cost around and the Nissan Leaf pack costs .  The study also assumed a gasoline price of per gallon (as of mid June 2011), that vehicles are driven 12,000 mi per year, an average price of electricity of per kWh, that the plug-in hybrid is driven in all-electric mode 85% of the time, and that the owner of PEVs pay to install a Level II 220/240 volt charger at home. The study also include hybrid electric vehicles in the comparison, and analyzed several scenarios to determine how the comparative net savings will change over the next 10 to 20 years, assuming that battery costs will decrease while gasoline prices increase, and also assuming higher fuel efficiency of conventional cars, among other scenarios.  Under the future scenarios considered, the study found that BEVs will be significantly less expensive than conventional cars ( to cheaper), while PHEVs, will be more expensive than BEVs in almost all comparison scenarios, and only less expensive than conventional cars in a scenario with very low battery costs and high gasoline prices.  The reason for the different savings among PEVs is because BEVs are simpler to build and do not use liquid fuel, while PHEVs have more complicated powertrains and still have gasoline-powered engines.  The following table summarizes the results of four of the seven scenarios analyzed by the study. According to a study by the Electric Power Research Institute published in June 2013, the total cost of ownership of the 2013 Nissan Leaf SV is substantially lower than that of comparable conventional and hybrid vehicles.  For comparison, the study constructed average hybrid and conventional vehicles and assumed an average US distance per trip distribution.  The study took into account the manufacturer's suggested retail price, taxes, credits, destination charge, electric charging station, fuel cost, maintenance cost, and additional cost due to the use of a gasoline vehicle for trips beyond the range of the Leaf. Despite the widespread assumption that plug-in recharging will take place overnight at home, residents of cities, apartments, dormitories, and townhouses do not have garages or driveways with available power outlets, and they might be less likely to buy plug-in electric vehicles unless recharging infrastructure is developed.  Electrical outlets or charging stations near their places of residence, in commercial or public parking lots, streets and workplaces are required for these potential users to gain the full advantage of PHEVs, and in the case of EVs, to avoid the fear of the batteries running out energy before reaching their destination, commonly called range anxiety.  Even house dwellers might need to charge at the office or to take advantage of opportunity charging at shopping centers.  However, this infrastructure is not in place and it will require investments by both the private and public sectors. Several cities in California and Oregon, and particularly San Francisco and other cities in the San Francisco Bay Area and Silicon Valley, already have deployed public charging stations and have expansion plans to attend both plug-ins and all-electric cars.  Some local private firms such as Google and Adobe Systems have also deployed charging infrastructure.  In Google's case, its Mountain View campus has 100 available charging stations for its share-use fleet of converted plug-ins available to its employees.  Solar panels are used to generate the electricity, and this pilot program is being monitored on a daily basis and performance results are published on the RechargeIT website.  s of 2013 , Estonia is the first and only country that had deployed an EV charging network with nationwide coverage, with 165 fast chargers available along highways at a minimum distance of between 40 to , and a higher density in urban areas. The importance to build the infrastructure necessary to support electric vehicles is illustrated by the decision of Car2Go in San Diego, California, that due to insufficient charging infrastructure decided to replace all of its all-electric car fleet with gasoline-powered cars starting on 1 May 2016.  When the carsharing service started in 2011, Car2Go expected 1,000 charging stations to be deployed around the city, but only 400 were in place by early 2016.  As a result, an average of 20% of the carsharing fleet is unavailable at any given time because the cars are either being charged or because they don’t have enough electricity in them to be driven.  Also, many of the company’s 40,000 San Diego members say they often worry their Car2Go will run out of charge before they finish their trip. A different approach to resolve the problems of range anxiety and lack of recharging infrastructure for electric vehicles was developed by Better Place.  Its business model considers that electric cars are built and sold separately from the battery pack.  As customers are not allowed to purchase battery packs, they must lease them from Better Place which will deploy a network of battery swapping stations thus expanding EVs range and allowing long distance trips.  Subscribed users pay a per-distance fee to cover battery pack leasing, charging and swap infrastructure, the cost of sustainable electricity, and other costs.  Better Place signed agreement for deployment in Australia, Denmark, Israel, Canada, California, and Hawaii.  The Renault Fluence Z.E. was the electric car built with switchable battery technology sold for the Better Place network.  The robotic battery-switching operation was completed in about five minutes. After implementing the first modern commercial deployment of the battery swapping model in Israel and Denmark, Better Place filed for bankruptcy in Israel in May 2013.  The company's financial difficulties were caused by the high investment required to develop the charging and swapping infrastructure, about million in private capital, and a market penetration significantly lower than originally predicted by Shai Agassi.  Less than 1,000 Fluence Z.E. cars were deployed in Israel and around 400 units in Denmark. Tesla Motors designed its Model S to allow fast battery swapping.  In June 2013, Tesla announced their goal to deploy a battery swapping station in each of its supercharging stations.  At a demonstration event Tesla showed that a battery swap operation with the Model S takes just over 90 seconds, about half the time it takes to refill a gasoline-powered car used for comparison purposes during the event.  The first stations are planned to be deployed along Interstate 5 in California where, according to Tesla, a large number of Model S sedans make the San Francisco-Los Angeles trip regularly.  These will be followed by the Washington, DC to Boston corridor. The REVA NXR exhibited in the 2009 Frankfurt Motor Show and the Nissan Leaf SV trim both have roof-mounted solar panels.  These solar panels are designed to trickle charge the batteries when the car is moving or parked.  Another proposed technology is REVive, by Reva.  When the REVA NXR's batteries are running low or are fully depleted, the driver is able to send an SMS to REVive and unlock a hidden reserve in the battery pack.  Reva has not provided details on how the system will work.  The Fisker Karma uses solar panel in the roof to recharge the 12-volt lead-acid accessory battery.  The Nissan Leaf SL trim also has a small solar panel at the rear of the roof/spoiler that can trickle charge the auxiliary 12-volt lead-acid battery. The existing electrical grid, and local transformers in particular, may not have enough capacity to handle the additional power load that might be required in certain areas with high plug-in electric car concentrations.  As recharging a single electric-drive car could consume three times as much electricity as a typical home, overloading problems may arise when several vehicles in the same neighborhood recharge at the same time, or during the normal summer peak loads.  To avoid such problems, utility executives recommend owners to charge their vehicles overnight when the grid load is lower or to use smarter electric meters that help control demand.  When market penetration of plug-in electric vehicles begins to reach significant levels, utilities will have to invest in improvements for local electrical grids in order to handle the additional loads related to recharging to avoid blackouts due to grid overload.  Also, some experts have suggested that by implementing variable time-of-day rates, utilities can provide an incentive for plug-in owners to recharge mostly overnight, when rates are lower. General Motors is sponsoring the Pecan Street demonstration project in Austin, Texas.  The project objective is to learn the charging patterns of plug-in electric car owners, and to study how a residential fleet of electric vehicles might strain the electric grid if all owners try to charge them at the same, which is what the preliminary monitoring found when the plug-in cars return home in the evening.  The Mueller neighborhood is the test ground, and as of 2013 , the community has nearly 60 Chevrolet Volt owners alone.  This cluster of Volts was achieved thanks to GM's commitment to match the federal government's $7,500 rebate incentive, which effectively halves the purchase price of the plug-hybrid electric cars. Electric cars and plug-in hybrids when operating in all-electric mode at low speeds produce less roadway noise as compared to vehicles propelled by an internal combustion engine, thereby reducing harmful noise health effects.  However, blind people or the visually impaired consider the noise of combustion engines a helpful aid while crossing streets, hence plug-in electric cars and conventional hybrids could pose an unexpected hazard when operating at low speeds. Several tests conducted in the U.S. have shown that this is a valid concern, as vehicles operating in electric mode can be particularly hard to hear below 20 mph for all types of road users and not only the visually impaired.  At higher speeds the sound created by tire friction and the air displaced by the vehicle start to make sufficient audible noise.  However, a 2011 study, commissioned by the UK Department for Transport (DfT) and conducted by the Transport Research Laboratory, found little correlation between pedestrian vehicle involvement density and noise level for the majority of vehicles.  In addition, the analysis found no evidence of a pattern in pedestrian vehicle involvement densities when only considering those accidents occurring on 30 mph or slower roads, or where the pedestrian was disabled.  A previous study did not found an increased pedestrian vehicle involvement density for electric and hybrid vehicles with respect to their conventional counterparts which raised the question as to whether added sound is necessarily required. Some carmakers announced they have decided to address this safety issue, and as a result, the new Nissan Leaf electric car and Chevrolet Volt plug-in hybrid, both launched in December 2010, as well as the Fisker Karma plug-in hybrid launched in 2011 launched in 2012, include electric warning sounds to alert pedestrians, the blind and others to their presence.  s of 2014 , most of the hybrids and plug-in electric and hybrids available in the United States, Japan and Europe make warning noises using a speaker system.  The Tesla Model S is one of the few electric cars without warning sounds, because Tesla Motors will await until regulations are enacted.  Volkswagen and BMW also decided to add artificial sounds to their electric drive cars only when required by regulation. The Japanese Ministry of Land, Infrastructure, Transport and Tourism issued guidelines for hybrid and other near-silent vehicles in January 2010.  In the United States the Pedestrian Safety Enhancement Act of 2010 was approved by the U.S. Senate and the House of Representatives in December 2010.  The act does not stipulate a specific speed for the simulated noise but requires the U.S. Department of Transportation to study and establish a motor vehicle safety standard that would set requirements for an alert sound.  A proposed rule was published for comment by the National Highway Traffic Safety Administration (NHTSA) in January, 2013.  It would require hybrids and electric vehicles traveling at less than 30 km/h to emit warning sounds that pedestrians must be able to hear over background noises.  According to the NHTSA proposal carmakers would be able to pick the sounds the vehicles make from a range of choices, and similar vehicles would have to make the same sounds.  The rules were scheduled to go into effect in September 2014.  However, in January 2015 the NHTSA rescheduled the date for a final ruling to the end of 2015.  Since the regulation comes into force three years after being rendered as a final rule, compliance was delayed to 2018. On 6 February 2013, the European Parliament approved a draft law to tighten noise limits for cars to protect public health, and also to add alerting sounds to ensure the audibility of hybrid and electric vehicles to improve the safety of vulnerable road users in urban areas, such as blind, visually and auditorily challenged pedestrians, cyclists and children.  The draft legislation states a number of tests, standards and measures that must first be developed for an Acoustic Vehicle Alerting Systems (AVAS) to be compulsory in the future.  The approved amendment establishes that the sound to be generated by the AVAS should be a continuous sound and should be easily indicative of vehicle behavior and should sound similar to the sound of a vehicle of the same category equipped with an internal combustion engine.\"  In April 2014 the European Parliament approved legislation that requires the mandatory use of the AVAS for all new electric and hybrid electric vehicles and car manufacturers have to comply within 5 years. Lithium-ion batteries may suffer thermal runaway and cell rupture if overheated or overcharged, and in extreme cases this can lead to combustion.  To reduce these risks, lithium-ion battery packs contain fail-safe circuitry that shuts down the battery when its voltage is outside the safe range.  When handled improperly, or if manufactured defectively, some rechargeable batteries can experience thermal runaway resulting in overheating.  Especially prone to thermal runaway are lithium-ion batteries.  Reports of exploding cellphones have been reported in newspapers.  In 2006, batteries from Apple, HP, Toshiba, Lenovo, Dell and other notebook manufacturers were recalled because of fire and explosions.  Also, during the Boeing 787 Dreamliner's first year of service, at least four aircraft suffered from electrical system problems stemming from its lithium-ion batteries, resulting in the whole Dreamliner fleet being voluntarily grounded in January 2013. Several plug-in electric vehicle fire incidents have taken place since the introduction of mass-production plug-in electric vehicles in 2008.  Most of them have been thermal runaway incidents related to the lithium-ion batteries and have involved the Zotye M300 EV, Chevrolet Volt, Fisker Karma, BYD e6, Dodge Ram 1500 Plug-in Hybrid, Toyota Prius Plug-in Hybrid, Mitsubishi i-MiEV and Outlander P-HEV.  s of 2013 , four fires after a crash have been reported associated with the batteries of all-electric cars involving a BYD e6 and three Tesla Model S cars. The first modern crash-related fire was reported in China in May 2012, after a high-speed car crashed into a BYD e6 taxi in Shenzhen.  The second reported incident occurred in the United States on October 1, 2013, when a Tesla Model S caught fire after the electric car hit metal debris on a highway in Kent, Washington state, and the debris punctured one of 16 modules within the battery pack.  A second reported fire occurred on October 18, 2013 in Merida, Mexico.  In this case the vehicle was being driven at high speed through a roundabout and crashed through a wall and into a tree.  On November 6, 2013, a Tesla Model S being driven on Interstate 24 near Murfreesboro, Tennessee caught fire after it struck a tow hitch on the roadway, causing damage beneath the vehicle. The U.S. National Highway Traffic Safety Administration (NHTSA) is conducting a study due in 2014 to establish whether lithium-ion batteries in plug-electric vehicles pose a potential fire hazard.  The research is looking at whether the high-voltage batteries can cause fires when they are being charged and when the vehicles are involved in an accident.  Both General Motors and Nissan have published a guide for firefighters and first responders to properly handle a crashed plug-in electric-drive vehicle and safely disable its battery and other high voltage systems. Common technology for plug-ins and electric cars is based on the lithium-ion battery and an electric motor which uses rare-earth elements.  The demand for lithium, heavy metals, and other specific elements (such as neodymium, boron and cobalt) required for the batteries and powertrain is expected to grow significantly due to the future sales increase of plug-in electric vehicles in the mid and long term.  s of 2011 , the Toyota Prius battery contains more than 20 lb of the rare-earth element lanthanum, and its motor magnets use neodymium and dysprosium.  While only 7 g of lithium carbonate equivalent (LCE) are required in a smartphone and 30 g in a tablet computer, electric vehicles and stationary energy storage systems for homes, businesses or industry use much more lithium in their batteries.  s of 2016 a hybrid electric passenger car might use 5 kg of LCE, while one of Tesla's high performance electric cars could use as much as 80 kg . Some of the largest world reserves of lithium and other rare metals are located in countries with strong resource nationalism, unstable governments or hostility to U.S. interests, raising concerns about the risk of replacing dependence on foreign oil with a new dependence on hostile countries to supply strategic materials. The main deposits of lithium are found in China and throughout the Andes mountain chain in South America.  In 2008 Chile was the leading lithium metal producer with almost 30%, followed by China, Argentina, and Australia.  In the United States lithium is recovered from brine pools in Nevada. Nearly half the world's known reserves are located in Bolivia, and according to the US Geological Survey, Bolivia's Salar de Uyuni desert has 5.4 million tons of lithium.  Other important reserves are located in Chile, China, and Brazil.  Since 2006 the Bolivian government have nationalized oil and gas projects and is keeping a tight control over mining its lithium reserves.  Already the Japanese and South Korean governments, as well as companies from these two countries and France, have offered technical assistance to develop Bolivia's lithium reserves and are seeking to gain access to the lithium resources through a mining and industrialization model suitable to Bolivian interests. According to a 2011 study conducted at Lawrence Berkeley National Laboratory and the University of California Berkeley, the currently estimated reserve base of lithium should not be a limiting factor for large-scale battery production for electric vehicles, as the study estimated that on the order of 1 billion 40 kWh Li-based batteries (about 10 kg of lithium per car) could be built with current reserves, as estimated by the U.S. Geological Survey.  Another 2011 study by researchers from the University of Michigan and Ford Motor Company found that there are sufficient lithium resources to support global demand until 2100, including the lithium required for the potential widespread use of hybrid electric, plug-in hybrid electric and battery electric vehicles.  The study estimated global lithium reserves at 39 million tons, and total demand for lithium during the 90-year period analyzed at 12–20 million tons, depending on the scenarios regarding economic growth and recycling rates. A 2016 study by Bloomberg New Energy Finance (BNEF) found that availability of lithium and other finite materials used in the battery packs will not be a limiting factor for the adoption of electric vehicles.  BNEF estimated that battery packs will require less than 1% of the known reserves of lithium, nickel, manganese, and copper through 2030, and 4% of the world’s cobalt.  After 2030, the study states that new battery chemistries will probably shift to other source materials, making packs lighter, smaller, and cheaper. China has 48% of the world's reserves of rare-earth elements, the United States has 13%, and Russia, Australia, and Canada have significant deposits.  Until the 1980s, the U.S. led the world in rare-earth production, but since the mid-1990s China has controlled the world market for these elements.  The mines in Bayan Obo near Baotou, Inner Mongolia, are currently the largest source of rare-earth metals and are 80% of China's production.  In 2010 China accounted for 97% of the global production of 17 rare-earth elements.  Since 2006 the Chinese government has been imposing export quotas reducing supply at a rate of 5% to 10% a year. Prices of several rare-earth elements increased sharply by mid-2010 as China imposed a 40% export reduction, citing environmental concerns as the reason for the export restrictions.  These quotas have been interpreted as an attempt to control the supply of rare earths.  However, the high prices have provided an incentive to begin or reactivate several rare-earth mining projects around the world, including the United States, Australia, Vietnam, and Kazakhstan. In September 2010, China temporarily blocked all exports of rare earths to Japan in the midst of a diplomatic dispute between the two countries.  These minerals are used in hybrid cars and other products such wind turbines and guided missiles, thereby augmenting the worries about the dependence on Chinese rare-earth elements and the need for geographic diversity of supply.  A December 2010 report published by the US DoE found that the American economy vulnerable to rare-earth shortages and estimates that it could take 15 years to overcome dependence on Chinese supplies.  China raised export taxes for some rare earths from 15 to 25%, and also extended taxes to exports of some rare-earth alloys that were not taxed before.  The Chinese government also announced further reductions on its export quotas for the first months of 2011, which represent a 35% reduction in tonnage as compared to exports during the first half of 2010. On September 29, 2010, the U.S. House of Representatives approved the Rare Earths and Critical Materials Revitalization Act of 2010 (H.R.6160).  The approved legislation is aimed at restoring the U.S. as a leading producer of rare-earth elements, and would support activities in the U.S. Department of Energy (US DoE) to discover and develop rare-earth sites inside of the U.S. in an effort to reduce the auto industry's near-complete dependence on China for the minerals.  A similar bill, the Rare Earths Supply Technology and Resources Transformation Act of 2010 (S. 3521), is being discussed in the U.S. Senate. In order to avoid its dependence on rare-earth minerals, Toyota Motor Corporation announced in January 2011 that it is developing an alternative motor for future hybrid and electric cars that does not need rare-earth materials.  Toyota engineers in Japan and the U.S. are developing an induction motor that is lighter and more efficient than the magnet-type motor used in the Prius, which uses two rare earths in its motor magnets.  Other popular hybrids and plug-in electric cars in the market that use these rare-earth elements are the Nissan Leaf, the Chevrolet Volt and Honda Insight.  For its second generation RAV4 EV due in 2012, Toyota is using an induction motor supplied by Tesla Motors that does not require rare-earth materials.  The Tesla Roadster and the Tesla Model S use a similar motor. With the exception of Tesla Motors, almost all new cars in the United States are sold through dealerships, so they play a crucial role in the sales of electric vehicles, and negative attitudes can hinder early adoption of plug-in electric vehicles.  Dealers decide which cars they want to stock, and a salesperson can have a big impact on how someone feels about a prospective purchase.  Sales people have ample knowledge of internal combustion cars while they do not have time to learn about a technology that represents a fraction of overall sales.  As with any new technology, and in the particular case of advanced technology vehicles, retailers are central to ensuring that buyers, especially those switching to a new technology, have the information and support they need to gain the full benefits of adopting this new technology. There are several reasons for the reluctance of some dealers to sell plug-in electric vehicles.  PEVs do not offer car dealers the same profits as gasoline-powered car.  Plug-in electric vehicles take more time to sell because of the explaining required, which hurts overall sales and sales people commissions.  Electric vehicles also may require less maintenance, resulting in loss of service revenue, and thus undermining the biggest source of dealer profits, their service departments.  According to the National Automobile Dealers Association (NADS), dealers on average make three times as much profit from service as they do from new car sales.  However, a NADS spokesman said there was not sufficient data to prove that electric cars would require less maintenance.  According to the New York Times, BMW and Nissan are among the companies whose dealers tend to be more enthusiastic and informed, but only about 10% of dealers are knowledgeable on the new technology. A study conducted at the Institute of Transportation Studies (ITS), at the University of California, Davis (UC Davis) published in 2014 found that many car dealers are less than enthusiastic about plug-in vehicles.  ITS conducted 43 interviews with six automakers and 20 new car dealers selling plug-in vehicles in California’s major metro markets.  The study also analyzed national and state-level J.D. Power 2013 Sales Satisfaction Index (SSI) study data on customer satisfaction with new car dealerships and Tesla retail stores.  The researchers found that buyers of plug-in electric vehicles were significantly less satisfied and rated the dealer purchase experience much lower than buyers of non-premium conventional cars, while Tesla Motors earned industry-high scores.  According to the findings, plug-in buyers expect more from dealers than conventional buyers, including product knowledge and support that extends beyond traditional offerings. In 2014 Consumer Reports published results from a survey conducted with 19 secret shoppers that went to 85 dealerships in four states, making anonymous visits between December 2013 and March 2014.  The secret shoppers asked a number of specific questions about cars to test the salespeople’s knowledge about electric cars.  The consumer magazine decided to conduct the survey after several consumers who wanted to buy a plug-in car reported to the organization that some dealerships were steering them toward gasoline-powered models.  The survey found that not all sales people seemed enthusiastic about making PEV sales; a few outright discouraged it, and even one dealer was reluctant to even show a plug-in model despite having one in stock.  And many sales people seemed not to have a good understanding of electric-car tax breaks and other incentives or of charging needs and costs.  Consumer Reports also found that when it came to answering basic questions, sales people at Chevrolet, Ford, and Nissan dealerships tended to be better informed than those at Honda and Toyota.  The survey found that most of the Toyota dealerships visited recommended against buying a Prius Plug-in and suggested buying a standard Prius hybrid instead.  Overall, the secret shoppers reported that only 13 dealers “discouraged sale of EV,” with seven of them being in New York.  However, at 35 of the 85 dealerships visited, the secret shoppers said sales people recommended buying a gasoline-powered car instead. The ITS-Davis study also found that a small but influential minority of dealers have introduced new approaches to better meet the needs of plug-in customers.  Examples include marketing carpool lane stickers, enrolling buyers in charging networks, and preparing incentive paperwork for customers.  Some dealers assign seasoned sales people as plug-in experts, many of whom drive plug-ins themselves to learn and be familiar with the technology and relate the car’s benefits to potential buyers.  The study concluded also that carmakers could do much more to support dealers selling PEVs. Several national and local governments around the world have established tax credits, grants and other financial and non-financial incentives for consumers to purchase a plug-in electric vehicle as a policy to promote the introduction and mass market adoption of this type of vehicles. In May 2009 the Japanese Diet passed the \"Green Vehicle Purchasing Promotion Measure\" that went into effect on June 19, 2009, but retroactive to April 10, 2009.  The program established tax deductions and exemptions for environmentally friendly and fuel efficient vehicles, according to a set of stipulated environmental performance criteria, and the requirements are applied equally to both foreign and domestically produced vehicles.  The program provides purchasing subsidies for two type of cases, consumers purchasing a new passenger car without trade-in (non-replacement program), and for those consumers buying a new car trading an used car registered 13 years ago or earlier (scrappage program). On June 1, 2010, The Chinese government announced a trial program to provide incentives up to 60,000 yuan (~US$8,785) for private purchase of new battery electric vehicles and 50,000 yuan (~US$7,320) for plug-in hybrids in five cities. s of 2010 , 17 of the 27 European Union member states provide tax incentives for electrically chargeable vehicles.  The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of PEVs and hybrid vehicles. In the UK the Plug-in Car Grant scheme provided a 25% incentive towards the cost of new plug-in electric cars that qualify as ultra-low carbon vehicles, capped at () until February 2016.  Both private and business fleet buyers are eligible for the government grant.  In December 2015, the Department for Transport (DfT) announced that Plug-in car grant was extended to encourage more than 100,000 UK motorists to buy cleaner vehicles.  The criteria for the Plug-in Car Grant was updated and the maximum grant dropped from GB£ (~) to GB£ (~).  The eligible ultra-low emission vehicles (ULEVs) must meet criteria in one of three categories depending on emission levels (CO emissions bands between 50 and 75g/km) and zero-emission-capable mileage (minimum of 10 mi ). A price cap is in place, with all Category 1 plug-in vehicles eligible for the full grant no matter what their purchase price, while Category 2 and 3 models with a list price of more than GB£ (~) will not be eligible for the grant.  Vehicles with a zero-emission range of at least 70 mi (category 1), including hydrogen fuel cell vehicles, will get a full GB£ (~), but plug-in hybrids (categories 2 and 3) costing under GB£ (~) will receive GB£ (~).  Under the extended scheme, some plug-in hybrid sports car will no longer be eligible for the grant, such as the BMW i8 because of its GB£ (~) purchase price tag.  The updated scheme came into force on 1 March 2016. Germany approved an incentive scheme in April 2016 with a budget of ().  The cost of the purchase incentive is shared equally between the government and automakers.  Electric car buyers get a () discount while buyers of plug-in hybrid vehicles get a discount of ().  Premium cars, such as the Tesla Model S and BMW i8, are not eligible to the incentive because there is a cap of () for the purchase price.  Only electric vehicles purchased after 18 May 2016 are eligible for the bonus and the owner must keep the new electric car at least nine months.  The same rule applies for leasing.  The online application system to claim the bonus went into effect on 2 July 2016.  s of 2016 , BMW, Citroën, Daimler, Ford, Hyundai, Kia, Mitsubishi, Nissan, Peugeot, Renault, Toyota, Volkswagen, and Volvo had signed up to participate in the scheme. In the United States the Energy Improvement and Extension Act of 2008, and later the American Clean Energy and Security Act of 2009 (ACES) granted tax credits for new qualified plug-in electric vehicles.  The American Recovery and Reinvestment Act of 2009 (ARRA) also authorized federal tax credits for converted plug-ins, though the credit is lower than for new PEVs. The federal tax credit for new plug-in electric vehicles is worth $2,500 plus $417 for each kilowatt-hour of battery capacity over 5 kWh, and the portion of the credit determined by battery capacity cannot exceed $5,000.  Therefore, the total amount of the credit allowed for a new PEV is $7,500.  Several states have established incentives and tax exemptions for BEVs and PHEV, and other non-monetary incentives. President Barack Obama set the goal of bringing 1 million plug-in electric vehicles on the road by 2015.  However, considering the actual slow rate of PEV sales, as of mid-2012 several industry observers have concluded that this goal is unattainable.  In September 2014 Governor of California Jerry Brown signed a bill, the Charge Ahead California Initiative, that sets a goal of placing at least 1 million zero-emission vehicles and near-zero-emission vehicles on the road in California by January 1, 2023. Ontario established a rebate between CA$ to CA$ (~ to ), depending on battery size, for purchasing or leasing a new plug-in electric vehicle after July 1, 2010.  The rebates are available to the first 10,000 applicants who qualify. Quebec offers rebates of up to CA$ () from January 1, 2012, for the purchase of new plug-in electric vehicles equipped with a minimum of 4 kWh battery, and new hybrid electric vehicles are eligible for a CA$ rebate.  All-electric vehicles with high-capacity battery packs are eligible for the full C$8,000 rebate, and incentives are reduced for low-range electric cars and plug-in hybrids. During the 1990s several highway-capable plug-in electric cars were produced in limited quantities, all were battery electric vehicles. PSA Peugeot Citroën launched several electric \"Électrique\" versions of its models starting in 1991, notably the Citroën C15, C25 and Berlingo and Peugeot J5 and Partner panel vans and the Citroën AX and Saxo and Peugeot 106 superminis.  The Saxo was the best-selling and most produced ever electric car at the time.  Other models were available through leasing mainly in California.  Popular models included the General Motors EV1 and the Toyota RAV4 EV.  Some of the latter were sold to the public and are in use still today.  In the late 2000s began a new wave of mass production plug-in electric cars, motorcycles and light trucks.  However, as of 2011 , most electric vehicles in the world roads were low-speed, low-range neighborhood electric vehicles (NEVs) or electric quadricycles. Pike Research estimated there were almost 479,000 NEVs on the world roads in 2011.  Just in China, a total of 200,000 low-speed small electric cars were sold in 2013, most of which are powered by lead-acid batteries.  An additional 600,000 low-speed small electric passenger vehicles were sold in China in 2015, and more than 700,000 during the first ten months of 2016.  s of 2015 , the GEM neighborhood electric vehicle is the market leader in North America, with global sales of more than 50,000 units since 1998. s of 2016 , there are over 60 models of highway-capable plug-in electric passenger cars and light-utility vans available in the world.  s of 2015 , there were 45 different plug-in electric passenger car models offered in Europe, 20 available in North America, 19 in China, 14 in Japan, and 7 in Australia.  There are also available several commercial models of plug-in motorcycles, all-electric buses, and heavy-duty trucks. Cumulative global sales of highway legal plug-in electric passenger cars and light utility vehicles passed the one million unit milestone in September 2015, and two million in December 2016.  The Renault-Nissan Alliance is the world's leading all-electric vehicle manufacturer.  The Alliance reached sales of 424,797 all-electric vehicles delivered globally at the end of 2016, including the pure electrics manufactured by Mitsubishi Motors, now part of the Alliance.  The Alliance, including Mitsubishi Motors i-Miev series, sold globally 94,265 all-electric vehicles in 2016.  Nissan alone totaled 275,000 global electric vehicle sales in December 2016.  Renault global electric vehicle sales passed the 100,000 unit milestone in September 2016. BYD Auto is the world's second largest plug-in electric car manufacturer with more than 188,000 units delivered in China through December 2016.  Its Qin plug-in hybrid is the company's top selling model with over 68,000 units sold in China through December 2016, making it the all-time best-selling plug-in electric car in the country.  s of 2016 , Tesla ranked as the world's third largest plug-in electric vehicle manufacturer with more than 186,000 electric cars worldwide since delivery of its first Tesla Roadster in 2008.  Its Model S has been the world's top selling plug-in car for two years running, 2015 and 2016.  Tesla global sales passed the 200,000 unit milestone in March 2017. Ranking next is Mitsubishi Motors with global sales of over 152,000 plug-in electric vehicles since 2009 through August 2016, consisting of all-electric cars of the Mitsubishi i-MiEV family, all-electric Mitsubishi Minicab MiEV utility vans and trucks, and the plug-in hybrid Mitsubishi Outlander P-HEV.  Next is General Motors with combined global sales since December 2010 of almost 113,000 vehicles through December 2015, consisting of over 106,000 plug-in hybrids of the Volt/Ampera family, over 4,300 Chevrolet Spark EVs, and over 2,400 Cadillac ELRs.  As of early November 2016, BMW has sold 100,000 plug-in cars, accounting for global sales its BMW i cars and BMW iPerformance plug-in hybrid models. BYD Auto ended 2015 as the world's best selling manufacturer of highway legal light-duty plug-in electric vehicles, with 61,722 units sold, mostly plug-in hybrids, followed by Tesla, with 50,580 units sold in 2015.  BYD was the world's top selling plug-in car manufacturer for a second year running, with 101,183 units sold in 2016, up 64% from 2015, and one more time followed again by Tesla with 76,243 units delivered.  However, in terms of sales revenue, Tesla ranked ahead with from its electric car sales in 2016, while BYD sales totaled from its electric car division.  In September 2016, combined sales of Tesla models totaled over 13,000 units worldwide, setting the record as the best monthly plug-in sales volume ever, by any automaker of plug-in cars. By mid-September 2015, the global stock of highway legal plug-in electric passenger cars and utility vans reached the one million sales milestone.  Sales of plug-in electric vehicles achieved the one million milestone almost twice as fast as hybrid electric vehicles (HEV).  While it took four years and 10 months to reach one-million PEV sales, it took more than around nine years and a few months for HEVs to reach its first million sales.  The global ratio between all-electrics (BEVs) and plug-in hybrids (PHEVs) has consistently been 60:40 between 2014 and the first half of 2016, mainly due to the large all-electric market in China.  In the U.S. and Europe, the ratio is approaching a 50:50 split.  All-electric cars have oversold plug-in hybrids, with pure electrics representing about 61% of the global stock of over 2 million plug-ins on the world's roads by the end of 2016, up from 58.9% at the end of 2015.  Cumulative global sales of highway-capable light-duty pure electric vehicles since 2010 achieved the one million unit milestone in September 2016.  The global stock of plug-in hybrid cars totaled almost 800,000 units at the end of 2016. The global stock of plug-in electric vehicles between 2005 and 2009 consisted exclusively of all-electric cars, totaling about 1,700 units in 2005, and almost 6,000 in 2009.  The plug-in stock rose to about 12,500 units in 2010, of which, only 350 vehicles were plug-in hybrids.  By comparison, during the Golden Age of the electric car at the beginning of the 20th century, the EV stock peaked at approximately 30,000 vehicles.  After the introduction of the Nissan Leaf and the Chevrolet Volt in late December 2010, the first mass-production plug-in cars by major carmakers, plug-in car sales grew to about 50,000 units in 2011, jumped to 125,000 in 2012, and rose to almost 213,000 plug-in electric cars and utility vans in 2013.  Sales totaled over 315,000 units in 2014, up 48% from 2013, In five years, global sales of highway legal light-duty plug-in electric vehicles have increased more than ten-fold, totaling more than 565,000 units in 2015.  Plug-in sales in 2015 increased about 80% from 2014, driven mainly by China and Europe.  Both markets passed in 2105 the U.S. as the largest plug-in electric car markets in terms of total annual sales, with China ranking as the world's best-selling plug-in electric passenger car country market in 2015.  About 775,000 plug-in cars and vans were sold in 2016, and cumulative global sales totaled an estimated 2.032 million plug-in cars and utility vans by the end of 2016.  The global market share of the light-duty plug-in vehicle segment achieved a record 0.86% of total new car sales in 2016, up from 0.62% in 2015 and 0.38% in 2014.  Despite the rapid growth experienced, the plug-in electric car segment represented just 0.15% of the 1.4 billion motor vehicles on the world's roads by the end of 2016, up from 0.1% in 2015. s of 2016 , with cumulative sales of more than 645,000 plug-in electric passenger cars, China has the world's largest fleet of light-duty plug-in electric vehicles, after having overtook during 2016 both the U.S. and Europe in terms of cumulative sales.  This figure accounts for both, domestically produced new energy passenger cars and imports.  The fleet of Chinese plug-in cars represents 43.0% of the global stock of light-duty plug-in vehicles at the end of 2016.  Among country markets, the U.S. ranks second, with over 570,000 plug-in passenger cars sold through December 2016, representing 28.1% of the global stock of plug-ins.  Japan has the world's third largest plug-in stock, with about 147,500 highway legal plug-in electric vehicles sold in the country between July 2009 and December 2016.  s of 2016 , about 637,500 plug-in electric passenger cars and vans have been registered in Europe, representing 31.4% of the global stock, the second largest after China.  The region was the world's largest light-duty plug-in market until October 2016. s of 2016 , more than 951,000 new energy vehicles have been sold in China since 2011, making the country the world's leader when all plug-in automotive segments are considered, including passenger cars, electric buses, and commercial plug-in heavy-duty trucks.  The stock of new energy vehicles in China achieved the 500,000 unit milestone in March 2016.  s of 2015 , China was the world's largest electric bus market with close to 173,000 plug-in electric buses, representing almost the entire global stock of plug-in buses.  The production of all-electric buses totaled 115,664 units in 2016, up 31% from 88,248 units produced in 2015. During 2014, four of the ten top selling countries achieved plug-in electric car sales with a market share higher than 1% of new car sales.  Also two small countries achieved this mark in 2014.  In 2015 nine countries achieved plug-in electric car sales with a market share equal or higher than 1% of total new car sales.'  In 2015 the European plug-in passenger car market share passed the one percent mark (1.41%) for the first time.  The following table presents the top 10 countries according to their PEV market share of total new car sales between 2016 and 2013.  The market share for two selected regions, Europe and California, and one autonomous territory, Hong Kong, is also shown. According to the U.S. Department of Energy, combined sales of plug-in hybrids and battery electric cars in the American market climbed more rapidly and outsold by more than double sales of hybrid-electric vehicles over their respective 24 month introductory periods.  A 2016 analysis by the Consumer Federation of America (CFA) found that 5 years after its introduction, sales of plug-in electric cars in the U.S. continued to outsell conventional hybrids.  An analysis by Scientific American found a similar trend at the international level when considering the global top selling PEVs over a 36-month introductory period.  Monthly sales of the Volt, Prius PHV and Leaf performed better than the conventional Prius during their respective introductory periods, with the exception of the Mitsubishi i-MiEV, which has been outsold most of the time by the Prius HEV over their 36-month introductory periods. According to Pike Research, global sales of plug-ins will surpass 1 million per year in 2017, after 7 years in the market and almost half the time it took hybrid electric vehicles to reach that sales threshold.  Toyota's chairman, Takeshi Uchiyamada, said in February 2017, that he \"expects the latest plug-in hybrid vehicles will catch on with consumers far more rapidly than the original Prius did.\"  As environmental awareness has become a major issue, and as technology for plug-ins has developed rapidly reducing costs, he expects Toyota to sell 1 million plug-in hybrids in less than 10 years, the time it took for sales of Toyota's conventional hybrid vehicles to hit that mark. Research published by Bloomberg New Energy Finance in February 2016 predicts that as battery prices continue to fall, light-duty electric vehicles without government subsidies will be as affordable as internal combustion engine cars in most countries by the mid-2020s.  As a result, the study forecasts that annual sales of electric vehicles will hit 41 million by 2040, representing 35% of new light duty vehicle sales.  In another scenario the study considers that if new carsharing services are successful, together with the adoption of autonomous cars, they could boost electric-vehicle market share to 50% of new car saless by 2040.  On the other hand, the Organization of the Petroleum Exporting Countries (OPEC) in its 2015 World Oil Outlook projected that the market share of battery electric cars and fuel cell cars will remain below 1% in 2040, while the share of hybrid electric cars is projected to grow from 1% in 2013 to 14% in 2040. BP in its 2017 Energy Outlook forecasts that the number of plug-in electric cars will rise from 1.2 million in 2015 to around 100 million by 2035 representing about 5.5% of the global fleet of passenger cars.  Around 25% of these plug-in electric vehicles will be plug-in hybrids and 75% will be pure battery electric cars.  The oil company also projects that automotive fuel demand for use in cars will continue to rise, despite efficiency improvements in conventional internal combustion engines and increased plug-in car adoption due to growth in demand for car travel from the growing middle class in emerging economies. New energy vehicle sales in China totaled 951,447 units between January 2011 and December 2016.  These figures include heavy-duty commercial vehicles such buses and sanitation trucks, and only include vehicles manufactured in the country as imports are not subject to government subsidies.  s of 2016 , the Chinese stock of plug-in electric vehicles consisted of about 540,000 all-electric vehicles (73.7%) and almost 193,000 plug-in hybrids (26.3%) sold since 2011.  s of 2015 , the global stock of plug-in electric buses was estimated to be about 173,000 units, almost entirely deployed in China, the world's largest electric bus market.  Of these, almost 150,000 are all-electric buses.  The Chinese electric bus stock grew nearly sixfold between 2014 and 2015. s of 2016 , cumulative sales of domestically produced highway legal plug-in electric passenger cars totaled 632,371 units since 2005.  s of 2016 , the Chinese plug-in stock represented 29.2% of the global light-duty plug-in stock.  Domestically produced cars account for 96% of new energy car sales in China.  A particular feature of the Chinese passenger plug-in market is the dominance of small entry level vehicles.  In 2015, all-electric car sales in the mini and small segments (A-segment) represented 87% of total pure electric car sales, while 96% of total plug-in hybrid car sales were in the compact segment (C-segment). In September 2016, the Chinese stock of plug-in passenger cars reached the same level of the American stock, and by November 2016, China’s cumulative total plug-in passenger vehicles sales had surpassed those of Europe, allowing China to become the market with the world's largest stock of light-duty plug-in electric vehicles, with almost 600,000 plug-in passenger cars.  China also surpassed the U.S. and Europe in terms of annual sales of light-duty plug-in electric vehicles, both in calendar years 2015 and current-year-to-date through November. New energy vehicle sales during 2014 reached 74,763 units, up 320% from 2013, and representing a market share of 0.32% of the 23.5 million new car sales sold that year.  Of these, 71% were passenger cars, 27% buses, and 1% trucks.  The BYD Qin plug-in hybrid, introduced in December 2013, ranked as the top selling plug-in electric car in China in 2014 with 14,747 units sold, and became the country's top selling passenger NEV ever.  Domestically produced new energy vehicle sales in 2015 totaled a record 331,092 units. Sales of plug-in passenger cars, excluding imports, totaled 207,380 units in 2015, making China the world's best-selling plug-in electric car country market in 2015.  The plug-in electric passenger car segment market share rose to 0.84% in 2015, up from 0.25% in 2014.  The top selling passenger models in 2015 were the BYD Qin plug-in hybrid with 31,898 units sold, followed by the BYD Tang (18,375).  As a reflex of the explosive growth of the Chinese plug-in electric car market in 2015, BYD Auto ended 2015 as the world's best selling manufacturer of highway legal light-duty plug-in electric vehicles, with around 60,000 units sold, ahead of Tesla (50,580).  During the first three quarters of 2016, BYD continued as the world's top selling plug-in car manufacturer with over 74,000 units delivered in China. The stock of new energy vehicles sold in China since 2011 achieved the 500,000 unit milestone in March 2016, making the country the world's leader in the plug-in heavy-duty segment, including electric buses and plug-in trucks.  Cumulative sales of new energy passenger cars achieved the 500,000 unit milestone in September 2016, excluding imports.  A total of 209,359 new energy passenger cars were sold in the first three quarters of 2016, up 122% year-on-year, consisting of about 145,000 all-electric cars, up 170% year-on-year, and about 65,000 plug-in hybrids, up 60% year-on-year.  The plug-in segment market share totaled 1.08% of new car sales during the period.  Three BYD Auto models topped the Chinese ranking of best-selling new energy passenger cars in 2016.  The BYD Tang plug-in hybrid SUV was the top selling plug-in car with 31,405 units delivered, followed by the BYD Qin (21,868), and the BYD e6 (20,605).  s of 2016 , the BYD Qin, with 68,655 units sold since its inception, remains the all-time top selling plug-in electric car in the country. s of 2016 , cumulative sales totaled 570,187 highway legal plug-in electric cars in the U.S. since the market launch of the Tesla Roadster in 2008, accounting for 28.1% of the global light-duty plug-in stock, down from about 40% in 2014.  s of 2016 , the U.S. has the world's third largest stock of plug-in passenger cars, after having being overtook by Europe in 2015 and China during 2016.  California is the largest U.S. car market, and accounts for approximately 48% percent of cumulative plug-in sales in the American market from 2011 to June 2016, and also accounts for about 50% of nationwide all-electric car sales and 47% of total plug-in hybrid sales.  The other nine states that follow California's Zero Emission Vehicle (ZEV) regulations have accounted for another 10% of cumulative plug-in car sales in the U.S. during the same period.  California's plug-in stock totaled about 270,000 units at the end of 2016. Nationwide sales climbed from 17,800 units delivered in 2011 to 53,200 during 2012, and reached 97,100 in 2013.  During 2014 plug-in electric car sales totaled 123,248 units, and fell to 114,248 in 2015.  A total of 157,181 plug-in cars were sold in 2016, up 37.6% from 2015.  The market share of plug-in electric passenger cars increased from 0.14% of new car sales in 2011 to 0.37% in 2012, 0.62% in 2013, and reached 0.75% of new car sales in 2014.  As plug-in car sales slowed down during 2015, the segment's market share fell to 0.66% of new car sales, and increased to 0.90% in 2016.  The highest-ever monthly market share for plug-in electric vehicles was achieved in December 2016 with 1.39% of new car sales.  The previous record was set in September 2016 (1.12%) marking the first time the plug-in segment market share passed the 1% mark in the U.S. December 2016 is also the best monthly plug-in sales volume on record ever, with 23,288 units delivered.  California plug-in sales achieved a 3.1% market share in 2015, 4.7 times higher than the U.S. s of 2016 , total sales are led by the Chevrolet Volt plug-in hybrid with 113,489 units, followed by the Nissan Leaf all-electric car with 103,597 units delivered.  Both plug-in cars were released in December 2010.  The Tesla Model S ranks as the third top selling plug-in electric car with an estimated 92,317 units sold through December 2016, followed by the Prius PHV, launched in February 2012, with 44,767 units.  Ranking fifth is the Ford Fusion Energi with 43,327 units, followed by the Ford C-Max Energi with 33,509 units delivered through December 2016.  Plug-in electric car sales in 2014 were led by the Nissan Leaf with 30,200 units.  The Tesla Model S was the top selling plug-in car in the U.S. in 2015 with 25,202 units delivered.  For a second year on a row, the Model S was the top selling plug-in car in the U.S. with about 29,156 units sold in 2016, followed by the Volt with 24,739, Model X with about 18,028, Ford Fusion Energi with 15,938, and the Nissan Leaf with 14,006 units. s of 2016 , the stock of light-duty plug-in electric vehicles in Japan is the world's third largest after China and the United States, with about 147,500 highway legal plug-in electric vehicles sold in the country since 2009.  The Japanese stock of plug-in vehicles represented 7.3% of cumulative global sales as of 2016 .  Plug-in segment sales climbed from 1,080 units in 2009 to 12,630 in 2011, and reached 24,440 in 2012.  Global sales of pure electric cars in 2012 were led by Japan with a 28% market share of the segment sales.  Japan ranked second after the U.S. in terms of its share of plug-in hybrid sales in 2012, with 12% of global sales.  A total of 30,587 highway-capable plug-in electric vehicles were sold in Japan in 2013. The plug-in segment sales remained flat in 2014 with 30,390 units sold, and a market share of 1.06% of total new car sales in the country (kei cars not included).  Sales totaled 24,660 units in 2015, consisting of 10,420 all-electrics and 14,190 plug-in hybrids.  The rate of growth of the Japanese plug-in segment slowed down from 2013, with annual sales falling behind Europe, the U.S. and China during 2014 and 2015.  The decline in plug-in car sales reflects the Japanese government and the major domestic carmakers decision to adopt and promote hydrogen fuel cell vehicles instead of plug-in electric vehicles. Sales of the plug-in electric segment in 2013 were led by the Nissan Leaf with 13,021 units sold, up from 11,115 in 2012.  The Mitsubishi Outlander P-HEV ranked second with 9,608 units sold in 2013.  The Leaf continued as the market leader in 2014 for the fourth year running with 14,177 units sold, followed by the Outlander P-HEV with 10,064 units, together representing about 80% of the plug-in segment sales in Japan in 2014.  In 2015 the Outlander plug-in hybrid was the top selling plug-in electric car in the country with 10,996 units sold, followed by the Leaf with 9,057 units.  Cumulative sales of plug-in electric cars since 2009 totaled 126,420 units at the end of 2015. Leaf sales in 2016 achieved a record of 14,795 units delivered, surpassing the previous record set in 2014.  Since December 2010, Nissan has sold 72,494 Leafs through December 2016, making the Leaf the all-time best-selling plug-in car in the country.  Between January and August 2016, a total of 4,162 Outlander P-HEVs were sold in Japan.  Sales of the Outlander plug-in hybrid fell sharply from April 2016 as a result of Mitsubishi's fuel mileage scandal.  Since its inception, sales of the plug-in hybrid totaled 34,830 units through August 2016. s of 2016 , about 637,500 plug-in electric passenger cars and vans have been registered in Europe, representing 31.4% of the global stock, the second largest after China.  Of these, more than 212,000 light-duty vehicles were registered in Europe in 2016 (33.2%).  s of 2016 , European sales of plug-in cars and vans are led by Norway with over 135,000 units registered, followed by the Netherlands with more than 113,000 units, and France with over 108,000.  Norway was the top selling plug-in country market in Europe in 2016 with 45,492 plug-in cars and vans registered, surpassing the Netherlands, Europe’s top market in 2015.  The other top selling European markets in terms of cumulative registrations, are the UK with about 91,000 units, followed by Germany with almost 75,000, and Sweden with over 30,500. Cumulative sales of light-duty plug-in electric vehicles in Europe passed the 500,000 unit milestone in May 2016.  Norway passed the 100,000th registered plug-in unit milestone in April 2016, France passed the same milestone in September 2016, and the Netherlands in November 2016.  The UK achieved the 100,000 unit mark in March 2017. s of 2015 , France ranked as the largest European market for light-duty electric commercial vehicles or utility vans, accounting for nearly half of all vans sold in the European Union.  The French market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015.  Denmark is the second largest European market, with over 2,600 plug-in electric vans sold in 2015, with an 8.5% market share of all vans sold in the country.  Most of the van sold in the Danish market are plug-in hybrids, accounting for almost all of the plug-in hybrid van sales across the EU. A total of 1,614 all-electric cars and 1,305 light-utility vehicles were sold in 2010.  Sales jumped from 2,919 units in 2010 to 13,779 in 2011, consisting of 11,271 pure electric cars and 2,508 commercial vans.  In addition, over 300 plug-in hybrids were sold in 2011, mainly Opel Amperas.  Light-duty plug-in vehicle sales totaled 34,333 units in 2012, consisting of 24,713 all-electric cars and vans, and 9,620 plug-in hybrids.  The Opel/Vauxhall Ampera plug-in hybrid was Europe's top selling plug-in electric car in 2012 with 5,268 units, closely followed by the all-electric Nissan Leaf with 5,210 units. The plug-in segment sales more than double to 71,943 units in 2013.  Pure electric passenger and light commercial vehicles sales increased by 63.9% to 40,496 units.  In addition, a total of 31,477 extended-range cars and plug-in hybrids were sold in 2013.  Registrations reached 104,746 light-duty plug-in electric vehicles in 2014, up 45.6% from 2013.  A total of 65,199 pure electric cars and light-utility vehicles were registered in Europe in 2014, up 60.9% from 2013.  All-electric passenger cars represented 87% of the European all-electric segment registrations.  Extended-range cars and plug-in hybrid registrations totaled 39,547 units in 2014, up 25.8% from 2013. During 2013 took place a surge in sales of plug-in hybrids in the European market, particularly in the Netherlands, with 20,164 PHEVs registered during the year.  Out of the 71,943 highway-capable plug-in electric passenger cars and utility vans sold in the region during 2013, plug-in hybrids totaled 31,447 units, representing 44% of the plug-in electric vehicle segment sales that year.  This trend continued in 2014.  Plug-in hybrids represented almost 30% of the plug-in electric drive sales during the first six months of 2014, and with the exception of the Nissan Leaf, sales of the previous European best selling models fell significantly, while recently introduced models captured a significant share of the segment sales, with the Mitsubishi Outlander P-HEV, Tesla Model S, BMW i3, Renault Zoe, Volkswagen e-Up! , and the Volvo V60 Plug-in Hybrid ranking among the top ten best selling models. In 2014 Norway was the top selling country in the light-duty all-electric market segment, with 18,649 passenger cars and utility vans registered, more than doubling its 2013 sales.  France ranked second with 15,046 units registered, followed by Germany with 8,804 units, the UK with 7,730 units, and the Netherlands with 3,585 car and vans registrations.  In the plug-in hybrid segment, the Netherlands was the top selling country in 2014 with 12,425 passenger cars registered, followed by the UK with 7,821, Germany with 4,527, and Sweden 3,432 units.  Five European countries achieved plug-in electric car sales with a market share higher than 1% of new car sales in 2014, Norway (13.84%), the Netherlands (3.87%), Iceland (2.71%), Estonia (1.57%), and Sweden (1.53%). In 2013 the top selling plug-in was the Leaf with 11,120 units sold, followed by the Outlander P-HEV with 8,197 units.  The Mitsubishi Outlander plug-in hybrid was the top selling plug-in electric vehicle in Europe in 2014 with 19,853 units sold, surpassing of the Nissan Leaf (14,658), which fell to second place.  Ranking third was the Renault Zoe with 11,231 units. For a second year running, the Mitsubishi’s Outlander P-HEV was the top selling plug-in electric car in Europe with 31,214 units sold in 2015, up 57% from 2014.  The Renault Zoe ranked second among plug-in electric cars, with 18,727 registrations, and surpassed the Nissan Leaf to become best selling pure electric car in Europe in 2015.  Ranking next were the Volkswagen Golf GTE plug-in hybrid (17,300), followed by the all-electric Tesla Model S (15,515) and the Nissan Leaf (15,455), the BMW i3, including its REx variant, (12,047), and the Audi A3 e-tron plug-in hybrid (11,791). The Netherlands was the top selling country in the European light-duty plug-in electric market segment, with 43,971 passenger cars and utility vans registered in 2015.  Norway ranked second with 34,455 units registered, followed by the UK with 28,188 units, France with 27,701 car and vans registrations, and Germany with 23,464 plug-in cars.  Eight European countries achieved plug-in electric car sales with a market share higher than 1% of new car sales in 2015, Norway (22.4%), the Netherlands (9.7%), Iceland (2.9%), Sweden (2.6%), Denmark (2.3%), Switzerland (2.0%), France (1.2%) and the UK (1.1%).  s of 2015 , almost 25% of the European plug-in stock was registered in the Nordic countries, with over 100,000 units registered.  In 2015, combined registrations in the four countries were up 91% from 2014. For the first time in the region, in 2015 plug-in hybrids (95,140) outsold all-electric cars (89,640) in the passenger car segment, however, when light-duty plug-in utility vehicles are accounted for, the all-electric segment totaled 97,687 registrations in 2015, up 65,199 in 2014, and ahead of the plug-in hybrid segment.  Also in 2015, the European market share of plug-in electric cars passed the 1% mark for the first time, with a 1.41% share of new car sales that year.  This trend continue during 2016.  Since April 2016 plug-in hybrids have outsold all-electric cars, and the gap has continued to widen.  Accounting for passenger plug-in car sales in Western Europe between January and July 2016, plug-in hybrids captured almost 54% of the region's plug-in market sales.  During 2016 the all-electric car segment ended with a market share of 0.57% of new car sales, while plug-in hybrids reached a market share of 0.73%. European sales of plug-in electric cars passed 200,000 units for the first time in 2016.  The plug-in segment achieved a market share of 1.3% of total new car sales in 2016.  Norway was the top selling plug-in car country in Europe in 2016 with 45,492 plug-in cars and vans registered, followed by the UK with about 36,907 units, France with 33,774, Germany with 25,154, the Netherlands with 24,645, and Sweden with 13,454.  France was the top selling market in the light-duty all-electric segment with 27,307 units registered, up 23% from 2015.  The plug-in car segment of ten European countries achieved a market share of new car sales above 1%, led by Norway with 29.1%, followed by the Netherlands with 6.4%, 6.4 %, Sweden with 3.5%, and Switzerland with 1.8%. The Renault Zoe was the best-selling all-electric car in Europe in 2016 with 21,735 units delivered, and also topped European sales in the broader plug-in electric car segment, ahead of the Outlander P-HEV, the top selling plug-in in the previous two years.  The Mitsubishi Outlander PHEV with 21,446 units sold was the second best-selling plug-in car, followed by the Nissan Leaf with 18,718.  The Outlander PHEV has been Europe's best-selling plug-in hybrid vehicle for four years in a row, from 2013 to 2016.  The top selling all-electric commercial van was the Nissan e-NV200 with 4,319 units registered.  s of 2016 , the Mitsubishi Outlander P-HEV is the all-time top selling plug-in electric car in the region with 80,768 units delivered, followed by the Nissan Leaf with 67,654 units, Renault Zoe with 60,625 units, Tesla Model S with 38,716, and the BMW i3 with 38,267 units.  The Renault Kangoo Z.E. is the all-time top selling all-electric utility van with 24,917 units. s of 2016 , a total of 135,276 light-duty plug-in electric vehicles have been registered, making Norway the European country with the largest stock of plug-in cars and vans, and the fourth largest in the world.  s of 31 2016 , and accounting for both new and used imports registrations, the Norwegian light-duty plug-in electric fleet consisted of 101,126 all-electric passenger cars and vans, and 34,383 plug-in hybrids.  These figures account for both new and used imports registrations.  Sales of used imports in Norway are significant, with more than 15,000 used imported electric cars have been registered from neighboring countries as of 2016 .  The government's target of 50,000 all-electric cars on Norwegian roads was reached in April 2015, more than two years earlier than expected, thanks to the successful policies implemented to promote electric vehicle adoption that include fiscal and non-monetary incentives.  The milestone of 100,000 light-duty plug-in electric vehicles registered was achieved in April 2016, and 100,000 all-electric vehicles in December 2016, representing about 10% of all pure electric cars that have been sold worldwide. The Norwegian fleet of plug-in electric cars is one of the cleanest in the world because 98% of the electricity generated in the country comes from hydropower.  Norway, with about 5.2 million people, is the country with the largest EV ownership per capita in the world.  In March 2014, Norway became the first country where over one in every 100 registered passenger cars is plug-in electric.  The plug-in car market penetration reached 2% in March 2015, passed 3% in December 2015, and achieved 5% at the end of 2016. Also the Norwegian plug-in electric vehicle market share of new car sales is the highest in the world.  The electric car segment market share rose from 1.6% in 2011, to 3.1% in 2012, and reached 5.6% of new car sales in 2013.  In 2014, the all-electric market share climbed to 13.8% accounting for plug-in hybrids.  The combined sales of new plug-in cars reached a market share of 23.4% in 2015.  A record market share of 29.1% was achieved in 2016.  The highest ever monthly market share for the plug-in electric passenger segment was achieved in January 2017 with 37.5% of new car sales.  Also in January 2017 the electrified passenger car segment for the first time ever surpassed combined sales of cars with conventional diesel or gasoline engines.  Sales of plug-in hybrids, all-electric cars and conventional hybrids achieved a combined market share of 51.4% of new car sales that month. Also, Norway was the first country in the world to have all-electric cars topping the new car sales monthly ranking.  The Tesla Model S has been the top selling new car four times, and the Nissan Leaf has topped the monthly new car sales ranking twice.  In March 2014 the Tesla Model S also broke the 28-year-old record for monthly sales of a single model regardless of its power source, surpassing the previous record set in May 1986.  In July 2016, when new car registrations are break down by type of powertrain, for the first time a plug-in hybrid, the Mitsubishi Outlander P-HEV, listed as the top selling new car.  In September 2016, the Tesla Model X ranked as the top selling new car model in Norway when registrations are broken down by type of powetrain.  The BMW i3 was the top selling new car model in the country in November 2016. A total of 50,875 plug-in electric vehicles were registered in Norway in 2016, consisting of 24,222 new electric cars, 5,281 used imported all-electric cars, 20,663 new plug-in hybrid cars, 607 new all-electric vans, and 102 used imported all-electric vans.  New light-duty plug-in registrations totaled 45,492 plug-in cars and vans registered, up 32% from 2015, and making Norway the top selling plug-in country market in Europe in 2016, surpassing the Netherlands, Europe’s top market in 2015. Registrations achieved a market share of 29.1% of all new passenger cars registered in 2016, with the all-electric car segment reaching 15.7%, down from 17.1% in 2015, and the plug-in hybrid segment climbed to 13.4%, up from 5.3% in 2015.  The Outlander PHEV ended 2016 listed as the best selling plug-in car in Norway with 5,136 units sold, the first time ever a plug-in hybrid topped the Norwegian list of top selling plug-in electric cars.  Ranking next were the Volkswagen e-Golf (4,705), Volkswagen Golf GTE (4,337), and Nissan Leaf (4,162).  When new car sales in 2016 are breakdown by powertrain or fuel, nine of the top ten best-selling models were electric-drive models, of which three models were plug-in hybrids, three were battery electric cars, three were conventional hybrids, and one was a diesel-powered car. s of 30 2016 , the Nissan Leaf remains as the all-time best selling plug-in electric car in the country with a total of 19,407 new Leafs registered since 2011.  When used imported Leafs are accounted for, there were 27,115 Leafs on Norwegian roads at the end of November 2016.  Ranking second is the VW e-Golf with 16,216 units registered followed by the Tesla Model S with 11,878 units.  s of 2016 , the Outlander PHEV is the all-time top selling plug-in hybrid car with 9,499 new units registered since 2013. s of 31 2016 , there were 113,636 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,903 range-extended and plug-in hybrids, 13,105 pure electric cars, and 1,628 all-electric light utility vans.  When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 115,193 units.  The country's electric vehicle stock reaches 151,752 units when fuel cell electric vehicles (30), mopeds (3,775), electric bicycles (32,496), and microcars (258) are accounted for.  A distinct feature of the Dutch plug-in market is dominance of plug-in hybrids, which represented 87% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2016. The Netherlands listed as the world's third best-selling country market for light-duty plug-in vehicles in 2015, with 43,971 units registered that year.  Until December 2015, the Netherlands had Europe's largest fleet light-duty plug-in vehicles.  Plug-in sales fell sharply during 2016, and as a result, by early October 2016, the Netherlands listed as the third largest European plug-in market, after being surpassed during the year by both Norway and France.  s of 2016 , the Netherlands had the second largest plug-in market concentration per capita in the world after Norway.  The stock of light-duty plug-in electric vehicles registered in the Netherlands achieved the 100,000 unit milestone in November 2016. Registrations of plug-in electric car represented a 0.57% share of total new car registrations in the country during 2011 and 2012.  During 2013 plug-in electric passenger car registrations totaled 22,415 units, climbing 338% from 2012, the highest rate of growth of any country in the world in 2013.  The segment's market share surged almost ten times from 2012 to 5.37% new car sales in the country during that year, the world's second highest in 2013 after Norway (5.6%).  The rapid growth of segment during 2013, allowed the Netherlands to reach a plug-in vehicle concentration of around 1.71 vehicles registered per 1,000 people, second only to Norway (4.04).  s of 2016 , the plug-in concentration had risen to 5.6 per 1,000 people.  The market share of the plug-in electric passenger car segment in 2014 fell to 3.86% of total new passenger car registrations, after the end of some of the tax incentives.  With 43,769 plug-in passenger cars registered in 2015, the segment market share rose to a record 9.7% of new car sales in the Dutch market in 2015, the second highest after Norway (22.4%). In November 2013, a total of 2,736 Mitsubishi Outlander P-HEVs were sold, making the plug-in hybrid the top selling new car in the country that month, representing a market share of 6.8% of all the new cars sold.  Again in December 2013, the Outlander P-HEV ranked as the top selling new car in the country with 4,976 units, representing a 12.6% market share of new car sales, contributing to a world record plug-in vehicle market share of 23.8% of new car sales.  The Netherlands is the second country, after Norway, where plug-in electric cars have topped the monthly ranking of new car sales.  The strong increase of plug-in car sles during the last months of 2013 was due to the end of the total exemption of the registration fee for corporate cars, which is valid for 5 years.  From January 1, 2014, all-electric vehicles pay a 4% registration fee and plug-in hybrids a 7% fee. The top 5 best-selling plug-in electric cars in 2015 were all plug-in hybrids, led by the Mitsubishi Outlander P-HEV (8,757).  The top selling all-electric car was the Tesla Model S (1,842).  Plug-in car sales achieved its best monthly volume on record ever in December 2015, with about 15,900 units sold, and allowing the segment to reach a record market share of about 23%.  The surge in plug-in car sales was due to reduction of the registration fees for plug-in hybrids.  From January 1, 2016, all-electric vehicles continue to pay a 4% registration fee, but for a plug-in hybrid the fee rises from 7% to 15% if its CO emissions do not exceed 50 g/km.  The rate for a conventional internal combustion car is 25% of its book value. As a result of the changes in the tax rules that went into force at the beginning of 2016, plug-in electric car sales declined significantly.  Sales during the first half of 2016 were down 64% from the same period in 2015.  A total of 9,185 plug-in passenger cars were registered in the first three quarters of 2016, consisting of 6,567 plug-in hybrids and 2,618 all-electric cars.  The market share of the plug-in car segment captured 3.2% of new car sales during the period.  s of 2016 , the Outlander P-HEV remained as the all-time top-selling plug-in car in the country with 25,984 units registered, followed by the Volvo V60 Plug-in Hybrid (15,804), Volkswagen Golf GTE (10,691), Volkswagen Passat GTE (7,773), Mercedes-Benz C 350 e (6,226), and the Tesla Model S (6,049). s of 2016 , the stock of light-duty plug-in electric vehicles registered in France totaled 108,065 plug-in cars and electric utility vans delivered since 2010, making the country the third largest plug-in market in Europe after Norway and the Netherlands, and the world's sixth.  s of 2016 , and accounting for registrations since 2010, the plug-in electric stock consisted of 61,686 all-electric passenger cars, 24,696 all-electric utility vans, and 12,857 plug-in hybrids.  The stock of light-duty plug-in electric vehicles registered in France passed the 100,000 unit milestone in October 2016. s of 2015 , France is the country with the world's largest market for light-duty electric commercial vehicles or utility vans.  Nearly half of the vans sold in the European Union are sold in France as a result of a national purchase incentive scheme, which French companies have embraced.  The market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015. Electric car registrations increased from 184 units in 2010 to 2,630 in 2011.  Sales in 2012 increased 115% from 2011 to 5,663 cars.  Registrations reached 8,779 electric cars in 2013, up 55.0% from 2012, and the all-electric market share of total new car sales went up to 0.49% from 0.3% in 2012.  In addition, 5,175 electric utility vans were registered in 2013, up 42% from 2012.  Sales of electric passenger cars and utility vans totaled 13,954 units in 2013, capturing a combined market share of 0.65% of these two segments.  Almost 1,500 plug-in hybrids were registered during 2012 and 2013, 666 units in 2012, and 808 units in 2013. A total of 15,045 all-electric cars and vans were registered in 2014, of which, pure electric passenger cars totaled 10,560 units.  This figure rises to 10,968 units if the BMW i3 with range extender is accounted for.  All-electric utility vans totaled 4,485 units registered in 2014.  All-electric cars captured a 0.59% market share of new car registered in France in 2014.  Plug-in hybrid car registrations totaled 1,519 units in 2014, almost doubling registrations from a year earlier.  Plug-in hybrid sales were driven by the Mitsubishi Outlander P-HEV, with 820 units registered in 2014.  The Zoe led all-electric vehicle registration in 2014, with 5,970 units registered, followed by the Kangoo Z.E. van with 2,657 registrations. A total of 27,701 light-duty plug-in electric vehicles were registered in France in 2015, consisting of 17,779 all-electric cars, 4,916 electric vans, and 5,006 plug-in hybrid cars.  All-electric cars captured a 0.9% market share of new passenger car registrations in 2015, and the entire plug-in passenger car market achieved a market share of 1.17%.  All-electric car registrations in 2015 continued to be led by the Renault Zoe (10,406), the electric utility van segment was led by the Kangoo Z.E. (2,836), and the plug-in hybrid segment was led by the Volkswagen Golf GTE (1,687).  A total of 33,774 light-duty plug-in electric vehicles were registered in France in 2016, making the country the third largest in Europe in 2016 after Norway and the UK.  France was the top selling European market in the light-duty all-electric segment with 27,307 units registered, up 23% from 2015.  During the first nine months of 2016 registrations consisted of 16,091 all-electric cars, 3,991 electric vans, and 4,858 plug-in hybrid cars.  The Renault Zoe continued as the top selling plug-in electric car with 8,163 units.  The plug-in passenger car market achieved a market share of 1.57% of new car sales during the first nine months of 2016. s of 2016 , the Renault Zoe is the all-time best-selling plug-in electric vehicle in the French market with 30,098 units registered since 2012.  Ranking second is the Kangoo Z.E. utility van with 15,032 units registered since 2010.  s of 2016 , the all-time top selling plug-in hybrid is the Volkswagen Golf GTE with about 2,500 units, followed by the Mitsubishi Outlander P-HEV with almost 2,000 registered. A total of 104,198 light-duty plug-in electric vehicles have been registered in the UK up until March 2017, including about 4,500 plug-in commercial vans.  Since the launch of the Plug-In Car Grant in January 2011, a total of 94,541 eligible cars have been registered through March 2017, and, as of 2016 , the number of claims made through the Plug-in Van Grant scheme totaled 2,938 units since the launch of the scheme in 2012.  Before the introduction of series production plug-in vehicles, a total of 1,096 all-electric vehicles were registered in the UK between 2006 and December 2010.  Before 2011, the G-Wiz, a heavy quadricycle, listed as the top-selling EV for several years.  s of 2016 , the UK had the fourth largest European stock of light-duty plug-in vehicles, and with 36,907 plug-in passenger cars registered in 2016, ranked as the second best selling European market that year after Norway. The British market experienced a rapid growth of plug-in car sales during 2014, driven by the introduction of new models such as the BMW i3, Tesla Model S, Mitsubishi Outlander P-HEV, and Renault Zoe.  Plug-in electric car registrations in the UK quadruple to 14,518 units in 2014.  In November 2014 the passenger plug-in segment's market share passed 1% of monthly new car sales for the first time in the UK.  The surge in demand for plug-in cars continued during 2015.  Plug-in electric car registrations in the UK totaled 28,188 units in 2015.  The plug-in electric car segment raised its market share of new car sales in 2015 to almost 1.1%, up from 0.59% in 2014.  The plug-in segment reached a record market share of 1.7% of new car sales in the UK, the highest ever.  The top selling models in 2015 were the Outlander P-HEV with 11,681 units, followed by the Leaf (5,236), and the BMW i3 (2,213). Plug-in car sales in March 2016 achieved the best monthly plug-in sales volume on record ever, with 7,144 grant eligible cars registered.  Registrations during the first six months of 2016 recorded the highest-volume half-year ever for plug-in electric car registrations.  During the first three quarters of 2016 a total of 29,185 plug-in electric vehicles were registered, of which, 28,035 cars were eligible for the Plug-in Car Grant.  Registrations consisted of 8,107 all-electric cars, up 16.6% year-on-year, and 21,078 plug-in hybrids, up 46.3% year-on-year.  During this period, sales of plug-in hybrids oversold pure electric cars, with the latter more than doubling sales of battery electric models.  During the first nine months of 2016 the plug-in car segment's market share reached 1.36% of new car sales.  While overall new car registrations year-to-date increased 2.6% from the same period in 2015, total plug-in car registrations during the first nine months of 2016 increased 36.6% from a year earlier.  The Outlander P-HEV continued to lead sales of the plug-in electric segment in 2016 with 9,486 units delivered.  The Leaf remained as the top selling all-electric car with 4,463 units registered. By mid-October 2016, sales of the Outlander P-HEV passed the 25,000 unit mark, accounting for about 50% of all plug-in hybrid sold in the UK since 2010.  s of 2016 , the Outlander plug-in hybrid continues to rank as the all-time top selling plug-in car in the UK with 26,600 units sold.  Cumulative sales of the Nissan Leaf, the second all-time best selling plug-in car and the top selling all-electric car ever, passed the 15,000 unit mark in September 2016, and also accounts for about 50% of total sales in the all-electric passenger car segment since 2010. s of 2016 , a total of 74,754 plug-in electric cars have been registered in Germany since 2010.  The country is the largest passenger car market in Europe, however ranks as the fifth largest plug-in market in Europe as of 2016 .  About 80% of the plug-in cars registered in the country through September 2016 were registered since January 2014.  The official German definition of electric vehicles changed at the beginning of 2013, before that, official statistics only registered all-electric vehicles because plug-in hybrids were accounted together with conventional hybrids.  As a result, the registrations figures for 2012 and older do not account for total new plug-in electric car registrations. The plug-in hybrid segment in the German market in 2014 experienced an explosive growth of 226.9% year-over-year, and the overall plug-in segment increased 75.5% from a year earlier.  Registrations of plug-in electric cars totaled 13,049 units in 2014.  The plug-in segment achieved a market share of 0.4% of new car sales that year.  The BMW i3 ended 2014 as the top selling plug-in electric car with 2,233 units registered.  The surge in sales continued in 2015, the plug-in hybrid segment grew 125.1% year-over-year, while the all-electric segment climbed 91.2% from the previous year.  In 2015, plug-in electric car registrations totaled 23,464 units, and the plug-in segment achieved a market share of 0.7% of new car sales.  The top selling models in 2015 were the Kia Soul EV (3,839), followed by the BMW i3 (2,271), and the Mitsubishi Outlander P-HEV (2,128). During the first three quarters of 2016, sales of plug-in hybrids surpassed sales of all-electric cars for the first time in the country.  A total of 17,074 units were registered, consisting of 7,678 all-electric cars and 9,396 plug-in hybrids.  The plug-in segment achieved a market share of 0.7% of new car sales.  The top selling models during the first eight months of 2016 were the Renault Zoe (1,836), BMW i3 (1,237), Tesla Model S (978), Audi A3 e-tron (908), and Volkswagen Golf GTE (852).  The introduction of the federal government purchase e-bonus, in effect since May 2016, did not produce immediate effect on plug-in car sales until September 2016, when registrations peaked to 3,061 units, consisting of 1,641 all-electric cars, up 76.6% year-on-year, and 1,420 plug-in hybrids, up 36.8% year-on-year.  Combined registrations of both type of plug-in accounted for 1.1% of new car registrations, allowing the German plug-in market share to pass the 1% mark for the first time during 2016. s of 2016 , a total of 30,525 plug-in electric vehicles have been registered in Sweden since 2011, consisting of 21,181 plug-in hybrids, 7,985 all-electric cars and 1,359 all-electric utility vans.  The Swedish plug-in electric market is dominated by plug-in hybrids, representing 69.4% of the Swedish light-duty plug-in electric vehicle registrations through December 2016.  Sweden has ranked among the world's top ten best-selling plug-in markets for two years running, 2015 and 2016, listed in both years as the ninth largest country market.  s of 2016 , the Swedish stock of plug-in cars and vans is the sixth largest in Europe. In September 2011 the Swedish government approved a 200 million kr program, effective starting in January 2012, to provide a subsidy of 40,000 kr per car for the purchase of 5,000 electric cars and other \"super green cars\" with ultra-low carbon emissions, defined as those with emissions below 50 grams of carbon dioxide (CO ) per km.  After renewing appropriations for the super green car rebate several times, from 2016, only zero emissions cars are entitled to receive the full premium, while other super green cars, plug-in hybrids, receive half premium. Registrations of super clean cars increased five-fold in July 2014 driven by the end of the quota of 5,000 new cars eligible for the super clean car subsidy.  A total of 4,656 plug-in super clean passenger cars were registered in 2014, representing a 1.53% market share of new passenger cars registered in the country in 2014.  Registrations of super clean cars were up 201% from 2013, while registrations of new passenger cars increased 12.7%.  The top selling plug-in electric cars in 2014 were the Mitsubishi Outlander P-HEV with 2,289 units and the Volvo V60 PHEV with 745.  A total of 8,908 light-duty plug-in electric vehicles were registered in 2015, up 80% from 2014.  The registered stock consisted of 5,625 plug-in hybrids, 2,962 all-electric cars and 321 all-electric utility vans.  The plug-in segment had a market share of 2.49% of new car sales in 2015.  The Mitsubishi Outlander P-HEV was the top selling plug-in car for a second year running with 3,302 units, followed by the Tesla Model S with 996 units. Registrations totaled 13,454 light-duty plug-in electric vehicles in 2016, consisting 10,257 plug-in hybrids, up 16.7% from 2015, 2,924 all-electric cars, up 4.8% year-on-year, and 273 all-electric vans down 15.9% from 2015.  Super clean car registrations totaled 12,995 units, up 51.4% from 2015.  The plug-in electric car segment achieved a market share of 3.5% of all new cars registered in 2016, the world's third largest after Norway and the Netherlands.  In 2016 the Volkswagen Passat GTE listed as the top selling plug-in car with 3,804 units, followed by the Outlander P-HEV (1,819), Volvo V60 PHEV (1,239), Volvo XC90 T8 (983), Tesla Model S (838), and Nissan Leaf (836).  The top selling all-electric utility van was the Renault Kangoo Z.E. with 171 units registered.  s of 2016 , the all-time top selling plug-in electric cars are the Mitsubishi Outlander P-HEV with 7,506 units registered, followed by the Volkswagen Passat GTE (4,075), Volvo V60 PHEV (3,239), Nissan Leaf (2,561) and Tesla Model S (2,099).  The Renault Kangoo Z.E. remained as the all-time the leader in the plug-in commercial utility segment with 1,024 units. Cumulative sales of plug-in electric cars in Canada totaled more than 27,000 new units through December 2016.  The Chevrolet Volt, released in 2011, is the all-time top selling plug-in electric vehicle in the country, with cumulative sales of 6,707 units through June 2016, representing over 30% of all plug-in cars sold in the country.  Ranking second is the Tesla Model S with 4,396 units sold, followed by the Nissan Leaf with 3,815 units delivered, both as of 2016 . Quebec is the regional market leader in Canada, with about 11,000 plug-in electric cars registered as of 2016 , of which, 55% are plug-in hybrids.  Registrations in the province totaled 3,100 units in 2015, representing a market share of 0.7% of new car sales, and 45% of total Canadian plug-in electric car sales that year.  In October 2016, the National Assembly of Quebec passed a new zero emission vehicle legislation that obliges major automakers selling cars in the Canadian province to offer their customers a minimum number of plug-in hybrid and all-electric models.  Under the new law, 3.5% of the total number of autos sold by carmakers in Quebec have to be zero emissions vehicles starting in 2018, rising to 15.5% in 2020.  A tradable credit system was created for those carmakers not fulfilling their quotas to avoid financial penalties.  Quebec aims to have 100,000 zero emission vehicles on the road by 2020. s of 2015 , there were 18,451 highway legal plug-in electric cars registered in Canada, consisting of 10,034 (54%) all-electric cars and 8,417 (46%) plug-in hybrids.  Until 2014 Canadian sales were evenly split between all-electric cars (50.8)% and plug-in hybrids (49.2%).  The Model S was the top selling plug-in electric car in Canada in 2015 with 2,010 units sold.  The Chevrolet Volt, with 2,544 units sold, ranked as the top selling plug-in car during the first three quarters of 2016. The world's top selling highway-capable all-electric car ever is the Nissan Leaf with global sales of more than 250,000 units through December 2016.  The United States is the world's largest Leaf market with 103,597 units sold through December 2016.  The other two top markets are Japan with 74,494 units delivered up until December 2016, and Europe with about 66,000 units through November 2016.  The European market is led by Norway with 19,407 new units registered up until December 2016, and the UK with 15,000 September 2016. Ranking second is the all-electric Tesla Model S, with global deliveries of more than 158,000 cars as of 2016 , with the United States as its leading market with about 92,300 units delivered through December 2016.  Norway is the Model S largest overseas market, with 11,763 new units registered through September 2016, followed by China with 5,524 units registered through September 2015.  The Tesla Model S has been the world's top selling plug-in car for two years in a row, 2015 and 2016.  The world's top selling all-electric light utility vehicle is the Renault Kangoo Z.E., with global sales of 25,205 electric vans delivered through December 2016. The following table presents global sales of the top selling highway-capable electric cars and light utility vehicles produced between 2008 and December 2016.  The table includes all-electric passenger cars and utility vans with cumulative sales of about or over 20,000 units since the introduction of the first modern production all-electric car in 2008, the Tesla Roadster. s of 2016 , the Volt/Ampera family is the world's best selling plug-in hybrid and the third best selling plug-in electric car after the Nissan Leaf and the Model S. Chevrolet Volt and Opel/Vauxhaul Ampera combined sales totaled about 134,500 units worldwide through December 2016, including just over 10,000 Opel/Vauxhall Amperas sold in Europe through June 2016.  Volt sales are led by the United States with 113,489 units delivered, followed by Canada with 8,884 units, both through December 2016.  The Netherlands is the leading European market with about 6,000 Amperas and Volts registered as of 2015 . Ranking next is the Mitsubishi Outlander P-HEV with around 119,500 units sold worldwide as of 2016 .  s of 2016 , Europe ranked as the Outlander P-HEV leading market with 65,529 units sold, followed by Japan with 33,730 units.  European sales are led by the UK with 26,600 units sold through December 2016, followed by the Netherlands with 25,984 units registered by the end of 2016. Ranking third is the first generation Toyota Prius Plug-in Hybrid with 75,400 units sold worldwide through April 2016.  The United States is the market leader with 44,767 units delivered through December 2016.  Japan ranks next with about 22,100 units, followed by Europe with 10,500 units, both, through April 2016.  The leading European market is the Netherlands with 4,134 units registered as of 30 2015 .  Production of the first generation Prius Plug-in ended in June 2015.  The second generation Prius plug-in hybrid, the Toyota Prius Prime, was released in the United States in November 2016. The following table presents cumulative sales through December 2016 of those plug-in hybrid models that have sold about 10,000 units since the introduction of the first modern production plug-in hybrid vehicle in December 2008, the BYD F3DM.\n\nGlobal storm activity of 2010 The global storm activity of 2010 includes major meteorological events in the Earth's atmosphere during the year, including winter storms (blizzards, ice storms, European windstorms), hailstorms, out of season monsoon rain storms, extratropical cyclones, gales, microbursts, flooding, rainstorms, tropical cyclones, and other severe weather events. The thunderstorm season for the Northern Hemisphere began this time of year, beginning on March 1, and ending on August 31. Fresh overnight snowfall on New Year's Eve and New Year's Day caused disruption in north east England, with roads across Northumberland, Tyne and Wear, County Durham and Teesside affected.  Snow also fell in parts of East Cumbria.  In places it was as deep as 10 cm and motorists were warned not to travel unless their journey was absolutely necessary. A major snow-related weather warning was put out in Alaska on December 30.  The expected snowstorm was probably part of the same weather system that hit the Russian Far East from December 30 to January 5. Between January 1 and 2, 2010, 50- and 70-year record low temperatures and snowfall hit northern China and Korea starting January 1.  Blizzards also hit Mongolia's Dundgobi province. A heavy storm surge hit northeastern New Brunswick on January 2, leading to over $750,000 in damage in the community of Port Elgin. Heavy record-breaking snow also fell in Moscow in early January 2010.  and light snow briefly fell in Greece and Turkey. On January 2, a weather front with a northerly wind brought heavy snow to the northwest of England.  Wythenshawe near Manchester Airport had 5 in of snow.  This affected road transport on January 2, particularly in the Greater Manchester area with conditions on the M60, M602 and M66 reported to be poor, while Snake Pass, which links Manchester with Sheffield was closed. As a snowstorm entered Scotland, a number of roads across the country were closed including three junctions of the M9 while the motorway was shut in both directions at the Newbridge Roundabout in Edinburgh during the evening because of heavy snow, and did not open again until the following day.  Problems were also reported on the A96 and the A939.  Rail services between Inverness and Central Scotland were also affected by poor weather. Possibly more than 200 people died in northern India, mostly in Uttar Pradesh after a cold snap and accidents in heavy fog around January 2 and 3. Heavy rain fell in parts of southwestern Brazil.  The worst affected municipality has been Angra dos Reis, about 150 km southwest of the city of Rio de Janeiro.  At least 35 people were killed at a resort on Ilha Grande: about 40 people were staying in the hotel which was buried under a mudslide, and the death toll is expected to rise further.  In Rio Grande do Sul, at least seven people are dead and 20 missing after a bridge collapsed due to heavy rains.  It is reported to be one of the worst mudslides in Reo De Janero's history. On January 4, heavy rain fell in Brazil. Heavy snow fell in northern China and grounded hundreds of flights on the 4th.  It and forced Hong Kong's Financial Secretary John Tsang and Monetary Authority Chief Executive Officer Norman Chan to cancel a trip to Beijing.  Beijing was hit by a blizzard starting the evening of the 2nd, with 70–80% of flights cancelled out of Beijing Capital International Airport on January 4.  Close to 20 cm of snow fell in the north of the city, and close to 25 cm in Seoul.  Schools across the area were closed, and Premier Wen asked local governments to ensure safe transportation, continued food supplies, and continued agricultural production.  Continued snowstorms were forecast for the city of Beijing and the province of Inner Mongolia.  It was also predicted on January 7 for snow to reach the provinces of Jiangsu, Anhui, Henan and Hubei on January 9 according to China National Radio.  One person was killed in Xinjiang Autonomous Region as a result of the storm.  Emergency services handed out extra cattle fodder in Tibet. Japan's Hokkaido island was hit by heavier snowfall, causing heavy travel disruption and some airport closures. By January 4 about 30 people, including 28 children and an elderly man, died from cold-related causes in the last 11 days across Bangladesh as snow and a cold wave swept over the north and centre of the nation.  Freezing fog also occurred on the 4th in Punjab, India.  Some parts of Bangladesh were the hardest hit with temperatures plummeting as low as 6 °C according to meteorologist Sanaul Haque, who predicted the cold wave may continue for another day. The New Brunswick villages of Upper Cape and Port Elgin were devastated as a massive hurricane-strength blizzard hit Nova Scotia, New Brunswick and Prince Edward Island on January 4, causing massive blackouts in its wake. Light snow briefly fell in Lebanon, Jordan, Israel and Palestine and further snow was reported in Niigata, Japan. By January 4 about 30 people, including 28 children and an elderly man, died from cold-related causes in the last 11 days across Bangladesh as snow and a cold wave swept over the north and centre of the nation.  Freezing fog also occurred on the 4th in Punjab, India.  Some parts of Bangladesh were the hardest hit with temperatures plummeting as low as 6 °C according to meteorologist Sanaul Haque, who predicted the cold wave may continue for another day. Heavy rain and severe cold hit southern Bangladesh and Italy. On the 4th, the UN issued a Green Alert in Albania as northern rivers swell with melt water In its monthly summary, Met Éireann, Ireland's weather service, said December was the coldest month for 28 years for most of the country and the coldest of any month since February 1986 at a few stations. In Scotland, Fife Council became the first local authority to confirm that its supply of grit was exhausted on January 4, after it received less than it had ordered from suppliers.  Government Ministers denied there was a shortage of grit and salt and insisted there were \"very substantial\" supplies for Scotland's roads. Italy suffered from both heavy rain in the south and heavy snow in the north. January 4 saw four villages in Sakhalin lose power as a result of a storm.  The 2,000 strong town of Tomari was worst hit.  Blizzard struck Sakhalin island, a narrow island in the stormy Sea of Okhotsk, off the coast of Siberia and just 25 miles north of Japan's snowy Hokkaido island. On the 4th, many motorways in Shandong were closed and 19 flights cancelled in the Yantai International Airport.  Eventually the No. 1820 train, carrying more than 800 passengers, started off at after being stranded for 12 hours. January 4 saw Seoul's heaviest snowfall since 1937 according to the Korea Meteorological Administration (KMA).  The blizzard dumped 25.8 cm of snow on the town. Avalanches and heavy snow hit the Russian's Sakhalin Island, which was smothered by a snow cyclone and blizzard, the Island's emergency officials said. Civil authorities were put on a major alert in the snow-torn province of Shandong on January 4 as more snow fell in both Shandong and Beijing.  Travel was affected as the snowstorm paralyzed Beijing on January 4, 2010. January 5 in northeastern China saw a smooth flow of traffic, with no gridlock or serious traffic accidents being reported after the snowstorm that caused traffic chaos on the 4th, according to the Beijing Traffic Management Bureau.  The city's bus and subway train services were up and running according as plan.  During the peak hours on Monday morning, the Beijing Subway Operating Company dispatched 20 additional trains to ease the heavy passenger flow.  The bus of all routes started off on time that morning, according to the Beijing Public Transport Holdings.  Inner Mongolia was still in a critical situation as teams batteled to clear severe rural snow drifts. 4–8 inches (10–20 centimeters) of snow fell in Beijing on the 5th, in the largest snowfall since 1951. The 2,000 weather modification offices in China, which are responsible for bombing the skies with silver iodide to induce rain or snow, were put into use.  Schools in Beijing and Tianjin closed and because the cities' traffic was in chaos.  The capital received the harshest Siberian winds in decades.  Temperatures for the 5th were forecast to plunge to −16 °C, a 40-year record low, after a daytime maximum of −8 °C.  The head of the Beijing Meteorological Bureau, Guo Hu, linked the blizzard-like conditions of the first week of January to unusual atmospheric patterns caused by global warming. Heavy snow started to fall in Seoul, South Korea and it was reported that a leading North Korean Communist party official had frozen to death, in his home, situated in the country's Sepo kun (or county). During the traffic peak hours of Monday morning, the Beijing Subway Operating Company dispatched 20 additional subway trains to ease the heavy passenger flow.  City workers were deployed to clean the snow in the city's main roads with about 15,710 tonnes of snow-dissolving agent, Beijing Environmental Sanitation Group Co., Ltd. official Zhang Zhiqiang said.  On the 4th, many motorways in Shandong were closed and 19 flights were cancelled at Yantai International Airport. In Inner Mongolia, 13 trains were delayed that Monday in Hohhot, the regional capital, according to the Hohhot railway authorities. The Met Office issued weather warnings for every region in the UK except the Northern Isles.  An extreme weather warning was issued for southern areas for overnight snowfall which could bring accumulations from 25 to .  BBC Weather and the Met Office also warned that temperatures in the Highlands of Scotland could drop to -20 C later in the week.  The Met Office also confirmed that the UK is experiencing the longest prolonged cold spell since December 1981.  The Harrogate district endured over six inches and had been a regular feature on BBC News broadcasts. A local record of 48 cm of snow was lying in Aviemore and 3 to of snow was recorded within the Cairngorms National Park.  Most parts of Scotland had further snowfalls during the night of January 4/5. Due to shortage of road grit conventionally made from rock salt, road grit was being made by or for road-gritters from cooking-type salt mixed with builders' sand; and the public bought up large amounts of cooking salt and table salt from food shops to put on their paths and drives.  The Government was reported to have reallocated reserve supplies of road salt and grit from Oxfordshire and Buckinghamshire and sent it to Cumbria and Fife due to the higher priority of even lower salt and grit reserves, along with the greater snow clearance work, according to Radio Oxford. A heat wave and/or unforeseen monsoon weather also hit parts of Australia in early 2010.  Victoria, the scene of horrific bushfires the year before, had a far colder summer, with hot weather arriving more than a month later than usual in 2009.  August 17 saw a dust storm at Laguna Mar Chiquita as a major drought hit Argentina, and flooding and hailstorms hit southeastern Australia and Queensland in March 2010.  The lack of winter precipitation in parts of China, however, contributed to a severe drought in the southwest.  Bolivia, Venezuela, Mali, Mauritania, Morocco and Spain have also seen periods of drought in 2009 and 2010.  On between May 12 and 26, both Mauritania, the Sénégal River Area and neighbouring parts of both Senegal and Mali faced both a drought and famine in 2010. Snow and fog occurred in Germany from January 3 to January 10.  The Finnish railways and Helsinki airport are disrupted by further snowfall and record low temperatures for the Helsinki region. On January 6, the London Borough of Harrow closed 58 schools and 73 flights were cancelled at Heathrow as 3 cm of snow fell at Heathrow Airport. Continued snowstorms were forecast for the city of Beijing and the province of Inner Mongolia.  Electricity rationing started on January 7.  Snow had started falling in Gangsu province by January 7.  It also was projected on January 7 to have reached the provinces of Jiangsu, Anhui, Henan and Hubei on January 9 according to China National Radio. The Chinese government said that China faced its worst ice risk in 30 years.  By the night of January 8–9, the major snowstorms predicted on January 7 for the provinces of Jiangsu and Anhui, had arrived and the emergency services were put on alert. Continued snowstorms were forecast for the city of Beijing and the province of Inner Mongolia.  Snow reached the provinces of Jiangsu, Anhui, Henan and Hubei on January 9.  One person was killed in Xinjiang Autonomous Region as a result of the storm. The synoptic situation in northern Europe settled to a steady northeast wind which brought snow showers and belts of snow. The British Army had to help stranded motorists in southern areas. The Met Office confirmed that 40 cm of snow fell in some parts of southern England. A \"severe\" warning issued by the Met Office was in place for every region in the UK.  Scottish First Minister Alex Salmond said Scotland was experiencing its worst winter since 1963. Further deaths in Wales, Shetland and Aberdeenshire were recorded.  Roads in the southeast were left with long traffic jams and abandoned cars.  8,000 schools were closed. In Eastern Parts, there were accumulations of 40 cm to 50 cm in places.  In Kent, six inches of snow fell in four hours in the early evening. By January 7, 2010, 22 people had died in the UK because of the freezing conditions. Heavy snow blocked many roads in the Irish Republic.  Dublin Airport closed on Wednesday January 6 and again on Friday January 9.  Cork Airport closed on Sunday January 10, at 6:30 pm and did not re-open until 12:30 pm on Monday January 11.  Dublin Bus canceled all services for a time.  Ireland West Airport Knock was also closed.  A new Irish record low temperature of -9 C was also recorded in Dublin, Ireland.  Temperatures in County Limerick dropped to -11.1 C .  Varying amounts of snow fell across Ireland on the 7th and 8th and road salt reserves began to run low. The snow fell heavily in some places of Spain including Prades (Tarragona, southern Catalonia) which received 120 cm of snow after a storm lasting over 30 hours.  The BBC News reported heavy snowfall as far south as Granada, in Spain. 25 Nepalese people, mostly children, died as a blizzard swept over most of Nepal on January 7.  Snowstorms probably also occurred in mountainous Bhutan and Sikkim, but no reports were forthcoming. Heavy snow also fell in Chicago on the 7th In Norway, temperatures hit −42 °C on the 7th in the central village of Folldala as snow fell across Scandinavia and -42.4 C at Tynset.  on the 8th as Kuusamo in Finland, the lowest registered temperature was -37.1 C .  Heavy blizzards and snow storms raged across Germany, Scandinavia and the northwestern parts of European Russia. In Poland, more snow hit the Lower Silesian Voivodeship, and nine people died across the country in a 48-hour period, bringing the total weather-related deaths to 139 since the start of November, a police spokesman said.  Most of the victims were vagrants, whose tragic lifestyle left them prone to the cold. Switzerland's 24.5 km Gotthard Road Tunnel reopened to trucks on the 8th, following heavy snowfalls over the previous 2 days. A heavy and snowy cyclone hit the Aleutian Islands on January 7 and 8.  Snow accompanied a cold wave moving south through North America on January 8, and Mexico City received snowfall accumulations.  In the mountainous regions, temperatures dropped to −10 °C, killing nine people.  The snow continued to fall on the 10th.  Bitter weather may have wiped out some Alaskan reindeer as temperatures and snow depths exceeded those of the extraordinarily harsh winter of 1963 to 1964. January 7 saw the snow and ice continuing to affect the snow-tormented counties of Oxfordshire, Warwickshire, Buckinghamshire, Gloucestershire and Northamptonshire.  Because of the dangerous conditions concerning the build-up of snow and black ice, Banbury Museum, the Cherwell District Council customer service centre and Cherwell District Council tourist information centre were closed.  Many Oxfordshire locations had between 1 and 2 ft of unmelted snow by then.  Cherwell District Council workers began using a mixture of table salt and sand instead of their diminishing road grit and rock salt supplies. Record overnight temperature lows of -22.3 C were recorded in Altnaharra, Caithnessshire, in the Scottish Highlands on the 7th.  as another area of low pressure over the North Sea started bringing further fronts of snow over the east of Great Britain on January 7, accompanied with cold, easterly winds.  The snow continued in to the 9th and the UK almost ran out of road salt, rock salt, table salt and road grit supplies due to the heavy demand from various agencies and local government bodies. By January 7, 2010 twenty-two people had officially died in the UK because of the freezing conditions.  The Department of Health wildly overestimated that the cold weather could cause up to 40,000 excess deaths in the UK if it continued much longer.  122 people had already died in Poland, most of them reportedly homeless and 11 had died in Romania.  Deaths in Bosnia and Austria luckily stood at only 4, with Kosovo only losing 1 life so far. The heavy snowfall across the British Isles between January 6 to 9, resulted in large-scale traffic disruption, closed airports, many canceled trains and hundreds of school closures.  A polar low developing in the English Channel brought fronts of snow over southern England before moving south and dissipating.  Two middle-aged men died after falling into a frozen lake in Leicester, in the English Midlands.  27 major companies in Britain were ordered to halt using gas on the 9th in order to maintain supplies amid unprecedented levels of demand and major companies had their gas turned off on the 8th, in the first such move since 2003, although there was no immediate danger for households of supplies running out.  The Automobile Association, a motor vehicle breakdown service, said they had dealt with about 340,000 breakdowns since December 17, including a local government snow plough that had overturned after it had hit black ice near Huddersfield.  A representative from Oxfordshire also confessed to the local media that they had a snowplough breakdown earlier in the day.  Several thousand schools remained closed and several of the weekend's Premier League football matches were cancelled. Overnight temperatures of -22.3 C were recorded in Altnaharra in the Scottish Highlands.  Heavy snow fell in the North of England throughout the day giving significant accumulations.  A high of −7.7 °C was recorded at Tulloch Bridge. Heavy snow fell in Denmark on the 8th.  By the evening of January 8, the Rügen Islands off the northeast coast of Germany was covered, on average, in 30 centimetres of snow while the capital Berlin was carpeted with snow and ice.  Autoclub Europa warned that chaotic traffic conditions could potentially leave large parts of Germany completely paralysed as the country prepared for further freezing conditions as forecasters warned that temperatures would drop below −20 degrees Celsius (−4 degrees Fahrenheit) overnight.  German officials had also acknowledged a shortage of grit and fear that high winds and drifting snow had closed may German autoroutes and roads, along with parts of Frankfurt airport.  As shortages in road salt and grit were already being feared, while the authorities recommended that people consider stocking up with a few days' worth of food and water.  The A5 autoroute leading from Baden-Württemberg into France was closed on Friday afternoon, leading to a queue of hundreds of lorries building up.  The road was opened again on Saturday morning, enabling careful drivers to proceed with their journeys.  Snow was also proving to be a major problem in Saxony and the Rhineland.  The country's nature protection group N.A.B.U. has recommended that people put out food for birds since the cold and snowy weather had ruined their usual feeding patterns. Meanwhile, some light snow also fell in Kosovo, Udmurtia and Ryazan Oblast, but heavier snow fell in many parts of Ukraine.  Temperatures fell as low as −30 °C and −29 °C in parts of Ukraine and similar temperatures occurred in neighbouring parts of Russia.  Heavy show would hit Moscow on the 9th. A husky dog race took place in Thetford on the 8th. On Saturday, the 9th of January, Banbury officer PC Paul Froggatt urged people not to try walking on the frozen Oxford Canal by the Castle Quay Shopping Centre in Banbury or in Oxford city.  Thames Valley Police had also logged cases of people falling through the ice in Milton Keynes, Bracknell and Aylesbury.  One man even tried to drive his car down a part of the frozen canal in Oxford and nearly died after it fell through the ice, while another man died on Teesside as he tried to rescue some puppies that were stranded on melting ice on the River Tees.  Oxford City Council also warned of the dangers of playing on the frozen canal and/or frozen lakes. A new wave of heavy snow was first reported in central and northern of France on the 9th, with both the Cotentin Peninsula and Paris being the worst-affected parts of France.  The French government said all non-essential travel should be avoided in these localities.  Significant snowfalls caused major delays to train services, blocked roads and some 15,000 people in areas around city of Arles experienced power cuts power lines collapsed under the weight 30 centimetres of snow.  Airlines cancelled a quarter of flights on Paris's main Charles de Gaulle airport.  Road salt and grit supplies were running low in some districts due to their unexpectedly heavy use. Heavy rain and melt water caused the worst floods in Albania in nearly 50 years, as 2, 500 homes being evacuated.  An artificial lake in the Kosinj valley overflowed after it rose by 8 centimetres in 1 hour.  The worst hit areas in Albania were the Shkodra district and to a lesser degree Tirana. On January 10, Croatia declared a national flood emergency as the Neretva river valley is flooded.  The town of Metkovic was also flooded as the river reached a near-record high level.  Northern Croatia also got flooded in related storm system.  Heavy hailstorms also hit the Dubrovnik that day. Snow hit the Czech Republic on January 10. During the early hours of January 10, light snow showers spread across parts of Central England and Wales.  The maximum temperature was -13.5 °C in Altnaharra and low of -18.4 °C was recorded at Kinbrace.  On the 11th rain, sleet and snow travelled northwards throughout the early hours.  Allenheads in England had fears over a potential 15 ft snowdrift.  A low of -21.0 °C was recorded in Altnaharra in Highland Region. More than 300 flights were cancelled in the 10th and 11th at Germany's Frankfurt Airport.  All three runways were cleared and being used by the 12th despite of light snow falls that day. The snowstorms and blizzards of the 11th brought widespread travel chaos to Germany, Austria, Switzerland and Hungary.  Airports, Motorways and railways were closed en masse due to heavy snow and ice.  Lithuania, Latvia, Estonia, Finland, Liechtenstein, the Czech Republic and Slovakia also reported some snowfall. The \"Banbury Guardian\" and \"Oxford Mail\" released videos and photos of Oxford and Banbury on the 11th as part of a name and shame campaign against the idiotic people walking on the frozen canal. January 10 saw heavy snow fall in Chicago. On January 10, Los Angeles Mayor Antonio Villaraigosa cancelled a trip to Washington to monitor L.A.'s response to the Southern California rainstorms.  Villaraigosa had been scheduled to address the U.S. Conference of Mayors' winter meeting at the nation's capital on Wednesday and was going speak about the success of the city's summer jobs program for at-risk youth. Freezing rain storms battered parts of Sichuan, Yunnan and Hubei, killing dozens of people. On January 10, 1 person died and 5,435 were evacuated after a snowstorm in Xinjiang Uygur Autonomous Region according to the Ministry of Civil Affairs.  A total of 261,800 people in 12 counties or cities were affected by the blizzards. On January 11, intense snowstorms hit a still beleaguered Europe.  Many cars and a lorry were stuck in drifting snow near the northern German town of Soehlen.  The German transport ministry warned people to travel on essential journey only.  Polish authorities reported that there were about 140 hypothermia deaths in Poland, that nearly 50,000 Polish residents without electricity, that the PKP trains were delayed by as much as 9 hours and that most of Poland's homeless shelters were overflowing.  Some shops were running out of drinking chocolate.  A similar situation was also occurring in the shops and homeless shelters of the Czech Republic.  Snow also fell in Belarus. Coal supplies ran low at power plants as the death toll rose to two in the strong snowstorm in the Altai and temperatures fell to −40 °C on January 12. On January 12, heavy snow caused hundreds of accidents, halted flights, and downed power lines in Poland; more than 160 people were trapped overnight on a frozen stretch of German roadway.  Hundreds of road accidents were also reported in Germany over the weekend, especially along the Baltic Coast, where two men were killed in a car they were driving when it hit a tree in Nordvorpommern after skidding in the treacherous conditions.  Austria, Switzerland and Liechtenstein were also badly hit by snowy and freezing conditions. Both the National Beef Association and National Farmers' Union Scotland asked for the Scottish Government to help rebuild steadings, which in some cases are not covered by insurance.  Heavy snowfall continued for a second day in Aberdeenshire, Aberdeen and Rutland. A further 5 to 10 cm of snow fell across the UK during the midday of January 12, including Stafford in Staffordshire.  Many schools were once again closed across England and Wales.  There were many road accidents and closures; the M25 motorway was down to one lane between Leatherhead and Reigate whilst roads stretching right across southern Great Britain were untreated causing havoc for commuters.  Gatwick and Birmingham airports were closed and many flights were delayed at Heathrow. During January 13, the weather system continued north affecting much of Northern England before reaching Scotland. Eight of Oxfordshire's 33,000-volt electricity substations, which were situated in Kennington, Kidlington, Wheatley, Wantage, Deddington, Eynsham, Berinsfield and Cholsey, near Wallingford failed apparently due to an electrical overload, causing several local blackouts; later saboteurs were officially blamed in the damage, and not an overload or mechanical failure as initially thought, according to \"Oxford Mail\".  The only way into the Wye Valley on the 13th, was by snow plough, tractor or quad bike.  Teachers and parents helped dig out Princes Risborough School as about 6 inches of snow falls across Buckinghamshire and Oxfordshire, according to \"Bicester Advertiser\" and \"Bucks Herald\" newspaper.  Peter Cornal of Royal Society for the Prevention of Accidents and Principal Jonathan Johnson of Aylesbury Vale Academy both warned people not to skate on Watermead Lake in Buckinghamshire. On January 14, southerly to south-easterly winds brought bands of snow to northern parts of Britain.  An inch of snow fell in Glasgow with further accumulations in areas outside the other towns and cities.  Travel was disrupted somewhat. About 440 Welsh schools were either fully or partially closed on Thursday the 14th. The English snowfall began to ease on the 14th and the British government ordered an inquiry into the road salt and grit shortage scandal crisis, travel disruption and the poor handling of the disaster by various British companies and agencies. As the thaw took hold in the UK and France, rain began to fall across the UK.  An 11-year-old girl named Naeemah Achha slid up on a patch of ice outside St. Michael from St. John Primary School in Blackburn, Lancashire on the 14th and died later in Royal Manchester Children's Hospital the next day.  Head teacher Kay Cant described her as an intelligent, vivacious, kind and popular girl.  Blackburn with Darwen Council's leader Michael Lee described her death as a very shocking incident. It was revealed on the 14th that all of the British county councils, London boroughs and unitary authorities were advised by the government to have six days' supplies of road salt in 2009.  Up to 30 councils rejected last year's offer of thousands of tonnes of de-icing salt at a reduced price, to use on the roads this winter according to the BBC.  The British Salt Director David Stephen made an offer out of the firm's 60,000 tonnes stockpile at Middlewich, Cheshire, in April 2009, but virtually no one to take up the offer.  Evidence suggests that only a few took up the government's advice.  The Highways Agency had 13 days worth of road salt, while Glasgow, Kirklees, Derbyshire, Derby City, Buckinghamshire, Cherwell District and the London Borough of Harrow took up the offer and raised their stock piles to seven days worth.  The Cherwell Borough, Bicester Parish, Oxfordshire, Slough borough, Harrow, Leicestershire, Buckinghamshire, Fife, Brighton and Hove unitary authority, Liverpool city, Pembrokeshire, Lancashire, Belfast city and Perth and Kinross councils nearly to run out of salt.  Scotland, Cumbria, Northumbria and Northern Ireland were getting priority access to salt due to the severity of the local landscape and storm activity. There were also road closures in Ceredigion including the A4120 at Ponterwyd, and the A4086 at Nant Peris in Gwynedd had been shut down due to hazardous driving conditions.  All of Caerphilly's 90 schools and Rhondda Cynon Taf's (RTC) 130 schools either were fully or partially shut except to pupils sitting there.  To Wales' Education Minister Leighton Andrews, Welsh First Minister Carwyn Jones and the WJEC exam board helped organise the Welsh GCSEs and A-level exam locations.  A Merthyr Tydfil exam was held in a local sports centre's hockey arena according to the BBC. Temperatures of +43.6C were reported in parts of Australia as a heat wave hit most of the country.  Heavy rain also fell in parts of Indonesia and Queensland, Australia. As the snow cleared from the United States, the British Isles and Europe, rain began to fall in place on the 14th and the snow also began to relent in both China, Korea and Japan on the 15th.  Light snow was reported in Latvia on the 14th and 15th. On the 15th, Greater Manchester Police said seven people had been injured when 10 cars crashed on the A627(M) in Oldham. Met Office severe weather warnings covered Wales and western parts of England and Scotland and snow was forecast for the Pennines and Scottish and Welsh hills.  The Environment Agency has warned about flooding in some areas of England and Wales.  BBC weather forecaster Sarah Keith-Lucas warned of localised flooding in western parts of the UK caused by melting snow, rain and gale-force winds. Later on, the Environment Agency warned that heavy rain and snow melting in slightly warmer temperatures meant that there was a risk of some localised flooding from drains, especially in Wales and parts of England, but homes were unlikely to be affected as 15 flood watches were put in place.  They warned that flood warnings for possible isolated river flooding in these areas, as heavy rain moves in from the south west on Friday evening.  The Met Office and said heavy rain would move in from the west accompanied by strong to gale-force winds on Saturday.  The Scottish Environment Protection Agency had five flood watches under way.  Thames Valley Police warned motorists about surface flooding and aquaplaning on Oxfordshire's roads. The 15th brought rain and thawing to much of southwest England and a pocket around Greater Manchester.  In the city of Manchester rain on frozen ground had overnight caused glazed frost which had largely (but not completely) gone by morning, and at midday the clouds were running fast from the south; the January snowfall was melting fast, but the pre-Christmas snowfall, which was trodden had half thawed to slush and then frozen hard to solid ice overnight, was slower to melt.  In Scotland the thaw melted fresh snow.  Flood warnings were issued. Snow fell in parts of southern Scotland and Yorkshire on the 16th.  The Cairngorm Ski Centre was closed as an army of diggers was trying to clear its car park after further snowfalls.  The river Derwent was put under a flood watch in places on the 16th. On January 17, Inner Mongolia Autonomous Region was given more than 850 tons of grain, edible oil and other daily supplies, 4,950 tons of coal and 900 tons of emergency cattle fodder. On January 17, both melting snow, heavy rain and frozen ground have caused flooding in much of England, Scotland and Wales.  The Environment Agency issued a total of 23 flood warnings were issued in England, one in force for Scotland and two in Wales.  In Scotland a landslide led to the temporally closure of the A76 in Dumfries and Galloway, while a section of the M74 was shut temporally shut due to flooding. Many properties in Maesteg, Bridgend, Ebbw Vale and Monmouth were also flooded.  Mid and West Wales Fire service sent extra appliances to the Carmarthen Bay holiday village at Kidwelly, where the flooding was described as 'major to severe'.  The River Wye, River Dee (\"Welsh: Afon Dyfrdwy\") in the lower Dee Valley, River Cherwell (at Banbury) and River Severn were all in full flood. Severn Trent Water, which provides water to central England and parts of Wales, said it had drafted in extra staff as ruptured and frozen pipes began to thaw.  Many pipes had ruptured in the Welsh Marches and a sewerage pipe had become blocked in outer Birmingham.  The company's senior operations manager, Fraser Pithie, said the firm had been called out to more than 470 burst and ruptured pipes since the 10th and their call centre staff had taken more than 10,000 calls. From Sunday, January 17 to Saturday, January 24, a series of five very powerful winter storms bore down on the Western United States (especially Southern California), and spread eastward on the 20th, piling more snow across the Great Plains and even as far as parts of Pennsylvania and New Jersey.  200 homes were evacuated as floods hit parts of California as up to 6 in of rain fell in the first of the storms.  Rescue and flood prevention services soon got everything under control, but one man was killed when a tree fell onto his house.  Two horses died after being hit by lighting in a Santa Barbara field.  Rare tornado warnings were enforced in parts of Southern California, including southern Los Angeles, Long Beach, southeastern San Diego County, and Anaheim.  Flash flood watches covered Los Angeles, Santa Barbara, and Ventura counties.  An EF-0 tornado was reported in southeastern San Diego County that lasted for 1 minute, 30 seconds.  Jim Rouiller, the senior energy meteorologist at Planalytics Inc., said that the storms (more specifically, the fifth storm, which was the strongest one) were the worst series of storms since 1997 (the one that caused the ).  The fifth storm also neared the record of the lowest pressure system ever experienced in San Diego, registering a minimum pressure of 973 mbar while it was in the Pacific.  Rouiller expected that evening's storm to bring from 4 to of rain, severe mudslides, a few tornadoes and heavy mountain snow ranging from 6 to across the Sierra Nevada.  A heavy rain storm hit Santee, California, on January 19. Heavy rain fell in Oxfordshire and King's Sutton, causing some flooding.  The River Cherwell nearly flooded Banbury and parts of Oxford.  Light snow fell in the Pennines the 19th. California state water officials warned on January 21 that one week of heavy rain and snow was not enough to end the drought, which is entering its fourth year by 2010.  According to measurements on the Thursday, the average water content of state's mountain ranges' snowpacks, considered the state's biggest \"de facto\" 'reservoir', was at 107% percent of normal capacity.  As the fourth and second strongest of the week's storms slammed into California, officials predicted as much as four feet (1.2 metres) of snow would fall in Northern California. Freshly fallen snow blanketed the north side of the San Gabriel Mountains on the morning after the fourth storm of the week dissipated on January 23, 2010, northwest of Wrightwood, California.  Snow was reported in many parts of California.  On January 24, the 5th and strongest storm caused heavy rain fell in parts of Los Angeles.  500 people were evacuated from a small village in La Paz County, Arizona due to a flash flood. On January 20, heavy rainstorms wreaked havoc in Haifa, Israel, as snow covered Mount Hermon in the Israeli-occupied Golan Heights. Both Wakehurst Place, near Ardingly, West Sussex and Bedgebury Pinetum, near Lamberhurst in Kent, were damaged.  Iain Parkinson, Wakehurst's woodland and conservation manager who has worked on the estate for 23 years, said it was the worst storm he had ever seen in the park. A new snow storm came over the Peter the Great Gulf, near Vladivostok on January 21.  The Onland Mountains and Sikhote-Alin Mountains were buried in snow as the weather system swept through Primorsky and Khabarovsk Krais, Russia.  Lake Khanka froze over and was then covered in snow.  The ice on the lake was reported to be several inches thick and the snow was spreading into China's Heilongjiang province. January 21 witnessed the final day of some Banbury schools remaining closed, after 2 weeks of general closure, in which most remained closed throughout the fortnight.  The video of people skating and walking on the Oxford Canal was shown on regional TV again, along with a nationwide online version.  Heavy rain fell in Oxfordshire, Warwickshire and Leicestershire as light snow fell in the Pennines on the 22nd. The snowstorm that began on Friday night, the 22nd was also hitting parts of Bulgaria, where a man suffering from a heart attack died in the country's northeastern region of Silistra, when the ambulance he was being carried on was road that was blocked by several snow drifts. Power, gas and water cuts occurred in Istanbul.  In Turkey's western region, near the Greek and Bulgarian borders, villages were cut off and all the major roads were blocked by heavy snow.  A bus overturned in Istanbul, injuring 10 people; Bulgarian authorities urged people to avoid travel due to heavy snowfall.  Dozens of stranded cars and trucks were abandoned due to heavy snow in eastern Bulgaria and a train was trapped near the border with Romania.  Some snow was also reported in neighboring parts of Romania. A total of four people died on the 24th and 25th as snowstorms hit Turkey.  The frozen body of 78-year-old Mehmet Aksit was found outside the small town of Akkisla in central Kayseri province after relatives reported him missing.  On the 24th, Nuri Turhan, an 81-year-old Korean War veteran's corpse was found in a mountainous region of Turkey's Aydin province, where he had gotten lost walking the day before and a 75-year-old man died of hypothermia in the northwestern Turkish province of Tekirdag. The 4th person was found dead in a fierce snowstorm that caused power outages in Turkey and traffic chaos in neighbouring Bulgaria on the 25th. Turkish climatologists working for Istanbul's Natural Disaster Coordination Committee told Turkey's \"Hurriyet Daily News\" that snowfall was predicted to continue until Monday evening and reach about 35 cm in parts of Istanbul. It was revealed that at least 22 people had died due to extremely cold temperatures in Romania in the last five days by January 25. From late 2009 through early 2010, a series of massive snow and ice storms that swept the Dakotas caused a number of Indian Reservations to lose power, heat, and running water for an extended period of time.  The storms were most severe in both Ziebach and Dewey counties, South Dakota.  The heavy snow, ice storms and low temperatures of January 26 led to Interstate 90 being closed from Chamberlain to the Minnesota border.  At nightfall on Monday the 25th, Interstate 29 was closed from Sioux Falls to the North Dakota border. Power company officials estimated that about 7,600 customers in South Dakota and 100 in North Dakota did not have power on Monday.  Some phone systems have also experienced brief telecommunications outages.  Kristi Truman, director of the North Dakota Office of Emergency Management was concerned about failing water and power supplies. Power outages began with a storm in December knocking down around 5,000 power poles, and has been accelerated by an ice storm on January 22 knocking down another 3,000 power lines on the reservation.  Among the tribes of South Dakota said to be suffering from the multiple storms are the Cheyenne River Sioux, Crow Creek Sioux Tribe, Flandreau-Santee Sioux Tribe, Lower Brule Sioux Tribe, Rosebud Sioux Tribe, Sisseton-Wahpeton Oyate and Standing Rock Sioux Tribe. \"There's been winters this bad before, but not with rain so bad it freezes the power lines and snaps the poles\", said Joseph Brings Plenty, the 38-year-old chairman of the Cheyenne River Sioux tribe.  The worst day was on the 28th, when a Native American froze to death in his home after running out of fuel. The reservations were further buffeted by the February 5–6, 2010 North American blizzard, which further depleted foodstocks and exacerbated the power problems. 25 Nepalese people, mostly children, died as a blizzard swept over most of Nepal on January 7.  Snowstorms probably also occurred in mountainous Bhutan and Sikkim, but no reports were forthcoming. On January 28 floods tore through the River Dnieper's and Prypiat River's floodplains.  Climatologists from the Ukrainian Hydro-meteorological Centre warned that the 2010 spring flood could be most severe in the past 10 years and that snow packs in some regions are more than usual.  They also told the Ukrainian news agency UKRINFORM that February would bring more snow.  Weather forecasters said that the abnormal winter in Ukraine has seen severe snowfalls being replaced with long-lasting frosts and temperatures as low as −30 °C and the predicted flooding was an inevitability, both along the Prypiat River in the Volyn Oblast and in Ternopil Oblast and the River Dnieper in Kiev as the snow layer is twice as high than in recent years. A stiff frost, temperatures of −2 °C to −3 °C and a light mist occurred in Oxfordshire and western Northamptonshire, during the night as a cold front crossed the UK.  Several cars crashed due to skidding on black ice. On January 28, blizzards and severe sub-zero temperatures that have killed more than a million livestock in Mongolia would force thousands of herders to migrate to shantytowns near the capital city of Ulan Bator.  Extreme winter weather that began in December and followed a hard summer drought that prevented farmers from stockpiling food for their livestock.  Dangerously heavy snow and temperatures as low as −40 °C have affected 19 of Mongolia's 21 provinces, with more than 14,000 Mongolian and Chinese Red Cross volunteers across the region scrambling to deliver emergency food aid to impoverished herders who have lost nearly all their cattle. The extreme weather had killed more than one million livestock, the herders' main source of income, according to the National Emergency Management Agency (NEMA) on the 29th.  The bad weather has also reduced food security, intensified poverty and increased domestic rural-urban migration for many families.  Central Dundgobi Province is one of those in the grip of a 'dzud' event (that is to say a 'major natural disaster').  A local herder named Javzmaa Batbold, a herder in Adaatsag County, Dundgobi province said that 120 of his 500 cattle had died so far.  His ger (a traditional Mongolian felt and wood dwelling) was completely covered in snow, and only when the door's side cleared off did it reveal a dwelling lay underneath.  The weather became rather extreme as 80 percent of Mongolia's territory was covered by snow, with depths of 20 to 90 centimetres. The State Emergency Commission issued an appeal to Mongolia's people to launch a campaign to offer aid to herders and called for the international community to donate food, medicines and equipment as well as funds to help herders.  Most cattle in Adaatsag county of Dundgovi Province (Central Gobi), Mongolia died of hypothermia on the 30th.  Fuel supplies were also running low. Snowstorms hit the east coast of the United States and the Sierra Nevada of California on the 30th and 31st. Heavy snow fell in the German Rhineland on the 30th and 31st.  Travel chaos was widespread as the snow spread into Hungary and the rest of Germany. Heavy snow storms also hit Luxembourg on January 31. The 30th saw heavy snow storms hit the East Coast of America. A nor'easter dumped tons of snow over much of central and eastern parts of the United States.  The nor'easter unleashed heavy snow over the Central United States, and left some places with over 15 in of snow.  It then moved eastward and made a sly turn up the coast of America, and left some places with over 12 inches of snow on Saturday, January 30, 2010.  Rain and floods also hit Southern California.  6 inches of rain and temperatures of 15 °C were recorded at Long Beach and L.A. on the 31st. On February 1, the Chinese government aid arrived in Mongolia and included 10 million Yuan's (1.46 million U.S. dollars') worth of food, portable power generators and quilts.  Mr. Miyeegombyn Enkhbold, Deputy Prime Minister of Mongolia and head of the State Emergency Commission, said the aid given showed an embodiment of brotherhood between China and Mongolia, and Chinese Ambassador Yu Hongyao said he was confident that Mongolians would overcome the disaster in time.  It was said to be the worst snow storms in both Mongolia and Inner Mongolia in at least 30 years.  The Red Cross Society of China has also announced a donation of 30,000 dollars for Mongolia, and Kazakhstan sent several hundred tonnes of food aid to the country.  Mongolia formally requested aid from the United Nations on the 2nd, after the extreme cold is calculated to have killed about 3% of the nation's 44,000,000 head of cattle thus far. On February 1, utility crews were working overtime to get power back to the 14,000 residents of Cheyenne River Sioux Reservation.  The wind chill factor averaged about −25° and there was about 1-foot of snow on the ground on average. A powerful Alberta clipper-type storm came from the north and headed towards an area of moist air left over from the previous day's thunderstorm over the Midwest on February 1, and was merging over central North America.  It was estimated that it could potentially drop up to 6 inches on the Canadian Prairies and North Dakota initially and eventually add 2 more inches as it hung around overnight. February 1 saw the earlier Californian storm track its way through northern Mexico and New Mexico.  It would shortly hit the east coast and cause chaos in the Washington, D.C., Virginia and Maryland areas by the February 6. A cyclone and heavy rain-induced floods hit the Canary islands on February 1 and 2.  An unusual event dropped 82 mm of rain and flooded out 27,000 homes over a 24-hour period.  Some mobile phones become inoperable due to storm-related interference and power supplies were cut overnight in some places.  The average February rainfall is only 36 millimetres. February 4 saw the worst winter in Beijing in 50 years, worst in Seoul in 70 years. The snow and frost activity was proving to be exceptionally bad that winter across northern India.  Official reports suggest that the states of Punjab, Bihar, Haryana and Uttar Pradesh have borne the brunt of the freezing temperatures in India. Officials at Canada's Vancouver Games were also worried about the lack of snow and frost as the city experienced its warmest January ever as the temperature never went below −2 °C.  The average daily high and low for January 2010 was 9.7 and 4.3 °C at the airport, which was way above the average high of 5 °C and the average low of −1 °C for January.  No snow was also recorded in January compared to an average of about 17 cm and an average snow depth of 1 cm. The east coast had records various record low temperatures in southern states such as Georgia Alabama and Florida in the US. The eastern seaboard, had like the Western Seaboard also suffered one of the worst winters on record.  An already snow bound Washington, D.C. was expected to receive up to another 20 inches of snow that weekend.  Snow was also threatening the American football championship event known as the Super Bowl. 11 were killed by avalanches in Farah, Bamyan, Ghor and Daykundi provinces between the 4th and the 8th a spokesman of the Afghanistan National Disasters Management Authority said.  The Afghan flash floods and avalanches left 20 others dead in the rest of the country on February 8.  The provincial authorities had summoned an emergency meeting and loya Jurga to discuss responses on February 8.  Both Shah Wali Kot and Shorandam districts were the worst affected.  Afghanistan generally expects about 400,000 people every year, according to experts at the United Nations Office for the Coordination of Humanitarian Affairs (OCHA). A series of avalanches caused by a storm in eastern Afghanistan killed 172 people on February 8 and 9. Power outages in both the Dakotas now covered only 100 rural electric customers and minimal numbers in Bismarck, North Dakota by February 5. The second big snowstorm of the winter hammered the Washington, D.C. area on February 5.  The El Niño phenomenon was blamed for the unusually high sea surface temperatures in the Pacific Ocean that moved east, thus pulling rainfall along with it.  Normally, El Niño brings increased rainfall across the east-central and eastern Pacific, leading to drier than normal conditions over northern Australia, Indonesia and the Philippines. The storm created historic snowfall totals in the Middle Atlantic states, rivaling the Knickerbocker Storm of 1922, as well as extensive flooding and landslides in Mexico.  The blizzard stretched from Mexico and New Mexico to New Jersey, killing forty-one people in Mexico, New Mexico, Maryland, Virginia and other places along its track. Some places across Eastern West Virginia, Maryland, Northern Virginia, Washington, D.C., and Delaware were buried in between 2 and of snow, causing air, rail, and Interstate highway travel to come to a halt.  On February 6, authorities said about 4 inches of rain fell in Hollywood Hills and 3.2 inches in Santa Barbara. The snow storms of February 6 and 7 left record levels of snow in many cities.  Approximately 2 ft of snow had fallen by midday on Saturday.  The mayor of Washington, D.C., and the governors of Virginia and Maryland declared states of emergency as the storm hit both Washington, D.C., Virginia, Maryland, West Virginia, Delaware, Pennsylvania and New Jersey.  Amtrak canceled several trains between New York and Washington, D.C., and also between Washington, D.C. and some southern destinations.  The storm arrived less than two months after a December storm dumped more than 16 in of snow in Washington, D.C..  Many power outages were also reported in the city. February 7 saw yet more major power outages in the Washington, D.C. region.  At least 420,000 homes were under a blackout as the snow felled trees and cut power lines.  Some 300,000 homes were without electricity in Maryland and neighboring Virginia, while Washington, D.C. was reporting an initial figure of 100,000 power outages for that day.  Emergency workers struggled to restore power as 2 ft of snow and a record snowfall of 3 ft fell on Maryland.  Transport was badly disrupted from West Virginia to southern New Jersey.  Washington, D.C., Virginia and Maryland have declared a short term state of emergency, allowing them to activate the National Guard in order to help cope with the storm's onslaught. A father and daughter of McKeesport were killed by carbon monoxide poisoning after improperly using a domestic petrol generator after a power outage.  On February 7 two children drowned trying to cross the swollen Chapulin River in the central state of Guanajuato, and other child deaths occurred in Angangueo, Zitácuaro, and Ocampo.  In total, twenty-eight deaths in the states of Michoacán, Mexico State, and the Distrito Federal (Mexico City) in Mexico have been attributed to the storm on February 6 and 7. On the 7th 10 people are dead in across Kandahar Province according to the Afghan Red Crescent Society (ARCS).  Najibullah Barith, ARCS's director in Kandahar complained of lack of the local and national resources. A series of avalanches caused by a storm in eastern Afghanistan killed 172 people on February 8 and 9. On the 7th 10 people are dead in across Kandahar Province according to the Afghan Red Crescent Society (ARCS).  Najibullah Barith, ARCS's director in Kandahar complained of lack of the local and national resources. 11 were killed by avalanches in Farah, Bamyan, Ghor and Daykundi provinces between the 4th and the 8th a spokesman of the Afghanistan National Disasters Management Authority said.  The Afghan flash floods and avalanches left 20 others dead in the rest of the country on February 8.  The provincial authorities had summoned an emergency meeting and loya Jurga to discuss responses on February 8.  Both Shah Wali Kot and Shorandam districts were the worst affected.  Afghanistan generally expects about 400,000 people every year, according to experts at the United Nations Office for the Coordination of Humanitarian Affairs (OCHA). A series of avalanches caused by a storm in eastern Afghanistan killed 172 people on February 8 and 9. The 2010 Salang avalanches consisted of a series of at least 36 avalanches that struck the southern approach to the Salang tunnel north of Kabul, Afghanistan on February 8 and 9 in 2010, burying over two miles of road, killing at least 172 people and trapping over 2,000 of travellers.  They were caused by a freak storm in the Hindu Kush mountains located in Afghanistan. Several heavy avalanches killed 15 and on a highway north of Kabul killed at least 15 and injured 55 on February 8 and 9, according to the Afghan Ministry of Public Health (MoPH).  A few other avalanches and landslides also hit other parts of the snow laden Hindu Kush at this time. 60 died and hundred were still missing on February 9 a treacherous mountain pass in Afghanistan series of avalanches smashed into and badly damaged an 2.6 km long Soviet-built Salang tunnel after several days of heavy snow in the Hindu Kush.  Afghan Interior Minister, Hanif Atmar 24 dead were found and 40 more were feared dead.  The Defence Minister, Abdul Rahim Wardak said 3,000 people had been trapped in vehicles along the mountain pass, that is at an altitude 3,400 m , but about 2,500 were rescued later that day.  The road passengers got trapped in their vehicles outside of the Salang Tunnel, 9 miles north of Kabul.  The International Security Assistance Force (ISAF) sent 2 helicopters to help evacuate people and drop essential supplies, a spokesman for the Afghanistan National Disasters Management Authority (ANDMA) said.  Only helicopters and pack horses could get there. A powerful Alberta clipper from western Canada and moist air from a line of thunderstorms over the mid-western United States merged over the central US on the 7th.  In some places this dropped 6 in over the Central and Eastern United States, much of it over areas already hit by the previous blizzard.  10 - was expected to fall on Washington, D.C., and up to 1 - of snow was forecast from New York City northward into New England.  Several inches of snow fell in both Washington, D.C. and New York City on the night of February 10 and 11. On February 8, there was nearly 45 cm of snow recorded at Reagan National Airport, and nearby Dulles International Airport recorded a local record of 80 cm .  The storms were dubbed \"Snowmageddon\", by US president Barack Obama, after an SUV in his motorcade carrying journalists was damaged by a snapped limb, injuring one person.  An area all the way from Pennsylvania to New Jersey, across the BosWash corridor and south down to Virginia, was under at least 60 cm of snow.  Parts of Northern Maryland had 90 cm .  The storm killed three people. Snow began to fall throughout Oxfordshire, North London, and Bavaria on February 8 and February 10.  Heavy winds and snow flurries also hit parts of Leicestershire, Northamptonshire, in Turkey, and Amur Krai, on February 9.  1 person suffered moderate injuries after slipping on ice in one of the rural northern districts of Amur Krai.  On February 10 a deep cyclone formed over the Adriatic Sea bringing heavy snowfall over large parts of Bulgaria, Serbia, Macedonia and lesser parts of Romania.  Severe weather warnings have been issued in these countries.  In Western Bulgaria it snowed non-stop for more than 48 hours, with up to a metre of snow accumulation in Kyustendil. United States federal government activities were ground to a halt by the, in some places 3 ft deep, snowfall in Washington, D.C. on the 10th. February 11 saw heavy snow east coast USA, but power workers resorted electricity supplies to 100,000 users in Maryland on February 11 and some more was predicted for the next day. On the 11th, about least 10 cm of snow fell in some areas, and strong winds caused drifting in places across Kent and East Sussex.  Snow also fell in Sheffield, Berkshire, Brighton and the Grampian Mountains.  Dover's coastguard helped dig out a stranded ambulance in the town.  Kent County Council said all primary routes and secondary routes where possible had been gritted on Wednesday afternoon ahead of the snow showers.  Kent Fire and Rescue Service said it was helping some motorists stuck in snow drifts in parts of the county including Margate, Dover and Lydd.  Chief Inspector Simon Black, of Kent Police, said that all main routs were passable, but some were down to a single snow-free lane and BBC Radio Kent said that drivers should make essential journeys.  Also 4 of its 4x4 vehicles were helping to transfer staff and patients to and from hospitals in Eastbourne and Hastings, in East Sussex.  The exit slip road off the M20, at junction 11a, closed because of the snow.  A Dover lad was moderately burnt after spilling paraffin he was trying to put into a portable domestic generator unit that day. Following a storm in the southeast United States on February 12–13, snow was on the ground simultaneously in all 50 U.S. states, an event believed never to have occurred previously.  Snow was confirmed in 49 states by February 13, and small patches of remnant snow on the north face of Mauna Kea in Hawai'i were confirmed soon after.  The snow on February 12–13 forced the cancellation of Bi-Lo Myrtle Beach Marathon XIII after the 5k portion had been run the previous night.  A record depth of 12.5 inches of snow fell within 24 hours in the Dallas–Fort Worth urban metro on the 13th.  The result of the massive snowstorm led to whole school districts being shut down and over 200,000 buildings were left without power, and forced to use fire and candles to light and heat their homes.  A 1 ft tall wind-induced wave swept 3 people off of various southern Californian beaches on the 14th, slightly injuring one.  Light snow fell in both parts of Hampshire and Fife, and heavily in the mountainous regions of Sakhalin Island on the 15th.  About 1800 out of 2400 flights at Hartsfield Jackson Atlanta Airport were cancelled on February 13.  Numerous minor car accidents happened all around the Metro Atlanta area. Moderate amounts of snow fell in Gwent and Northamptonshire on the 16th.  Weather forecasters warned of more snow predicted across South East Wales and parts of Central England.  Moderate snow fall was reported in both Oxfordshire, Northamptonshire, Leicestershire, the Malvern Hills, Pembrokeshire, Bristol, Aberdeenshire and Greater London on February 17 and 18.  The various London Authorities warned of growing travel chaos in London on the 18th. Over two inches of snow had fallen in part of Gwent, Pembrokeshire and Gloucestershire on the 18th.  The roadways in the Gloucestershire town of Dursley had been quickly covered in snow that as a cold weather front moved over the west of the county of Gloucestershire, Chief Inspector Steve Porter, of Gloucestershire Constabulary told motorists to take care and warned that many roads including the B4221 at Gorsley and the A4136 at Longhope had had major accidents.  Most of the roads would be closed due to heavy snow for the next few days.  Moderate amounts of snow also fell in part of Oxfordshire and Northamptonshire on the 19th. Heavy snowfall occurred in Wales and the Midlands between February 18 and 20. <ref name=\"Review of UK weather on 18/02/10\"> </ref><ref name=\"Review of UK weather on 19/02/10\"> </ref><ref name=\"Review of UK weather on 20/02/10\"> </ref> including in the Welsh town of Pant Glas, Gwynedd between the 18th and 20th. Heavy snow in Ireland forced the cancellation of the National Hurling League (NHL) hurling match between Kilkenny and Tipperary at Semple Stadium in Thurles on February 20.  A horse racing meeting in Naas was abandoned due to heavy snow the following day. Light snow fell in Oxfordshire overnight on the 20th and 21st, causing minor traffic disruption.  Manchester Airport was heavily disrupted late on the 20th, closed at 07.10am on the 21st, but opened later that morning after much heavy work clearing the snow off of the runways.  Flights were still disrupted by midday. A Eurostar train, with between 700, 740 and 800 passengers on board, broke down in Kent due to the extremely cold weather and heavy snow in the United Kingdom. The Paris-to-London service eventually left Gare du Nord railway station in Paris two hours late at about 8.15pm UK time after an unattended bag caused a security alert.  Everything was going according to plan, until they were two minutes away from Ashford at 10.15 pm, when the train suddenly stopped, the lights flicked briefly and all power failed, leaving the victims in complete darkness at around 10.45 pm.  Passengers reported that the lighting was flickering on and off in a \"very spooky manner\" throughout the journey, until the Eurostar suddenly stopped, and the lights finally went out completely.  It then became stiflingly hot inside the train as the ventilation system shut down due to the power outage on board the stricken Eurostar train.  The chaos grew when most of the toilets stopped working and the staff struggled to find emergency lighting switches.  Eventually the train manager walked through all the train carriages, using his emergency torch, announcing that another train and a bus were being sent to rescue them.  Passengers carrying luggage then had to clamber down ladders onto the tracks and then back up onto the rescue train that arrived alongside the failed one.  The rescue train arrived at St Pancras just after 2.30am on the 22nd, more than four-and-a-half hours later than the scheduled arrival time and the bus also took a few to Ashford, where they took local trains to London.  Eurostar technical spokesman, Mr. Bram Smets, said it has stopped in what he described as a \"major technical problem\".  Eurostar has launched a probe into the breakdown which left passengers stranded for 4 hours. Between 6 a.m. and 11 a.m. local time (and UTC), 108 mm of rain was recorded at Funchal weather station and 165 mm of rain at the weather station on Pico do Areeiro.  The average rainfall in Funchal for the whole of February is 88.0 mm .  Damage was confined to the southern half of the island. About 42 people were reported dead and about 68 to 100 injured as rain-induced floods and mudslides hit the Portuguese island of Madeira on the 20th, local authorities said.  The floodwaters tore down buildings, overturned cars and knocked down trees. The local civil protection service declared that it was \"overwhelmed\" by emergency calls, according to a duty police officer in an interview with a journalist from the Reuters news agency.  According to Portuguese media, the storms were the deadliest on the east Atlantic island since prior to October 1993, when 8 people died and 19 were injured. The Portuguese military sent specialist rescue teams to the island of Madeira on the 21st, when it was estimated that least 38 people were known to have died in the most extreme rainstorms in 17 years as tonnes of mud and stones brought down the slopes of the island, flooding the streets of the regional capital, Funchal, and other towns.  All utilities were knocked out across large swathes of the island.  Ribeira Brava was also badly flooded on the 21st.  Portuguese Prime Minister Jose Socrates, who was in Madeira, ordered an immediate rescue and aid mission for the island.  Portuguese President Cavaco Silva expressed his condolences in a televised statement. A Portuguese Navy supply ship, with a helicopter, food and medical equipment was sent to the islands that lie about 900 km/560miles from the Portuguese mainland.  The Island is very popular with foreign tourists.  Officials from the Portuguese Department of Emergencies said that emergency teams, 56 military rescuers, search dogs and 36 firefighters were being sent to the island straight away. The Island's authorities and the local civil protection board told CNN that one British tourist was missing.  10 British music students from a religious school in the Channel Islands were found alive in Funchal along with an Irish tourist as the city is thrown into meteorological chaos.  Local authorities informed the AFP news agency that 70 people were hospitalized. On February 22, 102 to 120 were confirmed injured and 42 were confirmed dead in Madeira.  2 British tourists were among the dead. Heavy snow fell in Oxfordshire, the English Midlands and Merseyside on the 23rd and heavy snow fell in the Scottish Highlands on the 24th. On February 24, a meteorological state of emergency was declared in 21 Mongolia's provinces due to the exceptionally cold conditions.  It is purported to be the coldest snap for 50 years and that a person has already died of hypothermia. A major winter storm started plowing its way through upstate New York, southern Vermont and Berkshire County, Massachusetts on the 23rd.  Colonie, New York saw a total of 17 inches of snow.  Hancock, Massachusetts had seen 9.5 inches, with the forecast total at 10-15 inches, and Stamford, Vermont had seen 9 inches. Salem County, New Jersey was hit by heavy snow fall on the 24th, and 4 to 12 inches of snow were predicted to fall the next day by weather forecasters.  Just under 1,000,000 were left without electricity in New England, Pennsylvania and New York. On February 23, snow fell in Southeast Texas.  Snow accumulated in counties north of Houston such as Conroe, Texas, which received 2 inches of snow.  Huntsville, Texas had seen about 2 to 3 inches as well as College Station.  Only a trace of snowfall fell in Houston, Texas. A winter storm warning was issued for counties north of Houston, while Houston had a winter weather advisory. The extreme winter storm spun around the Northeast by the 25th.  Oneonta, New York had seen 44.5 in of snow from this storm.  New York City had seen 20.9 in , and Philadelphia had seen 11.5 in of snow.  Washington, D.C. and Baltimore had remained under light rain.  Boston has seen 4.5 in of rain from this storm, and 9.2 in was reported in Belfast, Maine.  Major flooding prevailed along coastal New England.  The Shawsheen River at Wilmington, Massachusetts was at 8 feet above its banks, and the Merrimack River at Amesbury, Massachusetts was at 20 feet above banks.  On Friday morning and night, most of Massachusetts, New Hampshire and Maine were without power.  Sustained winds of up to 65 mph pounded the Cape Ann area of eastern Massachusetts.  Wind gusts of up to 95 mph were reported in Gloucester, Massachusetts and Rockport, Massachusetts.  Millions of trees were felled in the wind from Providence to Bangor.  Boston received 52 mph sustained winds and 74 mph gusts.  Between February 25 and 26 flood warnings covered most of Rhode island, the Pawtuxet River valley, Boston and Norfolk County in eastern Massachusetts.  Orange County, New York saw 2 to 3 feet of snow in Montgomery and Bullville. Widespread snowfall across Scotland brought extreme disruption.  61 cm of snow was recorded in Aviemore as hundreds of people were stranded in cars in Dunblane.  Schools and transport services were disrupted.  Two people died in Glencoe in an avalanche.  Temperatures reached −19.2 °C in Braemar.  Some people in Perthshire were stranded in their cars for 17 hours.  45,000 homes in Scotland were left without power.  30 schools were closed on the 25th. On February 26, heavy snow hit New York and more was forecast for the next day. On February 27, unusually heavy monsoon rains hit Haiti.  11 people died the town in Les Cayes, 3 died in the village of Torbeck, 5 died in the village of Les Cayes's Gelee as rain induced mudslides and flood waters hit them. In January 2010, heavy rain caused flooding which buried or washed away roads and railways leading to the ancient city of Machu Picchu, trapping over 2,000 tourists in addition to 2,000 locals.  Machu Picchu was temporarily closed, but it reopened on February 28, 2010.  Peru's rail crew has been hard at work since then, and Machu Picchu reopened to tourists on April 1, 2010. 51 were killed, 59 injured and 12 are missing in France, 6 killed in Germany, 3 killed and 2 severely injured in Spain, 1 killed in Portugal, 1 in Belgium, 1 in the Netherlands and 1 in England as heavy rainstorms hit the Bay of Biscay and central France on the 27th.  Winds of up to 140 km/h caused chaos as the storm moved from Portugal up through the Bay of Biscay, while a maximum gust of 228 km/h in Spain and 241 km/h in France were recorded.  Both Belgium and Denmark were hit by heavy rainstorms overnight as the windstorm moved further northeast.  The storm system moved across the Massif Central into the Brittany peninsula, and areas of France bordering Belgium and Germany are on alert for heavy rain and high winds.  An Italian was also injured when a hurricane-force wind capsized his yacht off the coast of Portugal.  The French President Nicolas Sarkozy expressed his condolences to relatives of victims, and said that he would visit the stricken area on Monday.  The French Prime Minister François Fillon said France would formally declare the storm a natural disaster, freeing up funds to help communities rebuild themselves.  Rail services were severely affected in northern Spain and a number of trains in western France were delayed because of flooded tracks in southern and western France.  British Airways canceled several flights and Air France said 100 of its flights had been cancelled from Charles de Gaulle airport in Paris.  Wind speeds hit 175 km/h at the top of the Eiffel Tower, French radio reported on the 28th.  The French meteorological service said that shortly after 1700 local time (1600 GMT) the storm passed into Germany, Belgium and the Netherlands, and there also were reports of high winds in the Swiss Alps on March 1.  Spain's Canary Islands, particularly La Palma, Gran Canaria and Tenerife, were also hit by the storm, although there was no great damage.  The French Departments of Vendée and Charente-Maritime were in ruins and over 1 million homes lost electricity supplies.  Television, radio and mobile phone service were also disrupted in some places. On February 28, the French storms killed 7 people who drowned in various villages in the Vendée region, while 3 seniors and a child were found dead in Charente Maritime near La Rochelle.  A man was killed by a falling tree branch in the south-western town of Luchon, where winds reached 90 mph.  At least five other people were reported missing and dozens others injured and a kid died in Northern Portugal 5 people were missing and 1 was injured in Aytre.  The French Interior Minister Brice Hortefeux declared the storm as a natural catastrophe.  The Météo France weather agency predicted the storm was by then heading towards Denmark. It was confirmed on March 1 that 45-52 (reports vary) French people had died, and that the French government had launched an equine in to why the incident had been so disastrously mishandled.  especially around La Rochelle and L'Aiguillon-sur-Mer.  Nicolas Sarkozy visited the coastal town of Aiguillon-sur-Mer, the hardest hit area of France where a total 25 had died in the storm. The 2010 Queensland floods saw inundation of the towns of Charleville, Roma, St George and Theodore among others.  The floods were caused by rainfall generated by a monsoon trough described by a Bureau of Meteorology forecaster as \"almost like a tropical cyclone over land\".  Over the period March 1–3, rainfall totals of between 100 – were observed in the area.  This water ran into already saturated rivers and creeks in the area.  Losses from cotton crops destroyed at Theodore and the area around St George and Dirranbandi are expected to be significant. The floods, described by the Queensland Minister for Primary Industries Tim Mulherin as the \"worst flood in 120 years\" are however expected to provide a billion dollar boost to the local economy, following the \"worst drought since Federation\" The floods have seen a large increase in the Australian plague locust population and the Australian Plague Locust Commission is concerned the locusts will head south and destroy what is expected to be a bumper winter grain crop.  On March 21, The 2010 Uganda landslide occurred in the district of Bududa in eastern Uganda on 1 March 2010.  The landslide was triggered by heavy rain between 12 pm and 7 pm that day.  At least 100 people were believed to have been killed. The Ugandan Red Cross stated that rescuers had recovered 50 bodies, whilst a Ugandan government minister has put the death toll at over 100.  The chairman of the eastern Bududa district suggested that the death toll could be as high as 300.  The landslide struck villages on the slopes of Mount Elgon, including Nameti, Kubewo, and Nankobe.  Officials and aid workers have warned that there may be further landslides, as heavy rain continues to fall in the region. AirTran Airways cancelled several flights out of Atlanta Airport due to bad weather on March 1 and 2. On March 1, the Environment Agency issued 169 flood warnings in over East Anglia, Yorkshire, Wrexham, Tyne and Wear, Cambridgeshire, Bedfordshire, Oxfordshire, Kent and West Sussex.  The Thames Flood Barrier closed twice in less than 24 hours to protect London from a combined high tide and tidal surge in the Thames estuary, where it had been predicted that the water would rise by another 50 cm.  Andy Batchelor, Tidal Area Flood Risk Manager, said that the Thames Barrier would close yet again should we need to protect the 1.25 million people living and working in London's floodplain. On March 2, heavy snowstorms struck the Virginias and parts of Maryland. The Ewaso Nyiro River in Kenya burst its banks and flooded a safari park's guest house on the 3rd, killing nine people. On March 4, 20 people died in Afghanistan, when an avalanche struck 2 villages in the Wakhan Mountains. Several dozen ships, some with nearly one thousand passengers were stranded in rapidly forming sea ice due to cold weather and strong winds in the Gulf of Bothnia, between Sweden and Finland and near the Baltic Sea.  Some of the ships were freed by icebreakers on March 5. On March 5, 20,000 people were left homeless when flood-induced landslides struck Bududa, Uganda. The 2010 Victorian storms were a series of storms that passed through much of the Australian state of Victoria on March 6 and 7, 2010.  One of the most severe storms passed directly over Greater Melbourne, bringing lightning, flash flooding, very large hail and strong winds to the state's capital.  It was described as a \"mini-cyclone\". On March 8, Catalonia saw its heaviest snowfall in 25 years; up to 50 centimetres fell in Barcelona.  Over 200,000 residents of North-Eastern Spain were left without power, and up to 500 passengers were evacuated from a train travelling to Southern France, which also was experiencing blizzard conditions, with many schools closing.  Nîmes and Perpignan were the worst-hit. A state of emergency was declared in Northern Spain, on the 10th of March, due to the unseasonal blizzards.  Forty roads in France were closed with snow reportedly 3 ft deep in some areas.  Italy also experienced the effects of the Arctic blast; there was traffic chaos in Siena, Assisi and Pisa, with snowstorms stretching as far south as Rome.  On the sunshine island of Majorca, havoc ensued with 6 major roads closing, due to unseasonal snowfall. On March 10, transportation and power supplies were disrupted by blizzards around Zagreb, hurricanes around Rijeka and a mixture of both in Dalmatia.  Both Istria, the northern Adriatic coast and Lika, had electricity supply problems, according to Croatian Power Company (HEP). Heavy rain and rapid snow-melt contributed to the failure of the Kyzyl-Agash dam in Kazakhstan on March 11, killing more than 40 people. On March 12, about 4,000 people were made homeless by floods in the Kazakh towns of Almaty, Kyzyl-Agash, and Zhylbulak.  A total of 30 people had been confirmed dead as the flood waters from, the dam that had burst on the 12th reached 2 meters deep on March 13. The American military officials closed Misawa Air Base on March 10, as a blizzard swept across northern Japan, dumping a record-setting 20 inches of snow on the base by mid-afternoon.  A cyclone with snowstorms approached Kamchatka coast from the Sea of Okhotsk on the 13th and preceded to move inland by the 15th, after which the storms died out in Eastern and Central Kamchatka with no casualties.  The Ust-Bolsheretsk, Petropavlovsk-Kamchatsky and Yelizovo districts of Kamchatka were badly hit by the high wind and snowfall. The fourth major winter storm to strike the East coast of the United States caused widespread flooding, severe beach erosion, and tree and structural damage from Virginia to Maine on the 13th.  The storm killed at least nine people, and left over 1 million without electricity.  After the storm, severe to record flooding occurred at the Pawtuxet River in Rhode Island and along the upper tributaries of the Mississippi River. On March 16, BBC Weather reported that areas of Portugal and Spain witnessed the highest recorded levels of precipitation, since World War II. Australia faced the worst flooding in 60 years as 134 people in the small rural town of Wanaaring, just south of the Queensland/New South Wales border, remains cut off on the 18th.  The Paroo River peaked at 4.8 meters and broke its banks on the 17th. A major sandstorm Kano in northern Nigeria on the morning of March 19.  The whole federal state was filled with sandstorms.  All flights into and out of the federal state were cancelled amid softy fears.  Similar events occurred over the border in the drought hit Niger.  The Harmattan wind caused others in Mauritania.  Rain fall had paradoxically gone down in the African Sahel region as it went up in most other parts of the World. Cyclone Ului made landfall in southeastern Queensland on the 21st, producing heavy rains and flooding. After nearly a month of flooding in the Croatian Kosinj Valley has been the centre of about 11 million Kuna/ (1,514,000 Euros) worth of damage in the region of Lika, Croat officials said on the 22 of March. Heavy sandstorms hit Beijing between March 20 and 22.  On March 23, a major storm sweeps of the lens west of the Bohai Sea and the Yellow Sea, and finally falls on the North Korea and South Korea.  The Xinjiang, Shanxi, Shaanxi and Hebei regions were already faceing drought and sandstorms earlier this month. A major sandstorms hit both Mauritania, Senegal, the Gambia, Guinea Bissau, Guinea and inland Sierra Leone.  Another gets southern Algeria, inland Mauritania, Mali and northern Côte d'Ivoire. A major hail storm hit Perth, Western Australia, more than 158,000 houses were blacked out at the height of the storm and estimates on the insurance damage are over a $1 billion. The UK was expecting one last shot at winter a huge snow storm is expected to drive up from the South West with rain turning to snow Midlands northwards.  The UK Met Office has issued severe weather warnings for the following regions: Northern Ireland, North West England, Wales, Yorkshire & Humber, West Midlands, S.W. Scotland, Lothian and Borders, East Midlands and North East England.  Temperatures are expected to go sub zero once again.  Snow is forecast from Cardiff, Wales northwards.  The warnings come a day after top British meteorologists from outside the UK Met Office predicted the UK's hottest summer on record for 2010 on the 28th. Snow fell in Scotland on March 30, stranding drivers on a motorway leaving them to dig out their cars, the MetOffice released weather alerts for only Scotland and Northern Ireland as the south wasn't as cold as forecasted however heavy snow is falling on higher ground in England and Wales, however temperatures in the South West of the UK fell dramatically around lunch time of the 30th of march and snow is falling in some areas in South Wales and South West England. The Met Office updated its warnings on March 30, the following areas of the UK are in severe weather warnings; Wales, Scotland and Northern Ireland.  The Met Office has also issued extreme weather warnings of Severe Blizzards, Severe Drifting Snow & Very Heavy Snowfall in Western Northern Ireland and North East Scotland where 40–50 cm of snow is now forecasted to fall with drifts over 5 feet tall.  Heavy thunder storms hit parts of Hampshire and temperatures begin to rise in Southern England and Greater London. On March 31, 48,000 Northern Irish, 24,000 Scottish and 150 Irish homes are without power as heavy snow falls in both Northern Ireland, parts of Staffordshire, and Scotland's Southern Uplands.  About 300 people are freed by rescue services when 120 vehicles ventured out during a blizzard in County Londonderry's Glenshane Pass.  Sadly, 17-year-old Natasha Paton, from Cleghorn in southern Lanarkshire dies as the coach she was in skids and crashes in the snow on the A73 road, outside Wiston, also in southern Lanarkshire.  She was om a trip to visit Alton Towers, Staffordshire on a trip from Biggar in Lanarkshire, that was organised by Lanark Grammar School The UK Met Office has issued severe weather warnings for the next day in the following regions:Northern Ireland, Northern England, Wales and Scotland. The storm follows the coldest winter in decades to hit the UK and this is the second most severe storm to hit the UK this year, the most severe was the January 6 snowfall in Southern England and South Wales, despite lower snow depth of 40 cm the south is less prepared and has a higher population density than Northern Ireland and Scotland so caused more chaos and damage to the UK economy.  First Great Western closed both Islip railway station and Bicester Town railway station for part of the day due to heavy snow fall on exposed stretches of track.  Snow also causes travel disruption for buses and cars in South Glamorgan. Both the Rhone and the Danube begin to swell at their upper sections due to heavy rain and snow related melt water on the 31st. On March 31, the UK Met Office issued severe weather warnings for April 1 in the following regions: Northern Ireland, Northern England, Wales and Scotland.  Heavy rain fell in Banbury and moderate snow fell in Scotland on the 2nd. April 1 and 2 had temperatures plummet to minus 50 degrees in Mongolia's Tuul valley, The peasant villager Urna said she bought \"400 bundles of grass and tons of feed so that we would be ready\" for any further bad weather.  According to the Mongolian Red Cross have reported that about 4,500,000 head of livestock perished as a result of the bad weather this year.  Tume, who lives in Ulan Bator said that he had noticed that there were several particle harsh winters in a row to.  He blamed climate change, but experts said that overgrazing by cattle had also child of most of the country's grassland. The April 2010 Rio de Janeiro floods and mudslides are an extreme weather event that has affected the State of Rio de Janeiro in Brazil in the first days of April 2010.  At least 212 people have died, 161 people have been injured (including several rescuers), while at least 15,000 people have been made homeless.  Meteolagists exspessed concern over the intensity of the storms and there aftermath.  The worst part was on the 5th and 6th. Two buildings suddenly fell into the River Buriganga in Nababerchar city's Kamrangirchar district during a heavy storm on April 17, 2010.  Bangladeshi police claimed the foundations were weakened by illegal lifting of sand by local sand traders. Heavy rain also hit the borders of Armenia and Azerbaijan in the evening. A major snowstorm hit New England after the western side of a low pressure system sank southward from eastern Ontario on April 27, merging with part of a second low.  Vermont was hit with the most snow, which totalled as much as 60 cm .  Close to 30,000 customers were left without power. Snow fell east of the Rocky Mountains in southern Alberta as the cold sector of a storm dumped over 25 cm of snow in the Calgary area.  Highway 2 was closed, ice coated some roads, and another 10 cm was expected to fall on Friday along with strong winds. One of the hottest seasons on record was recorded in India through May, prior to the monsoon season.  At least 250 people died from the country's heat wave. A major dust storm hit New Dehli, India on May 6, 2010. Heavy rain fell in western Azerbaijan on May 1.  By May 2, a rain-induced landslide in Alunitdag village in Dashkasan District (western Azerbaijan) covered the yards of two smallholdings.  Between May 2 and 3, two people were killed when their home collapsed in the village of Duzenli in Salyan District.  It happened at about 7:45 local time.  On May 5, further rain and flooding happened in most of the country's east. Heavy rainfall in parts of Poland raised the prospect of flooding in the town of Slubice. The 2010 South China floods began in early May 2010.  392 people died and a further 232 people were reported missing as of June 30, 2010; this included 57 people in a landslide in Guizhou.  53 of the deaths occurred from the flooding and landslides between May 31 and June 3, and 266 deaths occurred between June 13 and June 29.  424 people were killed by the end of June, including 42 from the Guizhou landslide; 118 more were killed and 47 left missing in the start of July, bringing the death toll to 542.  More than 72.97 million people in 22 provinces, municipalities and regions, including the southern and central provinces of Zhejiang, Fujian, Jiangxi, Hubei, Hunan, Guangdong, Guangxi, Chongqing Municipality, Sichuan and Guizhou were affected, while at least 4.66 million people were evacuated because of the risk of flooding and landslides in the latter half of June. The 2010 Central European floods were a particularly devastating series of weather events that occurred across several Central European countries during May, June and August 2010.  Poland was the worst affected.  Austria, the Czech Republic, Germany, Hungary, Slovakia, Serbia and the Ukraine were also affected. On May 16, in southern Serbia about 300 people were evacuated due to flooding after heavy rainfall in the country.  The flooded area was left with no electricity, telephone lines, or running mains water.  Two people drowned in a flooded Pčinja River. On May 17, one person died in the Hungarian town of Miskolc, while two others died in the Serbian town of Trgovište due to flooding created by heavy rainfall. Heavy rain began to fall in Guatemala City on the 24th, causing local rivers to flood.  Heavy rain was also reported in parts of Mexico, Honduras, and southwestern Brazil. As the storm moved inland, torrential rains triggered flash flooding and landslides in parts of Honduras on May 30.  At least 45 homes were destroyed and one person was killed in the country.  On May 31, the presidents of both El Salvador and Honduras declared a state of emergency for their respective countries.  Tropical Storm Agatha had picked up speed and strength over the Central American state of Belize, in the Atlantic basin, on May 31. The 2010 Indian heatwave was a period of extremely hot weather occurring during the summer of 2010 in India and much of South Asia.  Said to be the harshest summer since 1800, the heat wave killed hundreds of people due to heat exhaustion and food poisoning. On May 24, 400 people held a protest in the 1,500-strong village of Altit over the lack of aid provision, following a visit to the settlement by Pakistan's Prime Minister.  One protestor said he was sheltering in a Gilgit school building and that there were too few doctors in the camp.  The only reliable means of transport in the disaster zone was by Army helicopter.  About 32 villages in the Hunza–Nagar District were said by the local administration to be flooded.  The flooding lake was formed after a 2.5 m deep landslide in January. The hottest temperature ever recorded in Asia was reached in Mohenjo-daro, Sindh, Pakistan at 53.7 C , on May 26, and multiple cities in Pakistan saw temperatures above 43 C .  The previous record for Pakistan, and for all of Asia, was reached at 52.7 C in Sindh Province on June 12, 1919.  By May 27, after temperatures higher than 45 C hit areas across the country, at least 18 people died in Pakistan from the heat. Warsaw faced imminent rain-related flooding as the Vistula river burst its banks and inundated many nearby villages on May 20. The Vistula river burst its banks on May 21 and flooded nearby towns.  Warsaw was put on flood alert.  23,000 people were displaced in the worst Polish floods in 160 years.  EU officials said Poland would receive 100 million euros in aid from the European Union solidarity fund.  The Vistula was at a 160-year high and £2,000,000 worth of damage was done in Poland.  Austria, Hungary, the Czech Republic, Slovakia and Poland all witness heavy rainfall, as rivers begin to swell in Slovenia and eastern Germany. On May 23, the floods hit the city of Wrocław as the Ślęza river broke a dyke and flooded into the Kozanow neighborhood.  The Vistula River reached 7.8 m in height and, like the floods in general, was at a level not seen in 60 years. On May 24, levees failed or were tactically dynamited, southern Poland was hopelessly flooded, and the River Oder began to flood Germany.  $2,500,000,000 worth of damage was done in Poland as the river Vistula flooded along its entire length and Poland's death toll reached 15. By May 25, another £400,000,000 worth of damage was done and 15 Poles were reported dead.  Flooding also affected nearby parts of Germany. Parts of Lower Austria, the Polish town of Slubice and the German city of Frankfurt an der Order started to be flooded from the rivers Spree and Oder on May 27 due to heavy rain.  Heavy rain fell across the English Midlands. A large dust storm swept across both Libya and Egypt on May 26. On May 30, Guatemalan President Alvaro Colom declared a ‘state of calamity’ as the first tropical storm of the Eastern Pacific hurricane season, Hurricane Agatha flooded about 600 homes and killed 12 people the day after a volcanic eruption.  The rain made the volcanic ash set like concrete on and around Guatemala City and the city's main airport.  A total of thirteen people had already died in El Salvador the day before.  In El Salvador, one person drowned when a river flooded, according to civil protection agency spokesman Armando Vividor.  People living in or traveling to flood zones in the capital and five other cities were to relocate to shelters, according to a statement on the civil protection agency's website.  The Miami-based National Hurricane Centre issued an advisory that the storm had strengthened, with maximum sustained winds of 45 mph or more predicted during the next 48 hours. On June 1, the National Hurricane Centre stated that the remnants of Tropical Storm Agatha had only a low chance of regeneration in the western Caribbean Sea.  By the next day, the thunderstorm activity associated with Agatha in the western Caribbean had dissipated and it was only a bad storm.  The storm had severely damaged Guatemala's principal airport with rain induced floods.  It also resulted in the death of 1 person in Nicaragua, 152 people killed and 100 left missing in Guatemala (due to landslides), 13 deaths in El Salvador and 16 fatalities in Honduras.  On June 6, the remnants of Tropical Storm Agatha dissipated completely, after hitting Honduras. 25 died between June 2 and 4 in a heavy rainstorm in Pakistan's Punjab Province and a blizzard hit the Pakistani-held parts of Jammu and Kashmir. On June 2, police in the Czech Republic warned of vehicles’ aquaplaning after a man died in a flooded street due to it.  In Slovakia, a 38-year-old man died searching for another man who had fallen into a swollen river.  2,000 people were evacuated in the northern Hungarian town of Paszto.  Major flood alerts hit Serbia and Bosnia as their rivers swelled with floodwater.  Government officials in Serbia said the situation was \"critical\".  A Croatian drowned in a flooding Istrian river.  A total of 250 L/m2 of rain fell in Zupanja, Croatia, on June 3.  The Croatian government sent 5,000 1.5 L bottles to the Zupanja region, and the Croatian Red Cross launched an emergency appeal.  Meanwhile, there was more rain in Croatia and Slovenia.  On June 2, several heavy thunderstorms also hit the high Swiss Alps, accompanied by heavy snow in some places. The Hungarian flood level was critical on June 5. The 93 mph Cyclone Phet hit Oman's Masirah Island after 1,000 Omanis and 50,000 Pakistanis were evacuated on June 4.  (\"Phet\" (Thai: เพชร ) is a Thai word meaning diamond.)  Cyclone Phet made landfall at Thatta about 50 km south from Karachi, Pakistan on June 6, 2010 at about 16:30 GMT. A waterspout unexpectedly moved onshore as a tornado at Lennox Head, New South Wales, Australia on June 3.  40–100 houses were damaged, and several people were injured by the tornado. June 6 saw heavy rain hit the sun-scorched UK and Ireland.  Heathrow Airport had June 6‘s hottest spot at 28 C . On June 7, 150 Polish schools were closed, flood hit Slovakia received 25,000,000 Euros ($30,000,000) in EU aid, and river levels in Budapest reached 8.2 m .  The death toll included 1 Hungarian, 3 Slovaks, and 25 Poles. The Polish towns of Wilkow and Winiary flooded.  The river level in Warsaw reached 7.80 m and a local dam collapsed, flooding part of the city on June 8.  The Polish city of Sandomierz was cut off by flooding. The mayor of Janowiec appealed for aid and criticized the Polish government's handling of the affair.  42,000 people were affected and more than 12,000 houses were flooded in the province of Podkarpackie.  20 Poles were confirmed as dead as the Vistula river ran amok in Poland on June 9.  The general elections on June 20 were suspended due to the chaos caused by the floods. On June 8, a flood alert was issued in Belgrade as the river Danube was rising by 1 cm per hour in the city. On June 8, heavy thunderstorms hit the British Isles, ending the 6-day-long heat wave. About 42 people were killed by heavy monsoon-related landslides, winds and flash floods in the south of Bangladesh on June 15.  Some flooding was also reported in India's Tamil Nadu and West Bengal provinces. Over June 15 and 16 about 40 cm of rain fell in the French Côte d'Azur region.  At least 1,000 people were evacuated and spent the night in empty schools or other temporary shelters, and some 175,000 houses were estimated to have been left without power as rescue teams moved 436 prisoners from a flooded jail in Draguignan.  The rain was worst in Roquebrune-sur-Argens and Frejus; it continued into June 17.  Both rail and air travel in the region were interrupted, with 300 or so passengers who were traveling on a high-speed train between Nice and Lille trapped by floodwater spilling over the tracks near Nice.  The railway line between Toulon and Frejus was closed until the morning of June 17, due to water-induced subsidence of the track and debris on the line.  Fallen trees and landslides also blocked many roads for the next 24 hours.  Meteorologists said it was the worst such event in the region since 1827.  The ultimate death toll was 22. On June 21, the Salvadoran Red Cross Society said the El Salvador flood emergency appeal funds were nearly exhausted. The price of oil rose to $77 per barrel on June 24 as a cyclone began to form in the south-western Caribbean. On June 25, the Category 3 strength Hurricane Darby hit Mexico's Pacific coast (passing 395 km south-west of Acapulco) with hurricane-force winds reaching up to 35 km from the eye. On June 30, Hurricane Alex hit north-eastern Mexico.  An area 35 – north of the town of La Pesca was hit by winds up to 105 mph as the storm moved northward. The rain-swollen Gan River burst its banks and flooded Zhangshu, Jiangxi, China on June 21. June 27 to 29 saw the heaviest rainfall for 300 years in the Luolou township of the Chinese Guangxi Zhuang Autonomous Region.  6,673 people were affected; the town was cut off, schools were closed, and people traveled by boat. On July 30, Malteser International, the relief service of the Order of Malta, gave disaster relief to about 10,000 in the flood-ravaged Swat District of Pakistan.  The Khyber Pakhtunkhwa saw at least 400,000 people affected by the floods, which were the most severe ones since 1929.  The abnormally heavy rain killed 300 Pakistanis and a cholera epidemic was expected in its wake. Heavy rainfall happened in central and north-eastern Romania between June 22 and June 29. 1,870 people were evacuated from 10 villages in Suceava county, as the Siret river threatened to overflow on the afternoon of June 28.  Some 1,100 sheep were moved to higher ground in the mostly rural region.  Refugees were taken by local monasteries, schools, cultural centers, and relatives.  Gheorghe Flutur, president of Suceava county, said his region was one of the worst hit in the country.  Later that day, the Siret river threatened to break through the dikes protecting the town of Şendreni, as locals and emergency services reinforced the dikes with sandbags by the truck-full to prevent the river from breaking out and flooding the town. The north-eastern town of Dorohoi witnessed six deaths on June 29 as floods rose to just over 1 m in some places.  Several roads into Dorohoi remained either washed away or under water.  Ten people were killed by the floods in total.  The railway line to the Ukraine, electricity pylons, bridges, and roads were damaged across northern and eastern Romania. The Romanian counties of Alba, Arad, Bacău, Botoşani, Brașov, Cluj, Hunedoara, Iași, Mehedinţi, Neamţ, Olt, Prahova, Sălaj, Sibiu, Suceava, Timiş, Tulcea, Vâlcea, and Vrancea were flooded in late June.  Botoşani, Suceava, and Tulcea counties took the brunt of the damage.  Also affected was Chernivtsi province in the neighboring Ukraine. On June 29 and 30, Romanian Prime Minister Emil Boc toured the devastated regions in the north-east of the country.  Romanian Interior Minister Vasile Blaga told parliament that the losses were equivalent to 0.6% of GDP.  The agriculture ministry estimated that, of 2000000 ha of arable land, 12000 ha were damaged.  Moldova and the Ukraine had yet to assess the crisis in their flooded regions.  Russia gave 70 t of humanitarian aid to the Ukraine. On July 9, the European Union's Humanitarian Aid and Civil Protection representative, Kristalina Georgieva, let Belgium send aid to Romania.  The fortnight-long floods in Rumania killed 23–24 people, injured 43, made 7,000–18,000 homeless, and caused 60,000,000 Euros (US$76,000,000) of damage.  Romania is part of the EU‘s Civilian Solidarity Mechanism.  One Ukrainian was killed by the floods in the Ukraine's Chernivtsi region. On July 2, 2010, Hurricane Alex made landfall in Monterrey, Mexico, causing flooding in most of northern Mexico and killing 2 people. Unusually heavy rain hit Phnom Penh, causing transport chaos.  The chief of Phnom Penh Municipality's sewage system department said that several hours of rain had overwhelmed the city's drains and flooded most streets.  30 – of rain fell on June 2. On July 7, 2010, five people died and eight were missing after torrential rains caused flash floods in Huangyuan County, in China's overheating Qinghai Province.  The rain hit six towns at about 10 p.m. and lasted for about 40 minutes that night.  It triggered floods that cut transport links, phone, power, and water supplies.  Over 80 homes collapsed and 770 were flooded.  Disaster relief operations were under way during the heavy flooding, but the county government was still assessing the full losses and financial implications of the heat wave and flash flood.  Rumors that a homeless old man had been crushed to death in a partly collapsed house were denied by rescue workers. July 8 saw the highest temperature across the People's Republic of China so far in the summer of 2010, along with heavy rainstorms.  Local authorities and the National Meteorological Center also issued an orange flood alert in central and southwest China; the worst floods for 40 years hit these regions.  Part of the floods in southeastern Sichuan province was sewage from a failed sewer. Heavy rains hit the Hubei and Anhui provinces on July 8, and caused a 1 m deep flood which killed 1 person and made 500,000 homeless.  On July 9 over 27370 ha of farmland were flooded, 242 houses collapsed, and at least 10,157 residents were evacuated in Hubei province according to the Civil Affairs Bureau.  Anhui saw 10-year flood and 50-year temperature records, the latter increasing both air-conditioner sales and electricity consumption in Beijing and nearby cities.  Both the Golmud river and the Wenquan reservoir overflowed. July 8 saw the Indian states of Haryana, Punjab, and Himachal Pradesh flooded and arterial highways connecting Delhi to northern towns cut or directly threatened with flooding.  Delhi-Manali National Highway 21 remained closed for a second day after water from the rain swollen Beas river flooded in several places near Aut. A 160 ft breach opened up in the rain-swollen Sutlej Yamuna link canal and two army columns were deployed for rescue and evacuation missions in the disaster zone.  The overflowing Ghaggar-Hakra River also flooded Punjab's Patiala district, killing three people. As the floods eased in Central Europe and the Balkans, except for in Romania, temperatures began to climb across Western Europe and the UK between June 30 and July 2. On July 2 Brussels saw its hottest day since 1976.  France, Germany, and the Spanish resort Benidorm had record temperatures as part of Europe's July heat wave.  Several heavy thunderstorms hit the low Swiss Alps, accompanied by heavy sleet in some places. On July 3 a heat wave hit parts of Ryazan province and the cities of Bucharest and Budapest, killing a Romanian man with heatstroke.  Heavy thunderstorms hit the high Swiss Alps, accompanied by heavy snow in some locations. On July 11, heavy floods hit Haryana in India and damaged the archaeological site of Jognakhera, where ancient copper smelting had been found dating back almost 5,000 years.  The Indus valley civilization site was hit by almost 10 ft of water as the Sutlej Yamuna Link Canal overflowed. A thunderstorm with heavy rain hit Zürich and the Swiss-French border on July 10.  The storm also threatened to close the Avoriaz stage of the Tour de France cycle race. On July 11, temperatures skyrocketed in Vienna, Berlin, Munich, Amsterdam, Madrid, Lisbon, Zürich, and Bucharest.  More heavy thunderstorms hit the high Swiss Alps, accompanied by heavy snow in some places. On July 12, France and Belgium also saw record temperatures. Alpine and North Sea thunderstorms swept across south-east and north-west Germany respectively.  Heavy rain fall was also reported in parts of the Netherlands, Ireland, Normandy and the English Midlands on July 13. Thunderstorms hit the English Midlands, Oxfordshire, Ireland, and Northern Ireland.  The heatwave ended in the British Isles and north-west Europe. Heavy storms also hit Warsaw, Vienna and Kiev between July 14 and July 16. Heavy rain and thunderstorms hit the town of Samail in Oman's northern coastal mountain range on July 14.  On that day, heavy rain also fell in most of Iran's East Azerbaijan and West Azerbaijan provinces. The 2010 Milwaukee flood was a series of two disasters in the Milwaukee, Wisconsin area of the United States; they happened from July 15 to 23, 2010. Iowa, Wisconsin, Minnesota, Texas, Kansas and Nebraska had heavy thunderstorms with a bad tornado in Northfield, Minnesota on July 16.  Later in the day all of Manitoba, Nebraska, Kansas, Missouri, Tennessee, Mississippi, Wisconsin, and Minnesota had heavy storms.  The heat wave was at last dead and blown away by the storms. Flash flooding in Arizona swept a 12-year-old girl to her death on July 20, 2010. The 2010 Var floods were the result of heavy rainfall in southern France that caused severe floods in the Var department in the evening of July 15, 2010.  As well as generalized flooding, there were also flash floods.  Meteorologists said the floods were the worst in the region since 1827, with more than 400 mm of rain falling in less than 24 hours.  At least 25 people were killed, and 14 people were missing. Trees and chimney pots fell as winds hit parts of Wales and the Bristol Channel area, between July 15 and 16.  The winds hit Cardiff, Porthmadog, Pontcanna Fields near Cardiff, coastal Pembrokeshire, southern Monmouthshire, Cheltenham, Gloucester, Aberystwyth, Portmeirion, and Prenteg as heavy gales passed over South Wales and the Bristol Channel. On July 15, the regional representative of the American Red Cross had praise for the Chinese people, the Chinese Red Cross, and China‘s Ministry of Civil Affairs for their efforts in fighting the flood.  The People's Liberation Army (PLA) was praised for sending contingents of soldiers across the country to any location where a disaster had struck. Both the Xinjiang region in the northwest and Yunnan province in the south were affected by flooding.  In all, flood waters inundated parts of at least 24 of China's 34 provinces and regions on July 20.  By July 20, the Yangtze River at the Three Gorges Dam experienced its highest river discharge in 130 years, and the highest since the dam was built.  The dam's walls released 40000 m3 of water, while 30000 m3 per second of the river flow was held back in behind the dam, after water levels in the Reservoir had risen 4 m overnight.  The reservoir's water levels peaked at 158.86 m on the morning of July 23; the alarm level for the reservoir was 145 m .  All ferry service in the reservoir was halted when the total flow rate exceeded 45000 m3 per second, although the crest of the flooding passed the dam by July 24.  A second peak in the river arrived at the dam on July 28, when the peak flow from the dam was a record 56000 m3 per second. It was officially revealed on July 21 that more than 701 people were dead, and that 347 were missing due to the severe flooding in China so far; the floods caused the highest death toll since 1998, which saw the highest water levels in 50 years.  The floods have affected 100 rivers, 117 million people in 27 provinces, and seven cities.  9000000 ha of farmland and 645,000 houses have been destroyed, and 8,000,000 people have been evacuated (about the population of New Jersey). The overall damage totaled 142,200,000,000 Yuan or £13,700,000,000.  The Three Gorges Dam project prevented floods like those in 1998, which killed several thousands.  The shipping channels in the Three Gorges Dam were closed, but the dam wall was still holding for the time being. About 70 people were killed and 100,000 left homeless between July 19 and 25 in flash floods in Pakistan's south-western Baluchistan province. Due to the bad weather, an Airblue passenger jet crashed into the Margalla Hills outside Islamabad killing all 152 passengers on July 28. The Pakistan Meteorological Department said that over 200 mm of rain fell over a 24-hour period over a number of places in Khyber Pakhtunkhwa and Punjab.  A record breaking 274 mm rain fell in Peshawar over one 24-hour period; the previous record was 187 mm of rain in 24 hours, recorded in April 2009.  Many areas of Khyber Pakhtunkhwa saw more than 200 mm of rain over July 28 and 29, breaking a 35-year-old record.  Heavy rainfall was reported all over the Hindu Kush mountains.  80 died in Khyber Pakhtunkhwa province, mostly in the Swat Valley. A unnamed river flooded the town of Behrain, destroyed a hotel, and caused heavy damage. A recently built part of a dam in the Charsadda District collapsed; the ensuing wave killed two people and destroyed all the local farms. On July 31, the information minister of Pakistan's northwestern Khyber-Pakhtoonkhwa province, Mian Iftikhar Hussain, said the latest deaths were in this region.  Eight hundred died and thousands had diarrhea, fever and other waterbourne illnesses.  About 45 bridges were destroyed in the worst floods since 1928; this severely hampered rescuers, who lacked helicopters. On July 31, Peshawar in Pakistan was cut off and Pakistani-held parts of Kashmir were also flooded, while the abnormally strong monsoon killed 65 people in mountainous areas across the border in Afghanistan.  The EU pledged to give €30,000,000 to Pakistan in aid supplies.  1600 were confirmed dead in Pakistan by the end of August and 60 were dead in Afghanistan by July 31.  All 300,000 people in Peshwar were cut off by the flood waters, but helicopters were still able to land there, according to a Khyber-Pakhtoonkhwa provincial official (Mian Iftikhar Hussain).  It remained so on August 1, 2010. On August 1 floods occurred in south-east Afghanistan and continued in the Indus Valley.  1,100–1,400 Pakistani people had been killed and 27,000–30,000 were trapped on high ground, clinging to rooftops and trees.  India also suffered substantial dislocations of its road network as rescue workers used boats and helicopters in both nations.  There were 900 cases of water-carried illness in Pakistan; the town of Nowshera was flooded. The rain continued to fall on parts of Pakistan, Afghanistan, and Iranian Baluchistan, and snow blanketed the Pamir Mountains and Ladakh on August 2.  About 2,500,000 Pakistanis were now homeless according to the International Red Cross and Mian Iftikhar Hussain said there might be a cholera epidemic in his Khyber Pakhtunkhwa province. Some 4,000 soldiers were called in to help fight both the 6 rumoured and several known fires in the Moscow Oblast.  Over 5,000 people were evacuated from their homes and Vladimir Putin organised an emergency meeting for August 2 with the governors of the various regions in the Central and Southern Federal Districts devastated by the fires.  The grain harvest in the disaster zone was destroyed. On August 11, the U.S. Federal Emergency Management Agency (FEMA) issued a warning that about ⁄ of America's 80,000 dams and levees had a \"high\" or \"significant\" hazard to life and property if a catastrophic structural failure occurred. August 7 in China had 700 people known to have been killed and more than 1,000 people still missing and presumed dead as landslides demolished hundreds of buildings, including several seven stories high buildings in the remote Zhouqu county of Gansu province after several days of heavy rain in the county. The major landslides were triggered by torrential rains and covered an area of 5 km by 500 m .  45,000 people in Zhouqu county were evacuated as 7,000 soldiers, fire-fighters and medical staff deployed to tackle the disaster.  The worst floods for a decade killed over 2,100 and millions more were displaced nationwide. A report compiled by Lanzhou University in 2006 had warned that the felling of the forests around Zhouqu for coal mining and agriculture would cause heavy soil erosion and destabilize hillsides.  The construction of a major highway and more than 40 hydroelectric dam and recovery systems in the steep valleys acted to further destabilize the local geology, according to leading Chinese geologist Fan Xiao. The rain stopped on August 10 and the situation became less chaotic as the army and fire-fighters continued to look for survivors. August 10 saw China's state-run Xinhua news agency report that the authorities sent nearly 3,000 soldiers and about 100 medical experts to help in search and rescue effort in the Gannan and Tibetan Autonomous Prefectures in Gansu province after record floods hit the province.  Reports said Zhouqu county had at least 337 dead as rescuers searched for up to 1,148 others who were still missing.  Landslides up to five stories deep buried three villages, destroyed roads and bridges, disrupted mobile telecommunications equipment, and cut phone lines, water, and electricity supplies in parts of the region. The Chinese government sent experts by helicopter to survey the flooding in devastated villages, and examine how to blast open the remaining flood-induced blockages at the end of the valley, according to Chinese state television.  In total, about 875,000 homes were destroyed, 9,610,000 million people were evacuated, 22,000,000 acre of crops were destroyed, and tens of billions of dollars in damage were caused by the floods in 28 provinces and regions. The storms in north-east Jilin province also killed several people and left more than 100 people missing; heavy snowfall was reported in the Himalayas. August 8 had heavy rainfalls in southwest Kosovo, killing an elderly man, who fell into a swollen river on the Albania/Kosovo border. On August 20 the worst floods for 80 years hit Africa's Sahara Desert region.  The UN warned that Niger, Chad, Burkina Faso, Cameroon and northern Nigeria were also in the grip of the worst regional food crisis since 2006.  In the Sudanian Savanna city of Kano, Nigeria, over 2,000 families were displaced by floods, and in the nearby Jigawa region, an entire village was evacuated due to heavy flooding.  A Mauritanian child was swept away in a flood that damaged bridges and many homes in the mountain town of Aioun.  Heavy flooding happened around parts of Lake Chad. After weeks without rain, heavy downpours soaked Moscow and nearby areas, bringing further relief for the extended heat wave.  However, in Sarov (about 480 km east of Moscow) a new fire started near the country's top nuclear research center.  Earlier in August, radioactive and explosive materials were moved out of the facility due to the threat of forest fires; however, they were later returned when the threat lessened.  Over 3,400 fire-fighters battled the forest blaze, assisted by a special fire-fighting train.  The front end of the forest peat fire-fighting came near the town of Roshal (in the Shatursky district) at one point on August 13. Heavy rain and thunder storms hit the British Isles between August 13 and 22, leading to heavy flooding in parts of Sussex, Oxfordshire and Buckinghamshire.  The low cloud and rain led to the closure of the Bournemouth Air Festival on August 21 and 22.  The morning of August 23 saw heavy storms hit Pembrokeshire, the English Midlands and Argyll.  Some flooding was reported in Oxfordshire and Pembrokeshire.  As the day went on the storms lined up on the east coast between The Wash and Aberdeen, with isolated storms in south-west Gloucestershire, Oxfordshire, south Cambridgeshire, south-west Yorkshire, Leicestershire, and Moray. On September 1, floods hit the Niger River.  Flooded ruins were all that was left of three districts of the West African country's capital Niamey (Zarmagandaye, Lamorde and Karadje) It was the worst flooding by the Niger recorded since 1929. September 4 saw a landslide killing 12 people by hitting a bus, as record amounts of rain started falling in parts of Guatemala and south-eastern Mexico.  Thousands were evacuated from the Mexican Gulf coastline by the state of Tabasco, as the flooding continued to grow. September 5 saw up to 100 people trapped in a bus by a landslide as torrential rains continued to swamp parts of Guatemala.  Most were rescued, but 18 people were killed in the incident. On September 6, the region's first major tropical storm of the year killed at least 145 people in Guatemala, with at least 53 missing and thousands homeless as emergency crews struggled to reach isolated communities.  Tropical Storm Agatha washed away hundreds of roads and collapsed many bridges in the country.  The Chimaltenango department's governor, Erick de Leon, said that the landslides buried dozens of small rural Amerindian communities and killed at least 60 around the local village of Santa Apolonia alone.  Local volunteers worked desperately to recover the bodies of two brothers, aged four and eight, who were buried by a landslide in the village of Parajbei. September 6 also saw an overnight landslide (caused by a flood) burying around 100 people who were trying to rescue victims of the previous bus trapped by a landslide along the Inter-American Highway in the Guatemalan Highlands.  23 people were killed in the incident near the town of Santa Maria Ixtaguacan, and 12 had died in the original landslide.  The floods were estimated by the government to have killed 45 people on the highway so far, with 38 dying over the last two days of heavy rain and landslides far.  President Alvaro Colom declared a nationwide state of emergency and urged that citizens to stay off the nation's highways due to the likelihood of more landslides.  The week-long heavy rain and floods affected some 40,000 people in the country and caused $350–500 million in damage.  He also warned that 24,000 more people were at risk and the government was running out of funds. Thousands more fled their homes in Honduras after mudslides and landslips killed 15 on September 6.  State officials warned people to stay away from any swollen waterways as the rain-swollen reservoirs behind two dams near the capital of Tegucigalpa overflowed the dam walls' ramparts and spilled into a nearby river. Meanwhile, a weather forecast of yet more rain was issued across Central America; it prompted officials in Mexico to take precautions against rain induced landslides.  Heavy flooding made thousands homeless in the southern Mexican states of Veracruz, Tabasco, Chiapas and Oaxaca.  Mexican president Felipe Calderón pledged to send aid to the devastated regions on his Twitter page. Mexico's national power company opened floodgates on some hydroelectric dams in the region, worsening the flooding in some low-lying areas but causing no deaths and avoiding a catastrophic dam burst situation. The Pakistan government said on September 2 that 1,500 had died across Pakistan so far.  Basera (near Muzaffargarh in Punjab province), Jatti (near Thatta in Sindh province, Pakistan), and Larkana (in Sindh province) were still in chaos and choked with refugees on September 2.  The United Nations warned that up to 3,5000,000 children were at risk from water-borne diseases like cholera in the disaster areas and refugee camps. The Oxford mosque's Pakistan flood charitable fund reached £30,000 on September 4, 2010. On September 5 floods hit the squalid refugee camps in Azakhel, miles from Peshawar, Pakistan. On September 4 storms swept into the prefecture-level city of Luoyang in China's Henan province at 7 am.  A total of 15 villages were hit by gales, hail, flood and heavy rain.  Several small boats were sunk in a river running through a local scenic spot in Luoyang.  One person was reported dead, and electricity poles and telecommunication links were cut down in some villages as well as three bridges destroyed, farm crops destroyed, and several roads blocked, causing about 900,000,000 Yuan or $132,000,000 in material losses. A second day of heavy rain hit the UK on September 6.  August 2010 was the coolest since 1993, with the average maximum temperatures 1 C below normal.  The rainfall was much above average in both south-east England and Wales, and the third-wettest August on record had happened in East Anglia; over twice the average August rainfall occurred in 2010. The 2010 Victorian floods were a widespread series of flood events across the Australian state of Victoria from September 2 to 7.  Heavy rainstorms hit New Zealand's South Island on September 2; the same rain system then hit Victoria in Australia.  Flooding followed heavy rain across south-eastern Australia and caused the inundation of about 250 homes, hundreds of evacuations, and millions of dollars of damage.  Weather warnings were initially issued for Victoria on Thursday, September 2 and rain began to fall on that Friday, continuing through the weekend to Tuesday, September 7.  Heavy rain fell in most regions of the state, particularly higher altitudes in the state's west and north-east, flooding the upper reaches of many of Victoria's major rivers.  A state of emergency was declared with State Emergency Service crews arriving from Queensland, South Australia, and Tasmania.  In Skipton in the state's Western District, 20 properties were put on evacuation alert, while in the Central Highlands 120 people sought refuge in the town hall at Creswick and 30 people were evacuated from a caravan park in Clunes.  In northern Victoria, 150 extra police and 50 military personnel were deployed to assist with evacuations and sandbagging. Although Hurricane Igor was several hundred miles from the Leeward Islands on September 9, large swells produced by Igor swept two people out to sea in the U.S. Virgin Islands and Puerto Rico. Since the storms started on September 9, 144 people were killed and 13,000 affected people were moved to temporary relief shelters.  Five of El Salvador's fourteen departments were still seeing heavy rainfall.  More than 20 homes were destroyed, 24 bridges and 1,600 houses were damaged, as the town of San Vicente (48 km east of San Salvador) was buried under landslides on November 11, killing several people. Mexico experienced its worst rainy season on record starting beginning on September 10.  Mexico's President Felipe Calderón said that 900,000 people were affected in the latest tropical storms.  It was the worst such event since Hurricane Alex killed 22 and left 40,000 homeless in July 2010. September 13, 2010 saw Tropical Storm Julia hit the Cape Verde Islands with winds at 65 km/h . A record cloudburst hit northern India on September 18 and 19, causing deadly flooding. The 2010 Slovenian floods on the weekend of September 17–19 were caused by heavy rains in Slovenia, resulting in one of the worst floods in the country's history.  Among the regions affected were the capital Ljubljana, the Zasavje region, Laško, the Slovenian Littoral and the Lower Carniola region.  Initial damage was estimated to reach €15 million.  Three people were killed. On September 17, north-east England's firefighters, medics and emergency rescue teams did flood and rainstorm related training exercises at Hurworth Burn Reservoir, near Middleton St George in County Durham, as well as at Rothbury and Otterburn in Northumberland. Three Spaniards died as heavy rains hit southern Spain on September 17.  A man died when a wall collapsed in Bujalance in eastern Córdoba province, while a man and a woman died in Aguilar de la Frontera in the south of the province.  Storm warnings were announced by the national weather authority later in the day. Heavy rainstorm and snowstorms hit Invercargill, New Zealand on September 16 and 17. On September 18, heavy flooding closed a 100 m stretch covered in water up to 1.5 m deep on and off State Highway 1 north of Bulls as high winds and rain created major disturbances around the country.  State Highway 3 was entirely closed and there were also landslips on State Highway 4 at the Manawatu Gorge.  The NZ Transport Agency said State Highway 1 between Woodlands and Edendale in Southland had reopened after the ending of heavy snowfalls.  Similar weather incidents and some minor car crashes closed parts of State Highway 99.  Heavy floods under railway overbridge closed the highway near Marton, forcing a local diversion. Powerco restored supply to around 28,000 customers as a severe wind and rain storm continued to pound the North Island.  The storm had caused power cuts to around 45,000 customers through the middle of the North Island; the worst affected areas were Thames, Coromandel, Thames-Coromandel, Tirau and Putaruru, parts of the western Bay of Plenty, parts of Wanganui, Wairarapa, and Manawatu, Hauraki-Piako, and parts of Taranaki. Overnight between September 17 and September 18, snow and ice settled on Southland's roads causing slippery conditions on rural roads and treacherous conditions in Invercargill as the local police asked people to stay indoors and off the roads.  An autistic 7-year-old boy nearly died after falling in Paeroa, and hundreds of cars were damaged or destroyed nationwide.  A big tree fell on State Highway Five, the Napier-Taupo Road, about 3 km on the Napier side of the Tarawera Tavern.  Sheep died in the worst winter since 1996. The MetService reported more than 100 lightning strikes in the Hutt Valley and Wairarapa region yesterday, setting fire to a shed and igniting several trees early in the day. WeatherWatch.co.nz reported that some gusts had peaked at 154 km/h , collapsing power lines and cutting electricity in Warkworth, Remuera, Mangere and west Auckland.  The electricity supply line company Vector believes about 30,000 people lost power in the Auckland area and in Piha to the west of the city. Bermuda closed government-run schools for September 20 and 21 in anticipation of Hurricane Igor.  In addition to school closures, the Bermuda International Airport (BDA) was shut down in advance of the storm.  During the storm, the major causeway leading to the airport was shut down earlier than anticipated due to the threat of tornadoes. Despite initial fears that Hurricane Igor would cause immense damage across Bermuda, the storm passed after causing relatively minimal structural damage.  Heavy rains fell across the islands between September 18 and 19, amounting to 2.97 in ; sustained winds were recorded over hurricane force and gusts reached 93 mph .  An AWOS on St. David's Lighthouse recorded a peak sustained wind of 91 mph and gusts of 117 mph .  Although roughly 27,500 of the 35,000 residences across the island were without power, the damage was limited mainly to downed trees.  No reports of loss of life or serious injuries came after the storm's passage.  Officials in Bermuda stated that the biggest loss from Igor would be lessened tourism revenue following a mass exodus prior to the hurricane's arrival.  Catastrophe modeler AIR Worldwide indicated that insured losses in Bermuda were less than $100 million. September 19 saw thirteen more people perish in Uttar Pradesh because of the Yamuna river floods.  The maximum temperature in the capital city of New Delhi rose to 34.6 C .  Several food prices rose because of the destroyed farmland. Uttar Pradesh Chief Minister Mayawati demanded that a national government team be sent to assess the losses suffered during the floods.  After taking an aerial survey of the flood-hit areas, she requested that the national government provide 2,175 crore for a flood relief team. Not much happened in Pakistan on September 19, but the victims of the Pakistan flood struggled to rebuild on their own.  People swarmed around any cars, begging for help: their homes and everything were completely destroyed; their fields were turned into rubbish.  One villager said \"[n]obody has visited our village.  Nobody has helped us.  My house, my furniture & and everything else was destroyed.  We had no support from the government.\" On September 20, heavy rain and landslides inflicted a death toll of 63 in Uttarakhand; Naintal lost 11, Haridwar 7 and Pauri 3.  The river Ganges flooded several low-lying areas as it passed 2 m above the danger mark in Haridwar, as Har-Ki-Puari was completely submerged underneath the floodwaters.  The state-run rescue operation to the region said that seven people were still trapped under the debris of flattened houses; local volunteers and police were already there.  All the schools in the province were closed for three days.  Uttarakhand's Chief Minister, Ramesh Pokhriyal Nishank, requested further help from both the Bharatiya Janata Party leader, Sushma Swaraj, and the Indian prime minister. On September 22, the Yamuna river flooded the Yamuna Sports Complex and left mosquito-attracting pools of floodwater, further disrupting preparations for the Commonwealth Games. On September 22, in the morning after a night of steady rainfall Strathclyde Fire and Rescue received 117 calls about flooding in Kilbirnie, Glengarnock, Fairlie, and Largs in North Ayrshire, Scotland.  There were similar downpours in parts of Northumberland and Cumbria at the same time as the Scottish floods took place.  At one point later on September 22, 65 firefighters were called to flooding in the Rothesay and Helensburgh areas of Argyll and Bute.  Just after 9:00 pm on September 22, a storm-weakened wall collapsed over the Glasgow to Largs main railway line near Pencil View in Largs, forcing Network Rail to close the line. On September 19 the roof of the 2,000 seat Stadium Southland tennis stadium in Invercargill, New Zealand collapsed and imploded after a storm dumped a near-record amount of snow on it during a tennis match.  A tennis player narrowly evaded death when the air pressure's shock wave hit him as he loitered outside the stadium's entrance.  In contrast, in Christchurch the weather was sunny and bright, with a temperature of 15 C not -10 C .  State Highway 1 near Woodlands, Invercargill and Edendale was closed after a Fonterra truck jackknifed.  Fonterra had difficulty picking up milk from some areas due to tanker trucks jack-knifing on the roads; 400 farmers dumped their milk for hygiene reasons as it began to ferment and mold.  Similar automotive incidents closed most of State Highway 99 as 10 cm of snow hit Invercargill that day and blocked most of its streets.  The airport was closed and Air New Zealand crews were sent home.  Overnight snowfalls hit Lumsden and Queenstown for the second night running. The morning of September 20 saw roof collapses from heavy snow of the Wren building and neighbouring retailer Briscoes in Yarrow St, Invercargill. Heavy snow destroyed a 1000 m2 glasshouse at Eldon Gardens, Donald McDonald's drive in Tweed St, killing 2,000 young tomato plants.  The weight of the snow buckled the metal support poles of the glasshouse and most of the eight bays collapsed.  The NZ$100,000 glass house was 18 years old.  The snowfall in Invercargill was about deep, and was probably the worst in the town for at least 50 years. September 23 saw northern India's storm devastating railways with 22 Delhi (inbound and outbound) trains canceled and another 65 diverted due to the severe flooding in Delhi as the river Yamuna reached the 207.06 m danger mark.  The Old Yamuna Bridge between East Delhi and New Delhi was closed for safety reasons as the Yamuna river's level rose menacingly.  Residents in low-lying areas were evacuated to shelters in New Delhi. The Yamuna river reached a record high above the 495 ft danger mark in the worst monsoon rains for 30 years, leaving mosquito-attracting pools and coming dangerously near the Taj Mahal; more water was discharged from the Gokul and Okhla barrages. In Scotland, Grampian Police warned of treacherous conditions as flooding hit Aberdeenshire and Aberdeen on September 23.  Residents of the Meadows Care Home in Huntly, Aberdeenshire were moved to temporary shelder as the level of the River Deveron rose to danger level.  40 other local residents were given shelter at the Gordon Schools.  Grampian Fire and Rescue Service said a number of crews from Huntly, Elgin, and the former Tayside region, along with the British Red Cross and a local coast-guard team, were all involved in the evacuation.  Some schools were shut after several hours of torrential rain dramatically raised the level of a nearby Aberdeenshire river, while about 10 households in Soy Avenue and Durn Road, Portsoy were evacuated to a nearby care home.  Parts of North Ayrshire, Glasgow, Lothian, and Argyll and Bute were also affected.  Lothian and Borders Police later reported some flooding on West Maitland Street, Edinburgh, and High Street in Prestonpans. The Met Office said that 37 mm of rain had fallen in 12 hours overnight; Glasgow normally has 95 mm of rain in all of September.  Aberdeenshire also had 25 mm of rain in 12 hours, and it continued to rain for the rest of the day leaving an extra 10 – of rain by nightfall.  September 24 and 25 also saw patches of heavy rain hit various parts of Ireland, Belgium, and the Netherlands. In Hadejia and Kararar Rima, Nigeria, the flood victims slept wherever they could; the men searched for dry spots on the roads, while women and children kept piling into the houses still standing, as huge numbers of displaced people returned to flood-hit villages in northern Nigeria.  Over two million people were affected by the flood waters and more than fifty thousand families were made homeless.  Most of the houses were made of clay, so they dissolved in the flood waters. The Mississippi river started to rise and threatened St. Paul, Minnesota, with Mayor Chris Coleman declaring an emergency.  The previous week's rain caused the Minnesota and Mississippi rivers to reach flood levels.  Rising water levels after last week's heavy rainfall in southern Minnesota forced Water Street between Plato Boulevard and Minnesota 13 to be closed on September 28. In Canada, the financial assistance department announced that up to CA$300,000 per claim would be available to the people harmed by heavy rains and flooding in the North Island and Central Coast areas.  Areas including Port Alice, Port Hardy, Zeballos, and the Central Coast region's Bella Coola, Kingcome, and the Highway 20 corridor between Bella Coola and Anahim Lake would be covered. September 26 saw heavy rain (causing more massive flooding) moving into Bihar and devastating an area around Patna.  The Danapur Diyara district was the worst affected.  Locals complained about a lack of emergency housing and supplies.  The river Ganges breached its banks and flooded all low-lying areas, leaving many stranded due to flooded roads, collapsed bridges, and precautionary closures. September 27 and 28 saw the flood situation reaching its worst at Agra where the river water level touched 152.4 m on the morning of September 27.  Many neighborhoods and ghats, such as the Taj Ganj cremation ghat, were submerged after several thunderstorms brought very heavy rainfall.  Almost half of the city was without drinking water.  Tourists visiting the Taj Mahal were asked to stay far away from the raging river.  Settlements in the Balkeshwar and Dayalbagh areas have been inundated.  More than 30 villages in the Bah tehsil have also been badly hit by the Yamuna river floods. In Waikato, people started preparing for floods as heavy rain threatened to push already-swollen rivers past their limits.  Preparations were made for the worst as the region had already been hit by a storm the previous week.  While further rain did fall, the most serious flooding at the start of October 2010 turned out to be south of Waikato in the central region of New Zealand.\n\nBuildings and architecture of Brighton and Hove Brighton and Hove, a city on the English Channel coast in southeast England, has a large and diverse stock of buildings \"unrivalled architecturally\" among the country's seaside resorts.  The urban area, designated a city in 2000, is made up of the formerly separate towns of Brighton and Hove, nearby villages such as Portslade, Patcham and Rottingdean, and 20th-century estates such as Moulsecoomb and Mile Oak.  The conurbation was first united in 1997 as a unitary authority and has a population of about 253,000.  About half of the 20430 acre geographical area is classed as built up. Brighton's transformation from medieval fishing village into spa town and pleasure resort, patronised by royalty and fashionable high society, coincided with the development of Regency architecture and the careers of three architects whose work came to characterise the 4 mi seafront.  The previously separate village of Hove developed as a comfortable middle-class residential area \"under a heavy veneer of [Victorian] suburban respectability\": large houses spread rapidly across the surrounding fields during the late 19th century, although the high-class and successful Brunswick estate was a product of the Regency era.  Old villages such as Portslade, Rottingdean, Ovingdean and Patcham, with ancient churches, farms and small flint cottages, became suburbanised as the two towns grew and merged, and the creation of \"Greater Brighton\" in 1928 brought into the urban area swathes of open land which were then used for housing and industrial estates.  Many buildings were lost in the 1960s and 1970s, when Brighton's increasing regional importance encouraged redevelopment, but conservation movements were influential in saving other buildings. Much of the city's built environment is composed of buildings of the Regency, Victorian and Edwardian eras.  The Regency style, typical of the late 18th and early 19th centuries, is characterised by pale stuccoed exteriors with Classical-style mouldings and bay windows. Even the modest two-storey terraced houses which spread rapidly across the steeply sloping landscape in the mid-19th century display some elements of this style.  Extensive suburban development in Hove and the north of Brighton in the late 19th and early 20th century displays architectural features characteristic of those eras, with an emphasis on decorative brickwork and gables. Postwar developments range from Brutalist commercial and civic structures to pastiches of earlier styles.  Sustainable building techniques have become popular for individual houses and on a larger scale, such as at the long-planned New England Quarter brownfield development. Local and national government have recognised the city's architectural heritage through the designation of listed building and conservation area status to many developments.  Since 1969, 34 conservation areas have been created, covering areas of various sizes and eras; and more than 1,200 structures have listed status based on their \"special architectural or historic interest\". Brighton was originally an agricultural and fishing village surrounded by fields where sheep were farmed and corn was grown.  In the Saxon era, small buildings developed in an area bounded by four streets named after the points of the compass, and a church stood on higher ground inland.  Modest cottages for the fishermen stood on the beach below the cliffs and the now vanished South Street.  A thriving fishing industry contributed to the town's first period of growth in the 16th and 17th centuries, but development did not expand beyond the old boundaries.  The industry then contracted in the early 18th century, and depopulation occurred.  Labour and land for redevelopment accordingly became cheaper, and because good travel and communication routes were already established the town was well placed to grow rapidly again when sea-bathing became fashionable in the mid-18th century.  Little pre-18th century architecture remains in Brighton, therefore, although there are some individual buildings.  For example, 27 King Street in North Laine is cobble-fronted and retains a timber-framed interior which could be 17th-century.  Hove, meanwhile, was a single-street village with a manor house, some modest cottages and a church further inland.  Although St Andrew's Church remains in use and Hove Street survives, the manor house was demolished in 1936 and no other original buildings remain. Early-18th-century descriptions of the old town of Brighton (the present Lanes) concentrated on how small and low the houses were, and how the lower storeys were characteristically set slightly below ground level.  This, and the proximity of the houses to each other, may have offered protection against storms and flooding from the sea.  (In one of the earliest descriptions of Brighton - a letter dated 1736 - the rector of Buxted claims that \"we live here underground almost ... the second storey is finished something under 12 feet.\")  \"Huddling together\" may have also helped the houses survive to the present day: they were poorly built and had little structural integrity.  Typical Lanes buildings are timber-framed and plastered with load-bearing walls of bungaroosh with some flint.  Brick quoins and courses added strength, and façades were often studded with pebbles from the beach.  These would sometimes be coated with tar to keep water out, although this only became common in the early 19th century.  In The Lanes, such buildings can be seen at Bartholomews, Middle Street and Ship Street among others. Buildings of the 16th and 17th centuries and earlier can be found in the old villages absorbed by modern Brighton and Hove.  At St Wulfran's Church, Ovingdean, the 12th-century nave and chancel replaced a Saxon structure.  St Helen's Church at Hangleton retains 11th-century herringbone masonry and other ancient fabric.  The old parish churches of Patcham, Portslade, Preston, Rottingdean and Brighton itself all retain some features from the 12th to 14th centuries, although they were all subject to Victorian restoration.  Hove's oldest secular building is Hangleton Manor (now a pub), a Vernacular-style flint building with some 15th-century fabric.  Little has changed since the High Sheriff of Sussex rebuilt it a century later, and the dovecote outside it is 17th-century.  Other surviving manor houses and mansions in the old villages around Brighton and Hove include Preston Manor, Patcham Place, Stanmer House, Moulsecoomb Place and Ovingdean Grange, while Patcham and Rottingdean have well-preserved lesser houses such as Court House, Down House, Hillside and Southdown House, generally built of brick and flint in the 18th century. The first development outside the four-street boundary of the ancient village was in 1771–72, when North Row (soon renamed Marlborough Place) was built on the west side of the open land.  Some tarred cobble-fronted buildings survive there.  At the same time, inns were becoming established as fashionable venues: the Castle (demolished) and the Old Ship both had \"uncommonly large and expensive\" assembly rooms for dancing and high-class socialising.  The Castle's assembly rooms of 1754 were redesigned by John Crunden in 1776 in Classical style; in 1761 Robert Golden designed Palladian-style rooms for the Old Ship, later redecorated in a \"[Robert] Adamish\" style after Crunden's work at the Castle.  Robert Adam himself redesigned Marlborough House in 1786–87: with its elegant Neo-Palladian façade and \"spatially arresting interior\", it has been called the finest house of its era in the city. The Prince Regent visited Brighton regularly from 1783 and soon wanted a house.  A building near the Castle Inn was found, and Henry Holland extended it in \"a stilted Classical style\" in 1786–87.  The Royal Marine Pavilion, as it was called before its present name (the Royal Pavilion) was adopted, became increasingly important in the growing town as it became the centre of activities for the Prince and his entourage—and the focal point for his regularly changing architectural tastes.  Holland revamped the building in 1801–04 in a Chinese style, and the French-inspired interior was changed as well.  Meanwhile, William Porden added a \"monumental\" complex of stables (now the Brighton Dome complex) to the west in 1804–08, in an Indian style.  James Wyatt and later John Nash were then commissioned to alter the building again; Nash's work, finished in 1823, gave the building its present opulent Indo-Saracenic Revival/Orientalist appearance. The Prince Regent's patronage helped Brighton become a fashionable, high-class resort.  As it became more popular, it further outgrew its four-street boundaries.  Planned development, as opposed to ad hoc growth, started in the 1780s with North Parade and South Parade alongside Old Steine.  By the 1790s it spread well to the east along the East Cliff: New Steine (1790–95, but refaced in the 1820s) was the first sea-facing square, then came Bedford, Clarence and Russell Squares (all early 19th century) and Brighton's first crescent, Royal Crescent (1799–1802).  Powered by \"fashion, demand and the availability of capital\", the scale of building and architectural ambition kept growing—especially when the father-and-son architects Amon and Amon Henry Wilds and their associate Charles Busby arrived in the town.  They helped to develop the Regency style which now characterises the seafront.  Hanover Crescent, Montpelier Crescent, Park Crescent, the Kemp Town estate (Sussex Square, Lewes Crescent, Arundel Terrace and Chichester Terrace) and Brunswick Town (Brunswick Terrace, Brunswick Square and associated streets) were among their set-piece developments.  (The Brunswick estate was also the first significant development in the parish of Hove.)  Accordingly, by the early 19th century, Brighton was renowned for the splendour and \"strongly individual character\" of its architecture.  William Cobbett claimed in 1832 that it \"certainly surpass[ed] in beauty all other towns in the world\".  Due to the quantity and quality of work produced by the Wilds–Wilds–Busby partnership and the groundbreaking designs produced by Holland, Nash and Porden—which \"established a vocabulary of architectural elements\" that defined the entire Regency style—Brighton's early urban development was characterised by an \"overflowing of architectural inventiveness\". Around the same time, though, the first concerns were raised about the poor quality of houses on the edge of Brighton—especially on St James's Street, Edward Street and the roads running off West and North Streets.  Many reports and studies were made by the Corporation and outsiders over the next decades, but little action was taken.  There was, however, some slum clearance in 1845, when Queens Road was driven through the infamous Petty France and Durham districts to provide a direct link from the station to the town centre. The London–Brighton railway reached the coast in 1841, and westward and eastward links were soon built from Brighton railway station.  This was built in 1841 to David Mocatta's Italianate design, then added to in 1882–83 when H.E. Wallis added the dramatically curved train shed and F.D. Banister made further alterations, creating a building \"entirely characteristic of the greater Victorian railway station\".  The line to the east crossed the landmark London Road viaduct, a 28-arch, 400 yd , sharply curving brick structure which stood in empty fields when built by John Urpeth Rastrick in 1846. Development had not yet reached this part of Brighton because the ancient field system to the north and east of the town constrained its growth, as did the ownership by the Stanford family of most of the remaining land surrounding Brighton and Hove.  They carefully controlled its sale and development, releasing parcels of land gradually and ensuring that visually cohesive planned estates of high-quality housing were built.  The area's 19th- and early 20th-century housing accordingly has a clear pattern and \"a distinctive character\".  The poorest housing was to the east of Brighton (slum clearance around Carlton Hill, Albion Hill and Edward Street has replaced much of this); working-class housing for tradesmen, railway workers and other artisans spread to the northeast around Lewes Road, the viaduct and the station; middle-class developments lay north of the centre around London Road; and the highest-quality suburbs developed to the northwest of Brighton and north of Hove on the Stanford family's land.  As originally built, the inner suburbs were of variable architectural quality: small houses with very late Regency-style flourishes predominated, but scattered among these were small-scale industrial and commercial development (the latter especially along the main roads), a range of high-quality Victorian churches such as St Bartholomew's, St Martin's and St Joseph's, and institutional buildings such as workhouses, hospitals and schools.  Improving access to education was a particular priority for Brighton Corporation in the 19th century, so straight after the Elementary Education Act 1870 was passed it set up a school board, appointed Thomas Simpson as its architect and surveyor and provided several schools in suburban areas—most of which survive with little alteration.  Simpson also worked for the Hove school board from 1876, the enlarged Brighton and Preston board from 1878 and took on his son Gilbert to assist in 1890. The coming of the railway changed Brighton from an exclusive resort to a town popular with all classes of holidaymaker and permanent resident alike: the population grew by nearly 50% in the first decade.  The seafront remained the main attraction, so an array of features were added: pleasure piers, promenades, hotels, entertainment kiosks and an aquarium.  The West Pier and Palace Pier date from 1863 and 1891 respectively, although both were completed several years later; Madeira Drive was laid out in 1872 and received its \"signature cast-iron terrace\" (including a pagoda-shaped lift decorated with Greek gods) in the 1890s; Kings Road was widened in the 1880s; and large hotels began to line it even before this.  Early-19th-century hotels such as the Royal Albion, Royal York and Bedford were joined by an Italianate pair by John Whichcord Jr. (the Grand, 1864) and Horatio Nelson Goulty (the Norfolk, 1865).  Then in 1890 the vast Metropole Hotel by Alfred Waterhouse \"broke the orthodoxy of stucco along the seafront\" due to its prominent red-brick and terracotta façade.  Its deliberately different design caused shock and brought criticism, but the \"British Architect\" journal considered it \"a wonderful relief\" from the homogeneity of stuccoed Regency buildings.  Brighton's architecture was beginning to reflect trends in the country as a whole, but the Regency style and the Royal Pavilion's onion-domed, minaret-studded opulence continued to influence architecture throughout the town, and on the seafront in particular. Hove, meanwhile, was also developing rapidly—but its influences were different.  Although the Brunswick estate was successful, development of the neighbouring Adelaide Crescent stalled for more than 20 years and Decimus Burton's original design was scaled back.  Next came Palmeira Square ( 1855–1865), where the evolution from Regency to Victorian Italianate is clear, and there was some suburban development (called Cliftonville) around the new Hove railway station in the 1860s, but large tracts of land to the north and west remained undeveloped because of conditions in William Stanford's will.  Only in 1872 did these conditions expire, and over the next 30 years Hove developed into a comfortable, spacious, suburban town with \"a certain gentility\" which it still possesses.  Architects James Knowles and Henry Jones Lanchester were involved at first, and William Willett built the streets of ornately decorated gault brick villas they designed.  Next came H.B. Measures and Amos Faulkner, who introduced more architectural variety and preferred red brick; then local architects Thomas Lainson and Clayton & Black laid out further estates of spacious tree-lined avenues and large half-timbered houses in the Queen Anne Revival and Domestic Revival styles.  Public buildings were also provided, such as Hove Town Hall (1882; demolished 1966), a public library (1907–08) and Hove Museum and Art Gallery (a converted villa of 1877 designed in \"drab Italianate\" style by Thomas Lainson).  Good Gothic Revival churches of this era include Central United Reformed Church (1867 by Horatio Nelson Goulty), the \"dignified and grand\" Sacred Heart (1880–81 by John Crawley) and Holy Trinity (1863 by James Woodman).  Specialist building development company Medical Centre Developments bought the disused Holy Trinity in February 2016 for conversion into a medical centre. Residential growth continued in the interwar and postwar periods, and the distinctive zonal pattern of development continued.  Estates of council housing were built east and northeast of Brighton (at Whitehawk, Bevendean and Moulsecoomb, and in the redeveloped Carlton Hill inner suburb which had been subject to urban renewal); middle-class residential housing developed to the north in the Patcham and Preston areas; and suburbs such as Westdene, Withdean, Tongdean and West Blatchington to the northwest of Brighton and the north of Hove had an upper middle-class character.  The rapid interwar suburban growth was similar to that seen throughout southeast England, but it was particularly stimulated by the introduction of electric trains on the main railway route to London—bringing a quicker and much more frequent service and increasing the attractiveness of commuting.  Meanwhile, Brighton Corporation began major slum clearance operations in the 1930s when the government offered financial incentives.  Moulsecoomb and the Pankhurst Avenue area near Queen's Park, both started in the early 1920s, were the first council estates.  In the former, the South Moulsecoomb area was laid out first; its 478 houses, on 94 acre taken from the parish of Patcham in 1920, were designed along \"garden city\" lines with semi-detached houses set in large green spaces.  North Moulsecoomb's 390 houses, including many brick-built terraces at a much higher density, followed from 1926.  Brighton's first council flats were the four-storey Milner (1934) and Kingswood (1938) blocks, built as part of the Carlton Hill slum clearance programme. Several streets in central Brighton were also transformed by the Corporation in the 1920s and 1930s: they sought to improve the flow of traffic by widening main roads in the commercial heart of the town.  Western Road (1926–36), West Street (1928–38) and North Street (1927–36, and again in the 1960s) were all widened.  Many 19th-century buildings were demolished: on North Street, a mixture of shops, houses (some in \"squalid courtyards\") and inns disappeared, on West Street all buildings on the west side (mostly large houses of the late 18th and early 19th century, when the road was high-class) were removed, and the north side of Western Road was demolished.  Most buildings there were shops with tall 19th-century houses behind. Another 1930s development could have changed the Regency face of Brighton and Hove and redefined it along Modernist lines.  Wells Coates was commissioned to build a block of flats next to Brunswick Terrace.  The high-class speculative development was named Embassy Court and was completed in 1935.  Praise from the \"Architects' Journal\" was matched by Alderman Sir Herbert Carden, who campaigned for every other building along the seafront to be demolished and replaced with Embassy Court-style Modernist structures, all the way from Hove to Kemp Town.  He also wanted to demolish the Royal Pavilion and replace it with a conference centre.  This encouraged the formation of the Regency Society, the first of many local conservation and architectural interest groups. This era also saw a transformation in Brighton's leisure and entertainment venues as it continued to flourish as a popular resort.  Many large cinemas, theatres and dance halls were built, some in the fashionable Art Deco style: among them were the Savoy, the Astoria, the Regent, the Imperial Theatre and Sherry's Dance Hall—which was near another \"much-loved venue\", the SS Brighton complex.  Also in the Art Deco style were the Saltdean Lido and another open-air swimming pool at Black Rock.  Older buildings given a new look included the Brighton Dome (originally the Royal Pavilion's stables, built by William Porden) and the Brighton Aquarium.  Local architect John Leopold Denman designed many new buildings, typically in a \"well-mannered and individual\" Neo-Georgian style: most were for commercial use, such as 20–22 Marlborough Place, Regent House and the offices for the \"Brighton & Hove Herald\" newspaper at 2–3 Pavilion Buildings, but the Hounsom Memorial Church at Hangleton and the Downs Crematorium are also his.  The latter may have been inspired by Harry Stuart Goodhart-Rendel's St Wilfrid's Church on nearby Elm Grove.  Goodhart-Rendel, a native of Brighton, also produced \"his own inimitable response to Modernism\" at Princes House, a steel-framed building with red and blue patterned brickwork.  Several of its neighbouring commercial buildings on North Street are by Denman or the Clayton & Black firm. The urban area was not as badly affected by World War II bombing as some coastal towns, notably Eastbourne, but some buildings were damaged or destroyed.  The central arches of London Road viaduct had to be rebuilt after a direct hit left the tracks hanging in mid-air; the different coloured replacement brickwork is still visible.  St Cuthman's Church, built in 1937 on the new Whitehawk estate, was destroyed in 1943. The first council-owned tower blocks date from 1961, when four were built on the steep slopes of Albion Hill; Highleigh, opened on 16 May 1961, was the first.  Other tower blocks of ten or more floors stand in the Edward Street and Upper Bedford Street areas of Kemptown, where five were built in the mid-1960s to complete an urban renewal programme begun in 1926; Hollingdean, where the landmark Nettleton Court and Dudeney Lodge towers date from 1966; and Whitehawk, where the Corporation built four ten-storey blocks called Swanborough Flats in 1967.  Meanwhile, Hove had a high proportion of multi-occupancy residential buildings.  Thousands lived in small bedsits hidden \"behind the classic proportions [of] many of the older houses\": a report by the council in 1976 stated that 11,000 people in Hove lived in \"substandard housing\".  Given the lack of open land to build on, demolition and redevelopment was championed.  Based on Herbert Carden's pre-war suggestion, the whole of Brunswick Square, Brunswick Terrace and Adelaide Crescent were to be replaced by tower blocks after Hove Council approved plans in 1945, but public opposition was too great.  Two decades later, the Conway Street redevelopment scheme (1966–67) replaced 300 slum houses on an 11 acre site near the railway station with several tower blocks.  A committee was formed to ensure householders received a suitable price for their compulsorily purchased houses. The Borough Councils changed their emphasis in the 1970s towards \"densely packed low-rise flats\" such as Hampshire Court (Kemptown) and Ingram Crescent (Hove).  This new direction was not matched by private firms, which continued to build residential towers into the 1980s—especially in Hove.  Two of the city's tallest privately built blocks, Chartwell Court and Sussex Heights (the latter, at 334 ft , is Sussex's tallest tower block), sit on top of Brighton's largest postwar redevelopment scheme—the Churchill Square shopping centre.  This 11 acre development by Russell Diplock & Associates (1963–68) has been condemned as \"a disaster architecturally\": its vast scale and poor relationship to surrounding buildings made it \"very typical of its date\".  It was rebuilt as a covered shopping mall by Comprehensive Design Group (1995–98).  Most other postwar schemes, whether commercial, residential or mixed-use, have amounted to small-scale infill.  Brighton Square, a new pedestrian shopping square in the heart of The Lanes, dates from 1966 and is in harmony with the \"intimate\" surroundings in terms of scale and architecture.  Elsewhere in The Lanes, Postmodern Regency-style pastiche architecture characterises infill schemes at Nile Street (1987–89 by the Robin Clayton Partnership) and Duke's Lane (1979 by Stone, Toms & Partners).  A large site between Middle Street and West Street is covered by Avalon, a curvaceous double-fronted block of flats by Christopher Richards (2004–06). The largest redevelopment scheme in the city since Churchill Square has been the laying out of the New England Quarter mixed-use area on the site formerly occupied by Brighton railway works and Brighton station's car park.  The early buildings (2004–07 by Chetwood Associates; mostly residential) are \"standard 21st-century developers' fare\"; but a second phase of building (2007–09 by Feilden Clegg Bradley Studios), with retail buildings integrated with residential blocks under the name \"One Brighton\", is more distinctive.  BioRegional and the World Wide Fund for Nature's \"One Planet Living\" design principles were used to ensure the development was sustainable.  The best building, a residential block, comes to \"a dramatic sharp point\" at an acute road junction.  Sustainable design also informs smaller developments around the city: Conran and Partners' Atlanta Apartments (2007) in Bevendean have chestnut wood cladding, recycled copper and living roofs of sedum; the Sea Saw Self-Build scheme in Whitehawk (1993) consists of 24 timber-framed houses; the Hedgehog Housing development at Bevendean (2000) is similar; and a multiple award-winning scheme for the South London Family Housing Association at Hollingdean (1988) was also built according to sustainable principles. Since the present urban area's settlements first developed as fishing villages and downland hamlets, the local architecture has been influenced by characteristic styles and the use of materials rarely seen elsewhere.  Black glazed mathematical tiles and bungaroosh are unique to Brighton and its immediate surroundings, and tarred cobblestones with brick quoins, salt-glazed brickwork and knapped or plain flints were also common in early buildings.  Stucco—perfectly suited to seaside conditions—predominated throughout the 19th century, such that \"of nowhere else did it become so universally characteristic.\"  Bay windows, a common feature of seaside resorts, were treated distinctively; balconies, sometimes roofed, were included on most 19th-century houses; Victorian and Edwardian houses were often designed as villas, with elaborate porches and decorative gables; and terraced housing is prevalent.  The Regency style was so popular and influential that it persisted much longer than in other places, while Gothic Revival architecture is almost absent in secular buildings—although the style was popular for 19th-century churches, of which the city has a large, high-quality range. Bungaroosh, a low-quality composite material, was commonly used in construction in the 18th century.  The material contained miscellaneous objects such as broken bricks, lumps of wood, pebbles and stone; this mixture was then shuttered in hydraulic lime until it hardened.  Bungaroosh walls were often hidden behind stucco or mathematical tile façades, and are susceptible to water penetration.  Mathematical tiles, a similarly localised material, were designed to be laid overlapping each other, giving the appearance of brickwork.  Glazed black tiles are closely associated with Brighton, and survive on 18th- and early 19th-century buildings such as Royal Crescent, Patcham Place and the shop at 9 Pool Valley.  Other colours of tile are occasionally seen, such as cream (in the East Cliff area) and honey (commonly used by Henry Holland, including on his design for the original Marine Pavilion).  The tiles gave bungaroosh buildings an expensive-looking façade and were easier to work with than bricks. Rendered stucco façades \"are a defining characteristic of Brighton and Hove's historic core\".  Stucco gave the appearance of stone, left a smooth finish and could be worked into intricate patterns on mouldings, capitals, architraves and other embellishments.  It was used prominently on long, continuous terraces of houses, such as in the Brunswick and Kemp Town estates.  Rustication was sometimes used, especially at ground-floor level.  Typical decorative mouldings include standard features of Classical architecture such as columns of various orders, pilasters, parapets, cornices and capitals.  Stucco façades were not always well-regarded: writing in 1940, Louis Francis Salzman considered that stucco \"hides what architectural features [the buildings] may possess and produces dull uniformity, entirely lacking in character\". Brick buildings are common throughout the area.  Pale gault brick is characteristic of some mid-19th-century residential developments, such as the area around Grand Avenue in Hove and the Valley Gardens area of Brighton (both conservation areas).  Later in that century, smooth red brickwork became more common.  Yellowish stock bricks were popular in the 19th century for non-residential buildings and walls which were not readily visible.  Different coloured bricks, such as brown and grey-blue, were often used in quoins and dressings on walls made of flint or red bricks.  The layout of brickwork \"has a significant effect on a building's appearance\"; the Flemish bond pattern is encountered most frequently in the city.  On Victorian and Edwardian houses, brick chimney-stacks often served a decorative as well as a functional purpose, and were sometimes tall and ornate: examples include the Queen Anne-style houses at 8–11 Grand Avenue, Hove (1900–03, by Amos Faulkner). Stone was rarely used as a building material, as it was not prevalent locally.  Some churches and banks of the 19th and early 20th centuries were built of Bath or Portland stone, and Kentish ragstone was used for St Joseph's Church on Elm Grove, but few ordinary residential or commercial buildings have any stonework.  Artificial stone was sometimes used for exterior features such as cornices and columns, though, especially during the Victorian era.  Flint was historically a common building material as it was \"always readily available in Hove, Portslade, West Blatchington and Hangleton\".  Agricultural buildings and cottages used random (unknapped) flintwork extensively, as did all four parishes' ancient churches and others further east such as Ovingdean and Rottingdean.  Flints were collected from the beach and the South Downs or dug out of the fields, where they were often found near the surface.  A flint pit survived at Southern Cross near Portslade until the 20th century.  It became popular again as a building material in the early 19th century, by which time several styles of flintwork had developed: rounded pebbles in seafront buildings, whole flints in rural cottages and agricultural buildings, knapped (split) flints, and random flintwork with brick dressings.  The use of stone or brick quoins and dressings on flint walls, necessary for structural reasons, enhances the appearance of such buildings, \"sometimes to great decorative effect\".  Knapped flint was used particularly in farmhouses in nearby villages which later became part of the urban area: Court House and Down House in Rottingdean, Home Farmhouse in Withdean, Southdown House in Patcham and several houses in Ovingdean and Stanmer have them.  The Sussex dialect includes specialist words for types of flint: the irregular joints between randomly laid knapped flints are \"snail-creeps\", and rounded pebbles are \"pitchers\".  An old \"Brighton Vernacular\" style has been identified: small cottages with cobblestone walls laid in courses, whose windows and doors were edged with red brickwork.  Many examples of this style were demolished during the mid 20th-century slum clearance programmes. Weatherboarding is uncommon, but there are several examples at Stanmer and Patcham (barns and cottages) and in Meeting House Lane in The Lanes.  Nearby, 37a Duke Street—the oldest building on that road—is a \"remarkable\" late 18th-century house with a façade of painted wooden blocks imitating stonework.  Timber framing is also rare in the city, but modern self-build schemes at Sea Saw Way, Whitehawk (1993) and Hogs Edge, Bevendean (1997–2000) feature this structural system.  The latter development was built according to Walter Segal's self-build methods and has sustainable features including recycled paper insulation.  Waste House, a conceptual sustainable building within the University of Brighton Faculty of Arts campus in central Brighton, was built between 2012 and 2014.  Nearly 90% of its materials—from the timber-framed structure (made of reclaimed wood from building sites) and exterior walls formed of waste chalk and clay to the household-rubbish insulation (VHS cassettes, toothbrushes and denim offcuts)—were destined for landfill.  The project, which has won several architectural awards, attempts to show how unwanted materials can be used to create a viable and energy-efficient building. Concrete and steel framing became common in the 20th century: examples include the new Hove Town Hall, Brighton's police station and courthouse, and the original Churchill Square shopping centre.  Amex House, a corporate headquarters in the Carlton Hill area, was the first building in Britain to use glass-reinforced plastic.  The New England Quarter, an early 21st-century mixed-use development, has many buildings clad in an elastomic render with timber cladding and large areas of glass. Many of the city's old buildings have \"butterfly roofs\"—double-pitched, with a central depression between the slopes.  The oldest roofs tended to be laid with handmade clay tiles; slate tiles and mass-produced clay tiles were popular later.  Elaborately decorated gables characterise the roofs of many houses and villas of the Victorian and Edwardian eras, especially in suburban areas.  These are usually steep and triangular: curved and shaped gables are uncommon in the area.  Stucco, plaster, weatherboarding and woodwork were often used to decorate the face of the gable. Bow or bay windows were the \"chief architectural feature\" of Brighton's early houses.  Vertical sliding timber-framed sash windows with glazing bars were usually inserted into these, although casements were sometimes used—typically on the oldest or most modest buildings.  Casements would sometimes be given glazing bars as well.  Such bars were usually slim and had mouldings in various patterns.  The combination of partly recessed sashes and bow windows is characteristic of Brighton's Regency-era residential developments.  The Queen Anne Revival-style housing popular in Hove in the late 19th century had its own window pattern: two-part sashes with many panes on the upper section, separated by wider glazing bars than those used in earlier years.  Casement windows were popular on interwar Tudor Revival houses, as at Woodland Drive (a conservation area) in West Blatchington; and steel-framed Crittall windows are found in interwar Modernist buildings such as Embassy Court and the Moderne-style mansion flats at 4 Grand Avenue, Hove. Elaborate doorcases and porticos with Classical-style details are seen on many 19th-century houses, especially those built in the Regency era.  A typical form consisted of two columns with decorative mouldings, an entablature and a straight roof, all stuccoed, supporting a cast-iron balcony.  Suburban villas often feature brick and timber porches with gabled tiled roofs.  In central areas, many old houses have been converted into shops and have lost their original doorways in favour of glazed shopfronts. Balconies and canopied verandas are often seen on larger Regency- and Victorian-era houses in central Brighton and Hove.  Typically at first-floor level, made of Portland stone or lead-coated timber and surrounded by cast iron railings with elaborate patterns, they sometimes span entire terraces of houses.  They were provided to extend the living space of the drawing room, considered the most important room in the house for socialising during that era; accordingly they extended some way beyond the ground floor.  Many terraces and squares faced central gardens or the sea, so balconies would give uninterrupted views of these.  Queen Anne Revival and Arts and Crafts-style villas of the late 19th and early 20th centuries, especially in Hove and around Preston Park, featured wooden balconies with simple balustrades formed of upright timbers. Mouldings of various types were common external decorative features in the 18th and 19th centuries, especially on Regency-style buildings.  Many structural elements would typically feature moulded stucco work—pilasters, entablatures, pediments, brackets and courses—while other mouldings would be merely decorative.  Typical designs included shells, foliage (especially on capitals) and vermiculation.  The Ammonite Order is a Classical order found almost exclusively in Brighton and Hove, consisting of fluted columns topped by capitals whose volutes are shaped like ammonite fossils.  Architect Amon Henry Wilds used them extensively.  Pilasters and columns of the Corinthian order are also common.  Victorian and Edwardian buildings made use of intricately moulded courses and bracketed eaves.  Elaborate carved reliefs are found on some of John Leopold Denman's buildings of the 1930s as a result of his collaboration with sculptor Joseph Cribb.  In central Brighton, 20–22 Marlborough Place has a series of reliefs showing workers in the building trade, and 2–3 Pavilion Buildings have Portland stone capitals with scallops and seahorses. Terracotta was popular in the Victorian and Edwardian eras as an external decorative element, as was yellowish faience earthenware.  They were commonly used to top off a structure such as a wall or roof, in the form of finials, urns and caps.  Carved terracotta panels were also used to decorate façades, especially below windows: the former Hove Hospital (now Tennyson Court) has prominent examples of this. Basements are a very common feature of houses in Hove: it was customary for servants to live in them in the Victorian and Edwardian era.  According to a Hove Council survey in 1954, 2,573 houses were built with basements. Brighton's earliest council houses date from the 19th century.  Two landowners donated land around the present St Helen's Road in 1897, and simple polychromatic brick cottages were built to commemorate Queen Victoria's Diamond Jubilee.  Much council building took place in the 1960s and 1970s, often in the form of tower blocks.  In Hove, the Conway Redevelopment Scheme lasted from April 1966 until July 1967.  Hundreds of slum houses were replaced by five towers with between 54 and 72 flats each; the ten-storey Conway Court is the tallest.  Dark red and buff brickwork, small areas of blue plastic panelling and recessed balconies characterise the buildings.  About £2 million was spent.  In 1976–77, old council houses in the Ingram Crescent area off Portland Road were replaced by low-rise flats in a modern style with varied architectural features such as weatherboarding-style timber, dark brickwork and catslide roofs.  No council houses have been built since the 1980s, but in July 2010 the council announced plans to demolish Ainsworth House, a 1960s low-rise block in the Elm Grove area, and build a higher-density high-rise \"family complex\".  Planning permission was granted in April 2011, and the 15-home development called Balchin Court was opened in September 2013.  In November 2011 squatters occupied the building, which was in a dangerous condition because it contained asbestos. The shortage of building materials caused by the First World War prompted the government to seek alternatives.  Hundreds of prefabricated homes were built, especially on the outskirts of the urban area, but more innovative were the two all-metal houses built in 1923 on the Pankhurst estate.  The government paid half the cost of construction of the \"Weir Steel Homes\".  They were demolished in 1969.  In 1934, the New Zealand-based architecture firm Connell, Ward and Lucas built three Cubist houses on a hillside site on the Saltdean estate—among the earliest buildings of that style in Britain.  More were planned, in an attempt to demonstrate that the design could work on a large scale; but no more were built, although some later houses in the area adopted elements of the style.  Two of the three \"iconoclast machines for living\", as they were called in 1987, survive in much-altered form, \"forlorn among their conformist brothers and sisters\".  The starkly white-painted cubes were originally sold for £550. The fields around the ancient village of Hove were owned by a few large landholders, whose gradual release of land for development in the 19th and early 20th centuries contributed to the town's distinctive pattern of growth: individual architects or firms designed small estates with a homogeneous overall style but with much variation between them.  The Wick Estate's land was transformed between the 1820s and 1860s into the Brunswick Town estate, consisting of grand Regency/Classical-style squares and crescents of houses, with smaller versions in grid-pattern side streets.  Next came the Cliftonville estate, which filled the gap between Brunswick Town and Brighton.  Two-storey semi-detached stuccoed villas in the Italianate style, often with canted bay windows, characterised the early part of the estate—the long north–south roads between Church Road and the seafront.  Cliftonville (now Hove) railway station opened to the north in 1865, stimulating further development in a similar style.  A railway architect, F.D. Banister, designed most of Cliftonville, including number 42 Medina Villas (his own home during the 1850s) and three surrounding houses, whose Jacobethan red-brick exteriors and curved gables contrast with the surrounding villas.  The West Brighton estate's rapid development began in 1872 on land bought from the Stanford family, the area's largest landholders.  Until the Stanford Estate Act of Parliament was passed in 1871, no houses could be built on the land, despite tremendous pressure for growth; within 12 years, 550 acre were developed and Hove's housing stock had trebled.  Sir James Knowles and Henry Jones Lanchester were the principal architects, and William Willett built the houses to a high standard. Many flats and mansion blocks were built in Brighton, Hove and Portslade in the interwar and immediate postwar periods.  St Richard's Flats (mid-1930s, by Denman and Son), \"cottagey and jazzy at the same time\", are stuccoed with wooden balconies and a clay-tiled roof.  King George VI Mansions at West Blatchington consist of three long groups of three-storey brick and tile terraces forming a quadrangle around an area of open space; designed by T. Garratt and Sons in the \"Vernacular Revival\" style, they are little changed since their construction.  Wick Hall (1936) and Furze Croft (1937, by Toms and Partners) occupy the old gardens of the original Wick Hall mansion.  Their \"elegant\" form and high quality makes them \"well-respected local landmark[s]\".  Furze Croft retains its Crittall steel windows and is characteristic of the 1930s Moderne style.  Courtenay Gate occupies a prime site on Hove seafront; designed in 1934, it rises to seven storeys and has good architectural detail.  In The Drive in Hove, numbers 20 and 22 are brick- and stone-built flats which enhance the streetscape of this important residential road; number 22 was \"designed to resemble a castle\".  John Leopold Denman's Harewood Court (1950s), built for the Royal Masonic Benevolent Institution, is a seven-storey brick-built block in the Art Deco style.  Nearby, at the junction of The Drive and Cromwell Road, Eaton Manor dates from 1968–72, rises to eight storeys and contains over 100 flats.  It is described on the local list as \"handsome ... well articulated ... [and] an excellent example of the type\". For many years, convalescent homes and similar institutions have taken advantage of the mild climate and sea air.  The Convalescent Police Seaside Home in Hove was Britain's first when it opened in 1890 in a house in Clarendon Villas.  Almost immediately, architect J.G. Gibbins was engaged to design a purpose-built home on land nearby.  This plot on Portland Road was in \"a charming position, [...] open to the sea\" at the time.  William Willett erected the building, which opened in July 1893.  The red-brick home has gabled roofs, substantial chimney-stacks and a visually prominent entrance, and is a dominant presence on Portland Road.  The home moved to Kingsway in 1966, and East Sussex County Council converted the old building into the Portland House Nursing Home.  The French government paid for a large home to be built on the cliffs at Black Rock in 1895–98.  The château-like French Convalescent Home was converted into flats in 1999, but retains its slate mansard-roofed corner pavilions, gabled entrance and garden-facing colonnade.  The French Renaissance Revival style chosen by architects Clayton & Black contrasts with surrounding seafront developments.  St Dunstan's, a charity which looks after blind former members of the Armed Forces, is based at Ovingdean, and its rest and rehabilitation home is based on a prominent downland site overlooking the coast road.  The Burnet, Tait and Lorne Partnership's International Modern steel-frame and pale brick home has a cruciform plan with a symmetrical west-facing façade.  Some windows are recessed, and others are flanked by brown-tiled columns.  Described as \"slightly reminiscent of Charles Holden's London Underground stations\", its shape recalls that of a biplane.  A low chapel in front is topped by a Winged Victory sculpture.  On The Drive in Hove, the Grade II-listed number 55 (now flats) was a convalescent home called \"Catisfield House\" between 1939 and 1999.  It was run by the Rose Elizabeth Greene Charitable Trust: Miss Greene had left the original Catisfield House (in rural Sussex) in her will to house poor women recovering from stays in hospital.  It moved to Hove when larger premises were needed. The redevelopment of Brighton's three major commercial streets—North Street, West Street and Western Road—in the 1930s means that they are now characterised by distinctive interwar commercial buildings.  Western Road has \"a good run of large\" department stores and other shops: a ship-like Art Deco corner building by Garrett & Son (1934) incorporating Clayton & Black's Imperial Arcade (1924), the Moderne former Wade's (now New Look) and Woolworth's stores (1928), the British Home Stores (1931 by Garrett & Son; now Primark) and the Stafford's hardware shop (1930; now Poundland) in American-influenced and Continental European-influenced versions of the Classical style and both decorated with elaborate motifs, and the \"unusually palatial\" Neoclassical Boots the Chemist (1927–28; now McDonald's).  Covering the block between Dean and Spring Streets, its stone façade has four evenly spaced Ionic columns in the centre of the upper storey—originally a restaurant and tearoom which featured regular orchestral performances.  Mitre House is a monolithic red-brick and stone structure dating from 1935.  Now housing miscellaneous shops at ground-floor level, it originally incorporated the south coast's largest branch of International Stores, a car showroom and Brighton's branch of W H Smith below its five storeys of flats.  It replaced the 19th-century premises of Le Bon Marché, which after closure in 1926 were acquired by Brighton Corporation to house shops whose premises had been compulsorily purchased.  Older buildings survive on the south side, including two Classical-style bank branches—Thomas Bostock Whinney's Doric-columned Classical-style Bath stone Midland Bank (1905; now HSBC) and Palmer & Holden's heavily rusticated National Westminster Bank of 1925, with large arched windows flanked by pilasters and a prominent balustrade on the parapet.  The north side of North Street became the centre for bank and office buildings, though.  Survivors include Denman & Son's \"sombre Classical\" Barclays Bank branch (1957–59), a very late use of that style, the Modernist/Brutalist Prudential Buildings (1967–69, by the Prudential's in-house architect K.C. Wintle), originally that company's headquarters but now shops and a hotel; another Thomas Bostock Whinney-designed Midland Bank branch, built in 1902 with a colonnade of Tuscan columns and a balustrade at the top, typical of the Edwardian era; and the former National Provincial Bank branch by Clayton & Black and F.C.R. Palmer (1921–23; now a Wetherspoons pub), with intricate carving and use of detail throughout the Louis XVI-style Neoclassical stone façade.  Nearby at 163 North Street is \"the \"chef d'œuvre\" of Clayton & Black, an ebullient essay in Edwardian Baroque\", which they built in 1904 for an insurance company.  The Boots store which replaced the Regent Cinema in 1974 had a \"sculptural quality\" because of the way its steel frame projected beyond the glazed curtain walls.  Derek Sharp of Comprehensive Design Group undertook the work, but it the building was re-clad and redesigned in 1998, losing the original impact.  Waterstones bookshop opposite, designed for Burtons in 1928 by their in-house architect Harry Wilson, has a Classical theme with full-height pilasters. Several financial services companies made Hove their base in the late 20th century.  The Sussex Mutual Building Society's new head office on Western Road (1975), called \"one of the finest new office buildings in the locality\" in contemporary reports, is a well-lit slate-roofed building with a glazed clay mosaic mural depicting scenes from Sussex, designed by Philippa Threlfall.  The Alliance Building Society's three-storey steel-framed head office building at Hove Park was designed in the 1960s by Jackson, Greenen and Down, who gained the commission at the end of a competition started in 1956.  It had strong horizontal lines offset by granite columns and tall, narrow steel-framed windows.  On its opening in 1967, it was anticipated to be \"a great contribution to the architectural thought of the 20th century\"; but by the 1980s it was derided as a \"carbuncle\" and a \"white elephant\", its stark Modernist form having dated badly.  The merged and greatly enlarged Alliance & Leicester Building Society moved out in 1994 and the building was knocked down in 2001.  David Richmond and Partners' £65 million \"City Park\" scheme, consisting of houses and three curved-roofed office blocks rising to four storeys, replaced it.  The Legal & General insurance company moved there from their earlier home at the former Hanningtons furniture depository on Montefiore Road (now the Montefiore Hospital); architects Devereux and Partners had \"elegantly converted\" this 1904 building for its new purpose in 1972. High-tech offices of the 21st century include Exion 27 (built in 2001 by the Howard Cavanna consultancy), now used by the University of Brighton.  The exterior is panelled with aluminium cladding and has extensive areas of tinted glass.  Structurally, the building is steel-framed with steel and concrete floors and a large brise soleil.  The \"imposing\" 28000 sqft building was the city's first ultramodern commercial property and was intended for mixed commercial and industrial use, but its completion coincided with a slump in demand for high-tech premises. Brighton's first large-scale industry was the railway works, established next to the railway station in 1842.  Several extensions were built as demand grew for locomotive manufacture and repair: in 1889, the buildings had to be extended on iron piers across the floor of the steeply sloping valley.  After closure in 1957, some of the buildings were converted into a bubble car factory, which made 30,000 three-wheeled Isettas in the next seven years.  The whole site was cleared between 1962 and 1969, and the mixed-use New England Quarter now covers the area.  (The LBSCR also established a railway mission chapel for employees of the locomotive works; the flint-built Gothic Revival-style building on Viaduct Road is still in religious use, having been taken over by an Evangelical group.)  The British Engineerium in West Blatchington is a museum which occupies a mid-Victorian former water pumping station.  Its bold polychromatic brickwork, symmetrical High Victorian Gothic engine room building, visually dominant chimney and associated structures—all of which are listed—combine to form \"an unusually fine asset\" which is \"a splendid example of Victorian industrial engineering\".  A former brewery in the ancient village centre of Portslade dominates the surrounding flint buildings.  The \"characterful\" Classical/Italianate five-storey yellow-brick building was built in 1881 and is now in mixed industrial and commercial use.  The former Phoenix Brewery (1821) between Grand Parade and the Hanover district was historically significant but architecturally modest, apart from the later brewery office and adjacent Free Butt pub.  Closure came in the early 1990s, and the site was redeveloped for student housing.  Allen West & Co. Ltd, an electrical engineering company which was a major employer in northeast Brighton from 1910, built several distinctive factories on Lewes Road and the Moulsecoomb estate, especially in the 1940s and 1950s.  Most were demolished in the 1960s and 1970s, and the large warehouses of the Fairway Trading Estate occupy the Moulsecoomb site; but the company's wide brown-brick administrative and design office, built in 1966 on Lewes Road, was sold to Brighton Polytechnic and became Mithras House. Brighton's parish church, dedicated to St Nicholas, dates from the 14th century, St Andrew's Church at Hove is a century older, and the formerly outlying villages of Ovingdean, Hangleton, Rottingdean, West Blatchington and Portslade have even more ancient buildings at their heart.  Nevertheless, the defining characteristic of Brighton and Hove's religious architecture is the exceptional range of richly designed, landmark Victorian churches—particularly those built for the Anglican community.  The city's stock of such churches is one of the best outside London: this is attributable to the influence of fashionable society and the money it brought, and to the efforts of two Vicars of Brighton, Henry Michell Wagner and his son Arthur, to endow and build new churches throughout Brighton's rapidly developing suburbs and poor districts.  Both men were rich and were willing to pay for well-designed, attractive and even flamboyant buildings by well-known architects such as Benjamin Ferrey, Richard Cromwell Carpenter and George Frederick Bodley.  An early preference for the Classical style, as at Christ Church (now demolished) and St John the Evangelist's at Carlton Hill, gave way to various forms of Gothic Revival design—principally in the starkly plain form of the gigantic St Bartholomew's Church and the even larger St Martin's, whose fixtures and furnishings are classed among the best in England.  However, Charles Barry's imposingly sited St Paul's Church (1824), which began the Gothic trend, was not commissioned by the Wagners; nor were Hove's new parish church, the Grade I-listed All Saints (1889–91) or Cliftonville's St Barnabas' (1882–83), both by John Loughborough Pearson.  St Michael and All Angels Church, built in two stages by Bodley (1858–61) and William Burges (1893–95), was established by Rev. Charles Beanlands, a curate under Arthur Wagner at St Paul's.  The two parts, in different interpretations of the Gothic Revival style, harmonise well, and the interior (mostly by W. H. Romaine-Walker) is one of the city's grandest.  The present St Mary the Virgin Church is the second on the site: Amon Henry Wilds's Classical building collapsed during renovation and was replaced in 1877–79 by William Emerson's \"dynamic\" Early English/French Gothic design—his only church in England. Also characteristic of the Victorian era was the rebuilding or restoration of the area's ancient churches.  Richard Cromwell Carpenter rebuilt St Nicholas' Church from a ruined state in 1853–54, and Somers Clarke did more work in 1876.  George Basevi carried out an \"uninspiring\" neo-Norman revamp of the 13th-century St Andrew's Church in the 1830s, James Woodman and Ewan Christian \"over-restored\" St Peter's Church at Preston Village in 1872 and 1878, and the 11th- and 12th-century St Peter's Church at West Blatchington was initially rebuilt by Somers Clarke in 1888–91 and comprehensively extended in 1960 in a complementary style by John Leopold Denman.  The partly Saxon St Wulfran's Church, Ovingdean (the city's oldest building) was altered in the 1860s, although the overwhelming impression is that of a 12th-century Downland village church; and similar work was carried out at St Helen's Church in Hangleton in the 1870s, which nevertheless \"retains its medieval character\". Anglican churches continued to be built in the 20th century.  The stripped-down Modern Gothic of Edward Maufe's Bishop Hannington Memorial Church (1938–39), with its \"simple and gracious interior\", has been called \"Historicism at its most simplified\".  The Gothic Revival style was also used for Edward Prioleau Warren's Church of the Good Shepherd (1921–22) and Lacy Ridge's St Matthias Church (1907), with its round tower and hammerbeam roof.  Harry Stuart Goodhart-Rendel's widely praised St Wilfrid's Church of 1932–34 (closed 1980), which embraced architectural Eclecticism and Rationalism, used two-tone brick and reinforced concrete and had an unusual interior layout designed to make the altar highly visible.  John Betjeman said it was \"about the best 1930s church there is\".  Postwar churches are mostly Modernist in style: the Church of the Good Shepherd in Mile Oak (1967, by M.G. Alford) has two angular roofs with six irregular vertical windows mounted between them, and Bevendean's brick and knapped flint Church of the Holy Nativity (1963, by Reginald Melhuish) has a distinctive roof with two unequal upward slopes.  An exception is the 1950s St Mary Magdalene's Church on the Coldean estate, converted from an 18th-century barn in 1955 by John Leopold Denman and still wholly Vernacular in style. The city's 11 Roman Catholic churches range in style from the Classical St John the Baptist's Church (1832–35) in Kemptown—with monumental Corinthian columns and pilasters—to the varied Gothic Revival designs of St Joseph, St Mary Magdalen, the Church of the Sacred Heart and St Mary's at Preston Park (which has some Arts and Crafts elements).  The \"startling\" Romanesque Revival St Peter's Church at Aldrington (1915) has a landmark campanile, while Henry Bingham Towner's design for the Church of Our Lady of Lourdes, Queen of Peace at Rottingdean (1957) was a \"very conservative\" and simplified modern interpretation of the Gothic form.  Other postwar churches are vernacular or Modernist in style, such as St Thomas More Church at Patcham (1963)—distinguished by a wooden geodesic dome and large areas of glass. Nonconformist churches and chapels vary in age and style.  Holland Road Baptist Church in Hove (1887, by John Wills) is a landmark Purbeck stone Transitional Gothic Revival building—a rare design for that denomination, although the flint-built Florence Road Baptist Church near Preston Park (1894–95, by George Baines) is in the similar Early English style.  The same architect designed a smaller flint and brick chapel at Gloucester Place in 1904; its symmetrical façade was spoiled by wartime bomb damage to the miniature flanking towers. Strict Baptists meet at the starkly plain Neoclassical Galeed Chapel (1868).  Methodist church designs include Romanesque Revival (the Grade II-listed Hove Methodist Church, by John Wills in 1895 and featuring a prominent rose window), Early English Gothic Revival (E.J. Hamilton's 1897–98 building at Stanford Avenue in Preston Park, with stone-faced brickwork) and Modernist at Patcham (1968) and Dorset Gardens in Kemptown (2003).  Former chapels of that denomination include the Gothic Revival United Church in Hove (1904), the Renaissance-style church at nearby Goldstone Villas (converted into offices in 1968), W.S. Parnacott's distinctive Gothic-style stuccoed and pinnacled Primitive Methodist chapel (1886) in Kemptown, Thomas Lainson's Romanesque Revival church at nearby Bristol Road and James Weir's Free Renaissance design of 1894 on the main London Road. The headquarters of the Anglican Diocese of Chichester are in the grounds of Aldrington House, a Victorian villa now used as a mental health support centre.  The Diocese previously used two houses in Brunswick Square, but in 1995 James Longley & Co. of Crawley constructed the new building—Church House—to the design of architect David Grey and at a cost of £670,000.  It is in the Sussex vernacular style and makes extensive use of local materials.  The uppermost of the three storeys is hidden within a deep tiled roof with high-level windows.  The red-brick walls have contrasting string courses of dark blue brick. Brighton, Hove, Brunswick Town and Portslade have each had a town hall, but only those at Hove and Brighton are still in use and Hove's was rebuilt after a fire.  Medieval Brighthelmston used a building (called the Townhouse) which was more of a market hall, and a later building (1727) known as the Town Hall was principally used as a workhouse.  Work on the first purpose-built town hall began in 1830; Thomas Read Kemp laid the first stone, and Thomas Cooper designed it on behalf of the Brighton Town Commissioners (of which he was a member).  Brighton Corporation spent £40,000 to extend it in 1897–99, to the design of Francis May.  Its severe Classical design, with huge Ionic columns and wide staircases, was criticised in the 19th century, and May's infilling of the cruciform building's wings affected the composition's symmetry.  Nevertheless, English Heritage has awarded it Grade II listed status. Brunswick Town Hall, built on behalf of the Brunswick Square Commissioners, was the first town hall in the Hove area.  Its Classical-style stucco façade concealed stone and brickwork.  It cost £3,000 and opened in 1856.  The three-storey building served Brunswick Town and Hove jointly from 1873, when the Hove Commissioners moved in; but more space was needed, so leading Victorian Gothic Revival architect Alfred Waterhouse was controversially commissioned to design a new building on a large site bought from the Stanford Estate's land.  The Brunswick building, at 64 Brunswick Street West, passed into commercial use, is now part of the Brighton Institute of Modern Music, and is Grade II-listed. Waterhouse was thought by some Hove Commissioners to be too important an architect to design Hove's new town hall, but work went ahead in 1880 and it opened in 1882.  Local housebuilder J.T. Chappell executed Waterhouse's design, which was an elaborate Renaissance Revival-style red-brick and terracotta edifice with plentiful stonework and ornately mullioned and transomed windows featuring tracery and coloured glass.  A prominent clock tower supplied by Gillett & Johnston's predecessor company Gillett & Bland rose from the roof.  The building was destroyed by fire on 9 January 1966, leaving only the west side standing.  Restoration was considered, but by the 1960s Victorian architecture was considered old-fashioned and unworthy of preservation, and the remains were demolished by 1971 to make way for a replacement building. The Queen Anne-style Portslade Town Hall has not been used for that purpose since 1974, when Portslade Urban District became part of Hove; nevertheless part of the premises are still used by Brighton and Hove City Council.  The building was originally the Ronuk Hall and Welfare Institute—a social club and multi-purpose hall built for workers at the nearby Ronuk wax polish factory.  Gilbert Murray Simpson designed the red-brick building for the company in 1927; the first stone was laid in July of that year, and the hall opened in 1928.  It was lavishly furnished and decorated with paintings by well-known artists.  Portslade Urban District Council bought the \"impressive\" building for £36,500 in 1959.  Its main hall has two balustraded galleries. Brighton's police did not have a central headquarters building until 1965: they were based in the old Town Hall, then in the basement of Thomas Cooper's new building when that was built in 1830.  Brighton Borough Engineer Percy Billington's \"graceless\" police headquarters opened on 27 September 1965 on John Street in Carlton Hill.  At 64 St James's Street in Kemptown, an 1850s building with stone urns and a balustrade housed an early district police station.  In November 2008, a two-storey sustainable building replaced an existing police facility in Hollingbury.  Portslade had two police stations but neither remains in use: one at North Street existed by 1862 but was superseded by the St Andrew's Road station in 1905.  This was built with stables and a hayloft at the rear for the constables' horses.  The two-storey brick-built station is a \"good quality, dignified\" Queen Anne Revival-style building with a gabled façade and a hipped roof of clay. Until 1869, offenders facing court action were taken to various inns or to Brighton Town Hall.  On 3 July of that year, Charles Sorby's two-storey Tudor/Gothic brick and Bath stone hipped-roofed courthouse took over.  It still had influences of the Italianate style popular for courthouses 20 to 30 years previously.  Percy Billington designed a new law courts complex at a cost of £665,000 on a site next to the police station in Carlton Hill in 1967, and this replaced the original building on Church Street.  Billington's concrete structure, extended in 1986–89, faced the same criticism as the police station: in particular, the charge that the architecture \"failed to provide civic monuments of quality\".  In Hove, Holland Road has a modernist police station (1964) and courthouse (1971–72).  The latter cost £380,000 and has four courtrooms and office accommodation.  Designed by Fitzroy Robinson & Partners, the low-set, \"strongly horizontal\" building has a recessed lower storey and is built of brown-blue brick from Staffordshire. The city's main fire station faces the five-road junction of Preston Circus, near London Road viaduct.  Established on the site of a brewery of 1901, the building was redesigned in 1938; Graeme Highet won the commission in competition.  His plain brick exterior, curving gently round the road, combines \"restrained Modernism\" with more old-fashioned elements such as a canopied entrance and windows with prominent architraves. Sculptor Joseph Cribb provided carved reliefs for the main doors.  Portslade's former fire station operated from 1909 until about 1941 and passed into commercial use in 1972.  District Surveyor A. Taylor Allen's design was built by Ernest Clevett.  The \"attractive-looking building\" is of white brick and terracotta, and is surrounded by a wall with a multi-coloured brick pier supporting a large gas lamp.  There are decorative terracotta plaques and a gabled dormer window with terracotta finials.  In 1914, Hove Council took responsibility for firefighting within its boundaries and immediately sought a replacement for the existing fire station of 1879 in George Street.  Clayton & Black's \"elegant\" new fire station on Hove Street, completed in 1929 at a cost of £11,098, was inspired by one at Bromley—but the \"charming bellcote\" on the roof was a reference to the nearby Hove Manor, demolished soon afterwards.  The façade had a double archway.  The building became redundant in 1976 and was converted into flats in 1981 by architect Denis Hawes. Brighton's main hospitals are the Royal Sussex County Hospital (RSCH) in Kemptown and the Brighton General Hospital at the top of Elm Grove on Race Hill.  The former was built in several stages.  Charles Barry's original buildings (1826–28) are Classical and pedimented; William Hallett and Herbert Williams built three complementary extensions between them by 1853; Edmund Scott and F.T. Cawthorn added the similar Jubilee Building in 1887; Cawthorn built the prominently gabled Outpatients' Building in 1892; John Leopold Denman's Eye Hospital in 1935 is in his characteristic Neo-Georgian style; and Robin Beynon's 2002–05 work on the Audrey Emerton Building reflects Regency-style themes of stuccoed bowed façades.  Brighton General was originally the town's workhouse.  Designed in 1853 but not built until 1865–67, it is in a \"debased\" Italianate style with a long frontage flanked by pavilions.  George Maynard and J.C. & G. Lansdown were responsible.  More buildings were added in 1887, 1891 and 1898 to the rear. The Royal Alexandra Children's Hospital has occupied two buildings of markedly different architectural character.  Thomas Lainson's Queen Anne Revival-style building of 1880–81 in the Montpelier district was distinguished by its Dutch gables and much use of terracotta and red brick.  Clayton & Black added a colonnade and other parts in 1906, and a major extension (again with prominent gables) was undertaken in 1927 by W.H. Overton.  It closed in 2007 after its replacement opened next to the RSCH, and has been redeveloped for housing.  Lainson's building has been retained but the other parts were demolished in 2012.  The new hospital was designed by Building Design Partnership (scheme architect Ben Zucchi) in 2004–07.  Its \"boat-like form [is] evocative of Noah's Ark\" as it rises dramatically above the other RSCH buildings.  Features include low, child-height windows, a multicolour-panelled curved façade and an oversailing roof.  It cost £36 million, has three times the capacity of the old building and won a design award in 2008. Hove's first hospital was a \"classic Victorian building\" on Sackville Road, built in 1885–88 by John T. Chappell.  Architects Clarke & Micklethwaite designed the red-brick hospital, which had prominent chimneys on a slate roof, crow-stepped gables and a large terracotta panel with various inscriptions.  Closure was announced in 1994, and a local property development firm paid £550,000 for the building in 1998.  Under the guidance of scheme architect Christopher Dodd, it was converted into 37 housing association flats called \"Tennyson Court\", retaining all original architectural features.  A£5 million replacement, the Hove Polyclinic, opened in West Blatchington in October 1998.  Bryan Graham of architecture firm Nightingale Associates designed the facility, which is distinguished by a right-oriented round tower, several curved windows with decorative panels of opaque glass, and six-panelled doors.  Montefiore Hospital was founded in 2012 in the \"magnificent red-brick\" former Hanningtons furniture depository on Davigdor Road, Hove, built by Clayton & Black in 1904. Public halls, gentlemen's clubs and similar institutions were often designed to stand out from their surroundings, especially when they were expensively funded as memorials to individuals.  The former John Nixon Memorial Hall of 1912, by an unknown architect, contrasts with Kemptown's small-scale stuccoed terraces with its broad arched-windowed, red-brick façade and the Neo-Jacobean free-style treatment of its gabled roofline.  It was used as a church hall, as was the Edward Riley Memorial Hall in Carlton Hill (now the Sussex Deaf Centre)—a brown-brick building with a steep clay-tiled roof and high flint walls around it.  The Ralli Memorial Hall near Hove railway station introduces a red-brick Renaissance theme to the formal gault-brick villa architecture of the area.  A distinctive balconied porch and prominently mullioned and transomed windows also contribute to the building's character.  On the West Brighton estate, Samuel Denman's Grade II-listed Hove Club (1897) is another Jacobean-style red-brick building with prominent gables, which also features buttresses rising to form chimneys, a loggia entrance, stone mullions and transoms, Art Nouveau-style windows and ornate interior timberwork. The \"Buildings of England\" series called the \"majestic and intimate\" University of Sussex \"the best architecture of the second half of the 20th century\" in Brighton and Hove.  Although buildings are still being added on the 200 acre site, the original development by Basil Spence (1960–65) retains its original character—especially in the relationship between the buildings and the undulating downland landscape on the semi-rural site (carved out of the Stanmer estate).  Spence's buildings are \"post-1955 Modernist\", influenced by both Le Corbusier and the \"epic monumentality\" of Ancient Roman architecture.  They include a library, lecture rooms for arts and sciences, a non-denominational place of worship, an arts centre and Falmer House, the university's social centre.  All are articulated in red brick and concrete, with hollow vaults, concrete beams, arches and fins.  New buildings including numerous halls of residence have been added at various times by architects including Eric Parry, the RH Partnership, ADP Architecture, DEGW and H. Hubbard Ford. The University of Brighton's Moulsecoomb site consists of Mithras House, a former industrial building, and \"a collection of utilitarian modern buildings\" flanking Lewes Road.  Mithras House dates from 1966 and was built for industrial use; more prominent is the 300 ft , ten-storey slab of the Cockcroft Building.  Built entirely of concrete—mostly precast except for the lowest storeys—it has an east-facing entrance flanked by two-storey concrete piers and set below panels of flint.  The main elevations are \"busy\" with a regular rhythm of windows.  Long & Kentish's adjacent Aldrich Library (1994–96), curtain-walled with concrete and aluminium, is a \"light and elegant\" contrast to Cockcroft.  The curvaceous Huxley Building (2010) also adjoins.  The University also has a site at Grand Parade, which consists of the Phoenix Building and the former College of Technology.  The former, designed by Fitzroy Robinson Miller Bourne and Partners in 1976, forms a \"brutal intrusion\" into the early-19th-century terrace of Waterloo Place: only two of its 14 houses remain.  Now known as the \"Grand Parade Annexe\", the former College of Technology—a Modernist building with sections of unequal height and windows set in prominent concrete frames—was designed by Percy Billington between 1962 and 1967.  Described as \"one of Brighton's better postwar buildings\" for its sensitive relationship to its prominent curved site, the layout of its windows recalls the 19th-century terraces it adjoins.  It replaced the former Municipal School of Art by J.G. Gibbins, built in 1876–77 of brick, terracotta and granite in the 14th-century Italianate style. Brighton College is the only surviving building in the city by George Gilbert Scott: his Brill's Baths have been demolished.  Many additions have been made to his 14th-century Gothic-style flint and Caen stone complex, on which work started in 1848.  The design has been criticised by Harry Stuart Goodhart-Rendel and Nikolaus Pevsner, who called the ensemble \"joyless\" and preferred T.G. Jackson's \"lavishly Gothic\" additions of 1886–87, in which terracotta was used extensively.  BHASVIC is a \"splendid\" former grammar school on Old Shoreham Road in Prestonville.  Designed by S.B. Russell in 1911–12, in a Neo-Georgian/Queen Anne style with extensive red brickwork and wings joined to a central section by a series of staircases lit by round windows), it occupies a prominent corner site and retains its original iron gates with the emblems of Hove and Brighton Boroughs and East and West Sussex.  The Municipal Technical College on Richmond Terrace, north of Grand Parade (now flats) was designed in 1895–96 by the Brighton Borough Surveyor Francis May.  Extensions of 1909 and 1935 were in a complementary style with brick and dark terracotta, and the whole complex has been described as \"Free Jacobean\" in style.  Roedean School (1898–99), a girls' boarding school high on the cliffs towards Ovingdean, is a Free Jacobean composition by John William Simpson.  From the centre of the symmetrical range rise two identical towers.  Several wings then project forwards from this central block, each with a large gable end.  Simpson also designed the chapel in 1906, a sanatorium in 1908 and a library in 1911.  Hubert Worthington worked on a dining room extension in the 1960s.  St Mary's Hall, a private school affiliated to Roedean but closed since 2011, has a symmetrical façade with prominent gables and mullioned windows.  The design resembles simplified Tudor Revival, although it is early for that style (George Basevi designed it in 1836). Most secondary schools in the city date from the 20th century and have been extended regularly: examples include Patcham High School, Longhill High School at Ovingdean, Hove Park School and Blatchington Mill School.  The last two and Varndean School in Brighton were given £3 million between them in 1999–2000 to undertake major extensions because of the expanding school-age population in the city.  Cardinal Newman Catholic School in Hove dates from 1870–72 and was originally a convent.  Frederick Pownall designed the original Gothic Revival buildings, which have been added to many times in the 20th century.  There is also a Gothic Revival chapel of 1878.  Exterior features include a large oriel window above the entrance, with prominent mullions and transoms, and an array of tile-hung gables.  Falmer High School was rebuilt in 2010–11 as the Brighton Aldridge Community Academy to the design of Feilden Clegg Bradley Studios.  Plum-coloured and \"chalky white\" brick elevations join on the east side \"like an elaborate scarf joint\"; the north face is mostly glass, while the south side burrows into the hillside.  Flint is also used, reflecting the downland location.  The exterior walls are curved, and the timber-clad interior is open-plan and made up of many interconnecting spaces.  The building won the Royal Institute of British Architects' Regional Sustainability Award in 2012.  The Varndean campus of educational buildings, which includes primary, secondary and tertiary institutions, is centred on Gilbert Murray Simpson's Neo-Georgian quadrangled Varndean College of 1929–31. Gilbert Murray Simpson originally worked with his father in the firm Thomas Simpson & Son.  Thomas Simpson's former board schools of the post-1870 period (most were designed between 1880 and 1903) can be found throughout the city.  Architecturally, his schools are \"the best [such] works\" in Sussex.  His style evolved from the Queen Anne Revival typical of early board schools towards \"an Edwardian Free style\" in which the standard red brickwork is supplemented by pebbledashing, terracotta and stonework.  His rooflines became more elaborate over time as well.  The Finsbury Road School (1881; now flats) combines red and brown brickwork.  Connaught Road School in Hove (1884) and Elm Grove School in Brighton (1893) are in the Queen Anne Revival style; the former, now an adult education centre, combines yellow and red brick and terracotta-coloured render to create an \"elegant\" and \"distinctive\" façade.  Clayton & Black extended the building in 1903.  York Place School has been dated to 1895 and has two frontages; it is now integrated into City College Brighton & Hove's buildings, which are scheduled for redevelopment.  In Preston parish, Simpson built the Preston Road School (1880, with \"flamboyant pedimented gables\" and a large roof), the Downs School (a simpler building of 1890) and the dome-topped Stanford Road School (1893), which also has a tower.  Simpson's last board school, St Luke's at Queen's Park, was also the most elaborate.  Dated 1900–03, it has a separate swimming pool and caretaker's house, all in the same \"characterful Edwardian Free style\".  The extravagant t -shaped design features two wings with entrances set below timber-turreted towers, four gables to the rear, and an ornately decorated arched window in the third wing (the base of the t ).  Much use was made of stone. Aside from the former board schools, the city has many other primary schools in a range of styles.  St Christopher's School in Aldrington is housed in \"one of the most intact of a series of large 1880s villas\" that characterise the New Church Road area.  Original features include iron fixtures and stained glass.  Portslade Infants School was designed by E.H.L. Barker and opened on 23 July 1903.  The building has distinctive polychromatic walls with bands of red, black and blue bricks, and the steep roof continues this pattern by contrasting red tiles against black slates.  In contrast, the nearby St Nicolas' Church of England School, designed by the architect of St Bartholomew's Church Edmund Scott in 1867, is a simple Gothic Revival building of flint.  Anthony Carneys' design for the new Aldrington Church of England Primary School (1991) consisted of a \"cluster of buildings with a Dutch barn feel to the roofline\" and a rural ambience, despite the urban location.  The red-tiled, steeply pitched gabled roofs have inbuilt windows including an oculus, and the walls are of yellow and red brick. Throughout East Sussex, few original libraries survive in use.  In Brighton and Hove, only Hove's central library (1907–08, by Leeds architects Percy Robinson and W. Alban Jones) remains with little alteration.  The \"highly inventive\" Edwardian Baroque design features a domed upper storey and a rotunda at the rear.  The façade has egg-and-dart moulding.  Brighton's central library used to be in the early-19th-century complex of buildings designed by William Porden, which later became Brighton Museum & Art Gallery.  Distinguished by excellent interior tiling, it had long been too small but was not replaced until Jubilee Library opened in February 2005.  Bennetts Associates and Lomax, Cassidy & Edwards designed the \"carefully wrought but nonetheless striking\" building—a highly glazed \"box\" with a prominent brise soleil and side elevations laid with dark blue tiles resembling mathematical tiles.  As the main element in the regeneration of North Laine, it has been called \"the most important public building constructed in Brighton since the Royal Pavilion\".  Portslade Library, built in 1964, was \"a typical Sixties creation\" with little regard for disabled access: it was built on a sloping site, and steps lead down from the road to the entrance.  Its Modernist design drew comparison locally with Sputnik.  Hangleton's library (opened in 1962, although Hove Borough Surveyor T.R. Humble's plans date from 1958) is integrated into a residential building, and the same applies at Coldean.  The Archadia firm of architects designed a ground-floor library of 290 sqft with six housing association flats above, in which the windows are emphasised by panels of pale brick.  The complex opened in June 2008, replacing the original library of 1975.  Moulsecoomb library was designed by Percy Billington in 1964; its large roof seems to \"float\" as it overhangs the small single-storey structure.  Other modern libraries include Patcham (1933; extended in 2003), Westdene (1964) and Woodingdean (1959), for which planning permission to demolish and rebuild on a larger scale to include a doctor's surgery was sought in 2012.  Rottingdean's library is housed in the former vicarage, Saltdean's is part of Saltdean Lido, and Hollingbury library occupies the former County Oak pub (1950) which was made up of two prefabricated buildings. The Duke of York's Picture House is the oldest cinema still operating in England, and was one of the world's first when it opened in September 1910.  It is next to the fire station at Preston Circus and occupies the site of a 19th-century brewery.  The architects were Clayton & Black.  There are some Classical and Palladian touches on the elaborately decorated façade, notably in the four-arch colonnade, but the overall style is Baroque.  The symmetrical front elevation has full-height rusticated pilasters on the two end bays, giving them the appearance of towers.  The \"monumental\" Savoy Cinema (1930, by William Glen) just behind the seafront was later converted into a casino.  The 3,000-capacity building has a tall and prominent entrance in a free Art Deco style with some Classical touches.  Its Sussex bricks were given a white glaze, and the building was nicknamed \"the white whale\".  \"The most impressive of Brighton's interwar cinemas\", though, was the Regent—designed in 1921 by Robert Atkinson and replaced in 1974 by a commercial development.  It was Classical-style inside and out (the interior was the work of Walpole Champneys) and had a winter garden just below the roof.  Its replacement was the Odeon Kingswest, converted in 1973 from the Russell Diplock Associates-designed Brighton Top Rank Centre of 1965.  The \"intrusively aggressive\" Brutalist structure has no windows and a low, \"emphatically horizontal\" appearance, but its jagged roofline of bronze-coated aluminium shapes give it prominence on its corner site.  \"Hove's most opulent cinema\" (and its only purpose-built one) was the Granada (1933) at Portland Road in the Aldrington area.  F.E. Bromige designed the Art Deco building, whose \"striking angular tower\" and corner site made it a landmark.  The Art Deco theme continued inside.  Closure came in 1974 and the building became a bingo hall.  It was demolished in 2012 in favour of a mixed-use development.  Another 1930s cinema that became a bingo hall in the 1970s and later closed is the Astoria Theatre on Gloucester Place in Brighton.  Demolition was authorised in 2012, although as it is a Grade II-listed building the final decision lay with national government.  Edward A. Stone designed the building in a French Art Deco style with a steel-framed interior clad in pale stone blocks decorated with faience. The Brighton Dome complex incorporates the Studio Theatre, Corn Exchange and a concert hall.  It has occupied its large corner site at the junction of Church Street and New Road in North Laine since William Porden built it for the Prince Regent in 1804–08.  Borough Surveyor Philip Lockwood converted the buildings into an entertainment complex in 1867–73, then the next Surveyor Francis May and theatre architect Robert Atkinson did more work in 1901–02 and 1934 respectively.  Atkinson's additions included the theatre, which faces New Road.  All of these schemes retained the Indian/Islamic architectural influences of Porden's work.  Atkinson gave the concert hall an Art Deco interior, while May's interior work was \"of an eclectic Neo-Jacobean kind\".  Also on New Road is the Theatre Royal, another early-19th-century building remodelled several times subsequently.  Charles J. Phipps extended the theatre in 1866, and Clayton & Black gave the building its present appearance in 1894.  Their work includes a colonnade of cast iron columns of the Corinthian order, an exterior of \"vivid red brick\" and a series of dome-topped turrets on the roofline.  The former Brighton Hippodrome in The Lanes was designed as an ice rink in 1897, but Frank Matcham converted it into a theatre and indoor circus in 1901–02.  Elaborate Rococo-style interior decoration and Royal Pavilion-style onion domes above the stage contrast with a low-key exterior with short towers at each end and a coloured glazed awning.  Elsewhere, the Brighton Little Theatre occupies a Classical-style stuccoed former Baptist chapel of 1833, and the Emporium Theatre uses the former London Road Methodist Church-a Free Renaissance-style building designed in 1894 by James Weir and extended and refaced in 1938. Good examples of interwar pub architecture include the Neo-English Renaissance Good Companions (1939) on Dyke Road at Seven Dials, designed by the Tamplins Brewery's in-house architect Arthur Packham and featuring characteristic 1930s patterned brickwork, the Ladies Mile Hotel (1935) on Patcham's Ladies Mile estate, and Clayton & Black's ostentatious rebuild of the King and Queen on Marlborough Place.  Their 1931 design borrowed freely from Tudor vernacular elements, both standard and decorative: it features jettying, massive timber lintels, corbels in the form of gargoyles, elaborate carvings and a portcullis.  Also in central Brighton, the J D Wetherspoon-owned Bright Helm pub occupies a former office building on a corner site in West Street.  H.E. Mendelssohn's \"bold design\" of 1938 is often attributed incorrectly to his better known contemporary Erich Mendelsohn, as the curved stone and glass exterior evokes that architect's favoured Expressionist idioms.  John Leopold Denman transformed the Freemasons Tavern in Brunswick Town from a Classical-style mid 19th-century pub, similar to its neighbours, into a spectacularly elaborate restaurant with an ornately moulded Art Deco interior and a blue and gold mosaic exterior with Masonic imagery and bronze fittings.  Many older pubs in the city retain decorative reminders of the breweries to which they were tied.  Examples of Tamplins Brewery pubs include the Jolly Brewer on Ditchling Road (mosaic panelling and etched windows), the Dyke Tavern in Prestonville (several etched windows, some with gold inlay), the Victory in The Lanes (a green tiled façade with tiled lettering and etched windows), the Seafield in Hove (lettered ironwork) and the former Free Butt on Phoenix Place (an inscribed stone panel), which was the brewery tap.  The Connaught in Hove (1880) has a large panel advertising the Longhurst Brewery.  Many Portsmouth & Brighton United Breweries pubs have green tiled façades and leadlights, including the Horse and Groom (Hanover), the Long Man of Wilmington (Patcham), the Montreal Arms (Carlton Hill) and the Heart and Hand (North Laine).  Among nightclubs and similar venues, the building at 11 Dyke Road (latterly the \"New Hero\" club) stands out because of its elaborate French/Flemish Gothic Revival architecture.  It was built as a school in 1867 to the design of local architect George Somers Leigh Clarke.  The \"freely inventive\" building has red and brown brickwork, a steep roof and a prominent crow-stepped gable. Hove's main leisure venue is the King Alfred Centre on the seafront; facilities include swimming pools, gymnasia, a solarium and indoor sports.  Hove Borough Surveyor T.H. Humble designed the first section (Hove Marina) in 1937, then the firm of Scott Brownrigg & Turner built a large extension in 1980–82.  This faced particular criticism for its \"dreadful\" architecture and lack of harmony with its seafront location.  Controversial plans for wholesale development to the design of Frank Gehry, featuring two skyscrapers, came to nothing.  In Brighton, next to St Paul's Church and (latterly) the Top Rank Centre stood a well-loved leisure attraction called SS Brighton.  Built in 1934 and demolished in 1965, it was successively a swimming pool, ice rink, general sports venue, variety theatre and conference venue.  The exterior was Art Deco with a cream-coloured tiled façade, and the interior mimicked the design of an ocean liner.  Only in 1990 was the site developed, with a \"big and worthless hotel\". SS Brighton was also known as the Brighton Sports Stadium; genuine football stadiums used by Brighton & Hove Albion F.C. were the Goldstone Ground in Hove, the Withdean Stadium at Withdean and since 2011 the Falmer Stadium.  The Goldstone Ground was laid out on the Stanford family's land in Hove in 1901 for Hove F.C. but was taken on by Albion in 1902.  A.E. Lewer designed a pavilion and dressing rooms, and the West Stand was extended to the design of A. & W. Elliott in 1920.  The South Stand was reused from an event at Preston Park.  The stands were later replaced and refurbished several times, and floodlights were installed in 1961.  The site was controversially sold in 1995 and is now occupied by the Goldstone Retail Park.  Four vast warehouse-style units dominate the site.  The Withdean Stadium, originally a tennis venue and later used for athletics, was used from 1999 until 2011: temporary stands were added for its new purpose.  In 2011, the long-planned Falmer Stadium, on the edge of the city near the University of Sussex, was opened.  It was designed and built in 2009–11 by KSS Design Group.  Two \"breathtaking\" tubular arches support the steel and glass structure: they have no columnar support and a substantial breadth.  The stadium is set low into the landscape and can be seen clearly from the surrounding downland.  The Brighton & Hove Greyhound Stadium opened in 1928 on market garden land in West Blatchington, despite considerable opposition from Hove residents.  In 1939 the grandstands were lengthened and the former kennels removed.  New owners the Coral Leisure Group added a sports centre building in 1976–78 and a restaurant in the 1980s.  One stand was taken down in 1991, the former tote building was converted into offices in 1993 and a major refurbishment took place in 2000. The seafront was originally dominated by defensive structures and batteries, including some designed by James Wyatt.  As the threat of foreign invasion lessened in the 19th century, Brighton and Hove's seafront was redeveloped with pleasure and recreation as its focus, and from the 1860s it represented \"the idée fixe of how [a seafront] should look\".  Bandstands, elaborately roofed kiosks, shelters with decorative awnings, pale green railings and tall, ornate lamp-posts are found regularly along the whole seafront; most structures date from the late 19th century and many are Grade II-listed. The West Pier (1863–66 by Eugenius Birch), dedicated entirely to leisure and promenading, was \"one of the most important piers ever built\"—but after its closure in 1975 it decayed, caught fire twice and is now a rusting hulk stranded in the sea.  Many of its features were innovative, from the screw pile foundations developed by Alexander Mitchell to the Royal Pavilion-inspired Orientalist kiosks and other buildings which defined how seaside architecture should be.  Further dome-topped entertainment venues were added in 1893 and 1916; the first of these was built because a new rival had appeared closer to the centre of Brighton.  Between 1891 and 1901, £137,000 was spent on the Palace Pier.  It was built by Arthur Mayoh to a design by R. St George Moore, and many additions were subsequently made—starting with an elaborate Winter Garden (now the Palace of Fun) by Clayton & Black in 1910–11.  A funfair was built at the seaward end, 1760 ft from land, in 1938.  Domes, elaborate kiosks and ornate columns characterise the pier. Birch was also responsible for Brighton Aquarium (now the Sea Life Centre) in 1872.  The 21-bay double-aisled interior remains as built, but of his High Victorian Gothic-style work on the exterior only an \"attention-seeking clock tower\" survives, because the building was revamped in 1927–29 by the Borough Surveyor David Edwards.  He rebuilt it in pale artificial stone in the Louis XVI Neoclassical style.  Also in 1872, the long, straight Madeira Drive—which runs at sea level below the East Cliff—was greatly extended.  Borough Surveyor Philip Lockwood designed a \"superb\" two-storey arcaded promenade alongside the cliff; it includes a pagoda-roofed lift to Marine Parade.  Work took place in 1889–97, and Madeira Drive was extended further to Black Rock in 1905. Brighton Marina at Black Rock dates from 1971–76 and has little architectural interest: an \"insipid neo-Regency\" pastiche style was used for many of the residential buildings, and the wide range of commercial premises are dominated by a vast supermarket.  Module 2 Architects drew up a masterplan for these buildings in 1985.  Additional commercial development called \"The Waterfront\" (1999–2000 by Design Collective) pays no homage to existing architectural styles but has a \"distinctive arched roofline\".  The Marina faced opposition when it was proposed, and a proposed development consisting of a 28-storey tower block and hundreds of other homes—first agreed in 2007 and signed off again in 2013—continues to cause controversy. The esplanade at Hove is well known for its brightly coloured timber beach huts.  The first were installed in around 1930, 290 were in place by 1936 and there are now several hundred. Brighton railway station, a Grade II*-listed structure, was built in two parts.  Most of David Mocatta's stuccoed Italianate building of 1841 survives—albeit hidden by H.E. Wallis's extensions of 1882–83.  He added an elaborate iron porte-cochère over the forecourt and an impressive curved train shed, 21 bays and 597 ft long, at the rear.  Its glazed three-span roof is supported on octagonal fluted columns.  F.D. Bannister, the chief architect of the London, Brighton and South Coast Railway (LBSCR), made other alterations at the same time, such as removing an entrance colonnade designed by Mocatta.  The modern entrance has round-arched windows and doorways which recall its design.  The roof was comprehensively restored in 1999–2000.  Elsewhere in the city, the stations at Hove (original building), Kemp Town (demolished), London Road and Portslade were built to a common design in the 1850s–1870s.  The \"stately\" two-storey buildings are Italianate, reminiscent of a Tuscan villa, and have symmetrical layouts.  London Road station, by W. Sawyer in 1877, also has a wide staircase leading up to its entrance.  Moulsecoomb, newly built in 1980, was designed by the Chief Architect's Department of the Southern Region of British Railways.  Intended to be difficult to vandalise, it has two \"well-detailed\" Swiss chalet-style wooden and tiled buildings linked by a footbridge.  Preston Park's platform-level buildings were replaced in 1974 by flat-roofed timber and glass structures, although the yellow-brick street-level entrance survives.  Aldrington has basic shelters emphasising \"utility rather than elegance\". The Grade II*-listed London Road viaduct (1846) by John Urpeth Rastrick used 10 million yellow and red bricks, spectacularly spanned the undeveloped valley until terraced houses crowded round it, and made it possible for the LBSCR to reach Lewes and Newhaven.  \"The Builder\" of 1847 proclaimed Brighton to be \"immensely improved\" by the \"exceedingly striking\" structure.  A cornice and balustrade runs along its 1200 ft length.  Similar but smaller viaducts crossed Lewes Road (540 ft and 28 arches; demolished in stages in 1976 and 1983) and Hartington Road (three arches; demolished in 1973) as part of the Kemp Town branch line.  Further up Lewes Road, near Moulsecoomb, another Rastrick-designed viaduct of 1846 spans the dual carriageway at an acute angle.  It is built of blue brick and has three segmental-arched openings.  A concrete brace was inserted in one after wartime bomb damage.  Two more viaducts, both Grade II-listed and designed by Rastrick, cross New England Road.  The earlier, western viaduct (1839–41) carries the main line and was designed as a triumphal arch in stone and yellow brick.  It was given full Masonic honours when built.  A cast-iron arched bridge of 1851–54, cast at the nearby Regent Foundry, carried the now removed line to the goods yard and locomotive works.  It consists of four parallel ribs forming an arch with open spandrels.  There is a latticework parapet of iron and stone corbels. Trams (from 1901) and trolleybuses used to run in Brighton.  The Lewes Road Bus Garage was originally the Brighton Corporation Tramways depot; it retains windows etched with this name.  Wooden tram shelters survive on Dyke Road, Ditchling Road and Queen's Park Road.  They have been turned into bus shelters, and the same has happened in Old Steine with a series of trolleybus shelters designed in 1939 by Borough Surveyor David Edwards.  The cream-coloured structures have curved windows and flat roofs with similarly curved ends which oversail the shelter itself.  Their style is Streamline Moderne. The city has an array of free-standing clock towers in various styles.  The landmark Jubilee Clock Tower in the city centre has been called Brighton's \"second best known symbol\" after the Royal Pavilion; Preston Park, Queen's Park and Blaker's Park each have one; and a fifth was erected in the 1930s in Patcham to publicise the suburb's Ladies Mile Estate.  All except the Blaker's Park and Patcham clock towers are Grade II-listed.  John Johnson's design for the Jubilee Clock Tower of 1888 combined the Classical style with Baroque motifs and some High Gothic elements.  Pink granite, a copper fishscale dome, Corinthian columns and mosaic portraits of Royal Family members combine to give a \"supremely confident and showy\" design.  The tower has withstood fierce criticism and calls for its demolition, and is now a widely appreciated landmark.  Francis May's \"pompous\" clock tower, built in the newly laid out Preston Park in 1891–92, also combined some Classical and Gothic elements—this time using terracotta, pale brick and stone—but its style is closest in spirit to Neo-Flemish Renaissance.  A copper dome with a weather-vane tops the four-stage tower.  A London architect, Llewellyn Williams, won the commission for the Queen's Park clock tower in 1915; his three-stage design, on high ground, incorporates Portland stone (partly rusticated) and red brick, and also has a copper roof.  Blaker's Park, northeast of Preston Park, was laid out in 1893 when Sir John Blaker (later 1st Baron of Brighton) donated land.  He also paid £1,000 towards the construction in 1896 of a 50 ft red-brick and iron clock tower with a pale green exterior.  It is topped by a cupola with a dolphin-shaped weather-vane, and bears Blaker's monogram.  Patcham's clock tower, built of pale stone in an International/Modernist style, stands on a green amidst 1930s housing and forms an important landmark. Buildings have been lost to fire, damage or demolition since the urban area's earliest days, and the frequent replacement of buildings (even those with architectural merit) by Victorian-era speculators was particularly common along the seafront.  After World War II, Brighton's seaside resort function declined, demand for housing rose and it became an important regional commercial centre.  Pressure for redevelopment and the prevailing attitudes towards pre-20th-century architecture resulted in widespread demolition; many of the new buildings were architecturally unsuccessful because their scale, build quality and relationship with their surroundings were poor.  In other cases, large sites stayed vacant for decades pending redevelopment.  The city faces unusually severe geographical constraints—it lies between the English Channel and the South Downs (an Area of Outstanding Natural Beauty), and has continuous urban development to the east and west—and intense pressure for redevelopment continues.  Nevertheless, many buildings have also been saved—not least the Royal Pavilion, which was bought by the local authorities when Queen Victoria moved out and which faced another threat in the 1930s. National conservation groups such as The Victorian Society and The Georgian Group are active in the city, and the Regency Society was founded in 1945 to conserve Brighton's architectural heritage in a direct response to Herbert Carden's proposals for wholesale reconstruction.  Residents' groups such as the Regency Square Area Society undertake similar work at a local level.  The Victorian Society and The Georgian Group wrote a joint report in 1990 examining postwar developments in central Brighton in the context of the older surroundings.  It observed that the growth of Brighton as a commercial centre since World War II had damaged its character: \"grossly inappropriate commercial development\" was starting to dominate the traditional seaside resort architecture characterised by the Regency terraces and squares, the piers and the Royal Pavilion. The Royal Suspension Chain Pier (1822–23, by Captain Samuel Brown rn) became Brighton's first \"effective focal point\" after it became a fashionable seaside resort, but demolition was already under consideration by the time it was destroyed by a storm in 1896.  Only some oak foundations remain, and these are only visible at low tides.  Brown's 350 yd iron structure had Egyptian Revival towers at the landward end, and the landing stage was of Purbeck stone. Hove's original manor house was pulled down in 1936, despite its last owner offering it to the local council for less than its market value.  John Vallance built the Georgian-style L-plan house in the late 18th century.  Features included a curved porch on the inside of the \"L\", a cupola-style bellcote and a Chinese Chippendale staircase inside, and some of its flint and stonework may have come from an ancient chapel nearby.  Other historic Hove buildings lost in the 1930s include the Classical-style Well House at the chalybeate spring in St Ann's Well Gardens—the Ionic-columned, colonnade-fronted structure decayed as the spring ran dry, and was demolished in 1935—and the mid 18th-century Wick House.  This was owned by several important figures in local history, such as landowner Thomas Scutt, Rev. Edward Everard (associated with Brunswick Town and St Andrew's Church at Waterloo Street) and Sir Isaac Goldsmid, 1st Baronet.  Along with the neighbouring Wick Hall, designed and built between 1833 and 1840 by Decimus Burton, it was demolished in 1935 to make way for the Furze Hill mansion flats.  Burton's three-storey Wick Hall was Classical in style, with a prominent cornice, a parapet with ornate stone urns, and on the garden-facing elevation a curved bay faced with a series of Ionic columns.  Collectively, these four buildings were \"Hove's oldest and most important houses\". Postwar demolition and redevelopment has been extensive in places.  An especially infamous incident occurred in 1971, when Stroud and Mew's \"Regency Gothic\" Central National School in North Laine was knocked down hours before its listed status was granted: the letter was apparently delayed by a postal strike.  The building dated from 1830 and was founded by Vicar of Brighton Henry Michell Wagner.  Another school, the Brighton Asylum for the Blind on Eastern Road (designed by George Somers Clarke, architect of the similarly flamboyant Swan Downer School on Dyke Road) was \"tragically demolished\" thirteen years earlier.  Built in 1860–61, it was a precise and richly decorated interpretation of the Venetian Gothic style.  The Bedford Hotel, Thomas Cooper's \"distinguished\" Classical-style seafront hotel of 1829, was dominated by a series of Ionic columns.  Once Brighton's highest-class hotel, its future was undecided and redevelopment was under consideration when it burnt down in 1964.  The remains were quickly demolished and replaced by Sussex Heights, a white 24-storey, 334 ft Brutalist style building by Richard Seifert completed in 1968 and nowadays itself a Grade II listed building.  A different approach has been used more recently in some cases: historic and architecturally interesting façades have been retained while the rest of the site has been demolished and redeveloped.  Examples of this are the former Lewes Road United Reformed Church, whose façade now hides flats, and the Brighton Co-operative store on London Road.  Architects Bethell and Swannell designed the four-storey building, whose wide frontage is dominated by fluted columns of the Doric order.  In 2013 all but the façade was demolished in favour of student housing. Road schemes have long been a source of demolition and redevelopment: as early as 1902, part of the historic Brighton Brewery was removed to remove a notorious bottleneck (known locally as \"The Bunion\") on Church Road in Hove.  Large-scale projects then threatened several parts of central Brighton between the 1960s and 1990s, but all were abandoned.  A 1973 report by town planners Hugh Wilson and Lewis Womersley, which recommended large-scale demolition in North Laine in favour of a flyover and car park, was rejected.  The idea re-emerged in the late 1980s as the \"Breeze into Brighton\" Preston Circus Relief Road scheme, one of many ideas for the vacant Brighton Locomotive Works site now occupied by the New England Quarter; this would have replaced several buildings of historic interest on York Place and Cheapside, driven a trunk road through hundreds of houses and commercial buildings and sliced a corner off the listed Bedford Square on the seafront. In England, a building or structure is defined as \"listed\" when it is placed on a statutory register of buildings of \"special architectural or historic interest\" by the Secretary of State for Culture, Media and Sport, a Government department, in accordance with the Planning (Listed Buildings and Conservation Areas) Act 1990.  English Heritage, a non-departmental public body, acts as an agency of this department to administer the process and advise the department on relevant issues.  As of February 2001, Brighton and Hove had 24 Grade I-listed buildings, 70 with a status of Grade II* and 1,124 Grade II-listed buildings.  Brighton and Hove City Council issues periodic summarised updates of the city's listed building stock; the latest document was published in October 2013. Grade I, the highest status, indicates that a building is of \"exceptional interest\" and greater than national importance.  Grade II* is used for \"particularly important buildings of more than special interest\"; and Grade II, the lowest designation, is used for \"nationally important buildings of special interest\".  All three grades of listed status offer some protection against changes which would affect the structure's character, from interior restoration to demolition.  Proposed alterations require consent from the council, which set out its position in a document published in 1981: The fact that a building is listed does not mean that it will be preserved intact in all circumstances, but it does mean that demolition will not be allowed unless the case for it has been fully examined.  Alterations must preserve the character of the building as far as possible.  Listed Building Consent must be obtained from the council for any proposal to demolish or materially alter a listed building.  Failure to do so can result in an unlimited fine, 12 months' imprisonment, or both. Buildings listed at Grade I include the Royal Pavilion, Stanmer House, several churches, the wrecked West Pier, the main building at the University of Sussex and the principal parts of the Kemp Town and Brunswick estates.  Several other 19th-century residential developments have Grade II* status: among them are Royal, Park and Adelaide Crescents, Regency Square and Oriental Place.  Many more churches also have this grading.  Grade II-listed buildings and structures are varied: items of street furniture (such as parish boundary markers and lamp-posts) have been listed, as have dovecots, gazebos and chimneys; hundreds of houses and cottages, either individually or as part of terraces, are included; and churches, schools and other public buildings (such as Brighton Town Hall, Portslade railway station and many pubs) have also been given Grade II status. Listed buildings have occasionally been lost to fire or demolition, and are not always delisted (officially removed from the schedule of listed buildings).  The West Pier retains Grade I listed status despite its ruined, inaccessible condition; and permission to demolish a Grade II-listed house at 128 King's Road near Regency Square was granted in 2002 after it was damaged by fire.  Holy Trinity Church in Hove, declared redundant in 2010, has been threatened with demolition since 2008.  Elsewhere, in July 2010 the council announced they would move a Grade II-listed shelter on the seafront by 3 ft to reduce the danger to cyclists on an adjacent cycle lane. Since around 1990, the various councils (and later subsequently the city council) have surveyed the structural condition of all listed buildings and have provided funding \"to encourage the preservation of the city's historic building stock\", covering repairs to listed and other historic buildings, replacement of missing or damaged architectural or decorative features, and assistance to return at-risk buildings to suitable use.  As early as 2003, though, the city council reported that a change in the way grants were structured meant that financial help for specific buildings may decline in favour of spending money on enhancements to wider areas. The city of Brighton and Hove has 34 conservation areas, which are defined by Sections 69 and 70 of the Planning (Listed Buildings and Conservation Areas) Act 1990 as \"principally urban areas of special architectural or historic interest, the character or appearance of which it is desirable to preserve or enhance\".  About 18% of the urban area is covered by this designation.  Conservation areas vary in size from the 316.29 acre around Stanmer to the 1.43 acre Benfield Barn area.\n\nTechnological and industrial history of 20th-century Canada The technological and industrial history of Canada encompasses the country's development in the areas of transportation, communication, energy, materials, public works, public services (health care), domestic/consumer and defence technologies. The terms chosen for the \"age\" described below are both literal and metaphorical.  They describe the technology that dominated the period of time in question but are also representative of a large number of other technologies introduced during the same period.  Also of note is the fact that the period of diffusion of a technology can begin modestly and can extend well beyond the \"age\" of its introduction.  To maintain continuity, the treatment of its diffusion is dealt with in the context of its dominant \"age\". Technology is a major cultural determinant, no less important in shaping human lives than philosophy, religion, social organization, or political systems.  In the broadest sense, these forces are also aspects of technology.  The French sociologist Jacques Ellul defined \"la technique\" as the totality of all rational methods in every field of human activity so that, for example, education, law, sports, propaganda, and the social sciences are all technologies in that sense.  At the other end of the scale, common parlance limits the term's meaning to specific industrial arts. Metal mining also became significant industry during this period.  The International Nickel Company (Inco) was established in 1902 through the fusion of two companies.  A refinery using the Orford process was built in Port Colborne, Ontario in 1918 and then moved to Copper Cliff, Ontario, where that technique was replaced by the matte flotation process in 1948.  Hard rock gold mining became practical in 1887, with the development of the potassium cyanidation process, by Scott MacArthur, which was used to separate the gold from the ore.  This technique was first used in Canada at the Mikado Mine in the Lake-of-the-Woods Region again made accessible by the CPR.  The CPR also provided access to the B.C. interior, where lead, copper, silver and gold ores had been discovered in the Rossland area in 1891.  The ores were transported to Trail, B.C., where they were roasted.  After CPR built the Crowsnest Pass it purchased the Trail roasting facility and in 1899 built a blast furnace to smelt lead ore.  In 1902 the first electrolytic lead refining plant using the Betts Cell Process began operation in Trail.  The Consolidated Mining and Smelting Company of Canada Ltd. was founded as a CPR subsidiary and began to develop the Sullivan Mine with its lead, zinc and silver ores, in Kimberley in 1909. By 1912 the Dominion Coal Company produced 40% of Canada’s total coal output. In 1904 a company in Bowmanville, Ontario, began Canada’s first powdered milk production operation.  The large-scale home delivery of milk began in Toronto, Ottawa and Montreal in 1900. Railway and locomotive construction in the latter 19th century created a huge demand for steel.  The Bessemer furnace at the Algom steel mill in Sault Ste. Marie, Ontario went into operation in 1902.  The Montreal Rolling Mills Co, The Hamilton Steel and Iron Company, the Canada Screw Company, the Canada Bolt and Nut Company, and the Dominion Wire Manufacturing Company were consolidated in 1910 to form The Steel Company of Canada headquartered in Toronto.  With mills located in Hamilton and other cities, it was the largest producer of steel in Canada for most of the century.  Its competitor, the Dominion Steel Castings Company Limited founded in 1912, renamed the Dominion Foundries and Steel Company in 1917 and Dofasco in 1980, had its Hamilton facilities located next to those of Stelco. At the turn of the 20th century, a number of asbestos-related health concerns were identified, and its use was generally discontinued by the late 20th century. The modern version of plywood was invented in the US in 1905 in Portland, Oregon.  In 1913, the Fraser Mills in New Westminster, British Columbia, produced the first Canadian plywood, primarily from Douglas fir.  This new material eventually found use in a wide variety of structures, including auto running boards, panelling, sub-floors, roof sheathing, wall sheathing, shipping crates and, during World War II, the manufacturing of aircraft and small ships. The pulp and paper industry also developed during these years.  The closely related sulphate pulp process was introduced in Canada in 1907, when the Brompton Pulp & Paper Company began operation in East Angus, Quebec.  This process dominates the industry to this day.  The pulp slurry was fed in a continuous stream into a paper-making machine that flattened, pressed and dried it into newsprint on huge rolls many metres wide and containing thousands of meters of paper. Business and public administration was improved and simplified with the introduction of the typewriter, which acquired a familiar standardized form by about 1910, which features the \"qwerty\" keyboard, the typebar, ribbon, cylinder and carriage return lever.  Popular models in Canada were manufactured by the US Remington and Underwood companies, among others.  The introduction of the mechanical desk calculator complemented that of the typewriter.  Most machines used in Canada were manufactured in the US by companies such as Friden, Monroe, and SCM/Marchant.  The Gestetner copy machine, which used the stencil technique to reproduce copies of documents, was invented in England in 1881 by David Gestetner and quickly became popular in offices around the world, including those in Canada. Notable works of civil engineering realized the completion of: the New Westminster Bridge, Vancouver 1904, the Lethbridge Viaduct, Lethbridge, Alberta, 1909, the Spiral Tunnels, Hector to Field BC, 1909, the St. Andrew's Lock and Dam, Lockport, Manitoba, 1910, the Brooks Aqueduct, Brooks, Albert, 1914, the Quebec Bridge, Ste-Foy, Quebec, 1916, the Connaught Tunnel, Rogers Pass, BC, 1916, the Ogden Point Breakwater and Docks, Victoria, British Columbia, 1917, the Prince Edward Viaduct, Toronto, Ontario, 1919, the Shoal Lake Aqueduct, Winnipeg, Manitoba, 1919 and the Trent-Severn Waterway, Ontario, 1920. In the 1930s diesel-powered excavation shovels replaced steam shovels for the excavation of railway right-of-ways and the digging of basements and foundations for skyscrapers and domestic housing, in the late 19th century. It was the age of the skyscraper and the race to build the tallest structure in the British Empire set off a competition among cities across Canada.  Successive record holders included the Traders Bank of Canada, 15 floors, Yonge St, Toronto, 1905, the Dominion Building, 13 floors, Vancouver, 1910, World (Sun) Tower, 17 floors, Vancouver, 1912, the Canadian Pacific Building, 16 floors, Toronto, 1913, the Royal Bank, 20 floors, Toronto, 1915, the Royal Bank, Montreal, 1928, the Royal York Hotel, Toronto, 1929 and the Canadian Imperial Bank of Commerce, Toronto, in 1931. The construction of skyscrapers, grand hotels and other large buildings led to the development of central heating, an essential feature in Canada's cold climate.  In the 20th century such systems were used to provide heat to small communities such as university campuses, northern industrial towns or military bases.  Smaller systems were used in private homes.  Another technique, the convection method, was introduced to domestic dwellings at this time.  A metal furnace in the basement, using wood or coal as fuel, would heat air in a plenum which would rise by convection through a series of metal ducts into the rooms of the house above.  When the air cooled it would fall to the floor and return to the plenum through another series of metal return ducts.  In later years an electric fan was used to force the hot air from the plenum through the ducts. The introduction of the flush toilet in the US and Canada in the 1880s created a market that inspired the invention of rolled toilet paper.  The product was first produced in the US by the Albany Perforated Wrapping Paper Company in 1877.  The US Scott Paper Company began manufacturing toilette paper in 1902 and by 1925 Scott Paper was the largest manufacturer of toilette paper in the world.  As early as 1926 the Purex brand had been established in Canada and with the arrival of Scott Paper Canada in 1927 the White Swan brand was introduced. The introduction of medical x-rays during this period dramatically improved medical diagnostics.  Discovered by Roentgen in Germany in 1895, x-ray units were in operation at the Toronto and Montreal General Hospitals by the turn of the century.  The sphygmomanometer or blood pressure meter, that familiar device employing a cuff placed around the patients arm, found its way into the office of most Canadian doctors in the early 20th century.  The spread of bovine tuberculosis, a crippling childhood disease, was curbed through the introduction of pasteurized milk in Montreal and Toronto at the turn of the century.  This practice was soon followed by the dairy industry across Canada. Bayer began marketing the wonder drug of the age, Aspirin, in 1899.  It was an instant success and quickly became popular in Canada.  Originally sold as a powder, the tablet was introduced in 1914.  A very important step in the mass production of medical products was taken that same year when Dr. John Fitzgerald founded an institution that would be named the Connaught Laboratories in 1917, at the University of Toronto.  Initially the laboratories produced vaccines and antitoxins for smallpox, tetanus, diphtheria and rabies.  In 1922 after the Nobel Prize winning work on Dr. Banting and Dr. Best the facility began to manufacture insulin. In 1914 Dr. John Fitzgerald established laboratories in Toronto to produce vaccines for smallpox, rabies, diphtheria and tetanus.  The facility was named the Connaught laboratories in 1917 in honour of Prince Albert, the Duke of Connaught the recently retired Governor General.  Beginning in 1922 the laboratories began to mass-produce the newly discovered hormone insulin. The chlorination of municipal drinking water, a technique known to kill bacteria and thus make the water safer for human consumption, was introduced in Toronto in 1910.  It became widely used across Canada in the years that followed. The 13- and 18-pound muzzle-loading gun field gun with modern recoil and sighting systems were acquired at the turn of the century.  A notable acquisition was the first breech-loading gun, in Canadian use, the 13-pound quick-firing (Q.F.) and 18-pound Q.F. firing shrapnel and high-explosive rounds, in 1905.  The Royal Canadian Navy founded in 1910, took possession of its first ships, two tired steel-hulled former Royal Navy cruisers, the \"Rainbow\", in 1910, stationed at Esquimalt on the west coast and the \"Niobe\" at Halifax on the east coast. During the post-World War I era, a plethora of technologies were introduced, including the car, air service, air navigation, paved roads, radio, the telephone, refrigeration, wonder drugs and powered farming, mining and forestry equipment. The Ford Motor Company of Canada, founded in Windsor, Ontario in 1904, was the first major company to introduce the automobile to Canada.  It manufactured cars in that city and was the first company to use the assembly line manufacturing technique in Canada.  Facilities were established in the McGregor wagon factory in Walkerville (now part of Windsor), where the first vehicle off the line was a Model C. Production of the Model T was introduced in 1909 and by 1913 the company was manufacturing motors, the first internal combustion engines built in Canada, at the Windsor plant.  A number of different types, all based on US designs, were manufactured including, the Ford Model-A, Ford Model-C, Ford Model-K, Ford Model-N and Ford Model-T.  During WWII, the Canadian Military Pattern Truck, was built there.  Following the war, the Ford Meteor was assembled until production moved to the new Ford plant in Oakville in 1953, where production has continued until this day. In 1918 the McLaughlin Motor Company, Ltd. of Oshawa, Ontario and the Chevrolet Motor Company of Canada Ltd. merged to form General Motors of Canada and became a subsidiary of the US-owned General Motors Corporation.  The company manufactured Buicks, Oldsmobiles and Oaklands on its assembly line in Oshawa. Chrysler Canada, established in Windsor, began vehicle assembly in that city in 1936.  Studebaker Canada Ltd. manufactured cars and trucks at a plant in Hamilton, Ontario from 1947 to 1966. The automobile was a hit with Canadians.  In 1904 there were 535 cars in Ontario, by 1913 there were 50,000 in Canada, by 1916, 123,000, by 1922, 513,000 and by 1930, 1,076,000.  Of note was Thomas Wilby's Trans-Canada road trip, the first by automobile across Canada, from Halifax to Victoria, in 1912, on a series of highways that became known as the All Red Route.  As the car gained in popularity local automobile clubs were founded.  In 1913 nine of these clubs from across the country got together to form the Canadian Automobile Association. Cars required gasoline, and the first service station in Canada was built in Vancouver on Smythe Street in 1907.  Most early stations were informal curb-side affairs, and it was not until the twenties that the filling station as we know it began to appear, with Imperial Oil building architect-designed stations for its customers.  By 1928 Imperial had evolved three standard filling station designs for different locations: business district, urban residential and small town/leased property.  In the 1920s, gasoline itself was modified by the addition of tetraethyllead to reduce premature detonation of the gas-air mixture in the cylinder, commonly described as knock, in internal combustion engines.  Both health and environmental problems would later become associated with leaded gasoline. The popularity of the car also had a dramatic impact on urban infrastructure and roads in particular.  The dirt, gravel, tar and occasionally cobblestone that characterized most city roads was inadequate for the automobile and towns and cities and provinces across Canada began paving projects creating roads of asphalt and concrete that were more suitable.  The traffic light was also introduced to help regulate the congestion that began to arise in the twenties especially in larger cities like Toronto, Montreal and Vancouver.  The first in Canada was installed at the corner of Bloor St. and Yonge St. in Toronto in 1925.  The operation of cars on the newly paved roads in a snowy climate necessitated the use of a technology for keeping roads clear for cars in winter.  Municipalities and provinces acquired snowplows, fleets of trucks with steel blades attached to the front bumper to clear city and provincial roads.  The use of crushed rock salt for melting snow and ice on roads was also introduced during this period.  The technique was effective but unfortunately proved to be very corrosive to steel and concrete.  This had serious consequences for the undercarriage of the steel vehicles that used the roads in winter as well as the roads themselves along with bridges and parking lots. The popularity of the car in urban areas also lead to the introduction of the parking meter in the centre of most urban areas.  Invented in the US in 1935 it progressively found its way onto city streets across Canada in the years that followed. The car began to compete with the streetcar in the thirties and forties and many cities reduced or abandoned this service.  New suburbs were built without streetcar lines and urban diesel powered buses were used to provide public transport.  Only a handful of cities continued to maintain streetcar service into the fifties and beyond, most notably Toronto which to this day has a very elaborate public streetcar network. The auto-craze gave rise to a booming do-it-yourself car maintenance and repair movement with businesses specializing in car parts and tools becoming popular.  One of the notable firm in this field, the familiar, Canadian Tire, began operations in Toronto in 1922 and has become one of Canada's largest retailers. Long distance travel by aircraft became increasingly important and practical in the postwar years.  Taking off in a Vickers Vimy IV bomber from Lester's Field in St. Johns, Newfoundland on 14 June 1919, John Alcock and Arthur Whitten Brown, (Alcock and Brown) made the first trans-Atlantic flight, crash landing in Ireland 16 hours later.  The first cross-Canada flight began in Halifax on 7 October 1920 and ended in Vancouver ten days later. In the twenties and thirties the Canadian north was developed with the help of hundreds of small float equipped \"bush planes\" used to fly men and supplies to mining, forestry, trapping and fishing camps.  The first commercial air passenger flight in Canada was made in 1920, when two bush pilots flew a fur buyer from Winnipeg to The Pas, Manitoba.  National passenger air service was introduced by Trans-Canada Airlines beginning in 1937 and Canadian Pacific Airlines starting in 1942. Of note was the attempt by Britain to establish an airship service between that country and Canada and a related test flight by the British built dirigible the R-100 was made in July 1930.  After a successful crossing of the Atlantic the giant craft moored at a mast especially constructed for that purpose at St. Hubert near Montreal.  The ship flew on to Toronto before finally returning to Britain.  However, technical problems with the craft prevented further flights and the idea of a Trans-Atlantic lighter-than-air passenger service was abandoned. To facilitate the development of a national aviation service the Government of Canada created a kind of national highway in the sky called the Trans-Canada Airway consisting of airports, radio and weather services and lighting for night flying, at various locations across Canada.  Construction started in 1929 but was slowed by the depression.  The western leg from Vancouver to Winnipeg was completed in 1938.  The section from Winnipeg to Toronto and Montreal was inaugurated in 1939 and the extensions to Moncton, Halifax and St. John's completed in 1940, 1941 and 1942 respectively. In 1901, Guglielmo Marconi sent radio signals across the Atlantic Ocean.  He established a machine to produce electromagnetic waves at Cornwall in England and a machine to detect these waves at Signal Hill in St. John's Newfoundland.  On 12 December 1901 he announced that he had received the transmission of waves sent by the transmitter in England at the station in St John’s. In Montreal, in 1920, XWA (CINW (AM)) became the first commercial AM radio broadcaster in the world.  Both the AM transmitter and receiver used analog technology.  The following year CKAC became the first French- language AM radio broadcaster in Canada.  State operated national radio broadcasting chains were established beginning in the late 1920s, including the CNR National Radio Network, 1927, the Canadian Radio Broadcasting Commission Radio Network, 1932 and the Canadian Broadcasting Corporation Radio Network, 1936.  Private independent AM broadcast operations sprouted like mushrooms in cities large and small across Canada during the thirties and forties.  Canadian Marconi Company (CMC Electronics) formed in Montreal in 1903 and Northern Electric, manufactured radios for home use, the first mass-produced electronic equipment in Canada.  The circuits of these devises were based on analog technology. The teleprinter became a popular technology with telegraph companies beginning in 1922.  When used with the telegraph system it permitted the automated printing of thousands of telegraph messages and became the backbone of the telegram service offered by the Canadian National Telegraph Company formed in 1920 and the Canadian Pacific Telegraph Company. The wirephoto, was introduced in the US by Associated Press in 1935.  This technology permitted the transmission of a photograph by use of telephone wires and became widely used by newspapers for reporting purposes.  The technology was quickly introduced to Canada by Canadian Press (1917), which provided the service to newspapers across the country.  Canadian Press also became the exclusive provider of Canadian wirephotos for Associated Press. The Canadian film industry experienced mixed success during the twenties and thirties.  Film maker Ernest Shipman produced five features between 1920 and 1923 before meeting with financial failure.  The successful Canadian-owned Allen Theatre chain attained an important place in the exhibition market before being taken over by Famous Players Canadian Corporation (Cineplex Entertainment) in 1923.  The technology of the talking cinema or \"talkies\" was introduced to Canada in 1927 by that company to take advantage of the arrival of talking films produced in Hollywood.  The first Canadian \"talkie\" was \"The Viking\", an adventure story about sealing off the coast of Newfoundland, produced in 1931. Associated Screen News of Canada in Montreal produced two notable newsreel series, \"Kinograms\" in the twenties and \"Canadian Cameo\" from 1932 to 1953.  The regular production of short films by the newly created Canadian Government Motion Picture Bureau began in the 1930s.  British law encouraging filmmaking in the Commonwealth led Hollywood to circumvent the spirit of the concept by establishing film production companies to make American films in Calgary, Toronto, Montreal and Victoria.  These companies produced a small number of features but closed operations when the British law was changed to exclude their films.  In 1941, Odeon Theatres of Canada opened a new cinema chain to compete with Famous Players. The making of documentary films grew tremendously during World War II with the creation of the National Film Board of Canada in 1939.  By 1945 it was one of the major film production studios in the world with a staff of nearly 800 and over 500 films to its credit, including the very popular \"The World in Action\" and \"Canada Carries On\" series of monthly propaganda films. Plastics were also introduced during these years.  In Toronto, Plastics Ltd. began to produce Bakelite soon after its invention in 1909.  Another firm, Canadian Electro Products of Shawinigan, Quebec, invented polyvinyl acetate which was used in copolymer resins and water-based paints.  The wartime production of nitrocellulose naturally led to the manufacture at Shawinigan in 1932, of transparent cellulose film used for packaging.  What is now called fibreglass was invented in the US in 1938 at Owens-Corning by Russell Games Slayter and introduced to Canada shortly thereafter. Aluminum also became popular.  In 1902, attracted by the availability of cheap hydro power, the Aluminum Company of America established a Canadian subsidiary, the Northern Aluminum Company (Alcan) at Shawinigan Falls, Quebec to produce that metal using the electrolysis technique.  Corporate changes led to the creation of the Aluminum Company of Canada (Alcan) in 1925 and in 1926 the company constructed a giant aluminum smelter at a place it named Arvida, Quebec.  Once again the site was chosen for the availability of cheap hydro electricity and the proximity of a deep-water port at Bagotville for large ships carrying bauxite or aluminum ore.  World War II accelerated the demand for aluminum, which was the principal material in aircraft production and the Arvida facility was greatly expanded.  In 1958 another huge Alcan smelter was built at Kitimat, British Columbia. The growth in popularity of the car also created a need for rubber for automobile tires.  Accelerated by the emergency of World War II, a substantial synthetic rubber production industry was established at Sarnia, Ontario in the early forties.  The oil refineries there provided a ready source of raw materials.  In particular, the Suspensiod crackers operated there by Imperial Oil produced large quantities of hydrocarbon gases.  These were used by a new Crown enterprise, Polymer Corporation created in 1942, and associated private companies, St. Clair Processing Corporation Ltd., Dow Chemical of Canada Ltd., and Canadian Synthetic Rubber Ltd., itself a subsidiary of four Canadian rubber companies, Dominion, Firestone, Goodyear and Goodrich, to produce both GR-S and butyl type synthetic rubber.  Initially production was destined for wartime use on military vehicles but in postwar years output was quickly redirected to civilian automobile production. The closely related synthetic textile industry appeared in the years just after the First War.  The production of artificial silk, more properly known as viscose rayon, made from bleached wood pulp, began in Cornwall, Ontario in 1925, in a factory built by Courtaulds (Canada).  A year later Celanese Canada began making acetate yarn in a new plant in Drummondville, Quebec.  DuPont Canada was the first to manufacture nylon yarn in Canada at its factory in Kingston, Ontario in 1942.  This secret material was initially used for parachutes but following the war was used to make nylon stockings. Asbestos has long been known for its fibrous and heat resistant properties.  With the rise of the automobile, asbestos became an important material, being used to make brakes.  The world's largest asbestos mine, the Jeffrey Mine in Asbestos, Quebec had its beginnings in the 1878 when a local farmer W. H. Jeffrey began to mine the substance there.  Original mining methods were primitive and involved blasting, the use of chisels to remove the mineral from the rock by hand and a crane powered by one horse.  By 1895, 2300 tons of asbestos were being removed from the open pit mine per year.  The mine was purchased by the Johns-Manville Company of the US in 1918 and has since become the largest asbestos mine in the world, over two kilometres in diameter and 350 metres deep.  The material was used for insulation in buildings and ships and, of course, for automobile brakes.  However, serious health problems, including lung cancer, have been associated with its mining and use, and in recent years mining activity there has diminished. With the rail building era coming to an end, the rise of the automotive industry in southern Ontario provided the Hamilton steel mills of the Steel Company of Canada and the Dominion Foundries and Steel Company with a new market.  Dofasco introduced the basic oxygen steelmaking at its mills in Hamilton in 1954.  In the latter part of the century, Algoma, in Sault Ste. Marie, built coke oven batteries and blast furnaces, while phasing out the open-hearth and Bessemer steel-making process in favour of the basic oxygen steel-making. The industrial production of bread became notable during these years.  At the beginning of the 20th century it is estimated that only about 8% of Canadian wives bought bread commercially.  However, the industrial production of bread grew impressively and by the 1960s, 95% of homemakers purchased bread commercially.  One bakery of note, The Canada Bread Company Limited, was founded in 1911 as the result of the amalgamation of five smaller companies.  Industrial bakeries such as this were characterized by the use of large machines for the mixing of dough, which was placed in pans on slow moving conveyor belts that transported them through giant ovens, where they were baked.  Large automated packaging machines wrapped the finished loaves at great speed.  Improvements in transportation and packaging technology throughout the decades allowed a shrinking number of bakeries to serve every larger markets.  In 1939 there were about 3200 commercial bakeries across the country but by 1973 the figure stood at 1700, while in 1981 there were 1400. Meat packing grew to become Canada's most important food processing industry during this period.  In Calgary, Alberta, in 1890, Pat Burns established P. Burns and Company, which became the largest meat processor in western Canada.  In Toronto in 1896 the innovative Harris Abbatoir was established to export chilled sides of beef to the British market.  The industry grew rapidly during the war, supplying meat to Canadian and British troops overseas.  However, it underwent a period of consolidation in the twenties due to a loss of markets.  This led to the merger of two major players, William Davies and the Harris Abattoir, to form Canada Packers in Toronto.  By 1930, \"The Big Three\", meat packers in Canada were Canada Packers, Swift Canadian and P.Burns and Company in Calgary, Alberta. The increasing popularity of the electric refrigerator in Canadian restaurants and homes made it practical for manufacturers to make available various frozen foods.  The first such offering, a frozen strawberry pack was produced in Montreal and Ottawa beginning in 1932 by Heeney Frosted Foods Ltd. Cold breakfast cereal became increasingly popular during these years.  Wheat and later corn flakes were developed in the US by the Kellogg brothers in 1894 and the Kellogg Company was formed in 1906.  In London, Ontario the Canadian Corn Company purchased the rights to manufacture and distribute Toasted Corn Flakes for Canadian distribution.  In 1924 the American Kellogg Company purchased the London operation and formed Kellogg Canada Inc.  Since that time the company has manufactured and distributed in Canada a wide variety of breakfast cereals including Corn Flakes, 1907, Bran Flakes, 1915, All Bran, 1916 and Rice Krispies, 1928. Although neither the tin can nor soups were remarkable in any way in the thirties, the combination of the two in the form of the well known Campbell’s soup was very popular.  The Campbell Soup Company introduced its soup products to Canada in 1930, making them at its factory in Toronto on the lake shore. Instant coffee was another tasty innovation introduced during these years.  The inventor and world leader in the manufacture of instant coffee, the Swiss-based Nestlé Company began operations in Canada with the production of canned condensed milk at its plant, The Maple Leaf Condensed Milk Company, in Chesterville, Ontario in 1918.  Head office research invented instant coffee and began selling it around the world including Canada, as Nescafe in 1938.  It became hugely popular with allied troops during World War II.  In 1952 the instant chocolate drink, Nestle Quik, was introduced to Canada. The sanitary napkin and Kleenex brand facial tissue were introduced in the 1920s.  Kimberly, Clark and Co. (Kimberly Clark) formed in the US in 1872, invented cellucotton in 1914.  It used this material as the basis for a sanitary napkin and marketed the product as Kotex beginning in 1920.  Kleenex, initially intended for the removal of face cream, was introduced in 1924.  In 1925 the company formed what would become, Canadian Cellucotton Products Limited, for the marketing of these and other products in Canada and internationally.  The first practical electric razor, the Sunbeam \"Shavemaster\" and the Remington \"Close Shaver\" made available in the US in 1937 and in Canada shortly thereafter. With a base of caustic soda, the world's first oven cleaner, Easy-Off, was invented in Regina in 1932 by Herbert McCool and manufactured in his home in that city until 1946, when production shifted to Iberville, Quebec.  The product has since become the most popular oven cleaner in the world. The grand hotel continued to make a mark with new structures, including the Bigwinn Inn, Muskoka, Ontario, 1920, the Jasper Park Lodge, Jasper, Alberta, 1922, the Hotel Newfoundland, St. John's, Newfoundland, 1926, the Hotel Saskatchewan, Regina, Saskatchewan, 1927, the Prince of Wales Hotel, Waterton Lakes National Park, Alberta, 1927, the Lord Nelson Hotel, Halifax, Nova Scotia, 1928, The Pines, Digby, Nova Scotia, 1929, the Royal York Hotel, Toronto, 1929, the Chateau Montebello, Montebello, Quebec, 1930, the Nova Scotian Hotel, Halifax, Nova Scotia, 1930, the Charlottetown Hotel, Charlottetown, P.E.I. and the Bessborough Hotel, Saskatoon, Saskatchewan, 1935. In 1875 in Montreal, a McGill student, J. Creighton, established the basic rules for hockey as we know it today.  The world's first facility dedicated to hockey, the Westmount Arena was built in Montreal in 1898 while the first industrial refrigeration equipment for making artificial ice in Canada was installed in 1911 by Frank and Lester Patrick for their new arenas in Vancouver and Victoria.  The Mutual Street Arena, with its artificial ice surface, was built in Toronto in 1912.  With the development of wide span roof structures the construction of large indoor ice rink stadiums became possible.  These two technologies were used to build the Montreal Forum, home of the legendary Montreal Canadiens hockey team, in Montreal in 1924 and Maple Leaf Gardens home of the Toronto Maple Leafs, in that city in 1931.  Baseball's facilities were upgraded with construction of the new Maple Leaf Stadium on Lake Shore Boulevard in Toronto in 1926 and the De Lormier Downs Stadium (Hector Racine Stadium), in Montreal in 1927.  Civic Stadium, now Ivor Wynne Stadium, was built in Hamilton, Ontario in 1930, to host the British Empire Games held there that year. The construction of the very large, Basilica of Sainte-Anne-de-Beaupré, near Quebec city was completed in 1926. There were also advances in the lighting of public, commercial and industrial buildings.  In 1938, after decades of development in the US and Europe, General Electric in the US, and shortly thereafter Westinghouse, began to sell the fluorescent lamp.  Because of its lower power consumption its use was quickly adopted for large-scale applications.  These lights were quickly made available to the Canadian market through the Canadian subsidiaries of these two companies. Notable engineering works of the period included the Second Narrows Bridge, Vancouver, 1925, the R.C. Harris Filtration Plant, Toronto, Ontario, 1926, the Peace Bridge, Fort Erie, Ontario, 1927, the Champlain Bridge (Ottawa), 1928, the Ocean Terminals, Halifax, Nova Scotia, 1928, Sea Island Airport (Vancouver International Airport), Vancouver, 1929, the Ambassador Bridge, Windsor-Detroit, 1929, the Windsor-Detroit Tunnel, 1930, the Broadway Bridge, Saskatoon, Saskatchewan, 1932, the Île d'Orléans Bridge, near Quebec City, 1934, the Thousand Islands Bridge, Ontario, 1937, the Pattullo Bridge, Vancouver, 1937, the Lion's Gate Bridge, Vancouver, British Columbia, 1938, Malton Airport (Toronto Pearson International Airport), Toronto, 1938, the Blue Water Bridge, Sarnia, Ontario, 1938, the Queen Elizabeth Way, Ontario, 1939, the Rainbow Bridge (Niagara Falls), 1941, Dorval Airport, (Montréal-Pierre Elliott Trudeau International Airport), Montreal, 1941 and the Alaska Highway, Dawson Creek, British Columbia, 1942. Canada’s first major roller coaster the Crystal Beach Cyclone was built at the Crystal Beach Amusement park in 1927.  It quickly gained a reputation for its wild and even violent ride and one passenger, Amos Wiedrich was killed in 1938 when he stood up to take off his coat while the coaster was in motion. The dump truck and bulldozer were introduced during these years for a variety of earth moving tasks including road building and canal construction.  The dump truck was invented in Saint John, New Brunswick in 1920 by Robert T. Mawhinney and the bulldozer was developed in the US in 1923.  Both quickly became popular worldwide. Medical treatment benefited from the introduction of the electrocardiograph, used to diagnose heart problems, in large hospitals in the late twenties.  There were also important innovations with respect to the treatment of epilepsy during this period.  In Montreal, Dr. Wilder Penfield, with a grant from the US Rockefeller Foundation, founded the Montreal Neurological Institute at the Royal Victoria Hospital (Montreal), in 1934 to study and treat epilepsy and other neurological diseases. The military suffered a huge decline in the 1920s and 1930s.  The Royal Canadian Air Force founded in 1924, was largely a bush and float plane operation.  Only in the 1930s did it acquire a modest combat capability with a handful of British Armstrong Whitworth Siskin fighters and a squadron of Hawker Hurricane fighters as the clouds of war grew menacing.  The Royal Canadian Navy, perpetually starved for equipment acquired its first custom-built ships, the destroyers HMCS Saguenay (D79) and HMCS Skeena (D59) on May 22, 1931.  In 1929 the army began to retire its horses and was issued four 6-wheeled Leyland tractors in 1929, to tow its 60-pound guns.  Four 3-inch 20-cwt.  anti-aircraft guns were taken on strength in 1937.  As a reflection of this intense and diverse engineering activity, the Canadian Council of Professional Engineers was established in 1936.  This organization was renamed Engineers Canada in 2007. Canada was involved in the wartime Manhattan Project to build an atomic bomb, including the Montreal Laboratory for nuclear research by scientists from Britain, and American contracts with Canadian firms Eldorado Gold Mines for mining and processing uranium and a heavy water plant built by Consolidated Mining and Smelting (CMS) at Trail, British Columbia. The years following World War II introduced even more innovations, including television, the transistor radio, synthetic fabrics, plastic, computers, super highways, shopping centres, atomic energy, nuclear weapons, transcontinental energy pipelines, long range electric transmission, transcontinental microwave networks, fast food, chemical fertilizer, insecticides, the birth control pill, jet aircraft, cable TV, colour TV, the instant replay, the audio cartridge and audio cassette, satellite communications and continental air defence systems. In the early 1980s Canadian Satellite Communications (Cancom) assembled a package of Canadian and American television channels which it offered to remote communities throughout the northern regions of Canada.  The signals were distributed by Anik satellite and made available to the local populace through cable.  By the later part of the decade several hundred communities were using this service. There was also technological innovation in the telephone system.  The first trans-Atlantic telephone cable, jointly owned by the Canadian Overseas Telecommunication Corporation, British Post Office and AT&T, was brought into service in 1956, paving the way for telephone calls from Canada to Britain and Europe.  An improved cable, CANTAT was installed in 1961.  A similar service on the west coast, COMPAC, the Commonwealth Pacific Cable System was inaugurated in 1963, linking Canada by undersea telephone cable with New Zealand and Australia.  CN/CP Telecommunications introduced the well known Telex service to Canada in 1956.  Direct distance dialing was initiated in Canada in 1958, beginning with customers in Toronto and on 1 July of that year the Trans-Canada Microwave system, known as the Trans-Canada Skyway, went into service.  The concept and operation of a dedicated emergency telephone number originated in Canada, where the City of Winnipeg established the world's first 9-1-1 service in 1959.  The service eventually spread and was offered continent wide. The Anik (satellite) series of communications satellites initially built by Hughes Aircraft and operated by Telesat Canada starting in 1972 formed the basis of the world's first domestic satellite communications service.  Telesat has launched a large number of satellites including, Anik A1 - 1972, Anik A2 - 1973, Anik A3 - 1975, Anik B - 1978, Anik D1 - 1982, Anik C3 - 1982, Anik C2 - 1983, Anik D2 - 1984, Anik C1 - 1985, Anik E2 - 1991, Anik E1 - 1991, MSAT - 1996, Nimiq 1 - 1999, Anik F1 - 2000, Nimiq 2 - 2002, Anik F2 - 2004, Anik F1R - 2005, and Anik F3 - 2007. \"Dataroute\", the world's first national digital data system was introduced by CN/CP in 1973. After considerable political turmoil Canada acquired nuclear weapons from the Americans under a \"dual key\" arrangement on 1 January 1963.  Genie air-to-air rockets armed with atomic warheads were based at RCAF Stations Comox, British Columbia, Bagotville, Quebec, and Chatham, New Brunswick, as the primary weapon for the newly acquired CF-101 interceptor.  The nuclear armed BOMARC (Boeing Michigan Air Research Corporation) anti-aircraft missile was based at RCAF Stations North Bay, Ontario, and Lamacaza, Quebec.  In Germany, as part of Canada's NATO commitment, nuclear free-fall bombs were acquired for the RCAF CF-104 strike fighter and the Canadian Army in Germany took possession of a battery of Honest John surface-to-surface battlefield rockets armed with nuclear warheads.  By 1984 all these atomic weapons had been returned to the United States. While there were no accidents involving nuclear weapons in Canadian hands, there were at least two involving USAF aircraft flying in Canadian airspace.  On 14 February 1950 a USAF B-36 heavy bomber, serial 44-92075, carrying one Mark 4 (Fat Man type) atomic bomb experienced multiple engine failures while flying south off the coast of British Columbia and jettisoned the bomb over Squally Channel.  The crew bailed out and the plane flew on autopilot for another 330 km before crashing on a mountainside in the Kispiox Valley.  In eastern Canada on 10 November 1950, a USAF B-50 heavy bomber, serial 46-038, flying from Goose Bay, Labrador, to the United States, experienced engine trouble and in accordance with standard operating procedures, jettisoned the Mark 4 atomic bomb it was carrying over the St. Lawrence River, near Rivière-du-Loup.  The bomb's own 2200 kg conventional explosives blew it apart before it hit the water.  The plane flew on to a base in the US. Computers were introduced in a variety of areas at this time.  The National Research Council Canada experimented with fire-control computers towards the end of the war.  The University of Toronto Computer Centre, established in 1947, developed Canada’s first operational computer the University of Toronto Electronic Computer (UTEC) in 1951.  This was followed by the purchase of FERUT (Ferranti University of Toronto) computer, by the Computer Centre in 1952.  The NRC used this computer in the early fifties for the hydrographic modeling of the St. Lawrence Seaway then under construction.  Computers were also developed by other organizations, including the National Research Council, the NRC Computer (1954–1960), Ferranti Canada, mail sorter (1955), Computing Devices of Canada, the \"Kicksorter\" (1957–1963), the Defence Research Board, the DRTE (1960) and Sperry Canada, UMAC-5 (1962).  Avro Canada in Toronto worked unsuccessfully to develop the fire-control computer for the Velvet Glove air-to-air missile for the highly advanced but ill-fated Avro Canada CF-105 Arrow interceptor.  Avro Canada made extensive use of computers in calculations for aircraft design and manufacturing processes, including CNC.  Other military developers included the Royal Canadian Navy with its DATAR system for the command and control of warships. In the 1950s the Pinetree, Mid-Canada and DEW Line air-defence radar chains built across Canada relied heavily on computers.  Certainly the largest and most powerful computer in Canada at the time was the IBM/USAF developed AN/FSQ-7, installed in the late 1950s, 700 ft underground at RCAF Station North Bay, as the \"brain\" of the DEW Line System.  The machine contained 55,000 vacuum tubes, weighed 275 tons and occupied a half-acre of floor space.  It could perform 75,000 instructions per second. By 1958 there were 41 computers in operation in Canada with big business, universities or the military.  The most popular was the IBM 650, which was used by 19 organizations, including Canadair Limited, Canadian General Electric, Ford Motor Company of Canada, Great West Life Assurance Company, Imperial Oil Limited, Orenda Engines Limited and the University of Toronto.  Other models in use included the Bendix G-15, 4, the ALWAC III-E, 3, the IBM 705, 3, the UNIVAC II, 3, the Datatron 205, 2, the LGP-30, 2, the Borroughs E 101, 1, the IBM 704, 1, and the NRC 102A/D. One of the first commercial users of computers was Trans-Canada Airlines (TCA).  In 1961 Ferranti-Packard developed the ReserVec computer reservation system for TCA (now Air Canada).  This formed the basis for the Ferranti-Packard 6000 computer, and in 1963 two were sold in Canada, one to the Defence Research Establishment Atlantic, in Dartmouth, Nova Scotia and the other to the Toronto Stock Exchange. In 1961 the Royal Bank of Canada was the first bank in Canada to introduce computers for its operations, followed by the Toronto-Dominion Bank in 1962.  It was soon followed by the other large Canadian banks, including the Canadian Imperial Bank of Commerce and Bank of Nova Scotia.  When they introduced the credit card about the same time these records were kept on large central computers as well.  It was this experience with large computer systems linking hundreds of branch offices across the country that enabled the banks to introduce the ATM and the debit card, across Canada in the 1980s.  Computers were also introduced to control complex industrial processes.  Interprovincial Pipe Line Limited of Edmonton was one of the first Canadian companies to employ computers as a means of controlling the flow of gas in its very large pipeline system.  Atomic Energy of Canada Limited used computers to control atomic fission in its power reactors.  In 1977 the Toronto Stock Exchange became the first stock market in the world convert to electronic trading with the introduction of the its Computer Assisted Trading System.  Twenty years later, in 1997, the exchange closed its trading floor and converted to a fully automated, computer-driven trading system. Computers were also recognized as a tool for policing.  The Canadian Police Information Centre which was established in 1966 under the auspices of the RCMP, has operated, since that date, a national computer data base that provides information relating to criminal activity in Canada. During this period Canada Post applied computer readable codes to speed the delivery of mail.  On 1 April 1971 the postal code system was introduced.  The technique involved the use of a six-character geographic code placed on the envelope or parcel by the sender.  The code was in turn machine scanned by a computer-driven optical reader that signaled the sorting equipment to direct the item to the proper destination.  While technically effective, the introduction of the system lead to serious labour trouble at Canada Post for several years by unionized workers who were afraid of pay cuts or job loss (Postal codes in Canada). In transportation, several significant works were completed, including the Toronto Subway, 1954, the Trans-Canada Gas Pipeline, 1958, the St. Lawrence Seaway, 1959, Trans-Canada Highway, completed in 1962, the Montreal Subway, 1966, GO Transit, Toronto area, 1967 and Highway 401, Ontario, completed in 1968. On August 10, 1949, the Avro Canada C102 Jetliner, a mid-range four-engine passenger jet, made its first flight, just 13 days after the world's first and eight years before the US's first, the Boeing 707.  Trans-Canada Airlines (later Air Canada) and Canadian Pacific Airlines introduced jet passenger service with the de Havilland Comet, DC-8, DC9, B727 and B-737.  The B-747 was introduced by these companies in the early seventies.  In the sixties and early seventies De Havilland Aircraft of Canada in Toronto developed the DHC-7 Dash 7 and DHC-8 Dash 8 STOL aircraft.  These were used to provide passenger service to small city centre airports in Toronto, Ottawa and Montreal.  A number of international carriers also acquired these aircraft to provide similar services elsewhere in the world.  The first Canadian owned helicopter began operation in Canada on 12 March 1947.  On that date Photographic Survey Corporation took possession of a Bell Bell 47B-3, registration CF-FJA. The development of trans-oceanic aviation in the postwar years created a need for accurate weather information over the Atlantic and Pacific Oceans.  An international agreement in 1946 established 13 Ocean Weather Stations with Canada being responsible for two, ocean station Baker several hundred miles off the east coast from 1947 to 1950 and for ocean station Papa 900 mi off the coast of Vancouver from 1950 to 1981.  A number of ships, both converted RCN vessels and purpose built CCG weather ships were stationed at these points to gather weather information during these years.  Ships involved included, HMCS St. Stephen, HMCS St. Catherines, HMCS Stone Town, CCGS Vancouver, and CCGS Quadra. Of note was the transit of the Northwest Passage in 1954 by HMCS Labrador, Canada's first purpose built icebreaker, which was acquired that same year, in service with the Royal Canadian Navy. Of particular significance was the conversion from steam to diesel by Canada's two great railways.  Beginning in the mid fifties the CPR and Canadian National Railways began replacing their steam locomotives with diesel locomotives.  By 1960 the conversion was mostly complete. The Volkswagen was introduced to Canadians in 1952 and became very popular with drivers who wanted greater fuel economy than that provided by the larger cars then on the market.  It was sold in Canada until 1977.  The seat belt became a standard feature of domestic passenger cars in the late sixties.  The catalytic converter was also introduced during these years.  The first devices, designed to reduce air pollution from automobile exhaust, were installed in the 1975 model year for US cars manufactured in Canada.  Because of environmental concerns and the fact that it was not compatible with these converters, the major gasoline companies in Canada began to eliminate the sale of leaded gasoline that same year. Although Armand Bombardier invented the snowmobile, the initial production model, the B-7 dating from 1937 was a large 7 passenger vehicle.  It was not until 1959 with the development of the small gas engine that the individual snowmobile or Ski-doo was produced by Bombardier (Bombardier Recreational Products) in the company factory at Valcourt, Quebec.  A number of competitors in Canada and elsewhere entered the market and sales of snowmobiles skyrocketed with 2 million being sold worldwide between 1970 and 1973.  To this day, snowmobiles remain popular in Canada and other regions with snowy winters. Pedestrian walkways have become important features of some Canadian cities.  Climate controlled underground passageways and shopping malls have been features of the downtown cores of Toronto (PATH (Toronto)) and Montreal (Underground City, Montreal) since the mid-sixties.  Arguable the most unusual, is the Plus 15 system in downtown Calgary.  Initiated in 1970 it presently consists of 57 bridges and 16 km of enclosed climate controlled passageways suspended 15 ft above ground level which permit pedestrians to walk anywhere in the downtown core summer or winter without ever going outside. Beginning in the mid-1950s nuclear-generated electricity was developed under a partnership of industry and government at both the federal and provincial levels.  A demonstration power reactor, the NPD was built at Rolphton, Ontario in 1962, followed by a commercial-scale CANDU prototype at Douglas Point in 1968.  In 1971 electricity became commercially available from the large (ultimately 8-unit) Pickering station near Toronto, Ontario and, starting in 1977, the Bruce station (ultimately 8-units as well), near Kinkardine, Ontario.  These were followed by the Gentilly-2 Atomic Electric Plant, Trois-Rivières, Quebec and the Point Lepreau Atomic Electric Plant, Point Lepreau, New Brunswick both in 1982.  The electric current supplied by commercial hydro companies to consumers was changed and organizations like Hydro Ontario converted from 25 cycles to 60 cycles during the ten-year period from 1949 to 1959. The introduction of this technology was not without mishap.  On 12 December 1952 the experimental NRX reactor at Chalk River suffered a failure of its cooling system and underwent a partial meltdown.  On May 24, 1958, the newly commissioned NRU reactor also a Chalk River experienced a major accident when one of the uranium filled fuel rods caught fire and seriously contaminated the reactor building with radioactive debris. The modern era of oil production in Canada began in 1947 when Imperial made its major discovery at Leduc, Alberta.  The availability of oil and gas in Alberta, a half continent away from central Canada provided the impetus for the construction of two huge transcontinental pipelines to the eastern Canadian market.  The crude oil pipeline was the first to be built.  The construction of first section of the Interprovincial Pipeline from Edmonton to Regina to Superior Wisconsin in the US began in 1950 and was completed in an astonishing 150 days.  In 1953 the pipeline was extended through the US to Sarnia, Ontario and from Sarnia to Toronto in 1957.  At the time of its completion it was the longest pipeline in the world.  The oil and gasoline industry has grown tremendously since then, mainly to meet the demand for gasoline created by the popularity of the car and for home heating oil.  Major oil refineries have been built in Vancouver, British Columbia, Edmonton, Alberta, Sarnia, Ontario, Montreal, Quebec and Saint John, New Brunswick. The construction of the transcontinental oil pipeline was followed by that of the gas carrying Trans-Canada pipeline.  Work began in 1956 at the Alberta/Saskatchewan border advancing to Regina, Winnipeg and Port Arthur, Ontario in 1957.  Construction was not without mishap as during a pressure test in 1957 five kilometres of pipe blew up near Dryden, Ontario.  The line crossed the very technically difficult Canadian Shield north of Lake Superior and reached Toronto and Montreal in 1958.  At the time of its completion it was the longest pipeline in the world.  Political controversies related to the construction of the pipeline contributed to the fall of the St. Laurent Liberal government in 1957. Gas pipelines were also built in Alberta, the largest being that of Alberta Gas Trunk Lines, incorporated in 1954 and British Columbia, where the Westcoast Transmission Co. system was completed in 1957.  While large pipelines carried natural gas across the continent smaller distribution systems were necessary to carry gas into factories and individual homes, where it was used as a source of heat.  Very complex local understreet pipeline networks were constructed in cities across Canada to meet this requirement. Other energy projects of the period included the Lakeview Generating Station, Mississauga, Ontario, 1962, the W.A.C.Bennett (hydro) Dam, British Columbia, 1967, the Gardiner (hydro) Dam, Saskatchewan, 1968, the Churchill Falls Hydro Dam, Labrador, 1971, the Nanticoke Generating Station (largest coal-fired plant in North America), Nanticoke, Ontario, 1978 and La Grande 2 Hydro Dam, Quebec, 1979.  The energy crisis of 1973 had domestic repercussions with many consumers taking steps to reduce energy costs through the installation of improved home insulation and wood burning stoves. The existence in Alberta of large quantities of surface bitumen (oil) mixed with sand has been known for many years.  In the late 1970s the commercial production of synthetic crude oil from this bitumen began near Fort McMurray.  Construction at this site, by a company known as Syncrude, began in 1973 and the first crude oil was produced there in 1978.  The complex and costly production process involves scraping the sticky bitumen-laden sand from the surface, transporting it to a processing facility, removing the sand from the bitumen and upgrading the bitumen to a product known as light sweet crude.  The technical scale of the operation is very large.  Initially the sandy tar-like bitumen was scrapped from the ground using gigantic powered rotating mechanical wheels equipped with scraping buckets and the oil sand was placed on conveyor belts for transport to the processing plant.  However, the severely cold Albert winters caused the continuous breakdown of the machinery and a new technique was developed.  This involves the use of gigantic power shovels and dumptrucks to deliver the bitumen laden sand to the processing plant.  Once at the plant the bitumen is removed from the sand with a process that involves the use of hot water.  The bitumen is then subjected to fluid coking, hydroprocessing, hydrotreating and reblending.  Syncrude is the largest producer of synthetic crude oil from bitumen sand in the world and the largest producer of oil from a single site in Canada. The forestry industry underwent a notable process of mechanization in the postwar years.  The most visible change was the introduction of the chain saw.  When originally developed for modern use in the 1920s, this heavy, gasoline engine-driven machine required two men for its operation.  However, improvements in engine technology eventually made the saw small and light enough to be operated easily by one person.  In 1944 one of the first industrial users, Bloedel Stewart and Welch Ltd.  in British Columbia had 112 chain saws in operation, but their use accounted for only a small part of total forestry tree cutting.  In 1950 less than one percent of pulpwood in Canada was cut with chain saws.  However, by 1955 this figure had grown to more than 50%. Other machines were also introduced during this period.  The first feller buncher was used by the Quebec North Shore Paper Company in 1957.  Hydraulic tree shears were first used in 1966 by the Abitibi Pulp and Paper Company Limited (Abitibi-Consolidated).  Snowmobiles and tracked machines replaced animals for the hauling of logs.  In 1948 several Bombardier machines were employed to this end by the Ste. Anne Power Company Limited in Quebec.  In 1959 Timberland Machines of Woodstock, Ontario developed the Timberbuncher, a self-propelled machine which could move through the forest, cut a whole tree at its base (a process known as full tree harvesting) and, using a hydraulic arm, place it into a pile for hauling.  Machines that stripped the branches from felled trees, a process known as delimbing, were also introduced at this time. With the help of these technologies, the Canadian pulp and paper industry grew to become one of the major suppliers of newsprint in the world through the operations of companies such as MacMillan Bloedel Limited, Repap Enterprises Inc., Kruger Inc., Great Lakes Forest Products Ltd, British Columbia Forest Products Ltd., Consolidated-Bathurst Inc., Canadian Forest Products Ltd., CIP Inc., Domtar Pulp & Paper Products Group and Abitibi Consolidated. The use of pesticides was a prominent feature of postwar agriculture across Canada.  Insecticides based on fluorine, arsenic, rotenone, nicotine pyrethrum as well as herbicides using sulphiric acid, arsenites and salt and finally fungicides based on sulphur, mercury or copper have been very effective in controlling life forms that degrade agricultural output.  At the same time these compounds have also had a negative effect beyond their intended sphere of use.  DDT was registered for use in Canada from 1946 until 1985, when its use was banned.  The product was never manufactured in Canada.  Food irradiation, in particular the irradiation of potatoes to prevent sprouting while in storage, was approved for use in Canada in 1960. Potash-based chemical fertilizer became an important element of increased agricultural production in Canada and around the world in the postwar era.  In Saskatchewan techniques were introduced for the mining of the huge potash deposits found there.  These involve both \"dry\" and \"wet\", methods of mining.  The dry method involves the sinking of a vertical shaft and the use of large powered cutting machines to cut into the potash horizontally.  The wet technique known as solution mining is used to access potash at greater depths.  This involves drilling a vertical hole into the deposit into which is pumped hot water.  The liquid dissolves the potash underground and then returns to the surface, where another process separates the mineral from the water. Business administration underwent technological change.  The ball point pen was marketed in the US in October 1945 and in Canada shortly thereafter.  The IBM Selectric typewriter, introduced in 1961, quickly became popular with businesses in Canada, as did the Xerox photocopier in the 1960s. There was important progress in medical technology during this period.  In 1945 Dr. Stuart Stanbury established a National Blood Transfusion Programme for the Canadian Red Cross Society, thus making available to those in need, a dependable source of blood for medical purposes.  The associated test for blood typing was introduced at the same time.  Blood tests would become increasingly sophisticated in the coming years.  The electroencephalograph, used for the diagnosis of neurological disorders was introduced in major Canadian medical institutions in the late forties. The techniques for the mass production and distribution of vaccines and for the mass public inoculation were introduced during these years.  Polio was a disease that affected large numbers of Canadian children during the first part of the 20th century.  In the US, research by Dr. Jonas Salk in the late 1940s led to the discovery of vaccine for the prevention of this disease.  However, there was no technique for volume manufacture of the drug.  Connaught Laboratories of Toronto developed a synthetic culture known as \"medium 199\", which enabled the mass production of this polio vaccine beginning in 1952.  A successful all-Canadian mass inoculation of children using the new vaccine was undertaken in the spring of 1955, the first such mass public health campaign of its type in Canada.  Antibiotics such as penicillin were quickly made available to the general public in the postwar years. There was also progress with respect to the treatment of heart disease.  The pacemaker invented with significant Canadian participation was used to treat patients with arrhythmia.  For more serious problems open heart surgery became an option for patients and permitted the repair of faulty heart valves, the clearing of blocked coronary arteries and the resolution of other problems.  Canada's first heart transplant was performed on 31 May 1968, by Dr. Pierre Godin the Chief Surgeon at the Montreal Heart Institute, on patient Albert Murphy of Chomedy, Quebec a 59-year-old retired butcher suffering from degenerative heart disease.  The operation took place about six months after the world's first, by Dr. Christian Barnard.  Neurosurgery was introduced in a substantive way in the 1960s. Cancer patients were provided with a new option, radiation therapy, through what was popularly known as the \"Cobalt Bomb\", again developed with important Canadian input.  The use of radio isotopes for diagnostics was also introduced.  Chemotherapy also became a treatment option.  In 1960 the use of a subcutaneous arteriovenous shunt along with the artificial kidney machine allowed hemodialysis for patients with chronic renal failure. During these years the Montreal Neurological Institute pioneered the development of medical imaging technologies introducing Canada's first CAT scan in 1973, PET scan in 1975 and MRI in 1982.  The technique of medical ultrasonography also became widely available beginning in the late 1960s and was especially popular with expectant mothers interested in the health and sex of their fetus.  The number of these machines in use has grown greatly over the years.  In 2004 there were about 150 MRI units and about 350 CAT units in use in Canada. The corneal contact lenses first developed in 1949 gained mass appeal in Canada and elsewhere in the 1960s.  Made of polymethyl methacrylate (PMMA) they could be worn up to 16 hours a day. Developments in orthodontics made the straightening of the teeth of children with \"braces\" commonplace.  Children were also often on the receiving end of the tonsillectomy a fashionable surgical procedure during these years. The surgical replacement of body parts also became possible and was used to treat ailing kidneys and joints such as knees and hips.  The availability of cosmetic implants became popular during these years.  In 1962, in the US, Dow Corning developed the silicone gel-filled breast implant which was used by women for surgical breast augmentation.  The procedure was common in Canada.  In recent years implants containing saline solution have also become popular. Pharmaceuticals attained a high-profile.  The availability of the birth control pill in 1960 made it possible for women to protect themselves from unwanted pregnancy.  Stress could be treated with tranquilizers, such as valium, introduced in 1963.  The consumption of vitamins became widespread and supplements were added to staple foods such as milk and bread and were taken in pill form.  While most of these drugs were safe, one, thalidomide, had horrific consequences for its users.  Thalidomide was invented in West Germany in 1954 by Chemie Grunenthal as a sedative.  It was noted that the drug was particularly effective in treating the symptoms of morning sickness associated with pregnancy.  The drug was made available under prescription to Canadians beginning 1 April 1961.  Tragically it was discovered that the drug caused miscarriage and severe birth defects.  As a result, the drug was withdrawn from the Canadian market on 2 March 1962. The \"recreational\" use of \"soft drugs\" such a marijuana, LSD and hashish became part of the 1960s counter culture.  Marijuana was often produced locally using the hydroponic method. The car, cheap gasoline and postwar affluence created boom conditions for the expansion of suburbia.  Several standard designs for the single family home on a standard lot were reproduced cookie-cutter style row-upon-row in cities across Canada as subdivision after subdivision sprang up radiating from the central core.  The designs were thoroughly modern, reflecting the optimism of the era, usually with a peaked roof, asphalt shingles and a brick or wood siding exterior and included a living room, kitchen and occasionally dining room and two, three or four bedrooms and a full basement made of poured concrete or cinder block.  Floors were usually made of varnished hardwood planks and the walls and ceilings of gyprock.  Copper piping brought running water from the serviced street and copper wiring electricity from the rear lot line.  Clay tile pipe carried the sewage from the flush, sit toilet to the main sewer line running under the street.  There was usually a driveway beside the house for the family car, and less frequently a carport or garage. Most homes were equipped with a telephone often with a \"party\" line but these became rare by the 1960s.  A television set was also common in almost all homes by the end of the 1950s and the record player gave way to the hi-fi stereo.  Almost all kitchens were equipped with electric refrigerators and electric or less commonly gas, stoves.  Where there was gas it was usually piped to the home through a main line running under the street.  There were a variety of electrical \"labour saving\" devices including electrical mixers can openers and carving knives.  Central heating was a standard feature and coal, delivered to the home by a diesel powered truck, was the dominant fuel source in the early postwar years.  However, as the 1950s progressed coal gave way to oil and gas heating.  Home furnishings were almost all mass-produced and made from wood, fabric and various types of stuffing for cushions.  In the kitchen metal chrome tube chairs and formica topped tables were popular.  The small front and back yard were maintained with the help of a gasoline-powered lawn mower, and the hedge and bushes were trimmed with electric clippers.  In the early 1960s the high-rise apartment building began to make its appearance in large cities.  The self-supporting steel structures were usually seven stories or more, and large buildings contained hundreds of dwelling units.  Initially they were especially visible along Highway 401 in Toronto, Metropolitan Boulevard in Montreal and the north shore of English Bay in Vancouver.  Their construction was possible due to the introduction of the high-rise crane, which to this day remains a common feature of city skylines. The arrival of television had an effect on eating habits.  In 1953, C.A. Swanson & Sons introduced the TV dinner to the US market.  The pre-cooked food items, including, meat, potatoes and a vegetable were placed in the segments of an aluminum tray and frozen.  The consumer purchased the frozen product and heated it in the oven for about 25 minutes.  It could be eaten out of the tray.  In 1960, Swanson, a subsidiary of the Campbell Soup Company, built a factory in Listowel, Ontario to manufacture TV dinners and other Campbell frozen products for the Canadian market.  The electric toothbrush introduced in 1959 has become very popular in Canada since the 1990s. The steel aerosol spray can with the gas propellant, and \"crimp on nozzle\" was developed in the US in 1949.  It quickly became a favored type of packaging in Canada for a number of products including whipped cream, deodorant, bug spray and hair spray.  The gas propellant, usually a chlorofluorocarbon (CFC), became a target for environmental concern in the 1970s when research demonstrated that it had a harmful effect on the ozone layer in the atmosphere.  The international Montreal Protocol of 1989 banned the use of these substances and they were subsequently replaced with volatile hydrocarbons.  The disposable diaper was introduced to the Canadian market in 1972 by Procter & Gamble Canada Ltd.  The chemistry of hair colouring was introduced to the domestic scene in the US and Canada in 1956, when Clairol began marketing a home hair colouring kit for women that could be purchased for a modest price at a local pharmacy or grocery store.  Invented in Canada, the green plastic garbage bag (Bin bag) was introduced to Canadians in the late 1960s.  Patented in 1955 by Swiss engineer, George de Mestral, velcro, a two sided fastening technology based on hooks and loops, was introduced to a number of countries, including Canada, in the late 1950s.  In Montreal, Velek Ltd. acquired the exclusive marketing rights for velcro in North and South America and Japan. The format for sound recordings changed in the years just following the war.  In the US, Columbia Records introduced the long playing (LP) 33⁄ format in 1948.  Columbia made an agreement with Sparton Records, of London, Ontario, established in 1930, for the manufacture and distribution of its LP records in Canada.  Not to be caught short, RCA Victor in the US responded in 1949 with its own technological innovation the 45 rpm record (with the big hole in the centre) and manufactured and distributed this new format for the Canadian market through its Canadian subsidiary, RCA Victor of Canada, established in Montreal in 1929.  The video home system (VHS) released in 1976 by Victor Company of Japan, Limited (JVC), quickly became popular in Canada and was used to record TV programmes or to play VHS tapes of Hollywood movies that could be rented in neighborhood video stores that soon became a common feature of suburban strip malls.  In 2003 the popularity of DVD surpassed VHS and by 2006 the technology had become obsolete. The introduction of the credit card complimented the appearance of the shopping mall.  In 1968 a number of Canadian banks including the Bank of Nova Scotia, the Royal Bank of Canada, the Toronto-Dominion Bank and the Canadian Imperial Bank of Commerce began issuing the Chargex credit card to customers.  In 1977 these cards were reissued by the same banks under the VISA brand name.  The MasterCard credit card became available to Canadians in 1973. The booming growth of the suburbs led to the appearance of the shopping mall, a low-rise steel frame, commercial structure housing a number of retail outlets and surrounded by acres of asphalt parking lot for large numbers of cars.  The first in Canada included the Norgate Shopping Centre, Saint-Laurent, Quebec, 1949, the Dorval Shopping Centre, Dorval, Quebec, 1950, the Park Royal Shopping Centre, West Vancouver, British Columbia, 1950, the Sunnybrook Plaza, Toronto, 1951 and York Mills, Toronto, 1952. The hospitality industry was similarly affected and fast food drive-in restaurants began to appear.  In 1951 the first St. Hubert BBQ restaurant opened its doors on St-Hubert street in Montreal.  A&W opened its first Canadian operation in Winnipeg, Manitoba in 1957.  In 1959 Harvey's opened its first eatery on Yonge Street in Richmond Hill.  In Hamilton, Ontario, the first Tim Hortons restaurant opened in 1964.  The first McDonald's restaurant outside the United States was opened in Richmond, British Columbia in 1967 and Pizza Delight was founded in Shediac, New Brunswick, in 1968. Cinema attendance boomed after the war and with it innovations in cinema design.  The first double-screen cinema, The Elgin, opened its doors in Ottawa in 1946 and the drive-in cinema became popular after the war.  However, the long cold Canadian winters discouraged the widespread diffusion of this type of film exhibition.  The dramatic IMAX large-scale cinema format was invented as the result of developments in cinematic technology during Expo '67 in Montreal.  The world's first permanent Imax cinema, Cinesphere, was built at Ontario Place in Toronto in 1971.  Others were built in Vancouver for Expo '86 and at the Canadian Museum of Civilization in Gatineau, Quebec, in 1989.  By 1995 there were 129 Imax cinemas entertaining audiences around the world.  The audio cartridge and audio cassette became popular in the early 1970s with the cassette eventually winning the battle of the formats.  This compact medium led to the appearance of high quality in-car sound systems. The New Woodbine Racetrack for thoroughbred horse racing opened to the public in Toronto in 1956 (simply Woodbine after 1963) replacing the original Woodbine which was built in 1874.  Canada’s first purpose-built auto racing track, the Westwood Motorsport Park was built in Coquitlam, British Columbia in 1959.  The Mosport International Raceway, north of Bowmanville, Ontario opened to the public in 1961 and hosted the Canadian Grand Prix Formula 1 races from 1967 to 1977.  La Ronde became Canada’s largest amusement park when it opened in 1967 as part of Expo '67 in Montreal.  It is popular to this day for a number of roller coasters, including The Boomerang, Cobra, Goliath, Le Monstre and Vampire. Detergent, a replacement for soap, introduced in the postwar years, was used to keep clothes and dishes clean through the action of its active ingredient, tetrapropylene, a derivative of petroleum.  The popular Tide brand became available in 1948.  In 1964 permanent press fabrics were invented in the US by Ruth Rogan Benerito, a scientist at the Physical Chemistry Research Group of the Cotton Chemical Reactions Laboratory and introduced to Canada shortly thereafter.  The press resulted from the treatment of the fabric with formaldehyde.  Invented by DuPont scientist Dr. Roy J. Plunkett in 1938, polytetrafluoroethylene, a polymer considered the world’s most slippery substance, was introduced commercially as Teflon, in 1946 in the US.  It is used in a wide variety of applications, including as a non-stick coating on the cooking surface of pots and pans and is manufactured in Canada by DuPont in Mississauga, Ontario.  Krazy Glue (ethyl cyanoacrylate) was introduced to Canada in 1973. In the postwar years Canadian municipalities began treating raw sewage, which up to that time, with a few notable exceptions, had been allowed to flow directly from their sewer systems into nearby streams, rivers, lakes and oceans.  New facilities were added in Toronto including the Highland Creek Wastewater Treatment Plant in 1956 and the Humber Wastewater Treatment Plant in 1960.  Vancouver built a number of sewage treatment facilities including the Lions Gate Wastewater Treatment Plant in 1961, the Iona Island Wastewater Treatment Plant in 1963 and the Lulu Island Wastewater Treatment Plant in 1973.  The City of Ottawa built the Green's Creek Pollution Control Center (now Robert O. Pickard Environmental Centre) in 1961.  In 1970 the City of Montreal began the construction of a large sewer network which channeled all effluent to the treatment plant at Rivière-des-Prairies on the east end of the island and became operational in 1996.  By 1980, 64% of Canadians were served by sewage treatment, with the figure rising to 78% in 1997. Public and industrial concern with air pollution and acid rain led to measures being taken by a number of companies to cut back on harmful atmospheric emissions.  In 1972, Inco undertook steps to reduce emissions of SO2 and other gases by installing scrubbers and a 1250 ft chimney at its Copper Cliff smelter in Ontario. Bridges of note included the Angus L. Macdonald Bridge in Halifax 1954, the Oak Street Bridge, Vancouver, 1957, the Burlington Bay James N. Allan Skyway, 1958, Ogdensburg-Prescott International Bridge, 1960, the Queensborough Bridge, Vancouver 1960, the Sault Ste. Marie International Bridge, Sault Ste. Marie, Ontario, 1962, the Champlain Bridge, Montreal, 1962, the Lewiston-Queenston Bridge, Niagara Falls, Ontario, 1962, the Port Mann Bridge, Vancouver, 1964, the Macdonald-Cartier Bridge, Ottawa, 1965, the Pont de la Concorde (Montreal), 1966, the Prince Edward Viaduct, Toronto 1966, the Laviolette Bridge, Trois-Rivières, Quebec, 1967, the Saint John Harbour Bridge, Saint John, New Brunswick, 1968, the Dinsmore Bridge, Vancouver, 1969, the A. Murray McKay Bridge, Halifax, 1970, the Pierre Laporte Bridge, Quebec City, 1970, the Portage Bridge, Ottawa, 1973 and the Arthur Laing Bridge, Vancouver, 1976. This was also an era of gigantism, and there were both successes and failures.  Beginning in 1963, massive civil engineering works were undertaken in the St. Lawrence River in Montreal to build the site for the 1967 World's Fair, known as Expo 67.  The gigantic Red River Floodway, which opened in 1968, was designed to carry flood water from a rising Red River around the heart of the City of Winnipeg.  It was completed in 1968 and proved successful when used for the first time in 1969.  At the time of its completion it was the second largest earth-moving project in the world, after the Panama Canal. Northwest of Montreal thousands of acres of fertile farmland were expropriated to build the huge new Mirabel International Airport, which opened in 1975.  The facility was to be linked to the heart of Montreal with a fast train.  The train was never built and both passengers and air carriers stayed away in droves.  The site eventually became a quiet industrial airport, home to the production facilities for Bombardier regional jets.  On the other hand, the James Bay Project undertaken in Quebec at the same time was a booming success.  Several large dams on the La Grande River with their associated long distance transmission lines provide Hydro-Québec with an important source of electricity. The CN Tower, the world's tallest free-standing structure, was constructed in Toronto in 1975. Important skyscrapers, including Place Ville Marie (Royal Bank), Montreal, 1962, the Canadian Imperial Bank of Commerce Tower, Montreal, 1962, the Edifice Trust Royal (C.I.L. House), Montreal, 1962, the Toronto Dominion Bank Tower, Toronto, 1967, The Simpson Tower, Toronto, 1968, the Hôtel Château Champlain, Montreal, 1967, the Royal Trust Tower, Toronto, 1969, Royal Centre, Vancouver, 1972, Inco Superstack, Sudbury, Ontario, 1972, First Canadian Place, Toronto, 1975, Harbour Centre, Vancouver, 1976, the Complexe Desjardins, la Tour du Sud, Montreal, 1976, the Scotia Tower, Calgary, 1976, the Scotia Tower, Vancouver, 1977, Royal Bank Plaza, South Tower, Toronto, 1977 and the First Bank Tower, Toronto, 1979, represented significant architectural achievements during this period. The massive Saint Joseph's Oratory, the largest church in Canada, the construction of which began in 1924, was completed in 1967 on the north slope of Mont Royal in Montreal. Notable large sports facilities included, Empire Stadium, Vancouver, 1954, McMahon Stadium, Calgary, Alberta, 1960, the Montreal Automobile Stadium (Autostad) 1966, the Olympic Stadium, Montreal, 1976 and Commonwealth Stadium (Edmonton), 1978. Microelectronics became a part of everyday life during this period.  The personal computer became a feature of most homes, and the microchip found its way into a bewildering variety of products from cars to washing machines. In 1977 the first commercially produced personal computers were invented in the US: the Apple II, the PET 2001 and the TRS-80.  They were quickly made available in Canada.  In 1980 IBM introduced the IBM PC.  Microsoft provided the operating system, through IBM, where it was referred to as PC DOS and as a stand-alone product known as MS-DOS.  This created a rivalry for personal computer operating systems, Apple and Microsoft, which endures to this day.  A large variety of special-use software and applications have been developed for use with these operating systems.  There have also been a multiplicity of hardware manufacturers which have produced a wide variety of personal computers, and the heart of these machines, the central processing unit, has increased in speed and capacity by leaps and bounds.  There were 1,560,000 personal computers in Canada by 1987, of which 650,000 were in homes, 610,000 in businesses and 300,000 in educational institutions.  Canadian producers of micro-computers included Sidus Systems, 3D Microcomputers, Seanix Technology and MDG Computers.  Of note is the fact that these machines were based on digital technology, and their widespread and rapid introduction to Canada at the same time that the telephone system was undergoing a similar transformation would herald an era of rapid technological advance in the field of communication and computing. The laptop computer also appeared during these years and achieved notable popularity in Canada beginning in the 1990s.  In 1981 the first commercially available portable computer, the Osborne 1, became available.  Other models followed, including the Kaypro II in 1982, the popular Compaq Portable and Tandy Corporation TRS-80 Model 100 both in 1983, the IBM PC Convertible, 1986, the Macintosh Portable, 1989 and Power Book, 1991.  The latter models in particular were popular with both professionals and consumers. In the 1970s and 1980s word processing, a method for \"typing\" documents using a keyboard linked to a computer and a video screen, was developed.  Early machines were dedicated exclusively to this function and a notable Canadian contribution, the Superplus IV, produced by AES Data in Montreal in 1981, became widely popular.  However, the rise of the personal computer and the invention of PC-compatible word processing software, such as WordPerfect in 1982 and Microsoft Word in 1983, made stand-alone word processors obsolete.  Spreadsheet software also became popular for accounting purposes, notably Microsoft Excel, which was also introduced to the world and Canadian market in 1983.  These new machines with their new software quickly dominated the market and became an almost universal feature of any Canadian office. In 1987 there were considerable numbers of larger computers in Canada, including 25,000 mainframe and mini-computers.  But the most powerful of all were the supercomputers.  The Meteorological Service of Canada has been a noted user of large computers and has pioneered the Canadian use of supercomputers.  Machines used have included the Bendix G20, 1962, an IBM 360-95 scientific mainframe computer, 1967, its first supercomputer a CDC 7600 from Control Data Corporation, 1973, a Cray 1S supercomputer, 1983, a NEC supercomputer, 1993 and an IBM supercomputer in 2003.  At the time of its installation this latter machine was the most powerful computer in Canada. The Communications Security Establishment (CSE), Canada's \"electronic spy\" agency has been a notable user of supercomputers.  CSE code breaking capabilities degraded substantially in the 1960s and 1970s but were upgraded with the acquisition of a Cray X-MP/11 (modified) supercomputer delivered to the Sir Leonard Tilley Building in Ottawa, in March 1985.  It was, at the time, the most powerful computer in Canada.  In the early 1990s, the Establishment purchased a Floating Point Systems FPS 522-EA supercomputer at a cost of $1,620,371.  This machine was upgraded to a Cray S-MP superserver after Cray acquired Floating Point Systems in December 1991 and used the Folklore Operating System supplied by the NSA in the US.  These machines are now retired. Little information is available on the types of computers used by the CSEC since then.  However, Cray in the US has produced a number of improved supercomputers since then.  These include the Cray SX-6, early 2000s, the Cray X1, 2003 (development funded in part by the NSA), Cray XD1, 2004, Cray XT3, Cray XT4, 2006, Cray XMt, 2006 and Cray CX1, 2008.  It is possible that some of these models have been used by the CSEC and are in use today. In 2008, Canada’s most powerful research computer, an IBM supercomputer, was installed in Toronto.  The $20 million machine, about the size of an SUV, can make 12.5 trillion computations per second and will be used for proteomics research by the Ontario Cancer Institute, the Princess Margaret Hospital (specializing in cancer) and the University Health Network.  An IBM System x iData Plex supercomputer began operation at the University of Toronto in 2009.  However, the supercomputer used by Environment Canada for weather forecasting remains the largest in Canada. Canada's major telephone companies introduced digital technology and fibre optics during this period paving the way for more advanced business and customer telecommunications services. In 1976 Nortel developed the first digital private branch exchange (PBX) in the world.  That same year Nortel announced its \"Digital World\" project, which foresaw the development and market introduction of a complete family of digital switching, transmission and business communications systems. In 1977 Bell Canada began to conduct a fibre optic field trial to residential customers in Montreal.  The Manitoba Telephone System began to introduce fibre optics in that province beginning with customers in Elie, in 1981.  In 1984 Sasktel, the provincially owned telephone company in Saskatchewan, completed the construction of a 3,268 km.  long commercial fibre optics network to fifty-two communities in that province.  In 1984 CNCP Telecommunications began the construction of a trans-Canada fibre optic network.  By 2009, the national fibre optic system in Canada stretched 7000 kilometers across the country and included underwater fibre links to PEI (1985) and Newfoundland.  Eight fibres are reserved for Trans-Canada traffic. On 1 July 1985, Cantel and Bell Cellular began to offer cell phone service in Canada.  They used a technical standard known as CDMA, which was compatible with mobile phone systems in the US but not elsewhere in the world.  The fax began to make its presence felt in offices across Canada in the early 1980s. The \"Globe and Mail\" began to produce its contents in electronic form in 1979.  A year later in 1980, in order to enable the daily distribution of the Toronto-based paper across the country, in a bid to become Canada's \"national newspaper\", it began the transmission of its contents via Anik Satellite to regional offices, where it was printed and distributed. Bluetooth technology, developed in 1994 by Ericsson, was introduced to Canadian consumers at that time.  The technique permits the very short range radio communication of Bluetooth equipped electronic devices with each other for the purposes of information transfer.  It is designed to eliminate the need for wires and cables to connect such machines. The use of lasers became common throughout Canada during these years.  The devices are usually found as components of larger systems. Lasers are used in the field of telecommunications, where they are act as the modulated light sources for fibre optic systems.  The high frequency of the pulsed beams of light they produce enables the transmission of large quantities of information and the absence of an electromagnetic field around the fiber optic cable lessens transmission loss and increases the security of the data.  Lasers are also used in many mechanical manufacturing systems to start and stop processes, measure component size and monitor and maintain quality. In public they are commonly found at the retail checkout counter, where they scan bar codes.  They are also used to open and close doors for people and cars and are common in public washrooms, where they control the flow of water for taps and the flushing of urinals and toilettes. Their use in medicine has been growing.  They are used to \"burn\" plaque from clogged arteries, to remove the decayed portion of teeth in dental treatment and to treat vision problems related to retinal detachment and near sightedness (lasik surgery). The Lidar, a laser-based instrument, is used by meteorologists to determine the height of the cloud base. Smaller vehicles became popular in response to the oil crisis of 1973.  In 1981 the Chrysler K platform introduced by that company formed the basis for the compact K-car.  Front wheel drive was widely introduced to North America by the Big Three US automakers beginning in 1978, when the Plymouth Horizon and Dodge Omni, both with transverse mounted engines, became available.  A wide variety of front wheel-drive-models were quickly offered to Canadians by other car makers. The air bag safety feature was introduced during these years.  In the US, the 1973 model Oldsmobile Toronado was the first passenger car equipped with an air bag.  Ford introduced air bags as an option in the Tempo in the US in 1984 and Crysler made them a standard feature in 1988.  Air bags were made available in the Canadian market as this feature became available in US models. The semi-trailer truck (18 wheeler), became the dominant vehicle on the heavily used Highway 401 (Ontario).  Containerization, which had made headway in ocean shipping with the construction of terminals in Halifax, Montreal and Vancouver also led to the eventual elimination of the railway box car and began to make inroads in the trucking industry.  Light rail systems were built in Edmonton, Alberta in 1978, Calgary, Alberta, in 1981 and Vancouver, British Columbia in 1986. The Ontario Highway 407 Express Toll Road (ETR), opened in Toronto in 1997 to ease the burden of traffic on Highway 401 which passes through the heart of the city to the south.  Special technology is used to collect tolls without the use to toll booths.  Regular users can equip their cars with a transponder that sends a signal to highway sensors when the vehicle enters and leaves the road.  For those vehicles without a transponder, special electro-optical sensors read the number plate and a bill for the toll is sent to the vehicle owner in the mail. In 1996 GM Canada introduced the OnStar service to Canadian and US customers who chose this option when purchasing a new car.  Considered a safety feature, the service provides emergency services, vehicle diagnostics and directions to drivers on the road.  It is based on GPS technology as well as CDMA mobile phone technology provided in Canada by Bell Mobility.  There is a 24-hour emergency response centre in Oshawa for vehicles located in Canada. In the 1990s, a national weather radar surveillance system for aviation and general use was established.  The initial system, the Canadian weather radar network, established by Environment Canada, became operational in 1997 and consisted of 18 weather radars using the 5 centimetre wavelength (C-Band) and one using the ten-centimeter wavelength (S-Band).  In 1998 that organization received approval to add another 12 radars and to upgrade the system to the Pulse-Doppler radar standard. Also in 1997 the responsibility for air traffic control in Canada was transferred from Transport Canada to Nav Canada.  The very large and complex control system operated by that organization uses a number of technologies, including 1400 ground-based navigation aids, 46 radars and six Automatic dependent surveillance-broadcast systems, which are based on global positioning technology. Technological improvements have also enhanced rail safety during these years.  These include a large number of incremental changes to elements of rolling stock including wheels, axles, trucks, couplers and brakes.  There have also been improvements to tracks including continuous weld, concrete sleepers and switching techniques.  Techniques of traffic control and communication have also been improved. Bombardier's invention of a new class of aircraft, the regional jet or RJ, allowed airlines to introduce jet passenger service to smaller centres.  The design of this machine was facilitated through the use of computer-aided design software.  In 2009, Bombardier Aerospace of Montreal, in cooperation with the National Research Council Canada began using robots for the assembly of its aircraft. Factory farming of pigs and chickens in particular became a prominent feature of agriculture during these years.  Large numbers of these animals are crowded into very large barns with controlled environments in an effort to maximize their growth and hence profit for the farmer.  The use of antibiotics to fight infection, as well as the use of growth hormones is common.  The growth in the number of these farms has been dramatic.  The Fraser Valley in British Columbia is home to the highest concentration of such farms in Canada and the number of farms there increased from 56 in 1991 to 146 in 2001.  The growth of genetically modified crops also became common.  One of the most notable in this regard is canola.  Developed in Canada from rapeseed during the 1970s by Keith Downey and Baldur Stefansson it is used to produce oil that is low in erucic acid and glucosinolate and has become a major cash crop in North America.  A strain of canola with additional modification that made it resistant to herbicide was introduced in Canada in 1996. The use of computer-controlled robots in manufacturing (computer-aided manufacturing) as well as the closely associated, just-in-time inventory management technique, were pioneered in Canada by the auto manufacturers, who introduced them to improve efficiency.  They were put to use in new auto manufacturing plants built, by Honda Canada in Alliston, Ontario and Toyota Canada in Cambridge, Ontario (1988).  The techniques of robotic assembly-line manufacturing have improved over the years.  As of 2009 robots form the basis for automobile production in Canada with a number of facilities and companies using this technology, including (city, company, model): Geomatics, a term that originated in Canada, is a technique of computer-based mapping that integrates information from a variety of sources including, cartography, remote sensing (including images from RADARSAT), surveying, global satellite navigation systems, geodesy and photogrammetry.  In the new century it has become an important tool used by Canadians in a variety of endeavours including, commerce, the environment, search and rescue, urban planning, defence and natural resource management among other things.  The Geomatics Industry Association of Canada, founded in 1988, presently has a membership of more than 100 organizations. During these years Canada's unmanned space programme included the first launching of a Canadian earth observation satellite, RADARSAT-1 in 1995 and an improved version RADARSAT-2 in 2007.  Placed in polar orbits each of these satellites images almost the Earth's entire surface, every 24 days using a powerful synthetic aperture radar, SAR.  The images have both operational and scientific applications and their data is of use in geology, hydrology, agriculture, cartography, forestry, climatology, urbanology, environmental studies, meteorology, oceanography and other fields.  In 2009 the Canadian Space Agency announced a follow-up programme, RADARSAT Constellation, which will see the launching of three earth observation satellites, in 2014, 2015 and 2016 respectively, working as a trio to provide complete coverage of Canada's land and ocean surfaces as well as 95% of the surface of the world every 24 hours. The technology for offshore oil and gas extraction was introduced to Canada during these years.  The first of several projects off of Canada’s east coast was the massive Hibernia platform, a gravity base structure (GBS), built in Bull Arm, Newfoundland in the early 1990s.  The 1.2 million ton structure was towed into place, 315 km to the south east of St. John's, Newfoundland, over the Hibernia off-shore oil reservoir, where it was positioned resting on the ocean floor in 80 meters of water with its superstructure rising 50 meters above the surface of the sea.  In 1997 the facility began to pump oil from the sea bottom.  The oil is stored in giant on-board tanks and continuously off-loaded by a fleet of dedicated shuttle tankers which transport it to the shore-based oil refinery at Come-by-Chance, Newfoundland. The nearby Terra Nova, 350 km off shore, began to produce oil in 2002.  The platform itself rests on the ocean floor and pumps oil from the sea bottom.  However, unlike the Hibernia facility, the oil flows directly into a Floating Production Storage and Offloading (FPSO) vessel, the Terra Nova FPSO, where it is processed and stored.  The oil in the storage tanks is then removed by a shuttle tanker.  A third oil production facility in the same area, the White Rose (oil field), operated by Husky Petroleum, which began operation in 2005 also uses a Floating Production Storage and Offloading (FPSO) vessel, the SeaRose FPSO. Undersea pipelines, the first in Canada and part of the Sable Offshore Energy Project have been introduced to transport gas from undersea wells off the coast of Nova Scotia since 2000.  Gas was discovered near Sable Island in 1979 with the first platform and well head installed in the Thebaud field in 1999.  An on-shore gas treatment facility was built at Goldboro and connected to the wellhead with a 225 km long undersea pipeline and production began in 2000.  Other fields have been connected via undersea pipeline including the North Triumph, Venture, Alma and South Venture. Other notable energy works included, the ill-fated east coast Ocean Ranger drilling platform, which sank in a storm in 1982 with the loss of all aboard, the Nova Scotia Power Corporation tidal generating station, Annapolis, 1984 and the Darlington Nuclear Generating Station, Darlington, Ontario, 1990.  The construction of large-scale hydro-electric plants far from electric markets lead to the introduction of techniques for long distance electric power transmission.  These techniques were used at a number of sites, including the W.A.C Bennett hydro station in British Columbia in 1968, Churchill Falls, Labrador, 1971 and the Robert-Bourassa generating station, 1981, the La Grande-3 generating station, 1984, the La Grande-4 generating station, 1986 and the La Grande-2-A generating station, 1992, all in Quebec. Biotechnology involves modifying living organisms to serve human goals.  Biological techniques are derived from a number of sciences including, biology, chemistry, organic chemistry, biochemistry, genetics, botany, zoology, microbiology and embryology to name a few.  In 1987, a number of Canadian companies and university research organizations operating in the field of biotechnology established the Industrial Biotechnology Association of Canada, also known as BIOTECanada.  In 2005, the industry consisted of companies offering biotechnologically based services and products in the following sectors: human health (262 firms), agriculture (89 firms), food processing (54 firms), environment (33 firms), bioinformatics (16 firms), natural resources (21 firms) and aquaculture (15 firms).  As of 2010 BIOTECanada had more than 250 members. In 2005 noted bio-medical spenders on biotechnological research (research spending in millions of C$) included, Apotex Inc. (151.1), Pfizer Canada Inc. (147.5), GlaxoSmithKline Inc. (111.8), Merck Frosst Canada Ltd. (96.6), Biovail Corporation (88.9), AstraZeneca Canada Inc. (79.8) and Sanofi Pasteur Limited (76.6). Another product of biotechnology, genetically modified crops, are grown throughout Canada.  One of the most widely known, canola was developed in Canada through selective breeding in 1974.  Canola oil is used in food products and in non-food items such as lipstick, candles, bio-fuels and newspaper ink.  One of the largest suppliers of genetically modified seeds for food crops and animal feed in Canada is Monsanto Canada, established in Winnipeg in 1901. Medical treatment advanced during these years.  The use of lasers and computers became important parts of medical treatment.  Computers were essential in the development of new medical imaging devices such as the CAT scan, positron emission tomography and the MRI.  Minimally invasive surgery, also known as laparoscopic surgery, reduced surgical damage to patients.  Lasers were used with catheters for clearing blocked arteries and catheters with small cameras provided images of conditions inside the body.  Coronary bypass surgery became commonplace.  Laser eye surgery became popular in the 1990s and was used to improve visual acuity for the near-sighted.  New chemical chemotherapy combinations helped prolong the lives of cancer patients.  Techniques for the long-term application of medication through the use of a skin patch or implants appeared during these years. A large number of medical drugs (List of bestselling drugs) for treating a wide variety of ailments such as high blood pressure, high cholesterol levels, arthritis, allergies, anemia, depression, asthma, osteoporosis and diabetes, became available to Canadians during this period. The techniques for blood collection, processing and transfusion came under severe criticism in Canada during the 1980s, and led to Canada's worst-ever public health crisis.  Between 1980 and 1985, 2000 recipients of tainted blood provided by the Canadian Red Cross were infected with the HI virus.  Between 1980 and 1990, 30,000 Canadian transfusion recipients were infected with hepatitis C from tainted blood.  About 8000 of those who received bad blood have died or are expected to die as a result.  An investigation known as the Royal Commission of Inquiry on the Blood System in Canada was launched in 1993 and issued its final report in 1997. A private company, IVF Canada of Scarborough, was the first to begin offering in vitro fertilization (IVF) in Canada beginning in 1983.  Since that date the company has recorded a number of Canadian \"firsts\" in this field, including the first IVF pregnancy, first IVF twins, the first IVF triplets and the first baby born from a frozen embryo.  Beginning in 1998 male erectile difficulties could be treated with the use of Viagra and other medications. There were innovations in home design and construction during this period.  Houses generally became bigger.  New materials such as vinyl siding became common and often replaced more expensive brick for home exteriors.  The car port and garage became widespread features and the latter was often located close to the curb, creating a rather crowded streetscape.  The home dishwasher and the microwave oven were introduced.  Large-screen televisions usually of the cathode ray or projection type were found in many homes.  The Sony Walkman, introduced in 1979, quickly gained popularity as a means for listening to music on the go.  There were innovations in the field of domestic cuisine including the introduction of microwave popcorn.  In 1981, the development of the susceptor bag (a paper bag impregnated with an aluminum-coated polyester film), allowed popcorn therein to be popped in a microwave oven without scorching. The compact disc (CD) and the digital video disc (DVD) were introduced at this time.  The CD, which appeared in 1982, became a favourite format for musical recordings.  By 1986 most music stores in Canada had phased out the LP and replaced it with the CD.  It was also used for other purposes including data storage in the form of the CD-ROM.  A closely related format, the DVD, with greater storage capacity than the CD, was introduced in 1997.  In 1986, Americ Disc of Drummondville, Quebec began to manufacture CDs and after 1997, DVDs and has become one of the largest suppliers of this product in North America.  The infrared-based TV remote control became popular with Canadians in the early 1980s. Video games have become a wildly popular form of entertainment especially for youth, since the 1980s.  The earliest video game dated from 1947 and a number of devices were produced in the 1950s and 1960s.  However, it was the development of the computer chip that led to their popularization.  The coin-operated arcade game \"Pong\" introduced by Atari in 1972 was the first to become widely available.  The next phase of development included the introduction in the mid-1970s of the home console, first with a hardwired game, but then complemented in 1977 by “plug and play”, which allowed the use of game cartridges for variety.  Beginning in 1985 PC gaming became popular, exploiting the flexibility and increasing popularity of the personal computer.  In 1989 Nintendo released its Game Boy, the first of the hand-held electronic games.  Game imagery became more elaborate with the introduction of the 32-bit chip that was featured in the Sony PlayStation released in 1994.  The 128-bit, sixth generation of video games was born with the introduction of the Sega Dreamcast in 1998.  These technologies found a place in the Canadian consumer market from the moment of their introduction and Canadian companies have played a role in their development, with hardware makers like ATI Technologies developing high-powered video chips for game imagery and software companies developing a number of games. Gore-Tex, a breathable, waterproof textile, was patented in the US in 1980.  Clothing made from this product and designed for outdoor all-weather, sporting, athletic and recreational activity became available in Canada shortly thereafter. The theme park became popular in the 1980s and the technology of thrill is the main attraction.  In Toronto, Canada's Wonderland, Canada’s largest, opened its doors to the public in 1981 and is now tied for second place in North America as the theme park with the most roller coasters (List of roller coasters at Canada's Wonderland).  Galaxyland the world's largest indoor amusement park located in the West Edmonton Mall which opened in 1981 has garnered continent-wide attention.  The most popular thrill ride, the Mindbender (Galaxyland) is the largest indoor triple-loop roller coaster in North America.  The Drop of Doom was another featured attraction until closed in the early 2000s.  The Mall's World Waterpark, which opened in 1985 offers bathers a chance to cavort in the world’s largest indoor wave pool.  The technology of auto racetrack design and construction has been put to good use in Montreal at the Circuit Gilles Villeneuve, Canada’s premier auto race track and home of the Canadian Grand Prix Formula 1 motor car races since 1978.  The slot machine, so dear to gamblers, was introduced during this period.  Casinos have been built in Windsor, Caesars Windsor 1994, Niagara Falls, Niagara Fallsview Casino Resort 1996 and Orillia, Casino Rama 1996, Ontario, Montreal, Montreal Casino, Gatineau, Casino du Lac Leamy 1996 and Baie St. Paul, Casino de Charlevoix 1994, Quebec, Halifax, Casino Nova Scotia 1995, Nova Scotia and in Vancouver, the River Rock Casino Resort 2006, British Columbia. The world’s first automated teller machine (ATM) service was developed by the Sherwood Credit Union in Regina in 1977, at that institution’s North Albert Branch.  Other Canadian financial institutions followed this lead and by the 1980s the ATM was available throughout Canada. During the 1980s the bar code became a familiar feature on consumer products ranging from food to clothes as did the bar code scanner at the retail checkout counter.  These two technologies greatly improved the effectiveness of the check-out procedure and improved inventory management as well, through the associated computer accounting of stock.  This was one of the factors leading to the technique of just-in-time inventory management for retail, commercial and industrial undertakings. The payment of consumer purchases at the retail checkout counter through the use of an electronic debit card was introduced across Canada in 1994.  Known as Interac, the system allows the consumer to swipe his personal card and with the use of a personal identification number have the amount of the purchase electronically deducted from his or her bank account.  The service has since become very popular. Although the concept of recycling waste materials was not new, the Blue Box Recycling System for domestic refuse collection made the idea highly visible.  Initially developed by Laidlaw Waste Systems for the Kitchener, Ontario, in 1983, it was introduced to Ontario municipalities in 1986, by Ontario Multi-Material Recycling Incorporated (OMMRI), and promoted by Nyle Ludolph, who became known as the Father of the Blue Box.  The concept involved the use of blue plastic boxes which were distributed to home owners, who in turn filled them with recyclable refuse and placed them at curbside for weekly pickup.  The refuse was taken to specially designed plants, where materials were sorted and recycled.  The technique became popular in municipalities across Canada in the years that followed. During these years the municipal garbage dump evolved to become the sanitary landfill site.  A number of technologies, including clay and plastic liners were used to contain the smell and leachate.  The largest in Canada, the Keele Valley Landfill was operated by the City of Toronto from 1983 until 2002, when it was closed because it was full. Chlorofluorocarbons (CFCs), the gas propellants used in aerosol spray cans, became a target for environmental concern in the 1970s and 1980s when research demonstrated that they had a harmful effect on the ozone layer in the atmosphere.  The international Montreal Protocol of 1989 banned the use of these substances and they were subsequently replaced with volatile hydrocarbons. The problem with choice to reuse is still not available.  There are many products that don't need to be recycled for a hundred years, but are put out monthly. Large architectural works of note included BC Place, Vancouver, 1983, Petro-Canada Centre, West Tower, Calgary, 1984, the West Edmonton Mall, Edmonton, Alberta, 1986, Scotia Plaza, Toronto 1988, the Canterra Tower, Calgary, 1988, the Sky Dome, Toronto, 1989, Bankers Hall, Calgary, 1989, BCE Place–Canada Trust Tower, Toronto, 1990, the Bay Wellington Tower, Toronto, 1990, Tour du 1000 de la Gauchetière, Montreal, 1991, Tour IBM-Marathon, Montreal, 1992 and GM Place, Vancouver, 1995. New arenas for Canada's National Hockey League teams were built, including GM Place, Vancouver, home of the Vancouver Canucks in 1995, the Corel Centre in Ottawa, home of the Ottawa Senators and Molson Centre in Montreal, new home of the Montreal Canadiens, both in 1996. Significant new bridges included the Alex Fraser Bridge, Vancouver, 1986, the Skybridge (TransLink), Vancouver, 1989 and the Confederation Bridge, NB-PEI, 1997. In the earlier parts of Canada's history, the state often played a crucial role in the diffusion of these technologies, in some cases through a monopoly enterprise, in others with a private \"partner\".  In more recent times the need for the role of the state has diminished in the presence of a larger private sector. In the latter part of the 20th century there is evidence that Canadian values prefer public expenditures on social programmes at the expense of public spending on the maintenance and expansion of public technical infrastructure.  This can be seen in the fact that in 2008 the Federation of Canadian Municipalities estimated that it would take $123 billion to restore and repair aging urban infrastructure across Canada.\n\nList of former state routes in Georgia (U.S. state) This is a List of former state routes in the U.S. state of Georgia.  This list represents routes that traveled through the state but are no longer in operation, have been decommissioned, or have been renumbered. State Route 1E (SR 1E) was a route in Floyd County extending along present-day Old Cedartown Road, Park Avenue, Maple Avenue, East Sixth Street, and 2nd Avenue from US 27/SR 1 to US 27/SR 1, as well as US 411/SR 53.  Originally part of US 27/SR 1, it was originally designated as SR 1 before being reassigned as SR 1E in 1955.  US 27/SR 1 had been relocated along former SR 1 Spur, which is present-day US 27/SR 1 from Old Cedartown Road to present-day US 411 in 1938 with the old route remaining as SR 1 through Lindale.  When US 27 was upgraded and relocated in 1968 to the Rome Connector, SR 1E was truncated to end at the present-day interchange of Maple Avenue.  Essentially a business route for an unincorporated community south of Rome, the state no longer saw the value in retaining an old alignment and transferred control to Floyd County in 1985. State Route 3W (SR 3W) was a state highway in the city of Albany.  It traversed portions of Dougherty and Lee counties.  At least as early as 1919, SR 3 traveled on essentially the same path as it currently does in the Albany metropolitan area.  By the end of 1926, the segment of the highway from the Mitchell–Dougherty county line to Albany had a \"completed hard surface\".  By the end of 1929, US 19 was designated on this stretch of SR 3. By the end of 1946, SR 3W was projected to be designated from the western part of Albany to US 19/SR 3 north of the city.  By the middle of 1950, the entire length of SR 3W was hard surfaced.  By July 1957, SR 3 in the northern part of Albany was redesignated as SR 3 Conn. due to SR 3W being redesignated as part of SR 3. State Route 3W (SR 3W) was a state highway that existed in the city of Albany.  It traversed portions of Dougherty and Lee counties.  At least as early as 1919, SR 3 traveled on essentially the same path through the city as it currently does.  By the end of 1926, the segment from the Mitchell–Dougherty county line to Albany had a \"completed hard surface\".  By the end of 1929, US 19 was designated on it through the Albany metropolitan area. By the end of 1946, SR 3W was projected to be designated from the western part of Albany to US 19/SR 3 north of the city.  By April 1949, the southern part of SR 3W was hard surfaced, while its northern part had completed grading, but was not surfaced.  By the middle of 1950, the entire length of SR 3W was hard surfaced.  By July 1957, SR 3 in the northern part of Albany was redesignated as SR 3 Conn. due to SR 3W being redesignated as part of SR 3.  By June 1960, SR 3 Conn. was redesignated as part of the SR 3 mainline, and its old path was redesignated as the second SR 3W in the city.  In 1973, SR 3W was redesignated as part of SR 3, while its former path was redesignated as part of SR 133. State Route 3W (SR 3W) was a state highway that existed in the city of Thomaston, in Upson County.  At least as early as 1919, SR 3 traveled on essentially the same path through the city as it currently does.  By the end of 1926, the segment of SR 3 through Thomaston had a \"completed hard surface\".  By the end of 1929, US 19 was designated on this segment.  By June 1963, the path of SR 3 in Thomaston was split into SR 3W and SR 3E.  It was unclear as to which highway US 19 traveled on.  In 1987, SR 3W was redesignated as SR 3S. State Route 3E (SR 3E) was a state highway that existed in the city of Thomaston, in Upson County.  At least as early as 1919, SR 3 traveled on essentially the same path through the city as it currently does.  By the end of 1926, the segment of SR 3 through Thomaston had a \"completed hard surface\".  By the end of 1929, US 19 was designated on this segment.  By June 1963, the path of SR 3 in Thomaston was split into SR 3W and SR 3E.  It was unclear as to which highway US 19 traveled on.  In 1987, SR 3E was redesignated as SR 3N. State Route 3N (SR 3N) was a short-lived state highway that existed in the city of Thomaston, in Upson County.  At least as early as 1919, SR 3 traveled on essentially the same path through the city as it currently does.  By the end of 1926, the segment of SR 3 through Thomaston had a \"completed hard surface\".  By the end of 1929, US 19 was designated on this segment.  By June 1963, the path of SR 3 in Thomaston was split into SR 3W and SR 3E.  It was unclear as to which highway US 19 traveled on.  In 1987, SR 3E was redesignated as SR 3N.  In 1988, SR 3N in Thomaston was redesignated as the northbound lanes of SR 3. State Route 3S (SR 3S) was a short-lived state highway that existed in the city of Thomaston, in Upson County.  At least as early as 1919, SR 3 traveled on essentially the same path through the city as it currently does.  By the end of 1926, the segment of SR 3 through Thomaston had a \"completed hard surface\".  By the end of 1929, US 19 was designated on this segment.  By June 1963, the path of SR 3 in Thomaston was split into SR 3W and SR 3E.  It was unclear as to which highway US 19 traveled on.  In 1987, SR 3W was redesignated as SR 3S.  In 1988, SR 3S in Thomaston was redesignated as the southbound lanes of SR 3. State Route 3W (SR 3W) was a state highway that existed in Atlanta and Marietta.  It traversed portions of Fulton and Cobb counties.  At least as early as 1919, SR 3 traveled on essentially the same path through this area as it currently does.  By the end of 1926, US 41 had been designated on this segment of the highway.  The Atlanta–Marietta segment had a \"completed hard surface\". Late in 1937, SR 3 was split into two parts between Atlanta and the northwest part of Marietta.  US 41/SR 3 traveled northwest on the original path, while SR 3E traveled north-northwest on a more eastern path between the two cities.  By the end of the year, SR 3W was established, traveling northwest with US 41 on Marietta Street and Old Marietta Road.  By the end of 1946, SR 3W was redesignated as part of the SR 3 mainline. State Route 3W (SR 3W) was a short-lived state highway that existed in Atlanta and Marietta.  It traversed portions of Fulton and Cobb counties.  At least as early as 1919, SR 3 traveled on essentially the same path through these cities as it currently does.  By the end of 1926, US 41 had been designated on this segment of SR 3.  It had a \"completed hard surface\". Late in 1937, SR 3 was split into two parts between Atlanta and the northwest part of Marietta.  US 41/SR 3 traveled northwest on the original path, while SR 3E traveled north-northwest on a more eastern path between the two cities.  By the end of the year, SR 3W was established, traveling northwest with US 41 on Marietta Street and Old Marietta Road, while SR 3E traveled north-northwest on Hemphill Street and Northside Drive.  By the end of 1946, SR 3W was redesignated as part of the SR 3 mainline.  By June 1954, the second SR 3W in this area was designated between the two cities.  By June 1955, it was redesignated as part of SR 3. State Route 3E (SR 3E) was a state highway that existed in Atlanta and Marietta.  It traversed portions of Fulton and Cobb counties.  At least as early as 1919, SR 3 traveled on essentially the same path through these two cities as it currently does.  By the end of 1926, US 41 had been designated on this entire segment of SR 3, which had a \"completed hard surface\". Late in 1937, SR 3 was split into two parts between Atlanta and the northwest part of Marietta.  US 41/SR 3 traveled northwest on the original path, while SR 3E traveled north-northwest on a more eastern path between the two cities.  SR 3E's path from SR 120 in the east part of Marietta to US 41/SR 3 in the northwestern part of the city.  The rest of SR 3E was under construction.  By the end of the year, SR 3W was established, traveling northwest with US 41 on Marietta Street and Old Marietta Road, while SR 3E traveled north-northwest on Hemphill Street and Northside Drive.  All of SR 3E in the northern part of Atlanta was hard surfaced.  From the north part of the city to the northwest part, the highway had completed grading, but was not surfaced.  Later that year, all of SR 3E from Atlanta to northwest of the Fulton–Cobb county line had a completed hard surface. In 1940, nearly the entire segment of SR 3E in Marietta had a completed hard surface.  It was under construction from northwest of the Fulton–Cobb county line to the eastern part of Marietta.  By the end of the next year, the entire length of SR 3E had a completed hard surface.  By February 1948, SR 3E was moved off of Hemphill Avenue.  It, along with US 41 Temp. , followed US 19 on Spring Street, then traveled west on 14th Street and resumed the Northside Drive path.  By April 1949, US 41 Temp. /SR 3E's southbound lanes traveled on Hemphill Avenue.  By the middle of 1950, US 41 Temp. /SR 3E was shifted off of US 19 on Spring Street and 14th Street, and traveled on Hemphill Avenue again.  In 1952, US 41 Temp.  was redesignated as part of the US 41 mainline.  In 1985, SR 3E was decommissioned. State Route 3S (SR 3S) was a state highway that existed in the city limits of Marietta in Cobb County, Georgia.  By the end of 1965, it was designated from SR 3 to SR 5.  Between 1974 and March 1980, SR 3S was redesignated as SR 3 Spur. State Route 4 (SR 4) was a state highway that was formed at least as early as 1919.  It began at the Alabama state line, traveled to the east-southeast and intersected SR 1 in Rome.  It then curved to the southeast and reached its eastern terminus, an intersection with SR 3 in Cartersvile.  By the end of 1921, SR 4 was extended southeast into the main part of Cartersville, on a concurrency with SR 3 and headed north-northeast to an intersection with SR 2/SR 53 in Fairmount.  By the end of 1926, almost all of the Alabama–Rome segment, and the southern half of the 1921 extension, had a \"sand clay or top soil\" surface.  In the vicinity of Rome, the highway had a \"completed hard surface\".  About half of the Rome–Cartersville segment was under construction.  The easternmost part of this segment, and nearly the entire SR 3 concurrency, had a \"completed semi hard surface\".  By the end of 1929, the entire length of the original segment of SR 4 was redesignated as SR 20, with US 41W designated along the Rome–Cartersville segment.  US 41 was designated along the former SR 3 concurrency.  The 1921 extension was redesignated as SR 61. State Route 7W (SR 7W) was a state highway that existed in the south-central part of the state.  Between June 1960 and June 1963, the path of SR 7 through the Cordele area was split into SR 7W and SR 7E.  SR 7W traveled through the western part of the city.  In 1985, it was decommissioned. State Route 7E (SR 7E) was a state highway that existed in the south-central part of the state.  Between June 1960 and June 1963, the path of SR 7 through the Cordele area was split into SR 7W and SR 7E.  SR 7E traveled through the main part of the city, concurrent with US 41.  In 1985, it was redesignated as part of the SR 7 mainline. State Route 9E (SR 9E) was a 21.319 mi state highway.  It was originally constructed early in 1941.  In July 1981, as the extension of SR 400 had reached SR 60 south-southeast of Dahlonega, this designation was decommissioned. The routing that was followed by SR 9E starts where Hopewell Road splits from the current SR 9 north-northeast of Coal Mountain in Forsyth County, and parallels SR 400 very closely.  The road changes names to Lumpkin Campground Road as it enters Dawson County, passes by the North Georgia Premium Outlet Mall, then crosses SR 53 and SR 400 in rapid succession.  Just before intersecting with SR 136 the road changes names again to Harmony Church Road, then is called Auraria Road as it becomes SR 136.  The road crosses SR 400 once more to its west, parts ways with SR 136, then travels north through the community of Auraria into Lumpkin County to its northern terminus at SR 9/SR 52 west of Dahlonega. State Route 11E (SR 11E) was a state highway that existed entirely within Bibb County in the Macon metropolitan area.  It functioned like an eastern alternate route of US 41/SR 11/SR 49.  In 1953, it was established from US 41/SR 11/SR 49/SR 247 south-southwest of Macon to US 41/SR 11/SR 49 in the city.  Between June 1960 and June 1963, SR 49 was shifted eastward, off of US 41/SR 11 and replacing SR 11E. State Route 13W (SR 13W) was a state highway that existed in the Atlanta metropolitan area.  Between the beginning of 1945 and November 1946, it was established from US 19/SR 9 north of Atlanta to the southwestern corner of North Atlanta.  Between February 1948 and April 1949, its northern terminus was extended to US 23/SR 13 east-northeast of North Atlanta.  In 1971, SR 13W was redesignated as part of SR 141 (Peachtree Road/Peachtree Industrial Boulevard) and SR 13 Conn. State Route 15W (SR 15W) was a short-lived state highway that existed completely within the city limits of Athens.  It functioned like a bypass of downtown.  Between the beginning of 1945 and November 1946, it was established from US 129/US 441/SR 15/SR 24 north-northwest to US 129/SR 15.  Between February 1948 and April 1949, the path of SR 15 was shifted westward, replacing SR 15W. State Route 16S (SR 16S) was a state highway that existed in portions of Jefferson, Glascock, and Warren counties.  In 1942, it was established from an intersection with SR 16 west-northwest of Wrens to another intersection with SR 16 southeast of Warrenton.  A decade later, the path of SR 16 southeast of Warrenton was shifted southward, replacing the path of SR 16S.  The portion from southeast of Warrenton to northwest of Wrens was redesignated as SR 16 Conn., while the portion from northwest of Wrens to north of Louisville was redesignated as SR 16 Conn. State Route 20 (SR 20) was a very short-lived state highway that traveled from Gray to Sparta.  It was formed at least as early as 1919 and was decommissioned in 1921 and redesignated SR 22.  It began at an intersection with SR 11 in Gray.  It traveled to the east-northeast and entered Milledgeville, where it intersected SR 24 and SR 29.  It then went northeast to Sparta, where it met its eastern terminus, an intersection with SR 15 and SR 16. State Route 20 (SR 20) was a state highway in the east-central part of the state.  At least as early as 1919, part of SR 24 was established from Louisville to Waynesboro.  By the end of September 1921, the path of SR 24, from Waynesboro to Louisville, was shifted northwestward.  The former path of SR 24 was redesignated as SR 20.  Between October 1926 and October 1929, the path of SR 24, from Augusta to Louisville, was reverted to the Waynesboro–Louisville path, replacing SR 20.  SR 24's former path, on US 1, was redesignated as part of SR 4. State Route 26E (SR 26E) was a state highway that existed in the eastern part of Chatham County, in Savannah Beach, which is what Tybee Island was known at the time.  The roadway that would eventually become SR 26E was established between June 1963 and the end of 1965, as SR 26 Loop, between two intersections with US 80/SR 26 in the southern part of the city.  In 1969, it was redesignated as SR 26E.  In 1985, SR 26E was decommissioned. State Route 27S (SR 27S) was a state highway that existed in the central part of Dodge County, southwest of Eastman.  Between June 1963 and the end of 1965, it was established from US 341/SR 27 west of Eastman to US 23/US 341/SR 27 southeast of the city.  In 1977, it was decommissioned. State Route 28 (SR 28) was a state highway that traveled from Georgetown to Vienna.  It was established at least as early as 1919 and was decommissioned in 1937.  It began at an intersection with SR 39 in Georgetown.  It traveled to the northeast and intersected SR 1 in Lumpkin.  It curved to the east-southeast through Preston.  In Americus, the highway intersected SR 3 and SR 26.  It headed to the east and entered Vienna, where it met its eastern terminus, an intersection with SR 7.  By the end of 1926, a segment just east of Preston was under construction.  The eastern half of the Preston–Americus segment had a completed hard surface.  The Dooly County portion of the highway had a sand clay or topsoil surface.  By the end of 1929, a segment just east of Lumpkin and a segment just west of Preston had a sand clay or topsoil surface.  By the middle of 1930, SR 28 was extended northeast from Vienna to Hawkinsville.  A few months later, the Richland–Preston segment was under construction.  By 1932, US 280 was designated on the Richland–Americus segment.  Near the end of the year, the entire Vienna–Hawkinsville segment had a sand clay or topsoil surface.  By the end of 1937, s segment just southwest of Hawkinsville had a completed hard surface.  Just a few months later, all of SR 28 had been redesignated as SR 27. State Route 34 (SR 34) was a short-lived state highway in the western part of the state.  It was established at least as early as 1919 and was decommissioned in 1926.  When it was established, it extended from SR 1 and SR 16 in Carrollton to SR 8 in Villa Rica.  In 1926, its entire length consisted of a \"sand clay or top soil\" surface and was redesignated as a southern branch of SR 8.  Within three years, US 78S had been designated along the path of SR 8's southern branch.  A decade later, US 78S had been redesignated as US 78 Alt.  Nearly another decade later, SR 8 had been redesignated as SR 8 Alt.  By the beginning of 1953, US 78 Alt.  had been decommissioned.  By the middle of 1954, SR 8 Alt.  had been redesignated as SR 166 from Carrollton to northeast of the city and SR 61 from that point to Villa Rica, as they travel today. State Route 36 (SR 36) was a state highway that originally existed from Danielsville to Elberton, when it was established at least as early as 1919.  By the middle of 1930, its western terminus was shifted southwestward into Athens.  By the end of the year, the western terminus had been reverted to Danielsville and extended northwest to Commerce.  The Athens–Comer segment was redesignated as SR 82.  In 1941, all of SR 82 and the Comer–South Carolina segment of SR 36 had been redesignated as SR 72, while the Commerce–Comer segment of SR 36 had been redesignated as SR 98. State Route 42A (SR 42A) was a state highway in Atlanta.  It was entirely concurrent with US 29/US 78/SR 8/SR 10/SR 12 (Ponce de Leon Avenue).  It was formed in 1941 and decommissioned only five years later, in 1946.  It began at an intersection with US 19/SR 9 in the northern part of the city.  From there, US 29/US 78/SR 8/SR 10/SR 12/SR 42A traveled to the east to an intersection with SR 42.  Here, SR 42A ended, and US 29/US 78/SR 8/SR 10/SR 12 continued to the east. State Route 43 (SR 43) was a state highway that originally existed from a point north-northwest of Gainesville, north-northeast to Cleveland, and then northwest to Turners Corner, when it was established at least as early as 1919.  Its original southern terminus was at SR 11 north-northwest of Gainesville, and its original northern terminus was at SR 9 in Turners Corner.  By the end of 1921, SR 11 and SR 43 were swapped in this area.  SR 11 took over the entire route of SR 43, while SR 43 was shifted to travel from a point north-northeast of Gainesville northwest to SR 9 just northeast of Dahlonega.  By the end of 1926, the southern part of the new path had a sand clay or top soil surface, and the rest of it had a completed semi hard surface.  By the end of 1929, the Lumpkin County portion of the highway had a completed hard surface.  By the middle of the next year, all of the highway was completed.  In 1941, SR 43 was redesignated as SR 52. State Route 44 (SR 44) was a short-lived state highway that only existed from the Alabama state line, northwest of Jakin, to Brinson.  It was established at least as early as 1919, and was decommissioned by the end of 1921.  It was redesignated as part of SR 38. State Route 45 (SR 45) was a short-lived state highway in the north-central part of the state.  It was established at least as early as 1919 on a path from SR 8 and SR 13 in Lawrenceville, south-southeast to Loganville, east-southeast to SR 11 in Monroe, and east-northeast to SR 15 in Watkinsville.  By the end of 1921, the Lawrenceville–Loganville segment was redesignated as a southern extension of SR 13, the western terminus was extended west-southwest to what was known as \"Ingleside\" (now known as Avondale Estates), and the eastern terminus was shifted to the northwest to end at SR 8 west-southwest of Athens.  By the end of 1926, the segment from Ingleside (now labeled as \"Avondale\") to Loganville and the segment from Monroe to the Athens area were redesignated as US 78/SR 10, while the Loganville–Monroe segment was also redesignated as US 78 and SR 13 (and possibly SR 10). State Route 46 (SR 46) was a short-lived state highway that started at SR 32 and SR 33 in Sylvester to SR 7 and SR 35 in Tifton.  It was established at least as early as 1919.  By the end of 1921, the entire highway was redesignated as part of SR 50. State Route 50N (SR 50N) was a state highway that existed in the city limits of Albany, within Dougherty County.  The roadway that would eventually become SR 50N was established at least as early as 1919 as SR 32 from Dawson through Albany and into Sylvester.  By the end of 1921, SR 50 was designated across the state.  This truncated SR 32 at Ashburn.  By the end of 1926, the portion of SR 50 in the eastern part of Albany had a \"completed hard surface\". By the middle of 1930, from west of Albany to the Worth–Tift county line, the highway had a completed hard surface.  The western half of the Dougherty County portion of the Dawson–Albany segment had a completed semi hard surface.  In January 1932, the Dawson–Albany segment had a completed hard surface. Between February 1948 and April 1949, US 82 was designated on SR 50 through the Albany area.  Between June 1960 and June 1963, the path of SR& 50 through Albany was split into SR 50N and SR 50S.  SR 50N used Broad Avenue and Sylvester Road, while US 82/SR 50S used Oglethorpe Avenue and Albany Expressway.  In 1973, SR 50N was redesignated as SR 50 Conn., while SR 50S was redesignated as the SR 50 mainline. State Route 50S (SR 50S) was a state highway that existed in the city limits of Albany, within Dougherty County.  The roadway that would eventually become SR 50S was established at least as early as 1919 as SR 32 from Dawson through Albany and into Sylvester.  By the end of 1921, SR 50 was designated across the state.  This truncated SR 32 at Ashburn.  By the end of 1926, the portion of SR 50 in the eastern part of Albany had a \"completed hard surface\". By the middle of 1930, from west of Albany to the Worth–Tift county line, the highway had a completed hard surface.  The western half of the Dougherty County portion of the Dawson–Albany segment had a completed semi hard surface.  In January 1932, the Dawson–Albany segment had a completed hard surface. Between February 1948 and April 1949, US 82 was designated on SR 50 through the Albany area.  Between June 1960 and June 1963, the path of SR& 50 through Albany was split into SR 50N and SR 50S.  SR 50N used Broad Avenue and Sylvester Road, while US 82/SR 50S used Oglethorpe Avenue and Albany Expressway.  In 1973, SR 50N was redesignated as SR 50 Conn., while SR 50S was redesignated as the SR 50 mainline. State Route 52 (SR 52) was a state highway in Columbia and Richmond counties, in the Augusta metropolitan area.  Between 1919 and 1921, SR 52 was designated from the South Carolina state line, northwest of Augusta, and the South Carolin state line again on the northeastern edge of the city.  Before 1926 ended, the entire length of SR 52 had a \"sand clay or top soil\" surface.  By the middle of 1930, in an area northwest of Augusta, SR 52 was shifted to a more western alignment.  This segment was located about half of the distance from the Columbia–Richmond county line and the original segment.  The original part northwest of the split did not have a highway number.  The highway had a \"completed hard surface\" from there to Augusta.  The year ended with all of SR 52 having a \"completed hard surface\".  The original part's Richmond County portion did, too.  By February 1932, the entire length of the highway had a completed hard surface.  Near the end of the year, the northwestern part of SR 52 was shifted back to its original alignment and re-signed as SR 52.  The western extended part was re-designated as SR 104.  Before 1938, all of SR 52 was redesignated as the southern segment of SR 28. State Route 54B (SR 54B) was a state highway just south of Atlanta.  When it was established in between 1919 and 1921, it extended from an intersection with SR 16 in Sharpsburg northeast to an intersection with SR 54 in Fayetteville.  By the end of 1926, the northern half had a sand clay or top soil surface.  Nearly a decade later, the entire length of SR 54B was redesignated as a re-routing of the SR 54 mainline. State Route 56 Spur (SR 56 Spur) was a 6.6 mi spur route that existed entirely within the southeastern part of Richmond County.  Its route was entirely within the city limits of Augusta.  It's west–east section was part of Tobacco Road.  It was known as Doug Barnard Parkway for the rest of its length.  Its entire length was within the city limits of Augusta.  Its southern terminus was at an intersection with the SR 56 mainline (Mike Padgett Highway).  Its northern terminus was at an intersection with US 1/US 25/US 78/US 278/SR 10/SR 121 (Gordon Highway) in downtown Augusta.  Here, the roadway continues as Molly Pond Road.  The highway was decommissioned in 2014. State Route 60 (SR 60) was a short-lived state highway in the southeastern part of the state.  When it was established between 1919 and 1921, it extended from SR 27 in Sterling northeast to an intersection with SR 25 south-southwest of Darien.  By the end of 1926, the highway was removed from the state highway system.  This short segment of highway would later be used as part of SR 99. State Route 60 (SR 60) was a very short state highway that existed entirely within Walton County.  The roadway that would eventually become SR 60 was built between 1921 and the end of 1926 as an unnumbered road from SR 11 in Social Circle to SR 12 southeast of the city.  The entire length of the highway had a \"sand clay or top soil\" surface.  In 1930, this road was designated as SR 60.  At the end of 1940, SR 60 was redesignated as SR 181. State Route 62 (SR 62) was a short-lived state highway in the northeastern part of the state.  It was proposed between 1919 and 1921 on a path from SR 11 at a point northwest of Jefferson, at approximately the location of Talmo, northeast to SR 15 in Homer.  By the end of 1926, SR 62 was established on this same path, with US 129 having been applied onto SR 11.  Within three years, this short highway had been decommissioned. State Route 63 (SR 63) was a state highway that existed in the east-central part of the state.  When it was established between 1919 and 1921, it only extended from SR 30 in Ellabell to SR 26 at a point that approximates today's location of Eden.  By the end of 1926, its termini were shifted to SR 30 in Lanier to US 80/SR 26 in Blitchton.  By the end of 1929, the highway's western portion had a \"sand clay or top soil\" surface, while its eastern portion was under construction.  Also, the western terminus was shifted again, to Pembroke.  The next year, the eastern portion had a sand clay or top soil surface.  At the end of the year, US 280 was designated along the entire path of SR 63.  Before 1934 ended, the western portion had completed grading, but was not surfaced.  Just a few months later the eastern portion of SR 63 was under the same condition.  About six months later, the eastern terminus area had a completed hard surface.  Near the end of 1936, the rest of the highway also had a completed hard surface.  About a year later, SR 30's length southeast of Pembroke was swapped with the entire length of SR 63.  That meant that SR 63 eastern-most terminus was now at US 17/SR 25 southeast of Clyde.  The portion of this \"new\" path just southeast of Pembroke, as well as the entire segment southeast of Clyde, was under construction.  Within a year, those under construction segments had completed grading, but were not surfaced.  By the middle of 1939, a small portion of the highway farther to the southeast of Pembroke had the same conditions.  Before the year ended, the rest of the highway's length also had the same conditions.  A few months later, most of the highway's length that today is within the boundaries of Fort Stewart was under construction.  Later in 1940, the segment from US 17/SR 25 to SR 144 southeast of Clyde had a completed hard surface.  About five years later, Fort Stewart was established.  Most of the state highways that traveled within the area now covered by the base were removed.  Due to this, SR 63 was split into two short segments: one from the northern edge of the base to Pembroke and one from Richmond Hill to the southeastern edge of the base.  By the end of 1948, state highways were re-established through the base, thereby reconnecting SR 63 as a single highway.  One year later, the eastern terminus of the highway was extended south-southeast to Fancy Hall.  Before 1953 ended, the Keller–Richmond Hill segment was hard surfaced.  In 1954, the segment from the northern edge of Fort Stewart to Pembroke was hard surfaced.  At the end of the decade, the Fancy Hall–Keller segment was paved.  Before 1966 began SR 63 Spur had been established from SR 63 southeast of Richmond Hill east to Fort McAllister.  In 1967, SR 67's path south of Pembroke was shifted to the east, taking over the entire path of SR 63; therefore, SR 63 Spur was redesignated as SR 67 Spur. State Route 63 Spur (SR 63 Spur) was a short-lived spur route of SR 63.  Before 1966 began, SR 63 Spur was established from the SR 63 mainline southeast of Richmond Hill east to Fort McAllister.  In 1967, SR 67's path south of Pembroke was shifted to the east, taking over the entire path of SR 63; therefore, SR 63 Spur was redesignated as SR 67 Spur.  In the middle 1970s, SR 144 was extended east and south-southeast, taking over the route of SR 67 southeast of Fort Stewart; therefore, SR 67 was redesignated as SR 144 Spur. State Route 65 (SR 65) was a state highway that formerly exited in the extreme northeastern part of the state.  At least as early as 1919, a local road was established between the North Carolina and South Carolina state lines in Rabun County.  By 1921, this road was designated as SR 65.  By February 1932, the entire length of SR 65 had a completed hard surface.  Near the end of the year, all of SR 65 was redesignated as SR 105.  This roadway would eventually be redesignated as the northern segment of SR 28. State Route 68 (SR 68) was a short-lived state highway in the north-central part of the state.  When it was established between 1919 and 1921, it extended from SR 9 in Cumming southeast to SR 13 in Buford.  In January 1932, SR 68 was decommissioned and redesignated as part of SR 20. State Route 69 (SR 69) was a very short state highway located entirely in Towns County in the extreme northern part of the state.  The highway traveled from US 76/SR 2 north to the North Carolina state line, where it became North Carolina Highway 69 (NC 69).  SR 69 followed the roadway currently designated as the concurrency of SR 17/SR 515.  It was formed in 1930, and was redesignated as part of SR 17 between 1957 and 1960. Between 1919 and 1921, the roadway that would eventually become SR 69 was established as an unnumbered road from SR 2 in Hiawassee to the North Carolina state line north of Hiawassee.  By the end of 1930, this road was designated as SR 69, with a completed semi hard surface.  A few years later, the highway's location was shifted a few miles to the west.  Its southern terminus was still at SR 2, but began northwest of Hiawassee, as it currently travels.  By the end of 1935, the highway had a completed hard surface.  Twenty years later, SR 17 north of US 76/SR 2 was shifted to the west to travel concurrently with SR 69.  Between 1957 and 1960, SR 69 was decommissioned, while SR 17 stayed on this segment of highway. State Route 70 (SR 70) was a state highway that existed in the east-central part of the state.  When it was established between 1930 and 1932, it extended from Lincolnton to the South Carolina state line.  At this time, the highway was under construction.  In early 1934, SR 70 was extended south-southwest to US 78/SR 10/SR 17 north-northwest of Thomson.  By the end of 1937, the segment of SR 70 from Lincolnton to the South Carolina state line had a \"sand clay or top soil\" surface.  In 1938, a small portion of the highway southwest of Lincolnton had a \"completed hard surface\".  Late in 1941, all of SR 70 was redesignated as SR 43. State Route 72 (SR 72) was a state highway in the west-central and central parts of the state.  It was established in 1930 on a path from US 19/SR 3 north-northwest of Thomaston to SR 18 in Barnesville.  Later that year, the western terminus was shifted southward into Thomaston.  At the end of 1933, SR 72 was extended northeast to Jackson.  A few months later, it was extended again, this time north-northeast to Covington.  Three years later, the entire length of the Thomaston–Barnesville segment had a \"completed hard surface\".  Later that year, SR 72 was extended southwest to SR 41 in Woodland.  The next year, the eastern terminus was under construction.  By the middle of 1939, the then-western terminus had a completed hard surface.  At this time, the then-eastern terminus had a \"sand clay or top soil\" surface.  Near the end of 1940, SR 72 was extended north-northwest along SR 41 to Manchester and then west-northwest to SR 85 in Warm Springs.  It was possibly also extended west-southwest to US 27/SR 1 in Pine Mountain, but GDOT maps didn't show a highway number for this segment of highway.  The entire extension had a completed hard surface.  Meanwhile, the eastern terminus was under construction.  By the end of the year, the eastern terminus had a completed hard surface.  A few months later, the entire Newton County portion that didn't have a hard surface was under construction.  By the end of 1941, the entire highway was redesignated as SR 36.  The Newton County portion that didn't have a hard surface had completed grading, but was not surfaced. State Route 73W (SR 73W) was a state highway that existed in the southwestern part of Bulloch County.  Between July 1957 and June 1960, it was established and paved between two intersections with US 25/US 301/SR 73.  It traveled north-northwest to an intersection with SR 46 and then northeast to its northern terminus.  In 1993, it was decommissioned. State Route 73E (SR 73E) was a state highway that existed in the southwestern part of Bulloch County.  Between July 1957 and June 1960, it was established on US 25/US 301 as a redesignation of SR 73.  It straddled the intersection with SR 46/SR 119.  In 1993, it was reverted to be part of SR 73. State Route 82 (SR 82) was a state highway that existed from the northern part of Athens to Comer, when it was established in 1930.  It had previously been a temporary western terminus of SR 36 before being redesignated as SR 82.  In 1941, all of SR 82 and the Comer–South Carolina segment of SR 36 had been redesignated as SR 72. State Route 82 (SR 82) was a short state highway that existed entirely within Elbert County.  At the end of 1940, it was established from SR 77 northeast of Elberton east to the South Carolina state line east-northeast of the city.  By the end of 1941, the entire highway had a \"completed hard surface\".  In 1943, the eastern terminus of SR 82 was shifted north-northwest on the South Carolina state line to a point southeast of Hartwell.  This new path also had a completed hard surface.  In 1970, it was redesignated as SR 368. State Route 85W (SR 85W) was a state highway that existed from south of Shiloh to Woodbury.  The highway that would eventually become SR 85W was established in 1930 as part of SR 85 from south of Shiloh to SR 41 in Warm Springs.  By the middle of 1933, the portion of the highway from south of Shiloh to Warm Springs had a \"sand clay or top soil\" surface.  The next year, the segment of the highway just south-southwest of Warm Springs was shifted westward to a curve into the city.  At the end of 1936, two segments were under construction: around Shiloh and just west-southwest of Warm Springs.  By the middle of 1937, a portion from south of Shiloh to Warm Springs was under construction.  Near the end of the year, part of the Waverly Hall–Warm Springs segment had completed grading, but was not surfaced.  By the end of 1939, the segment from south of Shiloh to Warm Springs had a completed hard surface. In 1940, SR 163 was built from Warm Springs to Woodbury.  By the middle of 1941, SR 163's segment just northeast of Warm Springs was under construction.  In 1942, a portion of SR 163 northeast of Warm Springs had completed grading, but was not surfaced.  By the end of 1946, SR 85 was shifted eastward to a more direct path between Columbus and Manchester.  Its old path between south of Shiloh and Warm Springs was redesignated as a southerly extension of SR 163. By the middle of 1950, US 27 Alt.  was designated on SR 163 from south of Shiloh to Warm Springs.  By 1952, SR 163 was redesignated as SR 85W.  That year, SR 85W's segment south of Warm Springs was reverted to being designated as SR 163.  The next year, this was undone.  Also, the segment of SR 85W from Warm Springs to Woodbury had completed grading, but was not surfaced. Between 1960 and 1963, US 27 Alt.  was shifted eastward onto SR 85E from south of Shiloh to Manchester.  About thirty-three years later, SR 85W was redesignated SR 85 Alt.  State Route 85E (SR 85E) was a state highway between south of Shiloh and Woodbury.  In 1935, SR 85 was extended southeast on SR 41 to Manchester and then north-northeast through Woodbury.  At the end of 1936, part of SR 85 around Shiloh was under construction. In 1940, SR 85, from Manchester to approximately halfway between it and Woodbury, was under construction.  At the end of 1941, a portion of SR 85 just east-northeast of Manchester had a completed hard surface.  At this time, a portion of the highway from south of Woodbury had completed grading, but was not surfaced.  In 1943, a portion northeast of Manchester had a completed hard surface.  The next year, a portion south of Woodbury had a sand clay or top soil surface.  By the end of 1946, SR 85 was shifted eastward to a more direct path between Columbus and Manchester.  Its old path between south of Shiloh and Warm Springs was redesignated as a southerly extension of SR 163.  The entire length of SR 85 from south of Shiloh to Chalybeate Springs had a completed hard surface.  A small portion north-northeast of Chalybeate Springs had a sand clay or top soil surface; the portion from there to Woodbury had a completed hard surface.  Between 1946 and 1948, the Chalybeate Springs–Woodbury segment had a completed hard surface. Between 1955 and 1957, SR 85 from south of Shiloh to Woodbury was redesignated as SR 85E.  Between 1960 and 1963, US 27 Alt.  was shifted eastward onto SR 85E from south of Shiloh to Manchester.  About thirty-two years later, SR 85E was redesignated as part of the SR 85 mainline again. State Route 86 (SR 86) was a short-lived state highway in the North Georgia mountains region of the north-central part of the state.  In 1930, SR 86 was established from Blue Ridge northeast to the North Carolina state line west-northwest of Ivy Log.  In January 1932, the entire length of SR 86 was under construction.  The next month, the western terminus of SR 86 was shifted eastward to begin northwest of Morganton.  By mid-1933, the portion of SR 86 from northwest of Morganton to Mineral Bluff had a \"sand clay or top soil\" surface.  Later that year, the entire length of SR 86 had a completed semi hard surface.  In 1936, the entire length of SR 86 was under construction.  At the beginning of 1937, SR 86 was extended southeast to US 19/SR 9 in Porter Springs.  A few months later, SR 86's original segment had completed grading, but was not surfaced.  In late 1940, all of SR 86 was redesignated as SR 60.  About 37 years later, the original segment of SR 86, from Mineral Bluff to the state line was used for the path of SR 60 Spur. State Route 91W (SR 91W) was a state highway in the southwestern part of the U.S. state of Georgia.  It functioned like an alternate route of SR 91.  Between the beginning of 1945 and November 1946, it was established from SR 91 south-southwest of Albany to SR 3W just west of the city.  By February 1948, the entire length of the highway was hard surfaced.  In 1973, SR 234 was extended to the east, absorbing all of SR 91W. State Route 105 (SR 105) was a state highway that existed entirely within Rabun County in the northeastern part of the state.  The road that would eventually become SR 105 was established at least as early as 1919 between the North Carolina and South Carolina state lines.  By 1921, the highway was signed as SR 65.  By February 1932, the entire length of SR 65 had a \"completed hard surface\".  Near the end of the year, all of SR 65 was redesignated as SR 105.  By the end of 1934, the entire length of SR 105 was under construction.  By the middle of 1937, SR 105 had completed grading, but was not surfaced.  Before 1938, all of SR 105 was redesignated as the northern segment of SR 28. State Route 131 (SR 131) was a state highway that was located in Glynn and McIntosh counties in the coastal part of the state.  The highway that would eventually become SR 131 was established between 1919 and 1921 as SR 60 from SR 27 north-northwest of Brunswick to SR 25 south-southwest of Darien.  By the end of 1926, it was decommissioned.  SR 131 was established in 1936 on what is currently SR 99 on an eastern curve between intersections with US 17/SR 25 in Darien and Eulonia.  Later that year, the portion from Darien to approximately Meridian was under construction.  In 1938, this segment had a \"completed hard surface\".  From approximately Meridian to approximately Valona, the highway had completed grading, but was not surfaced.  In late 1939, SR 131 was established on a segment from US 84/SR 50 west-northwest of Brunswick to SR 32 northwest of Brunswick and on a segment from US 25/US 341/SR 27 southwest of Darien to US 17/SR 25 south-southwest of Darien.  However, there is no indication if these were separate segments of the highway or extensions.  The segment from approximately Valona to Eulonia was under construction.  A few months later, the northern terminus of the southern segment was shifted eastward to a southwest–northeast routing.  Also, the western terminus of the central segment was shifted south-southwest to connect with SR 32 at US 25/US 341/SR 27 north-northwest of Brunswick.  By the end of 1941, the southern segment was under construction.  The central segment was indicated to be \"on system–not marked or maintained\".  The Valona–Eulonia segment of the northern segment had completed grading, but was not surfaced.  In 1943, the entire length of all three segments of SR 131 were redesignated as an extension of SR 99.  SR 131 was moved to an alignment from South Newport to east-northeast of it.  The entire length of this new segment had a completed hard surface.  By the end of 1946, the highway was extended east-southeast to the Harris Neck National Wildlife Refuge.  This extension had a completed hard surface.  By the end of the decade, SR 131 was extended west-southwest to Jones.  In 1953, the entire length of this extension had completed grading, but was not surfaced.  About a decade later, this segment was hard surfaced.  In 1977, it was decommissioned.  Twelve years later, the eastern part was decommissioned, as well. State Route 134 (SR 134) was a state highway that was located in Telfair and Wheeler counties.  It was established in early 1937 from US 341/SR 27 southwest of Towns to SR 15 in Jordan.  This segment of highway remained virtually unchanged for over a decade, when it was given a \"sand clay, top soil, or stabilized earth\" surface.  By early 1949, SR 134 was established on a segment from SR 149 south-southeast of McRae to US 341/SR 27 northwest of Lumber City.  However, there is no indication if the two segments were separate or were connected via a concurrency with US 341/SR 27 between them.  By the middle of 1950, US 23 was designated on US 341/SR 27 between the two segments.  In 1953, the original segment of SR 134 had completed grading, but was not surfaced.  The next year, this segment was hard surfaced.  Between 1957 and 1960, SR 15 at this segment's eastern end was shifted farther to the east.  Its former path was redesignated as part of SR 19.  Between 1960 and 1963, the newer segment of SR 134 was paved.  It wasn't until 1988 that the entire length of the highway was decommissioned. State Route 143 (SR 143) was a state highway in the northwestern and north-central parts of the state. The roadway that would eventually become SR 143 was established at least as early as 1919 as part of SR 1 from LaFayette to Trenton.  By the end of 1921, SR 1 west of LaFayette was shifted to the east and off its former alignment.  SR 53 was indicated to be a \"contingent road\" from LaFayette to Calhoun, with a concurrency with SR 1 in LaFayette.  By the end of 1926, a portion of SR 53 northwest of Calhoun had a \"completed semi hard surface\".  The decade ended with SR 53's path west of Calhoun being shifted farther to the south.  The segment of SR 53 that had existed from LaFayette to Villanow was redesignated as part of SR 2. In 1930, the portion of SR 2 from approximately Naomi to Villanow had a completed semi hard surface.  In February 1932, this segment's eastern end was shifted slightly to the north.  In 1934, SR 2 was extended to an undetermined point northwest of LaFayette.  A few years later, SR 143 was established on SR 53's former path from Vilanow to Calhoun.  Later that year, SR 2 was extended farther to the northwest.  Its southeast part (northwest of LaFayette) was under construction, while its northwest part had completed grading, but was not surfaced.  By the middle of 1939, SR 2 was extended northwest to its intersection with SR 157.  The western two-thirds of its length in this area had completed grading, but was not surfaced.  Near the end of the year, SR 2 was extended north-northwest to an intersection with US 11/SR 58 in Trenton.  The eastern part of this extension was under construction. In early 1940, this last extension of SR 2 had completed grading, but was not surfaced.  Around the middle of the year, the portions of SR 2 both north-northwest and east-southeast of the SR 157 intersection had a \"completed hard surface\".  The next year, nearly the entire portion of SR 2 from just south-southeast of Trenton to just southeast of Cooper Heights had a completed hard surface.  Later that year, SR 143 was designated on an eastern alignment from SR 53 east of Fairmount to SR 5 and SR 53 in Tate.  By the end of 1946, SR 2 was extended to the Alabama state line.  Also, its segment from LaFayette to Villanow was hard surfaced.  By early 1948, all of SR 2 west-southwest of Trenton, the entire western segment of SR 143, and the eastern half of the eastern segment of SR 143, had a \"sand clay, top soil, or stabilized earth\" surface.  The western half of its eastern segment was indicated to be \"projected mileage\".  By the middle of 1949, SR 2 was shifted much farther to the north.  Its former alignment from the Alabama state line to Villanow was redesignated as a western extension of the western segment of SR 143.  A portion northwest of LaFayette was hard surfaced. By the middle of 1950, a portion just east-southeast of Trenton was hard surfaced.  In 1953, the portion west-southwest of Trenton and the portion from Sugar Valley to Calhoun were hard surfaced.  The eastern segment's portion east of the SR 156 intersection had completed grading, but was not surfaced.  The next year, this last segment was hard surfaced.  By mid-1955, the Gordon County segment of the western segment (from Villanow to Sugar Valley) and the portion of the eastern segment (west of the SR 156 intersection) had completed grading, but was not surfaced. By the end of 1960, the entire western segment was hard surfaced.  Nearly the entire part of the eastern segment west of the SR 156 intersection was decommissioned.  By the end of 1963, the decommissioned part of the eastern segment was re-instated.  In 1970, a portion of the eastern segment southeast of the SR 53 intersection was hard surfaced.  In 1973, the portion of the eastern segment was decommissioned was indicated to be \"under construction or projected mileage\".  In 1977, all of the western segment from the Alabama state line to northwest of Sugar Valley was redesignated as part of SR 136.  All of the western segment from northwest of Sugar Valley to Calhoun was redesignated as SR 136 Conn. The eastern segment from its western terminus to northeast of Sharp Top was redesignated as SR 379; northeast of this point to west of Tate was redesignated as part of SR 108; and from there to Tate was redesignated as SR 108 Conn. State Route 143 Connector (SR 143 Conn.) was a connecting route in the northwestern part of the state. The roadway that would eventually become SR 143 Conn. was established between 1963 and 1966 as an unnumbered road from SR 143 northwest of Sugar Valley to Resaca.  In 1972 it was designated as SR 143 Conn., ending at US 41/SR 3.  In 1977, all of SR 143 west of a point northwest of Sugar Valley and all of SR 143 Conn. were redesignated as parts of SR 136. State Route 148 (SR 148) was a state highway in the northwestern part of the state.  The highway that would eventually become SR 148 was established between 1919 and the end of 1921 as an unnumbered road from SR 1 in Fort Oglethorpe to SR 3 in Ringgold.  By the end of 1926, US 41 was designated on SR 3.  The entire length of the highway had a \"completed semi hard surface\".  In 1930, US 41W was designated on SR 1.  By the end of 1934, US 41W was decommissioned.  It was redesignated as part of US 27.  By the middle of 1939, the unnumbered road was designated as SR 148.  1940 ended with the entire length of SR 148 having a \"completed hard surface\".  By the end of 1949, SR 2 was shifted to the north, replacing the entire length of SR 148. State Route 148 (SR 148) was a state highway in the central part of the state.  SR 148 was designated between the middle of 1954 and the middle of 1955 from SR 18 east-southeast of Forsyth to SR 87 eas of Bolingbroke.  Its entire length had a \"completed hard surface\".  Between 1957 and 1960, Interstate 75 (I-75) and SR 401 were built on a northeastern bypass of Forsyth.  The southern terminus of this bypass connected with the western terminus of SR 148.  Between 1960 and 1963, I-75 (and possibly SR 401) was extended southeast to just northeast of Bolingbroke, replacing SR 148 northwest of this point.  Between 1963 and the end of 1966, I-75 was extended southeast through the Macon area, replacing SR 148 from northeast of Bolingbroke to east of this community (between the I-475 interchange northwest of Bolingbroke and SR 19 Spur east of the community, I-75 was under construction).  The remainder of SR 148 was redesignated as part of SR 19 Spur. State Route 154 (SR 154) was a state highway in the north-central part of the state.  It was established in late 1939 from an intersection with SR 156 in Blaine to SR 5 in Talking Rock.  The next year, it was under construction.  Before the year ended, it was established on an eastern alignment from SR 108 northeast of Jasper to SR 183 northwest of Dawsonville.  Around the middle of 1941, this new segment was under construction.  In 1942, the original segment had completed grading, but was not surfaced.  By the end of 1946, both segments of SR 154 were redesignated as parts of SR 136. State Route 158 (SR 158) was a state highway in the northwestern part of the state.  In 1940, SR 158 was established from US 41/SR 3 in Tunnel Hill to SR 71 in Varnell.  At the end of the next year, it was redesignated as part of SR 201. State Route 160 (SR 160) was a state highway in the central part of the state.  In 1940, SR 160 was established from SR 78 south of Adrian to SR 46 west of Oak Park.  Later that year, SR 160's segment from south of Adrian to SR 56 northeast of Soperton had a \"completed hard surface\".  The eastern part of the highway was under construction.  By the end of 1941, all of SR 160 was redesignated as SR 86, with the portion from northeast of Soperton to west of Oak Park having a completed hard surface. State Route 160 (SR 160) was a state highway in northern Clayton and southwestern DeKalb counties.  SR 160 was established from SR 54 in Forest Park to SR 42 south-southeast of Constitution.  By the end of 1949, it was extended south-southeast on SR 54 and then west-northwest to US 19/US 41/SR 3.  In 1995, the western segment was decommissioned completely, while the eastern segment was redesignated as SR 54 Conn. State Route 161 (SR 161) was a state highway in central Polk County and southwestern Floyd counties.  Its southern terminus was in Cedartown.  It proceeded northwest to Cave Spring to an intersection with US 411/SR 53.  It was established with a \"completed hard surface\" in 1939.  Between 1960 and 1963, it was redesignated as part of an extended SR 100. State Route 163 (SR 163) was a state highway in the west-central part of the state.  The highway that would eventually become SR 163 was established as SR 85 from south of Shiloh to SR 41 in Warm Springs.  By the middle of 1933, the entire highway had a \"sand clay or top soil\" surface.  The next year, the segment of the highway just south-southwest of Warm Springs was shifted westward to a curve into the city.  At the end of 1936, two segments were under construction: around Shiloh and just west-southwest of Warm Springs.  By the middle of the year, a portion from south of Shiloh to Warm Springs was under construction.  Near the end of the year, the segment form south of Shiloh to Warm Springs had completed grading, but was not surfaced.  1939 ended with the segment from south of Shiloh to Warm Springs having a completed hard surface. In 1940, SR 163 was built from Warm Springs to Woodbury.  By the middle of 1941, SR 163's segment just northeast of Warm Springs was under construction.  In 1942, a portion of SR 163 northeast of Warm Springs had completed grading, but was not surfaced.  By the end of 1946, SR 85 was shifted eastward to a more direct path between Columbus and Manchester.  Its old path between south of Shiloh and Warm Springs was redesignated as a southerly extension of SR 163. By the middle of 1950, US 27 Alt.  was designated on SR 163 from south of Shiloh to Warm Springs.  By 1952, SR 163 was redesignated as SR 85W. State Route 167 (SR 167) was a state highway that existed on a southeast-to-northwest path from the Savannah metropolitan area to Millen.  In early 1940, the highway was established on a path from US 17/SR 25 southwest of Savannah and then north-northwest to US 280/SR 26 west-northwest of the city.  In 1942, US 280 was truncated to the west-northwest; its path through this area was redesignated as an east-southeast extension of US 80.  SR 167 was extended west-northwest on US 80/SR 26 to just west-northwest of the Chatham–Effingham county line and then on a solo path to the north-northwest to the Effingham–Screven county line.  The original segment was indicated to be \"on system–not marked or maintained\".  The entire concurrency with US 80/SR 26 and its solo trek from there to Guyton had a \"completed hard surface\".  The next year, SR 167 was extended northwest to Millen.  In 1944, a small portion of the highway north-northwest of Guyton had a completed hard surface.  By the end of 1948, the original segment was indicated to have \"projected mileage\".  A small portion between Guyton and Egypt had a \"sand clay, top soil, or stabilized earth\" surface.  Another small portion northwest of that one had completed grading, but was not surfaced.  By 1952, the segment northwest of US 80/SR 26 was redesignated as a southeast extension of SR 17, with a portion southeast of Millen having completed grading, but not being surfaced.  By the middle of 1955, the original segment of SR 167 was decommissioned. State Route 170 (SR 170) was a state highway that was located in the northwestern part of the state, in Dade and Walker counties.  At the end of 1940, it was established on a path from SR 157 south-southeast of Trenton east-south to a point just west of SR 193 southeast of the city.  About a year later, the entire length was under construction.  In 1945, the eastern terminus was shifted north-northwest to end at an intersection with SR 193 east-northeast of Trenton.  By the end of 1948, the eastern terminus of the highway was truncated to end at a point east of Trenton.  By the end of 1951, the eastern terminus was extended north-northeast and then northwest to end at another intersection with SR 157 west of Chattanooga Valley.  This made the \"eastern\" terminus now the \"northern\" one.  Most of this extension had a \"sand clay, topsoil, or stabilized earth\" surface.  The northern part of it had completed grading, but was not surfaced.  The portion east of the southern terminus was hard surfaced.  In 1953, the entire extension had completed grading, but was not surfaced.  By mid-1955, it had a sand clay, topsoil, or stabilized earth surface.  By the middle of 1957, this extension had a \"topsoil or gravel, unpaved\" surface.  Between 1960 and the end of 1963, the northern part of the extension was paved.  At the end of the decade, the entire length of SR 170 was hard surfaced.  In 1974, SR 157 was shifted eastward, replacing all of the north–south portion of SR 170, with the old alignment being redesignated as SR 189.  The east–west portion of SR 170 was simply decommissioned. State Route 175 (SR 175) was a state highway that existed in the south-central part of the state, in Lowndes and Lanier counties.  In 1940, it was established from SR 122 and SR 125 in Barretts to US 84/SR 38 in Naylor.  By the end of 1948, a portion of the highway from east-southeast of Barretts (at the Lowndes–Lanier county line) to west-northwest of the SR 31 intersection northwest of Naylor had completed grading, but was not surfaced.  From this point to the second crossing of the county line it had a \"sand clay, top soil, or stabilized earth\" surface.  About five years later, the western terminus was truncated to a point west-northwest of the SR 31 intersection.  By the middle of 1955, it was further truncated to the intersection with US 221/SR 31.  By the end of 1963, the entire remaining segment of highway had a \"topsoil or gravel, unpaved\" surface.  In 1969, SR 175 was decommissioned. State Route 176 (SR 176) was a state highway in the northwestern part of the state.  In late 1940, it was established from SR 120 in Lost Mountain to SR 92 in New Hope.  In 1942, the highway was extended south-southeast to SR 6 in Powder Springs.  The extension was indicated to be \"on system–not marked or maintained\".  The next year, the southern part of the extension had a \"completed hard surface\".  By the end of 1948, the entire length of the extension was hard surfaced.  A portion of the original segment just west of Lost Mountain had a \"sand clay, top soil, or stabilized earth\" surface.  By the middle of 1950, this portion was hard surfaced.  In 1953, a portion west of Lost Mountain had completed grading, but was not surfaced.  Two years later, the New Hope–Lost Mountain segment had a sand clay, topsoil, or stabilized earth surface.  By the middle of 1957, this segment was indicated to have a \"topsoil or gravel, unpaved\" surface.  Before the decade ended, the New Hope–Lost Mountain segment was paved.  Also, an unnumbered road was built from Lost Mountain to US 41/SR 3 in Acworth.  In 1969, the New Hope-to-Lost Mountain segment was shifted to the northeast onto this unnumbered road.  Its former alignment was redesignated as SR 92 Conn. In 2010, SR 176 was decommissioned. This table shows the last alignment of the highway. State Route 179 (SR 179) was a state highway that existed in the southwestern part of the state.  It traversed Grady and Baker counties.  At the end of 1940, SR 179 was established from SR 111 in Calvary to US 84/SR 38 in Whigham.  In 1942, it was extended north-northwest to just north of the Grady–Mitchell county line, and then west to SR 97 in Vada.  The entire highway was indicated as being \"on system–not marked or maintained\".  By the end of 1949, SR 262 was established on the Grady–Mitchell county line, replacing the east–west part of SR 179.  By the middle of 1950, a portion north-northwest of Whigham was hard surfaced.  Two small portions between Whigham and the Grady–Mitchell county line had a \"sand clay, top soil, or stabilized earth\" surface.  By the end of 1951, the southern two thirds of this segment was hard surfaced.  A portion south-southeast of Whigham had completed grading, but was not surfaced.  The next year, this portion near Whigham was hard surfaced.  By the middle of 1954, the entire Calvary–Whigham segment was hard surfaced.  A portion south of the SR 262 intersection was shifted eastward and had a sand clay, topsoil, or stabilized earth surface.  About a year later, this portion was hard surfaced.  In 1987, SR 179 was decommissioned. State Route 179 Connector (SR 179 Conn.) was a connector route of SR 179 that existed in the southwestern part of the state.  In 1969, it was established between US 27/SR 1 east-southeast of Amsterdam to SR 111 and SR 179 in Calvary.  In 1987, SR 179 Conn. was decommissioned. State Route 181 (SR 181) was a short-lived state highway that existed entirely in Walton County.  The roadway that would eventually become SR 181 was built between 1921 and the end of 1926 as an unnumbered road from SR 11 in Social Circle to SR 12 southeast of the city.  The entire length of this road had a \"sand clay or top soil\" surface.  In 1930, this road was designated as the entire length of SR 60.  At the end of 1940, it was redesignated as SR 181.  At the end of 1941, it was redesignated as SR 213. State Route 205 (SR 205) was a state highway that existed in the north-central part of the state.  It was assigned to Bells Ferry Road in Cherokee County.  Between 1946 and the end of 1948, it was established between SR 92 at a point southwest of Canton and SR 5 in the city.  By the middle of 1955, all of the highway except for the southern terminus was hard surfaced.  The portion at the southern terminus had completed grading, but was not surfaced.  About two years later, this southern part was paved.  In 1970, SR 92's segment between Acworth and Woodstock was shifted southward, and SR 205 was extended southward on SR 92's old alignment to SR 92's new path.  In 1985, it was decommissioned. State Route 207 (SR 207) was a 2.1 mi state highway that existed in the central part of the state, completely within Oconee County.  It is now known as Hog Mountain Road.  In 1942, SR 207 was established from SR 53 west-northwest of Watkinsville to US 129/SR 15/SR 24 north-northeast of the city.  Its entire length was indicated to be \"on system–not marked or maintained\".  The next year, the entire highway had a \"completed hard surface\".  In 1983, it was decommissioned. State Route 209 (SR 209) was a state highway in the Athens area.  It existed entirely within Oconee County.  In 1942, it was established from US 78/SR 10 southwest of Bogart, north-northwest to US 29/SR 8 in Bogart, and then northeast to the Oconee–Clarke county line.  This northern terminus was just south-southeast of the Oconee–Clarke–Barrow–Jackson county quadripoint.  The entire length of the highway was indicated to be \"on system–not marked or maintained\".  The next year, the southern half of the highway had a \"completed hard surface\".  By the end of 1946, the southern terminus was shifted to another intersection with US 78/SR 10, but at a point south-southeast of Bogart.  The entire length of this new part was hard surfaced.  The northern terminus was truncated to the US 29/SR 8 intersection in Bogart.  In 1983, SR 209 was decommissioned. State Route 210 (SR 210) was a very short state highway that was located in Lookout Mountain.  At the end of 1941, it was established from SR 157 and SR 193 just west of the city limits of Lookout Mountain and then east and northeast to the Tennessee state line, at the Chattanooga city limits.  The next year, the entire length of the highway had a \"completed hard surface\".  Between 1955 and the middle of 1957, it was shifted to a different alignment.  It traveled from SR 157 northwest to the Tennessee state line, at the Lookout Mountain city limits.  This new alignment was paved.  Between 1963 and 1966, the northern terminus was shifted slightly to the east.  The highway then traveled on a south-southwest to north-northeast direction.  In 1977, SR 210 was redesignated as part of SR 189. State Route 213 (SR 213) was a state highway that existed in the central part of the state.  It traversed parts of Walton, Newton, Jasper, and Morgan counties.  The roadway that would eventually become SR 213 was built between 1921 and the end of 1926 as an unnumbered road from SR 11 in Social Circle to SR 12 southeast of the city.  Its entire length had a \"sand clay or top soil\" surface.  In 1930, it was designated as SR 60.  In 1937, part of SR 142 was established on a path from Farrar to Newborn.  At the end of 1940, SR 60 was redesignated as SR 181.  The segment of SR 142 was under construction.  At the end of the next year, SR 181 was redesignated as SR 213.  In 1942, SR 142's segment had completed grading, but was not surfaced.  The next year, SR 213 was designated on a southern segment.  It extended from SR 36 south-southwest of Covington, then southeast and east to SR 11 in Mansfield.  It may have also been designated from Mansfield east-northeast to SR 142 in Newborn and then north-northwest to SR 12 east of Covington; however, these segments were not specifically designated on maps.  The SR 142 segment had a sand clay or top soil surface.  By the end of 1946, the northern segment of SR 213 was redesignated as SR 229, which was also designated on the segment of roadway from Newborn to east of Covington.  The Mansfield–Newborn segment of SR 213 was hard surfaced.  By the end of 1948, SR 213 was extended east-southeast from Newborn to SR 83 in Pennington.  From its western terminus to just west-southwest of Mansfield, and from east-southeast of Newborn to Pennington, the highway had a \"sand clay, top soil, or stabilized earth\" surface.  From just west-southwest of Mansfield to east-southeast of Newborn, which included the SR 142 segment (with which SR 213 had a brief concurrency), it was hard surfaced.  In 1953, the segment of SR 213 from east-southeast of Newborn to Pennington was hard surfaced.  From its western terminus to just west of Mansfield, it had completed grading, but was not surfaced.  Between 1955 and the middle of 1957, the western end of SR 213 was paved.  At the end of the 1950s, all of the highway was paved.  In 1982, SR 213 was decommissioned. State Route 213 Spur (SR 213 Spur) was a spur route of SR 213 that existed entirely in Pennington, which is southwest of Madison, in Morgan County.  Between 1960 and the end of 1963, SR 213 Spur was established in Pennington from SR 213 to SR 83.  In 1982, it was decommissioned. State Route 214 (SR 214) was a state highway that existed in the central part of the state.  It was entirely within Macon County.  In 1942, it was established from SR 26 east-southeast of Fountainville to another intersection with SR 26 in Oglethorpe.  The next year, its entire length had a \"completed hard surface\".  The highway remained virtually unchanged for the next 40 years.  In 1982, SR 214 was decommissioned. State Route 214 Bypass (SR 214 Byp.)  was a bypass route of SR 214 just west of Oglethorpe.  Between 1963 and 1966, it was established from SR 26/SR 49 southwest of the city to SR 214/SR 214 Spur northwest of it.  In 1982, SR 214 Byp.  was decommissioned and redesignated as the southern part of SR 128 Byp.  State Route 214 Spur (SR 214 Spur) was a spur route of SR 214 that existed mostly within the city limits of Oglethorpe.  Between 1963 and 1966, SR 214 Spur was established from SR 214/SR 214 Byp.  northwest of Oglethorpe to SR 90/SR 128 in the city.  In 1982, SR 214 Spur was decommissioned. State Route 217 (SR 217) was a state highway that existed entirely within Macon County.  In 1942, it was established from SR 128 north of Oglethorpe to SR 127 south-southeast of Reynolds.  Between 1963 and 1966, the entire length of the highway had a \"topsoil or gravel\" surface.  In 1969, SR 217 was decommissioned. State Route 218 (SR 218) was a short state highway that existed in Walker and Catoosa counties.  It is currently known as Lakeview Drive.  In 1942, it was established from US 27/SR 1 in Lakeview to SR 146 southeast of that city.  Between November 1946 and February 1948, the entire highway was hard surfaced.  In 1985, SR 218 was decommissioned. State Route 221 (SR 221) was a state highway that existed in the west-central part of Jasper County.  In 1943, it was established from SR 16 west of Monticello to SR 11 in Prospect.  A decade later, the entire highway had completed grading, but was not surfaced.  Between 1957 and the end of 1960, the entire length was paved.  The highway remained virtually unchanged for the next two decades.  In 1983, it was decommissioned. State Route 222 (SR 222) was a short state highway that existed entirely within the southeastern part of Meriwether County.  Today, it is known as Jesse Cole Road.  In 1943, it was established from SR 85 east-northeast of Manchester to SR 173 north of that city.  Its entire length had completed grading, but was not surfaced.  By the end of 1948, its entire length had a \"sand clay, top soil, or stabilized earth\" surface.  Between 1955 and the middle of 1957, SR 85 was redesignated as SR 85E.  By the end of 1960, the entire length of SR 222 was paved.  In 1986, this highway was decommissioned. State Route 226 (SR 226) was a state highway that existed in the north-central part of the state.  It traversed parts of Dawson and Hall counties.  In 1943, it was established from SR 53 to SR 9E at two different points northwest of Gainesville.  By the end of 1946, its entire length was hard surfaced.  Between 1957 and the end of 1960, the southern terminus was truncated to the Hall–Dawson county line.  Between 1963 and 1966, it was further truncated to a point just west of the county line.  In 1968, yet another truncation left the southern terminus at the northern shore of Lake Lanier.  In 1980, SR 226 was decommissioned. State Route 229 (SR 229) was a state highway in the central part of the state.  It traversed parts of Walton, Newton, and Jasper counties.  The roadway that would eventually become SR 229 was an unnumbered road built between 1921 and the end of 1926 between SR 11 in Social Circle to SR 12 southeast of the city.  Its entire length had a \"sand clay or top soil\" surface.  In 1930, this road was designated as SR 60.  In 1937, part of SR 142 was established on a path from Farrar to Newborn.  At the end of 1940, SR 60 was redesignated as SR 181.  The segment of SR 142 was under construction.  At the end of 1941, SR 181 was redesignated as SR 213.  The next year, the SR 142 segment had completed grading, but was not surfaced.  In 1943, SR 213 was designated on a southern alignment, which may have included a portion from Mansfield east-northeast to SR 142 in Newborn and then north-northwest to SR 12 east of Covington.  However, these segments were not indicated on maps.  The SR 142 segment had a sand clay or top soil surface.  SR 229 was designated from SR 11 in Monticello to SR 142 north-northwest of Farrar.  The southern part of this segment had a \"completed hard surface\"; its northern part had a sand clay or top soil surface.  By the end of 1946, the northern segment of SR 213 was redesignated as part of SR 229.  It was also designated on a segment from Newborn to east of Covington; however, there was no indication if the three segments were connected by concurrencies with other highways or not.  The northern portion of the segment from Monticello to north-northwest of Farrar had completed grading, but was not surfaced.  By the end of 1948, the SR 142 segment was hard surfaced.  SR 229's segment from Newborn to east of Covington had a sand clay, top soil, or stabilized earth surface.  By the end of 1951, the northern segment of SR 229 was hard surfaced.  The portion from Monticello to north-northwest of Farrar was also hard surfaced.  Between 1957 and the end of 1960, the portion from Newborn to east of Covington was paved.  In 1982, SR 229 was decommissioned. Former SR 235 was a 2.2 mile loop road from SR 9 in what is now the Buckhead neighborhood of Atlanta, Georgia.  Heading South, it pulled off of SR 9 (Roswell Rd.) onto Habersham Rd NW, turning left onto Chatham Rd.  NW, following it to Andrews Dr. NW, turning right onto Andrews Dr. NW and following it until rejoining SR 9 (Peachtree Rd).  The road first appeared in 1944, and was deleted between 1961 and 1963, when it was converted to a local road. State Route 238 (SR 238) was a short state highway that existed in the west-central part of the state.  It was entirely within Troup County.  Between 1945 and the end of 1946, it was established from the Alabama state line west-southwest of LaGrange to US 29/SR 14 southwest of Lees Crossing.  By the end of 1948, the entire highway, except for the westernmost portion had a \"sand clay, top soil, or stabilized earth\" surface.  The next year, the western terminus also had that same type of surface.  By the middle of 1950, all of the highway was hard surfaced.  In 1975, SR 238 was decommissioned. State Route 239 (SR 239) was a state highway that existed in the northwestern part of the state.  It traversed the northwestern part of Walker County and the southwestern part of Chattooga County.  Between 1945 and the end of 1946, it was established from SR 48 in Cloudland to SR 157 southeast of Rising Fawn.  Its entire length was hard surfaced.  Almost exactly 30 years later, the southern portion of SR 157 was shifted southeastward, replacing all of SR 239. State Route 244 (SR 244) was a short state highway that existed in the west-central part of the state.  It was completely within Troup County.  Between 1946 and the end of 1948, it was established from the Alabama state line west-northwest of LaGrange to SR 109.  Its entire length was hard surfaced.  In 1975, it was decommissioned. State Route 245 (SR 245) was a state highway in Fannin County.  Between 1946 and 1948, SR 245 was designated from Mineral Bluff to McCaysville.  Each terminus had a completed hard surface; the central part had a sand clay, top soil, or stabilized earth surface.  The next year, the entire length of SR 245 was hard surfaced.  In 1977, SR 60's path from northwest of Morganton to the North Carolina state line was shifted westward, replacing all of SR 245.  Its former path from Mineral Bluff to the state line was redesignated as SR 60 Spur. State Route 248 (SR 248) was a state highway that existed in the east-central part of the state.  It traversed the north-central portion of Washington County and the east-central portion of Hancock County.  Between 1948 and the end of 1949, it was established from SR 102 north-northeast of Warthen to SR 16 in Jewell.  The southern half of the highway had a \"sand clay, top soil, or stabilized earth\" surface.  In 1953, this portion had completed grading, but was not surfaced.  The Hancock County portion was hard surfaced.  Between 1955 and the middle of 1957, the entire highway was paved.  In 1982, it was decommissioned. State Route 249 (SR 249) was a short-lived state highway.  Between 1946 and 1948, an unnumbered road was built from Murrayville to Dahlonega; it had a \"sand clay, top soil, or stabilized earth\" surface.  The next year, the unnumbered road between Murrayville and Dahlonega was designated as SR 249.  By the middle of 1950, all of SR 249 was hard surfaced.  By 1957, SR 60 was extended south-southwest on US 19/SR 9 into Dahlonega, then south-southeast to Gainesville, replacing all of SR 249. State Route 250 (SR 250) was a state highway that existed in Tattnall and Evans counties.  The roadway that would eventually become SR 250 was established between 1945 and the end of 1946 as an eastern segment of SR 64 from US 25/SR 73 south of Claxton to US 280/SR 30 east-southeast of Daisy.  This segment was indicated to be \"projected mileage\".  By the end of 1948, the southern terminus of this segment was completed grading, but was not surfaced.  By the end of 1949, SR 250 was established on a slightly different alignment.  It began at an intersection with US 25/US 301/SR 73 south of Claxton, at a point farther south than the eastern segment of SR 64 did.  Its eastern terminus was at SR 129 south-southeast of Claxton, in the northwestern part of Camp Stewart.  By the end of 1951, the portion of SR 64 on either side of the SR 250 intersection had a \"sand clay, topsoil, or stabilized earth\" surface.  In 1953, the entire Tattnall County portion of SR 64 had completed grading, but was not surfaced.  The northern terminus of it was shifted westward to end in Daisy.  By the middle of 1957, SR 250 was shifted northwest, replacing the entire length of the eastern segment of SR 64.  By the end of 1963, the entire length of SR 250 was paved.  In 1985, SR 250 was decommissioned. State Route 258 (SR 258) was a state highway that existed in the west-central part of the state.  It was entirely within Troup County.  Between February 1948 and April 1949, it was established from US 27/SR 1 west-northwest of Hogansville to US 29/SR 14 in that city.  In 1953, the entire length of the highway was hard surfaced.  Between June 1963 and the end of 1966, it was redesignated as a southern extension of SR 54. State Route 259 (SR 259) was a state highway in the southeastern part of the state.  It traversed the northwestern part of Camden County and the southeastern part of Brantley County.  Between February 1948 and April 1949, it was established from SR 252 in Tarboro to US 84/SR 50 in Atkinson.  The Camden portion of the highway had a \"sand clay, top soil, or stabilized earth\" surface.  Between 1963 and the end of 1966, the entire length had a \"topsoil or gravel\" surface.  In 1968, the northern half of the Brantley County portion was hard surfaced.  In 1978, the rest of the highway was hard surfaced.  At the beginning of 1980, SR 259 was decommissioned. State Route 261 (SR 261) was a state highway that existed in the eastern part of the state.  It was entirely within Long County.  Between February 1948 and April 1949, it was established from the Altamaha River on the Wayne–Long county line to US 25/US 301/SR 23 south of Glennville.  By August 1950, it was extended northeast to an intersection with SR 196 at a point south-southeast of Glennville.  By the end of 1951, the southern terminus of the highway was shifted northwest to be just north-northwest of the Wayne–Long–Tattnall county tripoint.  In 1952, the southern terminus of SR 261 was reverted to its former location.  The northern half of the highway had completed grading, but was not surfaced.  Between 1957 and the end of 1960, the southern terminus was truncated slightly to the northeast.  Between 1963 and the end of 1965, the southern terminus was once again reverted to its former location.  At this time, the entire highway had a \"topsoil or gravel\" surface.  In 1967, the northern half was hard surfaced.  In 1981, SR 261 was decommissioned. State Route 263 (SR 263) was a state highway that existed in the central part of the state.  It was entirely within Taylor County.  Between February 1948 and April 1949, it was established from SR 128 north of Reynolds to US 19/SR 3 south-southwest of Salem.  In 1953, the southern half of the highway was hard surfaced.  By the middle of 1955, the northern half had a \"sand clay, topsoil, or stabilized earth\" surface.  By mid-1957, this segment was paved.  In 1987, SR 263 was decommissioned. State Route 265 (SR 265) was a very short state highway that existed in the south-central part of the state.  It was entirely within Telfair County.  Between February 1948 and April 1949, it was established from SR 117 east-northeast of Jacksonville to SR 149 northeast of that town.  Between September 1953 and June 1954, the entire highway was hard surfaced.  In 1976, the portion of SR 149 south of the SR 265 intersection was shifted northeastward, replacing all of SR 265. State Route 267 (SR 267) was a short state highway that existed in the west-central part of the state.  Between April 1949 and August 1950, it was established from SR 41 south of Geneva to US 80/SR 22 west-southwest of it.  The entire length of the highway had a \"sand clay, top soil, or stabilized earth\" surface.  In 1953, the northern terminus was shifted slightly to the west-southwest.  Between July 1957 and June 1960, the entire length was paved.  By the middle of 1963, the northern half of the highway was redesignated as part of SR 355.  In 1997, SR 267 was decommissioned. State Route 269 (SR 269) was a short state highway that existed in the east-central part of the state.  The highway was completely within Taliaferro County; however, the southern part traveled on the Warren–Taliaferro county line.  Between April 1949 and August 1950, the highway was established from SR 12 southeast of Crawfordville to SR 47 in Sharon.  Its entire length had a \"sand clay, top soil, or stabilized earth\" surface.  By the end of 1951, the entire highway was hard surfaced.  In 1983, SR 269 was decommissioned. State Route 276 (SR 276) was a short state highway that existed in the eastern part of the state.  It was entirely within Long County.  Between April 1949 and August 1950, it was established from a point west-northwest of Ludowici to US 25/US 301/SR 23 northwest of the city.  Between 1963 and 1966, the southern terminus was shifted slightly westward.  At this time, the entire length of the highway had a \"topsoil or gravel\" surface.  In 1981, SR 276 was decommissioned. State Route 277 (SR 277) was a short-lived state highway that existed in the central part of the state.  It was completely within Laurens County.  Between April 1949 and August 1950, it was established from the Dodge–Laurens–Bleckley county tripoint to US 80/SR 19/SR 26 in Dublin.  The entire Dexter–Dublin segment was hard surfaced.  Between September 1953 and June 1954, the southern terminus of the highway was truncated to just west of Dexter.  By the middle of 1955, the southern terminus was reverted to just south-southeast of its former location.  By mid-1957, the southern terminus was shifted to its original location.  By the middle of 1960, SR 277 was redesignated as an eastern extension of SR 257. State Route 287 (SR 287) was a short north–south state highway that existed in the central part of the state.  It was completely within Taylor County.  Between April 1949 and August 1950, SR 287 was established from a point just south of the Macon–Taylor county line southeast of Reynolds to SR 96 east of that city.  In 1952, the southern terminus was truncated to the county line.  The next year, the entire highway was hard surfaced.  By the middle of 1954, the southern terminus was truncated slightly.  By the middle of 1955, the southern terminus was reverted to the county line.  Near the end of the decade, the southern terminus was truncated again to the point that it was in 1954.  Between 1963 and 1966, the southern terminus was reverted once again to the county line.  In 1987, SR 287 was decommissioned. State Route 289 (SR 289) was a state highway that existed in the southeastern part of the state.  It traveled along the Appling–Jeff Davis county line.  Between 1950 and 1952, it was established from US 23/SR 15 south-southwest of Graham to US 341/SR 27 in the city.  In 1953, the central portion of the highway was shifted eastward to a more direct path between its termini.  The portion of the highway north of the Big Satilla River had completed grading, but was not surfaced.  Between 1960 and the middle of 1963, the portion south of the river was given the same treatment.  In 1970, the portion north of the river was hard surfaced.  Between 1978 and March 1980, SR 289 was decommissioned. State Route 290 (SR 290) was a short north–south state highway that existed in the southwestern part of the state.  It was entirely within Quitman County.  Between 1950 and 1952, it was established as an S-shaped highway from Hatcher to US 82/SR 50 west-southwest of Springvale.  In 1952, the southern terminus was shifted westward.  This put the highway on a nearly due north–south direction.  Between September 1953 and June 1954, the entire length of the highway was hard surfaced.  In 1981, SR 290 was decommissioned. State Route 291 (SR 291) was a short state highway that existed in the southwestern part of the state.  It was located completely within Quitman County.  Between August 1950 and the end of 1951, it was established as an S-shaped highway from Morris to US 82/SR 50 nearly due wet of Springvale.  In 1952, the southern terminus was shifted slightly.  This put the highway on a nearly due southwest–northeast direction.  The next year, the southern terminus was shifted slightly to the northwest.  By the middle of 1954, the southern terminus was extended slightly to the west.  The entire length of the highway was hard surfaced.  Between 1963 and 1966, the alignment of the highway was shifted to become a J-shaped highway.  In 1981, SR 291 was decommissioned. State Route 294 (SR 294) was a short state highway that existed in the northwestern part of the state.  It traveled completely within Bartow County.  The roadway that would eventually become SR 294 was established in 1952 as SR 294N from Allatoona Dam east of Cartersville to SR 20 northeast of the city.  The next year, the entire length of SR 294N was hard surfaced.  Between June 1955 and July 1957, it was redesignated as SR 294.  Between 1963 and 1966, it was again redesignated as SR 294N.  This roadway would eventually become SR 20 Spur. State Route 294N (SR 294N) was a short state highway that existed in the northwestern part of the state.  It traveled completely within Bartow County.  It was established in 1952 from Allatoona Dam east of Cartersville to SR 20 northeast of the city.  The next year, all of SR 294N was hard surfaced.  Between June 1955 and July 1957, the highway was redesignated as SR 294.  Between 1963 and 1966, SR 294 was again redesignated as SR 294N.  In 1994, SR 294N was redesignated as SR 20 Spur. State Route 294S (SR 294S) was a short state highway that existed in the northwestern part of the state.  It traveled completely within Bartow County.  Nearly the entire highway was within the city limits of Emerson.  In 1952, it was established from US 41/SR 3 in Emerson to just west of Red Top Mountain State Park in the far northeastern part of the city.  The next year, the entire highway was hard surfaced.  By the middle of 1955, US 41/SR 3 in the area was shifted eastward; the western terminus of SR 294S was then at SR 293.  In 1977, SR 294S was decommissioned. State Route 295 (SR 295) was a short-lived state highway in the city of Atlanta.  Between September 1953 and June 1954, it was established on what was listed on maps as simply \"Expressway\" (a predecessor of I-75/I-85/Downtown Connector) from US 19/US 41/SR 3 at Lakewood Avenue to University Avenue.  Between 1955 and the middle of 1957, it was decommissioned. State Route 300 (SR 300) was a state highway that existed in the central part of the state.  It followed a route between Monticello and US 129/US 441/SR 24, near the Rock Eagle State 4-H Club Center north of Eatonton.  It was established in 1960.  Later that year, a small portion at the eastern terminus was paved.  By 1967, the section from its western terminus to the intersection with SR 142 was paved.  In 1970, the entire length of the highway was paved.  By 1983, the highway was decommissioned and given to local authority. State Route 304 (SR 304) was a north–south state highway that was located in the east-central part of the state.  It was completely within Columbia County.  Between June 1955 and July 1957, it was established from US 221/SR 47 just north of Appling north-northeast to US 221/SR 104/SR 150 in Pollards Corner.  It was completely concurrent with US 221.  The entire length of US 221/SR 304 was paved.  In 1987, SR 47 between Appling and Leah was shifted eastward to travel concurrently with US 221.  This necessitated a decommissioning of SR 304. State Route 312 (SR 312) was an east–west state highway that was located in the southwestern part of the state.  Between July 1957 and June 1960, it was established from US 27 Bus. /US 84 Bus. /SR 38 in Bainbridge to US 84/SR 38 in Whigham.  The portion of SR 312 from Bainbridge to SR 262 north-northeast of Climax was paved.  From that point to Whigham had a \"topsoil or gravel, unpaved\" surface.  By the middle of 1963, the eastern part of the highway was also paved.  In 1980, SR 312 was decommissioned. State Route 318 (SR 318) was a west–east state highway that existed in the north-central part of the state.  It traveled completely within Dawson County.  Between July 1957 and June 1960, it was established from the Dawson Demonstration Forest and Wildlife Management Area south-southwest of Dawsonville to SR 53 southeast of that city.  The entire length of the highway was paved.  By the middle of 1963, it was extended south-southeast on a concurrency with SR 53, then solely east and southeast to War Hill Park northeast of Chestatee.  In 1971, the western terminus was truncated to SR 9 south of Dawsonville.  In 1980, the eastern terminus was truncated to SR 53.  In 1985, SR 318 was decommissioned. State Route 319 (SR 319) was a state highway that existed in the north-central part of the state.  It traversed the northeastern part of Barrow County and the south-central part of Jackson County.  Between July 1957 and June 1960, it was established from SR 211 north-northwest of Statham to US 129/SR 24 about halfway between Arcade and Jefferson.  The entire highway was paved at this time.  The highway was virtually unchanged for the next 30 years.  In 1990, it was decommissioned. State Route 321 (SR 321) was a short-lived state highway that existed in the eastern part of the state.  It traversed the northwestern part of Bryan County and the southeastern part of Bulloch County.  Between July 1957 and June 1960, it was established from US 280/SR 30/SR 63 in Pembroke north-northeast to SR 119 west-northwest of Blitchton.  The entire Bryan County portion was paved, while the entire Bulloch County portion had a \"topsoil or gravel, unpaved\" surface.  By the middle of 1963, the Bulloch County portion was paved.  SR 321 was designated on a separate segment from US 80/SR 26 south-southeast of Stilson, then northeast and north-northwest to SR 119 southwest of Guyton.  From the southern terminus of this segment to the turn to the north-northwest had a topsoil or gravel, unpaved surface; while the rest of it was paved.  There was no indication if the two segments were connected via concurrencies with SR 119 and US 80/SR 26 or if they were two separate segments.  By the end of 1966, SR 119's segment at the northern terminus of the original segment was redesignated as part of SR 46.  SR 321's southern segment was extended on a direct connection with the newer segment.  The central portion of the newer segment was hard surfaced.  In 1967, SR 119 was re-routed southward, replacing all of SR 321.  The former path of SR 119 through Stilson was redesignated as SR 119 Conn. State Route 322 (SR 322) was a state highway in the central part of the state.  Between 1957 and the end of 1960, it was established from US 1/SR 4/SR 46 in Oak Park then south-southeast to SR 292 east of Lyons.  In the middle of the 1960s, its entire length was redesignated as an eastern extension of SR 86. State Route 333 (SR 333) was a north–south state highway that existed in two separate segments in the state.  The highway traversed portions of Thomas, Mitchell, Dougherty, Lamar, Pike, Spalding, Henry, and Clayton counties. Between June 1960 and June 1963, the highway was established on US 19 from the Florida state line to Camilla.  This truncated SR 35, which was concurrent with US 19 from the Florida state line to Thomasville.  The segment of US 19 between Thomasville and Meigs, with which SR 3 was concurrent was redesignated as US 19 Bus.  SR 333 was established on a sole routing from Camilla to the eastern part of Albany, while US 19/SR 3 traveled on a slightly more western path.  SR 333 was also established on US 19/US 41 from SR 16 in Griffin to an indeterminate location between Jonesboro and Hapeville.  From Griffin to Lovejoy and in Jonesboro, SR 3 traveled on a more eastern path.  Between Lovejoy and Jonesboro and from north-northwest of Jonesboro, US 19/US 41/SR 3/SR 333 traveled concurrently.  By 1966, US 19 between Camilla and Albany was shifted eastward to travel concurrently with SR 333.  It was unclear if the northern terminus of SR 333 was truncated to Lovejoy or not.  That year, SR 333 was indicated to be \"projected mileage\" from an unnumbered road in the southern part of Barnesville, then west-northwest and north-northwest through Aldora, then north-northeast past US 41/SR 7, then north-northwest through Milner, then northwest and north-northwest past US 19/SR 3 south of Griffin, then north-northwest through the western part of Griffin to connect with the US 19/US 41/SR 3/SR 333 intersection with SR 92 in the northern part of the city.  The next year, US 341's path through the Barnesville–Aldora area was shifted southwestward to travel concurrently with SR 333 from just south of Barnesville to US 41/SR 7 Conn. just north of the city.  SR 333 was indicated to be projected mileage and under construction from this intersection to the US 19/US 41/SR 3/SR 92/SR 333 intersection in Griffin.  In 1968, the highway was indicated to be projected mileage from the US 19/US 82/SR 50S/SR 333 and US 19/SR 3W intersections in Albany.  The under construction segment from just north of Barnesville to south of Griffin was completed.  The next year, the portion of SR 333 from just north of Barnesville to Griffin was decommissioned. In 1970, all of SR 333 north of Griffin was also decommissioned.  In 1974, a freeway was built in Albany, with SR 333 designated on it.  Three years later, US 19 through the main part of Albany was shifted northeast to travel concurrently with the SR 333 freeway.  By March 1980, US 82 in Albany was also shifted onto the freeway.  Later that year, the northern terminus of SR 333 was truncated to the US 19/US 19 Bus. /US 82/US 82 Bus. /SR 50/SR 50 Bus. /SR 62/SR 333 interchange in Albany, with SR 50 shifted onto the freeway.  In 1982, all of SR 333 that remained was redesignated as SR 300. State Route 333 Spur (SR 333 Spur) was a proposed spur route of SR 333 that was planned to be put inside the city limits of Albany.  In 1976, it was indicated to be \"projected mileage\" from the SR 333 freeway just north of the Clark Avenue interchange and northeast to Turner Field Road.  In 1980, it was deleted, never having been built. State Route 336 (SR 336) was a state highway that existed in the northeastern part of the state.  Between June 1960 and June 1963, it was established from SR 328 east of Avalon to SR 17 in the southeastern part of Toccoa.  The entire highway was paved.  In 1982, the highway was decommissioned. State Route 340 (SR 340) was a state highway that existed in the Atlanta metropolitan area.  It traversed the northeastern part of Douglas County and the south-central part of Cobb County.  The roadway that would eventually become SR 340 was established in 1952 as an unnumbered road from US 78/SR 8 in Austell to SR 3 in Fair Oaks.  Between June 1960 and June 1963, this road was designated as SR 340.  The entire length of the highway was paved.  In 1983, SR 5 was re-routed on a more southerly track, replacing all of SR 340. State Route 342 (SR 342) was a 5.240 mi state highway that existed in the north-central part of the state.  It was completely within Dawson County.  Between June 1960 and June 1963, it was established from SR 183 southeast of Juno to SR 52 southeast of Amicalola, on the southern edge of the Chattahoochee-Oconee National Forest.  The entire highway was paved.  In 1982, it was decommissioned. State Route 343 (SR 343) was a short-lived state highway that existed in the northeastern part of the state.  It was completely within Rabun County.  Between June 1960 and June 1963, it was established on a concurrency with US 23, US 441, and possibly SR 15 from Tallulah Falls and Wiley.  The entire path of this concurrency was paved.  By the end of 1965, it was decommissioned, with US 23/US 441/SR 15 all traveling on SR 343's former path. State Route 344 (SR 344) was a state highway that existed in the northwestern part of the state.  It traversed portions of Floyd and Bartow counties. The highway that would eventually become SR 344 was established at least as early as 1919 as part of SR 4 from Rome to Cartersville.  By the end of 1926, a portion of the highway from just east of Rome to a point northwest of Cartersville was under construction.  In the northwestern part of Cartersville and farther to the west, a portion of the highway had a \"completed semi hard surface\".  Within three years, the segment of SR 4 was redesignated as part of SR 20, with US 41W designated on it.  The portion of the highway just east of Rome had a \"completed hard surface\".  The highway was under construction northwest of Cartersville. By the middle of 1930, the entire Rome–Cartersville segment had a completed hard surface.  Before the end of 1934, US 41W between Rome and Cartersville was redesignated as part of US 411.  In 1953, a small portion of SR 20 in the northern part of Cartersville was hard surfaced.  A few years later, all portions of SR 20 that had been built were paved.  Between 1960 and 1963, US 411 between Rome and Cartersville was shifted on a more southerly routing, concurrent with SR 344, which was commissioned at this time; SR 20 remained on the old alignment.  In 1977, SR 344 was decommissioned, and SR 20 was shifted onto US 411 between Rome and Cartersville.  SR 20's old alignment was redesignated as part of SR 293. State Route 346 (SR 346) was a short east–west state highway that existed in the north-central part of the state.  It was completely within Jackson County.  Between June 1960 and June 1963, it was established from US 129/SR 11 in Talmo to SR 82 Spur northeast of the city.  the entire highway was paved.  In 1966, SR 82 Spur and SR 82 swapped paths in the area.  In 2004, SR 346 was decommissioned. State Route 349 (SR 349) was an east–west state highway that existed in the northwestern part of the state.  It traveled entirely within the northern part of Walker County.  Between June 1960 and June 1963, it was established from SR 193 in Flintstone to US 27/SR 1 in Rossville.  In 1986, it was decommissioned. State Route 350 (SR 350) was a state highway that existed in the Athens – Clarke County metropolitan area.  It was entirely in Clarke County and the city limits of Athens.  Between June 1960 and June 1963, it was established from US 129/SR 15 in the northwestern part of the city to US 29/SR 8 in the northeastern part.  The entire divided highway was paved.  By the end of 1965, US 29 was designated on SR 350 from the US 129/SR 15 interchange, which also has US 29 Temp.  and US 441 Temp. , to the US 29/SR 8 interchange.  US 441 Temp.  was designated on it from the US 129/SR 15 interchange to the US 441/SR 15 Alt.  interchange.  A western extension of SR 350, ending at US 29/US 78/SR 8/SR 10, was under construction.  Also, SR 350 was under construction east-southeast just slightly from the US 29/SR 8 interchange.  In 1966, SR 350 was decommissioned.  US 29 was designated on the freeway from the western terminus to where it, as well as SR 8, depart the freeway.  This interchange also had SR 8 Bus.  and SR 106.  SR 8 was designated on the entire length of the freeway.  Its former path through the city was redesignated as SR 8 Bus., still concurrent with US 78/SR 10. State Route 351 (SR 351) was a 13 mi state highway that existed in the Atlanta metropolitan area.  It traversed portions of Clayton and Henry counties.  Between September 1953 and June 1954, the roadway that would eventually become SR 351 was established as an unnumbered road from SR 138 in Jonesboro to US 23/SR 42 east-northeast of Flippen.  Between June 1960 and June 1963 SR 351 was designated on this road.  In 1985, it was decommissioned. State Route 353 (SR 353) was a north–south state highway that was located in the south-central part of the state.  It traversed the northwest portion of Coffee County, the extreme northeastern part of Irwin County, and the southeastern part of Ben Hill County.  Between June 1960 and June 1963, the roadway that would eventually become SR 353 was established as an unnumbered road built from SR 158 west of Douglas, then north and northwest to SR 268 west-southwest of Broxton.  By the end of 1965, SR 353 was designated on this road and extended northwest to the Coffee–Irwin county line.  In 1966, SR 353 was proposed to be extended northwest to SR 206 north-northeast of Wray in the southeastern part of Ben Hill County.  In 1973, the highway was extended on this planned path.  In 1980, it was extended south-southeast around the southwestern part of Douglas to SR 135.  In 1988, SR 206 was shifted southeast, replacing all of SR 353. State Route 357 (SR 357) was a 15.5 mi north–south state highway that was located in the west-central part of the state.  It was completely within Muscogee County and the city limits of Columbus.  In April 1932, the roadway that would eventually become SR 357 was built as an unnumbered road from the main part of Columbus east to the western edge of Fort Benning.  Later that year, SR 103 was designated on this road, with a \"completed hard surface\".  In 1952, an unnumbered road was built from SR 103 in the eastern part of Columbus north-northwest to US 27 Alt. /SR 85.  Between June 1963 and the end of 1965, SR 103's southern terminus was truncated to Buena Vista Road and Brennan Road in the main part of Columbus.  Its former path on Buena Vista Road was redesignated as SR 357.  The unnumbered road built a decade before was also numbered as part of SR 357.  In 1969, SR 357 was extended south-southwest to SR 85 south of Columbus (now within Fort Benning).  This extension replaced SR 1 Spur.  In 1983, SR 357 was decommissioned. State Route 359 (SR 359) was a short lived state highway that existed completely within Chatham County, mostly within the city limits of Savannah.  Between June 1963 and the end of 1965, it was established from just north of Hunter Air Force Base south of the city to US 17/US 80/SR 25/SR 26S in downtown, traveling on Abercorn Street and 37th Street.  In 1968, the entire highway was redesignated as part of SR 204. State Route 361 (SR 361) was a north–south state highway that was located in the central part of the state.  It was completely within Bibb County, mostly in the city limits of Macon.  Between June 1963 and the end of 1966, the roadway that would eventually become SR 361 was built as Hartley Bridge Road and Mt. Pleasant Church Road south of Macon.  In 1967, SR 361 was established from US 41/SR 49/SR 247 south of Macon, west on Hartley Bridge Road and Mt. Pleasant Church Road, then north-northeast on Fulton Mill Road, Heath Road, Tucker Road, and Foster Road, and then northeast on Bass Road to SR 87 east-southeast of Bolingbroke.  In 1972, US 23 was shifted onto SR 87.  In 1976, US 129 onto US 41/SR 49/SR 247 south of Macon.  In 1982, SR 361 was decommissioned. State Route 363 (SR 363) was a north–south state highway that was located in the southwestern part of the state.  It was completely within Early County.  The roadway that would eventually become SR 363 was built in 1952 as an unnumbered road from US 84/SR 38 in Saffold to SR 39 in the southern part of Blakely.  The next year, the northern terminus of this road was shifted to SR 62 in the western part of Blakely.  In 1966, the northern terminus was shifted back to its original location.  In 1967, SR 363 was designated on this road.  In 1985, all of SR 363 except for the southern piece was decommissioned.  This southern portion was redesignated as part of SR 370. State Route 363 Spur (SR 363 Spur) was a spur route of SR 363 that existed entirely in the southwestern part of Early County.  Between June 1963 and the end of 1966, an unnumbered road was built west-southwest from Cedar Springs.  In 1967, SR 363 Spur was designated on this road.  In 1985, when SR 363 and SR 363 Spur were decommissioned, SR 273 was extended west-southwest of Cedar Springs.  This replaced the eastern part of SR 363 Spur.  What was the western part was redesignated as SR 273 Spur. State Route 364 (SR 364) was an east–west state highway that was located in the southern part of the state.  It traversed portions of Thomas and Brooks counties.  In 1966, it was established from US 84/SR 38 west of Boston to US 84/SR 38 west of Quitman.  Its entire length was hard surfaced.  In 1982, it was decommissioned. State Route 366 (SR 366) was a north–south state highway that was located in the northeastern part of the state.  It was completely within Hart County.  In 1967, it was established from an intersection with SR 51 and SR 77 west of Hartwell, then northwest on a concurrency with SR 77 and solely north-northwest to Interstate 85 (I-85) northeast of Lavonia and just south of Tugaloo State Park.  The entire highway was hard surfaced.  In 1990, SR 77's path in the Lavonia area was shifted northeast, replacing all of SR 366.  Its former path was redesignated as SR 77 Conn. State Route 367 (SR 367) was an east–west state highway that was located in the east-central part of the state.  It was completely within Chatham County in the Savannah metropolitan area.  Between June 1963 and the end of 1965, SR 26 Loop was established from US 80/SR 26 in Whitemarsh Island, then southeast over Turner Creek, then northeast and north-northeast to US 80/SR 26 in Wilmington Island.  Its entire length was hard surfaced.  In 1969, it was redesignated as SR 367.  In 1985, it was decommissioned. State Route 373 (SR 373) was an east–west state highway that was located in the northwestern part of the state.  It was completely within Gordon County.  Between June 1963 and the end of 1965, the roadways that would eventually become SR 373 were built as unnumbered roads.  One extended from Calhoun to Cash.  The other extended from Cash to SR 53 in Sonoraville.  In 1972, SR 373 was designated on both of these roads, starting at SR 156 in Calhoun.  In 1977, it was decommissioned. State Route 375 (SR 375) was a very short-lived state highway that existed in the west-central part of the state.  It traversed portions of Quitman and Stewart counties.  Between June 1963 and the end of 1965, the roadway that would eventually become SR 375 was built as an unnumbered road from Florence north-northeast to Omaha, and then eastward to US 27/SR 1 south-southeast of Louvale.  In 1968, this road was extended south-southwest to SR 27 in Georgetown.  In early 1972, this road was designated as SR 375.  Later that year, it was redesignated as a northern extension of SR 39. State Route 375 Connector (SR 375 Conn.) was a connector route of SR 375 that existed entirely in Stewart County in the west-central part of the state.  In 1970, the roadway that would eventually become SR 375 Conn. was built as an unnumbered road from Florence to US 27/SR 1 in Lumpkin.  In early 1972, this road was designated as SR 375 Conn.  Later that year, it was redesignated as SR 39 Conn. State Route 379 (SR 379) was a short-lived east–west state highway that was located in the north-central part of the state.  It was completely within Pickens County.  The roadway that would eventually become SR 379 was established in 1941 as an eastern segment of SR 143 from SR 53 east of Fairmount to SR 5 and SR 53 in Tate.  By the end of 1946, the eastern half of this segment had a \"sand clay, top soil, or stabilized earth\" surface.  The western half of it was indicated to be \"projected mileage\". By the end of 1960, nearly the entire part of this highway west of the SR 156 intersection was decommissioned.  By the end of 1963, this decommissioned part was re-instated.  In 1970, a portion of it southeast of the SR 53 intersection was hard surfaced.  In 1973, this portion was indicated to be \"under construction or projected mileage\".  In 1977, all of SR 143 from its western terminus to northeast of Sharp Top was redesignated as SR 379; northeast of this point to west of Tate was redesignated as part of SR 108; and from there to Tate was redesignated as SR 108 Conn. In 1981, SR 379 was decommissioned. State Route 381 (SR 381) was a north-south state highway located in Paulding County in the northwestern part of the state.  The roadway that would eventually become SR 381 was built in 1939, when SR 92 was extended from Hiram to Acworth.  By the end of 1948, the entire length of SR 92 that would become SR 381 was hard surfaced.  In 1966, the Dallas–New Hope segment of SR 92 was shifted to the southeast.  Its old alignment became SR 92 Spur.  In 1972, the Hiram–New Hope segment of SR 92 was shifted east.  Its old alignment between New Hope and Cross Roads became a northeast extension of SR 92 Spur.  In 1979, SR 92 Spur was redesignated as SR 381.  In 1990, SR 381 was decommissioned. On April 4, 1977, Southern Airways Flight 242 attempted a landing on this stretch of highway near New Hope.  The DC-9 crashed, killing the flight crew, 60 passengers, and eight people on the ground.  It also destroyed a gas station, grocery store, and other structures. State Route 387 (SR 387) was a very short-lived state highway that existed in the Atlanta metropolitan area.  It traversed portions of Fulton and Clayton counties.  In 1990, it was established on Camp Creek Parkway from Interstate 285 (I-285) in East Point to I-85 in College Park.  The next year, it was decommissioned. State Route 407 Loop (SR 407 Loop) was a state route that was an related route State Route 407, a unsigned designation along I-285 (similar to Georgia State Route 404 Spur).  It traveled off I-285 (now Glenridge Drive), and then turned left onto Dunwoody Peachtree Road by Saint Joseph's Hospital of Atlanta to I-285 (SR 407) once again.  The route was officially removed in 1994. State Route 701 (SR 701) was a short-lived state highway that existed in the west-central part of the state.  It was entirely in Troup County.  In 1970, it was indicated to be \"projected mileage\" from a point west of LaGrange to SR 109 south-southeast of Glenn.  In 1973, the entire length of the highway was hard surfaced.  In 1975, SR 109 was shifted southwestward, replacing all of SR 701. State Route 701 Spur (SR 701 Spur) was a short-lived spur route of SR 701 that existed in the west-central part of the state.  It was entirely in Troup County.  In 1970, it was indicated to be \"projected mileage\" from the Alabama state line west of Abbottsford to SR 701 in that community.  In 1973, the entire length of the highway was hard surfaced.  In 1975, it was redesignated as SR 109 Spur. State Route 704 (SR 704) was a proposed state highway that was planned for the northern part of Chatham County.  In 1973, it was proposed from SR 30 west-northwest of Monteith to SR 21 north-northwest of that community.  Between January 1979 and March 1980, the path of SR 30 in the Monteith area was shifted northward, replacing the proposed path of SR 704. State Route 705 (SR 705) was a state highway that existed in the north-central part of Cobb County.  It was proposed between 1961 and 1966 from SR 5 southwest of Marietta to SR 5 in southern Cherokee County.  By 1973, the interchange with I-75 and a short piece of SR 705 northeast of it were built.  In 1976, SR 705 between I-75 and the northern intersection with SR 5 was built and redesignated as SR 5 Conn. In 1984, SR 5 Conn. was redesignated as SR 5 Spur.  SR 5's path north of Marietta was shifted westward, onto Interstate 75 (I-75) and I-575, replacing all of the proposed portion of SR 705. State Route 707 (SR 707) was a proposed state highway that was planned for the southeastern part of Midway.  In 1974, it was proposed from US 17/SR 25 and SR 38 at the eastern terminus of US 82 to SR 38 east-southeast of Midway.  Between January 1979 and March 1980, the path of SR 38 east of Midway was shifted southward, replacing the proposed path of SR 707.  US 82 was extended along this path. State Route 713 (SR 713) was a state highway.  In 1977, I-575/SR 713 was proposed from I-75 north of Marietta to just south of the Cherokee–Pickens county line.  It was completed from SR 140 south of Canton to SR 20 east of the city.  The next year, the entire Cobb County portion (except for the southern end) of I-575/SR 713 was under construction.  By March 1980, I-575/SR 713 was completed to SR 92 southwest of Woodstock and one exit to the south in the Canton area.  SR 713 was proposed to be extended northwest to SR 5 in Talking Rock.  SR 713 Spur was proposed from SR 5 to SR 713 at the northern terminus of I-575.  Later that year, the southern completed part of I-575/SR 713 was extended south to I-75.  The next year, SR 5's path from south of Nelson to southeast of Talking Rock was shifted westward, replacing the northern extension of SR 713.  In 1982, I-575/SR 713 was under construction from SR 92 southwest of Woodstock to south of Canton and from east of Canton to I-575's northern terminus north-northwest of Ball Ground.  SR 713 was proposed to be extended northwest to SR 5 west-northwest of Talking Rock and north to the southern terminus of SR 719 at SR 5.  The next year, I-575/SR 713 was completed from SR 92 southwest of Woodstock to south of Canton.  In 1985, I-575 was completed northeast to a point southwest of Ball Ground.  SR 5's path from Talking Rock to south-southwest of Ellijay was shifted westward, replacing all of SR 713. State Route 713 Spur (SR 713 Spur) was a proposed spur route of SR 713.  Between the beginning of 1979 and March 1980, it was proposed from SR 5 to SR 713 at the northern terminus of Interstate 575 (I-575).  In 1981, SR 5's path from south of Nelson to southeast of Talking Rock was shifted westward, replacing SR 713 Spur. State Route 719 (SR 719) was a proposed state highway.  Between the beginning of 1979 and March 1980, it was proposed from SR 5 south-southwest of Ellijay to US 76/SR 5 northeast of that city.  In 1981, SR 719 was under construction.  In 1983, SR 5 in Gilmer County was shifted eastward, replacing the proposed path of SR 719. State Route 721 (SR 721) was a short-lived state highway that existed in portions of Spalding and Butts counties.  At least as early as 1919, SR 7 was established on essentially the same path as SR 721 would eventually travel.  Between the end of September 1921 and October 1926, US 41 was designated on this path.  Between the beginning of 1959 and the beginning of 1964, US 41 and SR 7 were shifted onto the new western bypass of the city, and off of this path.  Between the beginning of 1974 and the beginning of 1980, SR 721 was designated on two segments: the former path of US 41/SR 7 in Griffin and from High Falls Road east of the city to SR 16 west-southwest of Jackson.  This last intersection was just to the east of the Spalding–Butts county line.  It was proposed between the two segments.  In 1983, the path of SR 16 east of Griffin was shifted southward, replacing SR 721. State Route 726 (SR 726) was a state highway In 1983, it was proposed as a western bypass of Powder Springs, Clarkdale, and Austell, from an unnumbered road south-southwest of Powder Springs to US 78/SR 5/SR 8 southwest of Austell.  In 1986, US 278/SR 6 in the Powder Springs–Austell area was shifted westward, onto the path of SR 726 and the northern part of the unnumbered road in Powder Springs.  The former path from Powder Springs to Austell was redesignated as SR 6 Bus.  In 1988, a proposed northern rerouting of US 278/SR 6 was designated as a second iteration of SR 726.  In 1990, SR 726 was completed.  The next year, the path of US 278/SR 6 in the Powder Springs area was shifted northward, onto the former path of SR 726. This table shows the completed section of SR 726. State Route 728 (SR 728) was a proposed state highway that was planned for the eastern part of the McRae area.  In 1994, it was proposed as an eastern bypass of McRae, from US 280/SR 30 south-southwest of the city, around to US 319/US 441/SR 31, just north of where they split from US 280/SR 30 northeast of Helena.  As of the 2013 Telfair County map, the bypass was still proposed. State Route 730 (SR 730) was a proposed state highway.  In 1982, it was proposed as an eastern bypass of LaFayette, from southeast of the city to north-northeast of it.  In 1988, US 27/SR 1 in the area was shifted eastward, onto the path of SR 730.  Its former path through the city was redesignated as US 27 Bus. /SR 1 Bus.  State Route 732 (SR 732) was a proposed state highway that was planned for the southwestern part of the Athens metropolitan area, almost entirely in Oconee County.  In 1983, the southwestern part of the Athens Perimeter Highway, designated as SR 732, was proposed to be the final connecting piece of the freeway.  In 1987, the Athens Perimeter Highway was completed, with SR 10 on the southern part. State Route 733 (SR 733) was a proposed state highway in the northeastern part of Gilmer County.  Between 1977 and 1980, it was proposed between two intersections with US 76/SR 5 northeast of Ellijay.  Between 1984 and 1991, US 76/SR 5/SR 515 was shifted onto the path of SR 733, replacing it. State Route 734 (SR 734) was a proposed state highway in the northeastern part of Gilmer County.  Between 1977 and 1980, SR 734 was proposed from US 76/SR 5 south-southwest of Cherry Log, across US 76/SR 5 north-northeast of Cherry Log, and then to another intersection with US 76/SR 5 in Lucius.  Between 1984 and 1991, US 76/SR 5/SR 515 was shifted onto the path of SR 734, replacing it. State Route 736 (SR 736) was a short proposed state highway that was planned in Augusta.  In 1982, it was proposed to connect the eastern terminus of the John C. Calhoun Expressway (which would later carry SR 28), at 15th Street, with the western terminus of Greene Street, at SR 4 (13th Street).  In 1985, SR 28 was shifted southwest, off of Broad Street northwest of 5th Street, and onto John C. Calhoun Expressway, the proposed path of SR 736, and Greene Street. State Route 741 (SR 741) was a proposed state highway that was planned in Crawfordville.  In 1983, it was proposed as a western bypass of Crawfordville, from SR 22 southwest of the city to SR 22 northwest of it.  In 1985, the path of SR 22 in Crawfordville was shifted westward, replacing the proposed path of SR 741. State Route 744 (SR 744) was a proposed state highway in the western part of Polk County.  In 1987, it was proposed as an eastern bypass of Cedartown, from US 27/SR 1/SR 100 south-southwest of the city to US 27/SR 1 north-northeast of it.  In 1991, the path of US 27/SR 1 through the area was shifted eastward, replacing the path of SR 744. State Route 744 Spur (SR 744 Spur) was a proposed spur route of SR 744 that partially existed in the city limits of Cedartown.  In 1987, it was proposed from US 27/SR 1/SR 100 in the city to the proposed path of SR 744 southeast of it.  In 1991, US 278/SR 6 was shifted southward, out of the main part of the city, replacing the spur route. State Route 746 (SR 746) was a state highway that existed in the east-central part of Floyd County, just to the east of Rome.  In 1985, it was proposed from US 411/SR 20 southeast of Rome north-northeast and northwest to SR 53 at the eastern terminus of SR 53 Spur.  In 1990, the portion of SR 746 between US 411/SR 20 and SR 293 was built.  Two years later, all of the original completed and proposed portions of SR 746 from US 411/SR 20 to SR 53 and the entire length of SR 53 Spur were redesignated as parts of SR 1 Loop.  At this time, a separate segment of SR 746 was proposed from SR 20 west-northwest of Rome south-southeast and southeast to US 27/US 411/SR 1/SR 53 just north-northeast of Six Mile, east-northeast to SR 101 south-southeast of Rome, and east-northeast and north-northeast to US 411/SR 20 at SR 1 Loop.  Nearly a decade later, the proposed western terminus was shifted westward on SR 20 to begin at a point east-northeast of Coosa.  In 2005, the entire length of SR 746 was cancelled. This table shows the completed portion of SR 746. State Route 747 (SR 747) was a short-lived state highway that existed just north of Newnan.  In 1984, a northern bypass of Newnan was established from US 29/SR 14 just north of the city to SR 34 just northeast of it.  However, it was unnumbered.  The next year, a western extension of this bypass, designated as SR 747, was proposed to have a western terminus at SR 34 west of the city.  In 1986, this bypass was then proposed as SR 34 Byp.  The next year, it was re-proposed as SR 747.  In 1988, it was completed as SR 747 from US 27 Alt. /SR 16 to US 29/SR 14.  The next year, this bypass was entirely redesignated as SR 34 Byp. State Route 748 (SR 748) was a proposed state highway In 1987, it was proposed as part of an eastern bypass of Rockmart, from SR 113 east-northeast of the city south and south-southeast to US 278/SR 6 east-southeast of Van Wert.  In 1990, US 278/SR 6 was shifted northeast from Van Wert on SR 113 and southeast on the proposed path of SR 748, with SR 101/SR 113 concurrent with them to Yorkville. State Route 754 (SR 754) was a state highway that existed in Cobb and Cherokee counties.  The roadway that would eventually become SR 754 was established at least as early as 1919 as part of SR 5.  SR 5's path between Marietta and northeast of Canton was shifted onto Interstate 575 (I-575), replacing SR 713 on that path.  The portion between Marietta and west-northwest of Lebanon was redesignated as SR 754.  In 1986, the southern terminus was truncated to just north of Piedmont Road north of Marietta.  The next year, the southern terminus was re-extended to the northern terminus of SR 5 Spur.  In 1991, the northern terminus was truncated to SR 92 southwest of Woodstock.  In 1995, the southern terminus was truncated to just south of the Cobb–Cherokee county line.  In 1997, the southern terminus was truncated to the county line itself.  In 2003, the highway was decommissioned. State Route 758 (SR 758) was a proposed state highway that was planned in the southeastern part of Macon.  In 1985, it was proposed as a southeastern rerouting of US 80, from the intersection of US 80/SR 22 (Eisenhower Parkway) and US 41 Bus. /US 129/SR 11/SR 49 east-northeast and north-northeast to US 23/US 80/US 129 Alt. /SR 19/SR 87 (Emery Highway).  In 1998, the entire length of SR 758 (except for the westernmost portion) was canceled.  The western part was built as an eastern extension of Eisenhower Parkway, but as an unnumbered road. State Route 759 (SR 759) was a very short-lived state highway that existed entirely within the northern part of Jackson County, to the east of Commerce.  In 1989, it was proposed as an eastern bypass of the main part of Commerce, from US 441/SR 15 south-southeast of Commerce to another intersection north-northeast of the city.  In 1991, SR 759 around Commerce was completed.  The next year, the path of US 441/SR 15 in the Commerce area was shifted eastward, replacing SR 759.  The former path of US 441, on SR 334 and SR 98, was redesignated as US 441 Bus.  State Route 765 (SR 765) was a proposed state highway that existed in the Homer area.  In 1991, it was proposed as an eastern bypass of Homer, from US 441/SR 15/SR 164 south-southeast of Homer to US 441/SR 15 north-northeast of the city.  In 2004, the path of US 441/SR 15 in the Homer area was shifted eastward, onto the proposed path of SR 765. State Route 768 (SR 768) was a proposed state highway in Paulding County.  In 1987, it was proposed as a southern bypass of Dallas, from west-southwest of the city to US 278/SR 6/SR 120 southeast of it; this replaced the proposed path of SR 6 Byp.  In 1990, US 278/SR 6/SR 120 was routed on the proposed path of SR 768. State Route 768 Spur (SR 768 Spur) was a proposed spur route of SR 768 that was planned to be placed in the east-central part of Paulding, north-northwest of the city limits of Hiram.  Between the beginning of 1982 and the beginning of 1987, it was planned to be designated between the proposed path of SR 768 and US 278/SR 6/SR 120, at the point where they met the western terminus of SR 360.  In 1990, US 278/SR 6/SR 120 was shifted southward, onto the proposed path of SR 768, with SR 120/SR 360 shifted onto the proposed path of SR 768 Spur. State Route 771 (SR 771) was a short-lived state highway that existed in the north-central part of Fannin County.  In 1986, it was proposed from SR 5 south of McCaysville to the Tennessee state line north-northwest of the city.  By 1996, it was established on this proposed path.  In 2001, it was decommissioned. State Route 773 (SR 773) was a proposed state highway that was proposed partially for the town of Tallulah Falls.  In 1986, it was proposed as a western cut-off, on a more direct path between two intersections with US 23/US 441/SR 15 from south of Tallulah Falls and into the city.  The next year, the path of US 23/US 441/SR 15 in the Tallulah Falls area was shifted westward, onto the proposed path of SR 773.  The former path was redesignated as SR 15 Loop. State Route 789 (SR 789) was a proposed state highway in Polk and Paulding counties.  In 1987, it was proposed as a northeastern bypass of Yorkville, from north-northeast of Yorkville to east of it.  In 1993, US 278/SR 6 was shifted northeast from Yorkville onto the former proposed path of SR 789 State Route 793 (SR 793) was a proposed state highway in the north-central part of Carroll County and the southern part of Haralson County.  In 1988, it was proposed as a western bypass of Bremen, from south-southwest of the city to north-northwest of it.  In 1993, US 27/SR 1 in the Bremen area was shifted westward, onto the path of SR 793.  The former path was redesignated as US 27 Bus. /SR 1 Bus.  State Route 811 (SR 811) was a proposed state highway in the central portion of Haralson County.  In 1989, it was proposed from US 27/SR 1 south-southeast of Buchanan north-northwest across US 27/SR 1, and curved around the east side of the city to a point north-northwest of it.  In 1992, US 27/SR 1 was rerouted onto the proposed path of SR 811 and was shifted east of the city.  The former path was redesignated as US 27 Bus. /SR 1 Bus.  State Route 813 (SR 813) was a proposed state highway in the northern part of Walker County.  In 1994, it was proposed as a western bypass of the Chickamauga and Chattanooga National Military Park, from US 27/SR 1 east-northeast of Chickamauga to SR 2 west-southwest of Fort Oglethorpe.  In 2001, US 27/SR 1's path through the Chickamauga and Chattanooga National Military Park was shifted westward, onto the path of SR 813. State Route 816 (SR 816) was a proposed state highway that was proposed in the northern part of Sandersville, in the central part of Washington County.  In 1988, it was proposed as a northern bypass of Sandersville, from Deepstep Road northwest of the city to SR 88 east-northeast of it.  The next year, the proposed path of SR 816 was extended southwest to SR 24 west of the city.  In 1991, the path of SR 88 was extended westward, replacing the proposed path of SR 816. State Route 817 (SR 817) was a proposed state highway that was planned form Gwinnett, Barrow, and Oconee counties.  In 1989, a southern bypass of the Dacula–Athens area, designated as SR 817, was proposed from US 29/SR 8 (and what was then the eastern terminus of SR 316) west-southwest of Dacula to the southwest corner of the Athens Perimiter Highway.  In 1991, SR 817's path from west-southwest of Dacula to SR 11 north of Bethlehem was completed as an eastern extension of SR 316.  In 1993, SR 817's path from north of Bethlehem to US 78/SR 10 southeast of Bogart was also completed as an eastern extension of SR 316, with US 29 shifted onto its entire length, from what was the western terminus of the proposed path of SR 817.  SR 8 was shifted onto US 29/SR 316 from southeast of Russell to southeast of Bogart.  In 1996, SR 817's path in the southwestern part of Athens was completed as an eastern extension of SR 316, with US 29/US 78/SR 8 concurrent with it. State Route 818 (SR 818) was a proposed state highway that was planned for the Watkinsville area.  In 1990, a western bypass of the city, then proposed as a northern extension of SR 186, was planned from US 129/US 441/SR 24 north-northeast of Bishop to US 129/US 441/SR 15 in the southern part of Athens.  In 1992, this bypass was then proposed as SR 818.  In 1995, the path of US 129/US 441/SR 24, with SR 15 north of SR 24's northern terminus, was shifted westward, replacing the proposed path of SR 818.  The former path of US 129/US 441/SR 24 was redesignated as US 129 Bus. /US 441 Bus. /SR 24 Bus.  State Route 822 (SR 822) was a state highway that existed entirely within the city limits of Dublin.  In 1989, it was established from just south of the CSX railroad tracks in Dublin north-northwest to US 80/US 319/SR 26/SR 29/SR 31 (Bellevue Avenue).  In 1992, the path of SR 31 in Dublin was shifted east-southeastward, off of US 319/US 441/SR 19 and US 80/US 319/SR 26/SR 29, and onto the path of SR 822. State Route 826 (SR 826) was a proposed state highway that was planned in the western part of Eatonton, in the central part of Putnam County.  In 1988, it was proposed as a western bypass of the city, from US 129/SR 44 in Warfield to US 129/US 441/SR 24 north of Eatonton.  In 1992, the path of US 129/US 441/SR 24 in this area was shifted westward, replacing the proposed path of SR 826. State Route 828 (SR 828) was a proposed state highway that was planned in the central part of Jenkins County.  In 1989, it was proposed as a northeastern bypass of Millen, from SR 21 east of the city to US 25/SR 121 north-northwest of it.  In 1995, the path of SR 21 in the Millen area was shifted northward, replacing the path of SR 828. State Route 829 was a proposed state highway that was planned just south of Sylvania.  In 1988, an unnumbered road was built from US 301/SR 73 at the southern terminus of SR 73 Loop south-southwest of Sylvania to SR 21 southeast of the city.  In 1990, the unnumbered road south of Sylvania was designated as SR 829.  In 1993, the path of SR 21 in the Sylvania area was shifted south-southwest, replacing the path of SR 829 and then routed on US 301/SR 73 Loop.  Its former path was redesignated as SR 21 Bus.  State Route 831 (SR 831) was a proposed state highway in the southeastern part of Decatur County.  In 1989, it was proposed as an eastern bypass of Attapulgus, from east-southeast of the city to north-northwest of it.  In 1995, US 27/SR 1 was shifted onto this bypass.  Its former path was redesignated as SR 1 Bus.  State Route 835 (SR 835) was a proposed state highway that was planned for the northern part of Habersham County.  In 1989, it was proposed as an eastern bypass of Hollywood and Turnerville, from SR 17 just south of Hollywood to US 23/US 441/SR 15 south-southwest of Tallulah Falls.  In 1993, the path of US 23/US 441/SR 15, from Hollywood to Tallulah Falls, was shifted eastward, onto the proposed path of SR 835. State Route 838 (SR 838) was a proposed state highway in the north-central part of Early County.  In 1992, it was proposed as an eastern bypass of Blakely, from south-southeast of the city to north-northeast of it.  The next year, US 27/SR 1 in the Blakely area was shifted eastward, onto the path of SR 838.  The former path through the city became US 27 Bus. /SR 1 Bus.  State Route 844 (SR 844) was a proposed state highway in the north-central part of Laurens County.  Between the beginning of 1966 and the beginning of 1972, it was proposed as a western bypass of Dublin, from US 319/US 441/SR 31 south of the city to US 441/SR 29 north-northwest of it.  By the beginning of 1977, this highway was canceled. State Route 847 (SR 847) was a proposed state highway in the central part of Randolph County.  In 1993, it was proposed as a southeastern bypass of Cuthbert, from south-southwest of the city to north-northeast of it.  The next year, the path of US 27/SR 1 through the Cuthbert area was shifted eastward, onto the proposed path of SR 847.  The former path was redesignated as US 27 Bus. /SR 1 Bus.  State Route 863 (SR 863) was a proposed state highway that was planned for the central part of Effingham County.  In 1993, it was proposed as a western bypass of Springfield, from SR 21 south-southeast of the city to another intersection with SR 21 northwest of it.  In 1997, the path of SR 21 in the Springfield area was shifted westward, replacing the proposed path of SR 863. State Route 876 (SR 876) was a proposed state highway for the south-central part of Banks County, southeast of Hollingsworth.  In 1992, it was proposed as a southeastern bypass of Hollingsworth, from US 441/SR 15 south-southeast of Hollingsworth to SR 198 southeast of the community.  In 1997, the path of US 441/SR 15, from Homer to Cornelia, was shifted eastward, onto the proposed path of SR 876. State Route 877 (SR 877) was a proposed state highway that was planned for the north-central part of Banks County.  In 1992, a cutoff, north-northeast of Hollingsworth, was proposed as an unnumbered road from Hollingsworth to US 441/SR 15 north-northwest of it.  Two years later, the cutoff north-northeast of Hollingsworth, was then proposed as SR 877 and extended to SR 105 just east of the southern end of its concurrency with US 441/SR 15.  In 2002, SR 877 was canceled. State Route 899 (SR 899) was a proposed state highway that was planned as a northern bypass of Gray.  Between the beginning of 1996 and the beginning of 2010, it was proposed from US 129/SR 11/SR 18/SR 22 southwest of Clinton, at the west end of SR 18's concurrency with US 129/SR 11/SR 22, to SR 22 east-northeast of Gray.  In 2016, this bypass was canceled. State Route 901 (SR 901) was a proposed state highway that was planned within the city limits of Athens.  In 1992, it was proposed as a slightly western rerouting of US 129/US 441/SR 15 in the southern part of Athens.  Its path was from US 129/US 441/SR 15 southwest of their southern interchange with what is now SR 10 Loop north-northwest to Timothy Road just north of the freeway.  In 2001, the path of US 129/US 441/SR 15 in the southern part of Athens was shifted westward, onto the proposed path of SR 901 south of the freeway. State Route 932 (SR 932) was a proposed state highway that was planned inside the city limits of Gray.  In 1993, it was proposed as a southern bypass of the main part of the city, from US 129/SR 11/SR 18/SR 22 in the southwestern part to SR 22 east-northeast of the city.  The next year, the proposed path of SR 932 was truncated to SR 18 in the southeastern part of Gray.  In 1998, the path of SR 18 in Gray was shifted southward, replacing the proposed path of SR 932. State Route 1011 (SR 1011) was a short proposed state highway that was planned for the southeastern part of Columbia County.  Between January 1964 and January 1970, it was proposed from SR 28 west-northwest of Martinez east-northeast to Blackstone Camp Road.  By January 1975, it was canceled.  Between January 1997 and January 2009, this proposed path would be used as a northward shifting of the southern terminus of Blackstone Camp Road.  Its former path was renamed simply Old Blackstone Camp Road. State Route 1056 (SR 1056) was a short state highway that existed in the central part of Martinez.  Between January 1997 and January 2009, it was established on Davis Road from either Executive Center Drive or King Road west-northwest to SR 104 (Washington Road).  By January 2013, it was decommissioned. State Route 1082 (SR 1082) was a state highway that existed entirely in Martinez with portions proposed in Evans and Martinez.  Between January 1997 and January 2009, it had three proposed segments and one signed portion.  The westernmost segment was proposed from SR 104 (Washington Road) at the northern terminus of Towne Centre Drive east-southeast to Rountree Way, just southeast of Columbia Industrial Boulevard.  The second portion was from Rountree Way, just northwest of Columbia Industrial Boulevard, east-northeast slightly to Columbia Industrial Boulevard.  The third portion was from Blue Ridge Drive, just southwest of its southern intersection with Halifax Drive, to Old Evans Road, just south-southeast of Old Petersburg Road.  The fourth portion, which was indicated to be signed, was on Old Petersburg Road, from Old Evans Road east-southeast and southeast to Baston Road.  By January 2013, the second and third segments were canceled, and the fourth segment was decommissioned.  By January 2017, the first segment was canceled. State Route 1109 (SR 1109) was a proposed state highway that was planned for the central part of Effingham County.  Between the beginning of 1997 and the beginning of 2010, it was proposed as a northern bypass of Springfield, from SR 21 north-northwest of the city to SR 119 in the northern part of the city.  Between the beginning of 2012 and the beginning of 2015, SR 1109 was canceled.\n\nRadiation effects from the Fukushima Daiichi nuclear disaster The radiation effects from the Fukushima Daiichi nuclear disaster are the observed and predicted effects resulting from the release of radioactive isotopes from the Fukushima Daiichi Nuclear Power Plant after the 2011 Tōhoku earthquake and tsunami.  Radioactive isotopes were released from reactor containment vessels as a result of venting to reduce gaseous pressure, and the discharge of coolant water into the sea.  This resulted in Japanese authorities implementing a 20 km exclusion zone around the power plant, and the continued displacement of approximately 156,000 people as of early 2013.  Trace quantities of radioactive particles from the incident, including iodine-131 and caesium-134/137, have since been detected around the world. The World Health Organization (WHO) released a report that estimates an increase in risk for specific cancers for certain subsets of the population inside the Fukushima Prefecture.  A 2013 WHO report predicts that for populations living in the most affected areas there is a 70% higher risk of developing thyroid cancer for girls exposed as infants (the risk has risen from a lifetime risk of 0.75% to 1.25%), a 7% higher risk of leukemia in males exposed as infants, a 6% higher risk of breast cancer in females exposed as infants and a 4% higher risk, overall, of developing solid cancers for females. Preliminary dose-estimation reports by WHO and the United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) indicate that, outside the geographical areas most affected by radiation, even in locations within Fukushima prefecture, the predicted risks remain low and no observable increases in cancer above natural variation in baseline rates are anticipated.  In comparison, after the Chernobyl accident, only 0.1% of the 110,000 cleanup workers surveyed have so far developed leukemia, although not all cases resulted from the accident.  However, 167 Fukushima plant workers received radiation doses that slightly elevate their risk of developing cancer.  Estimated effective doses from the accident outside of Japan are considered to be below, or far below the dose levels regarded as very small by the international radiological protection community.  The United Nations Scientific Committee on the Effects of Atomic Radiation is expected to release a final report on the effects of radiation exposure from the accident by the end of 2013. A June 2012 Stanford University study estimated, using a linear no-threshold model, that the radioactivity release from the Fukushima Daiichi nuclear plant could cause 130 deaths from cancer globally (the lower bound for the estimate being 15 and the upper bound 1100) and 199 cancer cases in total (the lower bound being 24 and the upper bound 1800), most of which are estimated to occur in Japan.  Radiation exposure to workers at the plant was projected to result in 2 to 12 deaths.  However, a December 2012 UNSCEAR statement to the Fukushima Ministerial Conference on Nuclear Safety advised that \"because of the great uncertainties in risk estimates at very low doses, UNSCEAR does not recommend multiplying very low doses by large numbers of individuals to estimate numbers of radiation-induced health effects within a population exposed to incremental doses at levels equivalent to or lower than natural background levels.\" Preliminary dose-estimation reports by the World Health Organization and United Nations Scientific Committee on the Effects of Atomic Radiation indicate that 167 plant workers received radiation doses that slightly elevate their risk of developing cancer, however like the Chernobyl nuclear disaster that it may not be statistically detectable.  After the Chernobyl accident, only 0.1% of the 110,000 cleanup workers surveyed have so far developed leukemia, although not all cases resulted from the accident Estimated effective doses from the accident outside Japan are considered to be below (or far below) the dose levels regarded as very small by the international radiological protection community. According to the Japanese Government, 180,592 people in the general population were screened in March 2011 for radiation exposure and no case was found which affects health.  Thirty workers conducting operations at the plant had exposure levels greater than 100 mSv.  It is believed that the health effects of the radioactivity release are primarily psychological rather than physical effects.  Even in the most severely affected areas, radiation doses never reached more than a quarter of the radiation dose linked to an increase in cancer risk (25 mSv whereas 100 mSv has been linked to an increase in cancer rates among victims at Hiroshima and Nagasaki).  However, people who have been evacuated have suffered from depression and other mental health effects. While there were no deaths caused by radiation exposure, approximately 18,500 people died due to the earthquake and tsunami.  Very few cancers would be expected as a result of the very low radiation doses received by the public.  John Ten Hoeve and Stanford University professor Mark Z. Jacobson suggest that according to the linear no-threshold model (LNT) the accident is most likely to cause an eventual total of 130 (15-1100) cancer deaths, while noting that the validity of the LNT model at such low doses remains the subject of debate.  Radiation epidemiologist Roy Shore contends that estimating health effects in a population from the LNT model \"is not wise because of the uncertainties\".  The LNT model did not accurately model casualties from Chernobyl, Hiroshima or Nagasaki; it greatly overestimated the casualties.  Evidence that the LNT model is a gross distortion of damage from radiation has existed since 1946, and was suppressed by Nobel Prize winner Hermann Muller in favour of assertions that no amount of radiation is safe. In 2013 (two years after the incident), the World Health Organization indicated that the residents of the area who were evacuated were exposed to so little radiation that radiation induced health impacts are likely to be below detectable levels.  The health risks in the WHO assessment attributable to the Fukushima radioactivity release were calculated by largely applying the conservative Linear no-threshold model of radiation exposure, a model that assumes even the smallest amount of radiation exposure will cause a negative health effect. The World Health Organization (WHO) report released in 2013 predicts that for populations living around the Fukushima nuclear power plant there is a 70% higher relative risk of developing thyroid cancer for females exposed as infants, and a 7% higher relative risk of leukemia in males exposed as infants and a 6% higher relative risk of breast cancer in females exposed as infants.  With the WHO communicating that the values stated in that section of their report are relative increases, and not representative of the absolute increase of developing these cancers, as the lifetime absolute baseline chance of developing thyroid cancer in females is 0.75%, with the Radiation-induced cancer chance now predicted to increase that 0.75% to 1.25%, with this 0.75% to 1.25% change being responsible for the \"70% higher relative risk\":These percentages represent estimated relative increases over the baseline rates and are not absolute risks for developing such cancers.  Due to the low baseline rates of thyroid cancer, even a large relative increase represents a small absolute increase in risks.  For example, the baseline lifetime risk of thyroid cancer for females is just (0.75%)three-quarters of one percent and the additional lifetime risk estimated in this assessment for a female infant exposed in the most affected location is (0.5%)one-half of one percent. The WHO calculations determined that the most at-risk group, infants, who were in the most affected area, would experience an absolute increase in the risk of cancer (of all types) during their lifetime, of approximately 1% due to the accident.  With the lifetime risk increase for thyroid cancer, due to the accident, for a female infant, in the most affected radiation location, being estimated to be one half of one percent[0.5%].  Cancer risks for the unborn child are considered to be similar to those in 1 year old infants. The estimated risk of cancer to people who were children and adults during the Fukushima accident, in the most affected area, was determined to be lower again when compared to the most at-risk group—infants.  A thyroid ultrasound screening programme is currently (2013) ongoing in the entire Fukushima prefecture; this screening programme is, due to the screening effect, likely to lead to an increase in the incidence of thyroid disease due to early detection of non-symptomatic disease cases. About one-third of people (~30%) in industrialized nations are presently diagnosed with cancer during their lifetimes.  Radiation exposure can increase cancer risk, with the cancers that arise being indistinguishable from cancers resulting from other causes. In the general population, no increase is expected in the frequency of tissue reactions attributable to radiation exposure and no increase is expected in the incidence of congenital or developmental abnormalities, including cognitive impairment attributable to in-utero radiation exposure.  No significant increase in heritable effects has been found in studies of the children of the survivors of the atomic bombings of Hiroshima and Nagasaki or in the offspring of cancer survivors treated with radiotherapy, which indicates that moderate acute radiation exposures have little impact on the overall risk of heritable effects in humans. As of August 2013, there have been more than 40 children newly diagnosed with thyroid cancer and other cancers in Fukushima prefecture.  18 of these were diagnosed with thyroid cancer, but these cancers are not attributed to radiation from Fukushima, as similar patterns occurred before the accident in 2006 in Japan, with 1 in 100,000 children per year developing thyroid cancer in that year, that is, this is not higher than the pre-accident rate.  While controversial scientist Christopher Busby disagrees, claiming the rate of thyroid cancer in Japan was 0.0 children per 100,000 in 2005, the Japan Cancer Surveillance Research Group showed a thyroid cancer rate of 1.3 per 100,000 children in 2005 based on official cancer cases.  As a point of comparison, thyroid cancer incidence rates after the Chernobyl accident of 1986 did not begin to increase above the prior baseline value of about 0.7 cases per 100,000 people per year until 1989 to 1991, 3 to 5 years after the accident in both the adolescent and children age groups.  Therefore, data from Chernobyl suggests that an increase in thyroid cancer around Fukushima is not expected to begin to be seen until at least 3 to 5 years after the accident According to the Tenth Report of the Fukushima Prefecture Health Management Survey released in February 2013, more than 40% of children screened around Fukushima prefecture were diagnosed with thyroid nodules or cysts.  Ultrasonographic detectable thyroid nodules and cysts are extremely common and can be found at a frequency of up to 67% in various studies.  186 (0.5%) of these had nodules larger than 5.1 mm and/or cysts larger than 20.1 mm and underwent further investigation.  None had thyroid cancer.  An RT report into the matter was highly misleading.  Fukushima Medical University gives the number of children diagnosed with thyroid cancer as of December 2013 as 33 and concluded, \"[I]t is unlikely that these cancers were caused by the exposure from I from the nuclear power plant accident in March 2011\". Thyroid cancer is one of the most survivable cancers, with an approximate 94% survival rate after first diagnosis.  That rate increases to a 100% survival rate with catching it early. A 2013 article in the Stars and Stripes asserted that a Japanese government study released in February of that year had found that more than 25 times as many people in the area had developed thyroid cancer compared with data from before the disaster. As part of the ongoing precautionary ultrasound screening program in and around Fukushima, (36%) of children in Fukushima Prefecture in 2012 were found to have thyroid nodules or cysts, but these are not considered abnormal.  This screening programme is, due to the screening effect, likely, according to the WHO, to lead to an increase in the incidence of the diagnosis of thyroid disease due to early detection of non-symptomatic disease cases.  For example, the overwhelming majority of thyroid growths prior to the accident, and in other parts of the world, are overdiagnosed (that is, a benign growth that will never cause any symptoms, illness, or death for the patient, even if nothing is ever done about the growth) with autopsy studies, again done prior to the accident and in other parts of the world, on people who died from other causes showing that more than one third (33%+), of adults technically has a thyroid growth/cancer, but it is benign/never caused them any harm. Thyroid cancer is one of the most survivable cancers, with an approximate 94% survival rate after first diagnosis, and that rate increases to a 100% survival rate with catching it early.  For example, from 1989 to 2005, an excess of 4000 children and adolescent cases of thyroid cancer were observed in those who lived around Chernobyl; of these 4000 people, nine have died so far, a 99% survival rate. A survey by the newspaper Mainichi Shimbun computed that there were 1,600 deaths related to the evacuation, comparable to the 1,599 deaths due to the earthquake and tsunami in the Fukushima Prefecture. In the former Soviet Union, many patients with negligible radioactive exposure after the Chernobyl disaster displayed extreme anxiety about low level radiation exposure, and therefore developed many psychosomatic problems, including radiophobia, and with this an increase in fatalistic alcoholism being observed.  As Japanese health and radiation specialist Shunichi Yamashita noted: A survey by the Iitate, Fukushima local government obtained responses from approximately 1,743 people who have evacuated from the village, which lies within the emergency evacuation zone around the crippled Fukushima Daiichi Plant.  It shows that many residents are experiencing growing frustration and instability due to the nuclear crisis and an inability to return to the lives they were living before the disaster.  Sixty percent of respondents stated that their health and the health of their families had deteriorated after evacuating, while 39.9% reported feeling more irritated compared to before the disaster. Summarizing all responses to questions related to evacuees' current family status, one-third of all surveyed families live apart from their children, while 50.1% live away from other family members (including elderly parents) with whom they lived before the disaster.  The survey also showed that 34.7% of the evacuees have suffered salary cuts of 50% or more since the outbreak of the nuclear disaster.  A total of 36.8% reported a lack of sleep, while 17.9% reported smoking or drinking more than before they evacuated. Experts on the ground in Japan agree that mental health challenges are the most significant issue.  Stress, such as that caused by dislocation, uncertainty and concern about unseen toxicants, often manifests in physical ailments, such as heart disease.  So even if radiation risks are low, people are still concerned and worried.  Behavioral changes can follow, including poor dietary choices, lack of exercise and sleep deprivation, all of which can have long-term negative health consequences.  People who lost their homes, villages and family members, and even just those who survived the quake, will likely continue to face mental health challenges and the physical ailments that come with stress.  Much of the damage was really the psychological stress of not knowing and of being relocated, according to U.C. Berkeley's McKone. On 24 May 2012, more than a year after the disaster, TEPCO released their estimate of radioactivity releases due to the Fukushima Daiichi Nuclear Disaster.  An estimated 538,100 terabecquerels (TBq) of iodine-131, caesium-134 and caesium-137 was released.  520,000 TBq was released into the atmosphere between 12 and 31 March 2011 and 18,100 TBq into the ocean from 26 March to 30 September 2011.  A total of 511,000 TBq of iodine-131 was released into both the atmosphere and the ocean, 13,500 TBq of caesium-134 and 13,600 TBq of caesium-137.  In May 2012, TEPCO reported that at least 900 PBq had been released \"into the atmosphere in March last year [2011] alone\" up from previous estimates of 360-370 PBq total. The primary releases of radioactive nuclides have been iodine and caesium; strontium and plutonium have also been found.  These elements have been released into the air via steam; and into the water leaking into groundwater or the ocean.  The expert who prepared a frequently cited Austrian Meteorological Service report asserted that the \"Chernobyl accident emitted much more radioactivity and a wider diversity of radioactive elements than Fukushima Daiichi has so far, but it was iodine and caesium that caused most of the health risk – especially outside the immediate area of the Chernobyl plant.\"  Iodine-131 has a half-life of 8 days while caesium-137 has a half-life of over 30 years.  The IAEA has developed a method that weighs the \"radiological equivalence\" for different elements.  TEPCO has published estimates using a simple-sum methodology, s of 25 2012 TEPCO has not released a total water and air release estimate. According to a June 2011 report of the International Atomic Energy Agency (IAEA), at that time no confirmed long-term health effects to any person had been reported as a result of radiation exposure from the nuclear accident. In a leaked TEPCO report dated June 2011, it was revealed that plutonium-238, −239, −240, and −241 were released \"to the air\" from the site during the first 100 hours after the earthquake, the total amount of plutonium said to be 120 billion becquerels (120 GBq) — perhaps as much as 50 grams.  The same paper mentioned a release of 7.6 trillion becquerels of neptunium-239 – about 1 milligram.  As neptunium-239 decays, it becomes plutonium-239.  TEPCO made this report for a press conference on 6 June, but according to Mochizuki of the Fukushima Diary website, the media knew and \"kept concealing the risk for 7 months and kept people exposed\". According to one expert, the release of radioactivity is about one-tenth that from the Chernobyl disaster and the contaminated area is also about one-tenth that of Chernobyl. A 12 April report prepared by NISA estimated the total release of iodine-131 was 130 PBq and caesium-137 at 6.1 PBq.  On 23 April the NSC updated its release estimates, but it did not reestimate the total release, instead indicating that 154 TBq of air release were occurring daily as of 5 April. On 24 August 2011, the Nuclear Safety Commission (NSC) of Japan published the results of the recalculation of the total amount of radioactive materials released into the air during the incident at the Fukushima Daiichi Nuclear Power Station.  The total amounts released between 11 March and 5 April were revised downwards to 130 PBq for iodine-131 (I-131) and 11 PBq for caesium-137 (Cs-137).  Earlier estimations were 150 PBq and 12 PBq. On 20 September the Japanese government and TEPCO announced the installation of new filters at reactors 1, 2 and 3 to reduce the release of radioactive materials into the air.  Gases from the reactors would be decontaminated before they would be released into the air.  In the first half of September 2011 the amount of radioactive substances released from the plant was about 200 million becquerels per hour, according to TEPCO, which was approximately one-four millionths of the level of the initial stages of the accident in March. According to TEPCO the emissions immediately after the accident were around 220 billion becquerel; readings declined after that, and in November and December 2011 they dropped to 17 thousand becquerel, about one-13 millionth the initial level.  But in January 2012 due to human activities at the plant, the emissions rose again up to 19 thousand becquerel.  Radioactive materials around reactor 2, where the surroundings were still highly contaminated, got stirred up by the workers going in and out of the building, when they inserted an optical endoscope into the containment vessel as a first step toward decommissioning the reactor. A widely cited Austrian Meteorological Service report estimated the total amount of I-131 released into the air as of 19 March based on extrapolating data from several days of ideal observation at some of its worldwide CTBTO radionuclide measuring facilities (Freiburg, Germany; Stockholm, Sweden; Takasaki, Japan and Sacramento, USA) during the first 10 days of the accident.  The report's estimates of total I-131 emissions based on these worldwide measuring stations ranged from 10 PBq to 700 PBq.  This estimate was 1% to 40% of the 1760 PBq of the I-131 estimated to have been released at Chernobyl. A later, 12 April 2011, NISA and NSC report estimated the total air release of iodine-131 at 130 PBq and 150 PBq, respectively – about 30 grams.  However, on 23 April, the NSC revised its original estimates of iodine-131 released.  The NSC did not estimate the total release size based upon these updated numbers, but estimated a release of 0.14 TBq per hour on 5 April. On 22 September the results were published of a survey conducted by the Japanese Science Ministry.  This survey showed that radioactive iodine was spread northwest and south of the plant.  Soil samples were taken at 2,200 locations, mostly in Fukushima Prefecture, in June and July, and with this a map was created of the radioactive contamination as of 14 June.  Because of the short half-life of 8 days only 400 locations were still positive.  This map showed that iodine-131 spread northwest of the plant, just like caesium-137 as indicated on an earlier map.  But I-131 was also found south of the plant at relatively high levels, even higher than those of caesium-137 in coastal areas south of the plant.  According to the ministry, clouds moving southwards apparently caught large amounts of iodine-131 that were emitted at the time.  The survey was done to determine the risks for thyroid cancer within the population. On 31 October the Japanese ministry of Education, Culture, Sports, Science and Technology released a map showing the contamination of radioactive tellurium-129m within a 100-kilometer radius around the Fukushima No. 1 nuclear plant.  The map displayed the concentrations found of tellurium-129m – a byproduct of uranium fission – in the soil at 14 June 2011.  High concentrations were discovered northwest of the plant and also at 28 kilometers south near the coast, in the cities of Iwaki, Fukushima Prefecture, and Kitaibaraki, Ibaraki Prefecture.  Iodine-131 was also found in the same areas, and most likely the tellurium was deposited at the same time as the iodine.  The highest concentration found was 2.66 million becquerels per square meter, two kilometers from the plant in the empty town of Okuma.  Tellurium-129m has a half-life of 6 days, so present levels are a very small fraction of the initial contamination.  Tellurium has no biological functions, so even when drinks or food were contaminated with it, it would not accumulate in the body, like iodine in the thyroid gland. On 24 March 2011, the Austrian Meteorological Service report estimated the total amount of caesium-137 released into the air as of 19 March based on extrapolating data from several days of ideal observation at a handful of worldwide CTBTO radionuclide measuring facilities.  The agency estimated an average being 5,000 TBq daily.  Over the course of the disaster, Chernobyl put out a total of 85,000 TBq of caesium-137.  However, later reporting on 12 April estimated total caesium releases at 6,100 TBq to 12,000 TBq, respectively by NISA and NSC – about 2–4 kg.  On 23 April, NSC updated this number to 0.14 TBq per hour of caesium-137 on 5 April, but did not recalculate the entire release estimate. On 12 October 2011 a concentration of 195 becquerels/kilogram of Strontium-90 was found in the sediment on the roof of an apartment building in the city of Yokohama, south of Tokyo, some 250 km from the plant in Fukushima.  This first find of strontium above 100 becquerels per kilogram raised serious concerns that leaked radioactivity might have spread far further than the Japanese government expected.  The find was done by a private agency that conducted the test upon the request of a resident.  After this find Yokohama city started an investigation of soil samples collected from areas near the building.  The science ministry said that the source of the Strontium was still unclear. On 30 September 2011, the Japanese Ministry of Education and Science published the results of a plutonium fallout survey, for which in June and July 50 soil samples were collected from a radius of slightly more than 80 km around the Fukushima Daiichi plant.  Plutonium was found in all samples, which is to be expected since plutonium from the nuclear weapon tests of the 1950s and '60s is found everywhere on the planet.  The highest levels found (of Pu-239 and Pu-240 combined) were 15 becquerels per square meters in Fukushima prefecture and 9.4 Bq in Ibaraki prefecture, compared to a global average of 0.4 to 3.7 bq / kg from atomic bomb tests.  Earlier in June, university researchers detected smaller amounts of plutonium in soil outside the plant after they collected samples during filming by NHK. A recent study published in Nature found up to 35 bq / kg plutonium 241 in leaf litter in 3 out of 19 sites in the most contaminated zone in Fukushima.  They estimated the Pu-241 dose for a person living for 50 years in the vicinity of the most contaminated site to be 0.44 mSv.  However, the Cs-137 activity at the sites where Pu-241 was found was very high (up to 4.7Mbq / kg or about 135,000 times greater than the plutonium 241 activity), which suggests that it will be the Cs-137 which prevents habitation rather than the relatively small amounts of plutonium of any isotope in these areas. On 21 April, TEPCO estimated that 520 tons of radioactive water leaked into the sea before leaks in a pit in unit 2 were plugged, totaling 4,700 TBq of water release (calculated by simple sum, which is inconsistent with the IAEA methodology for mixed-nuclide releases) (20,000 times facility's annual limit).  TEPCO's detailed estimates were 2,800 TBq of I-131, 940 TBq of Cs-134, 940 TBq of Cs-137. Another 300,000 tons of relatively less-radioactive water had already been reported to have leaked or been purposefully pumped into the sea to free room for storage of highly radioactively contaminated water.  TEPCO had attempted to contain contaminated water in the harbor near the plant by installing \"curtains\" to prevent outflow, but now believes this effort was unsuccessful. According to a report published in October 2011 by the French Institute for Radiological Protection and Nuclear Safety, between 21 March and mid-July around 2.7 × 10 Bq of caesium-137 (about 8.4 kg) entered the ocean, about 82 percent having flowed into the sea before 8 April.  This emission of radioactivity into the sea represents the most important individual emission of artificial radioactivity into the sea ever observed.  However, the Fukushima coast has some of the world's strongest currents and these transported the contaminated waters far into the Pacific Ocean, thus causing great dispersion of the radioactive elements.  The results of measurements of both the seawater and the coastal sediments led to the supposition that the consequences of the accident, in terms of radioactivity, would be minor for marine life as of autumn 2011 (weak concentration of radioactivity in the water and limited accumulation in sediments).  On the other hand, significant pollution of sea water along the coast near the nuclear plant might persist, because of the continuing arrival of radioactive material transported towards the sea by surface water running over contaminated soil.  Further, some coastal areas might have less-favorable dilution or sedimentation characteristics than those observed so far.  Finally, the possible presence of other persistent radioactive substances, such as strontium-90 or plutonium, has not been sufficiently studied.  Recent measurements show persistent contamination of some marine species (mostly fish) caught along the coast of Fukushima district.  Organisms that filter water and fish at the top of the food chain are, over time, the most sensitive to caesium pollution.  It is thus justified to maintain surveillance of marine life that is fished in the coastal waters off Fukushima.  Despite caesium isotopic concentration in the waters off of Japan being 10 to 1000 times above concentration prior to the accident, radiation risks are below what is generally considered harmful to marine animals and human consumers. A year after the disaster, in April 2012, sea fish caught near the Fukushima power plant still contain as much radioactive Cs and Cs compared to fish caught in the days after the disaster.  At the end of October 2012 TEPCO admitted that it could not exclude radioactivity releases into the ocean, although the radiation levels were stabilised.  Undetected leaks into the ocean from the reactors, could not be ruled out, because their basements remain flooded with cooling water, and the 2,400-foot-long steel and concrete wall between the site’s reactors and the ocean, that should reach 100 feet underground, was still under construction, and would not be finished before mid-2014.  Around August 2012 two greenling were caught close to the Fukushima shore, they contained more than 25,000 becquerels a kilogram of cesium, the highest cesium levels found in fish since the disaster and 250 times the government’s safety limit. In August 2013, a Nuclear Regulatory Authority task force reported that contaminated groundwater had breached an underground barrier, was rising toward the surface and exceeded legal limits of radioactive discharge.  The underground barrier was only effective in solidifying the ground at least 1.8 meters below the surface, and water began seeping through shallow areas of earth into the sea. Radiation fluctuated widely on the site after the tsunami and often correlated to fires and explosions on site.  Radiation dose rates at one location between reactor units 3 and 4 was measured at 400 mSv/h at 10:22 JST, 13 March, causing experts to urge rapid rotation of emergency crews as a method of limiting exposure to radiation.  Dose rates of 1,000 mSv/h were reported (but not confirmed by the IAEA) close to the certain reactor units on 16 March, prompting a temporary evacuation of plant workers, with radiation levels subsequently dropping back to 800–600 mSv/h.  At times, radiation monitoring was hampered by a belief that some radiation levels may be higher than 1 Sv/h, but that \"authorities say 1,000 millisieverts [per hour] is the upper limit of their measuring devices.\" Prior to the accident, the maximum permissible dose for Japanese nuclear workers was 100 mSv per year, but on 15 March 2011, the Japanese Health and Labor Ministry increased that annual limit to 250 mSv, for emergency situations.  This level is below the 500 mSv/year considered acceptable for emergency work by the World Health Organization.  Some contract companies working for TEPCO have opted not to use the higher limit.  On 15 March, TEPCO decided to work with a skeleton crew (in the media called the Fukushima 50) in order to minimize the number of people exposed to radiation. On 17 March, IAEA reported 17 persons to have suffered deposition of radioactive material on their face; the levels of exposure were too low to warrant hospital treatment.  On 22 March, World Nuclear News reported that one worker had received over 100 mSv during \"venting work\" at Unit 3.  An additional 6 had received over 100 mSv, of which for 1 a level of over 150 mSv was reported for unspecified activities on site.  On 24 March, three workers were exposed to high levels of radiation which caused two of them to require hospital treatment after radioactive water seeped through their protective clothes while working in unit 3.  Based on the dosimeter values, exposures of 170 mSv were estimated, the injuries indicated exposure to 2000 to 6000 mSv around their ankles.  They were not wearing protective boots, as their employing firm's safety manuals \"did not assume a scenario in which its employees would carry out work standing in water at a nuclear power plant\".  The amount of the radioactivity of the water was about 3.9 M Bq per cubic centimetre. As of 24 March 19:30 (JST), 17 workers (of which 14 were from plant operator TEPCO) had been exposed to levels of over 100 mSv.  By 29 March, the number of workers reported to have been exposed to levels of over 100 mSv had increased to 19.  An American physician reported Japanese doctors have considered banking blood for future treatment of workers exposed to radiation.  Tepco has started a re-assessment of the approximately 8300 workers and emergency personnel who have been involved in responding to the incident, which has revealed that by 13 July, of the approximately 6700 personnel tested so far, 88 personnel have received between 100 and 150 mSv, 14 have received between 150 and 200 mSv, 3 have received between 200 and 250 mSv, and 6 have received above 250 mSv. TEPCO has been criticized in its provision of safety equipment for its workers.  After NISA warned TEPCO that workers were sharing dosimeters, since most of the devices were lost in the disaster, the utility sent more to the plant.  Japanese media has reported that workers indicate that standard decontamination procedures are not being observed.  Others reports suggest that contract workers are given more dangerous work than TEPCO employees.  TEPCO is also seeking workers willing to risk high radiation levels for short periods of time in exchange for high pay.  Confidential documents acquired by the Japanese Asahi newspaper suggest that TEPCO hid high levels of radioactive contamination from employees in the days following the incident.  In particular, the Asahi reported that radiation levels of 300 mSv/h were detected at least twice on 13 March, but that \"the workers who were trying to bring the disaster under control at the plant were not informed of the levels.\" Workers on-site now wear full-body radiation protection gear, including masks and helmets covering their entire heads, but it means they have another enemy: heat.  As of 19 July 2011, 33 cases of heat stroke had been recorded.  In these harsh working conditions, two workers in their 60s died from heart failure. On 19 July 2013 TEPCO said that 1,973 employees would have a thyroid-radiation dose exceeding 100 millisieverts.  19,592 workers—3,290 TEPCO employees and 16,302 employees of contractor firms—were given health checks.  The radiation doses were checked from 522 workers.  Those were reported to the World Health Organization in February 2013.  From this sample, 178 had experienced a dose of 100 millisieverts or more.  After the U.N. Scientific Committee on the Effects of Atomic Radiation, questioned the reliability of TEPCO´s thyroid gland dosage readings, the Japanese Health Ministry ordered TEPCO to review the internal dosage readings. The intake of radioactive iodine was calculated based on the radioactive cesium intake and other factors: the airborne iodine-to-cesium ratio on the days that the people worked at the reactor compound and other data.  For one worker a reading was found of more than 1,000 millisieverts. According to the workers, TEPCO did little to inform them about the hazards of the intake of radioactive iodine.  All workers with an estimated dose of 100 millisieverts were offered an annual ultrasound thyroid test during their lifetime for free.  But TEPCO did not know how many of these people had received a medical screening already.  A schedule for the thyroid gland test was not announced.  TEPCO did not indicate what would be done if abnormalities were spotted during the tests. Within the primary containment of reactors 1, 2, 3 and 4, widely varying levels of radiation were reported: Outside the primary containment, plant radiation-level measurements have also varied significantly. On 25 March, an analysis of stagnant water in the basement floor of the turbine building of Unit 1 showed heavy contamination. On 27 March, TEPCO reported stagnant water in the basement of unit 2 (inside the reactor/turbine building complex, but outside the primary containment) was measured at 1000 mSv/h or more, which prompted evacuation.  The exact dose rate remains unknown as the technicians fled the place after their first measurement went off-scale.  Additional basement and trench-area measurements indicated 60 mSv/h in unit 1, \"over 1000\" mSv/h in unit 2, and 750 mSv/h in unit 3.  The report indicated the main source was iodine-134 with a half-life of less than an hour, which resulted in a radioactive iodine concentration 10 million times the normal value in the reactor.  TEPCO later retracted its report, stating that the measurements were inaccurate and attributed the error to comparing the isotope responsible, iodine-134, to normal levels of another isotope.  Measurements were then corrected, stating that the iodine levels were 100,000 times the normal level.  On 28 March, the erroneous radiation measurement caused TEPCO to reevaluate the software used in analysis. Measurements within the reactor/turbine buildings, but not in the basement and trench areas, were made on 18 April.  These robotic measurements indicated up to 49 mSv/h in unit 1 and 57 mSv/h in unit 3.  This is substantially lower than the basement and trench readings, but still exceeds safe working levels without constant worker rotation.  Inside primary containment, levels are much higher. By 23 March 2011, neutron radiation had been observed outside the reactors 13 times at the Fukushima I site.  While this could indicate ongoing fission, a recriticality event was not believed to account for these readings.  Based on those readings and TEPCO reports of high levels of chlorine-38, Dr. Ferenc Dalnoki-Veress speculated that transient criticalities may have occurred.  However, Edwin Lyman at the Union of Concerned Scientists was skeptical, believing the reports of chlorine-38 to be in error.  TEPCO's chlorine-38 report was later retracted.  Noting that limited, uncontrolled chain reactions might occur at Fukushima I, a spokesman for the International Atomic Energy Agency (IAEA) \"emphasized that the nuclear reactors won’t explode.\" On 15 April, TEPCO reported that nuclear fuel had melted and fallen to the lower containment sections of three of the Fukushima I reactors, including reactor three.  The melted material was not expected to breach one of the lower containers, causing a serious radioactivity release.  Instead, the melted fuel was thought to have dispersed uniformly across the lower portions of the containers of reactors No. 1, No. 2 and No. 3, making the resumption of the fission process, known as a \"recriticality,\" most unlikely. On 19 April, TEPCO estimated that the unit-2 turbine basement contained 25,000 cubic meters of contaminated water.  The water was measured to have 3 MBq/cm3 of Cs-137 and 13 MBq/cm3 of I-131: TEPCO characterized this level of contamination as \"extremely high.\"  To attempt to prevent leakage to the sea, TEPCO planned to pump the water from the basement to the Centralized Radiation Waste Treatment Facility. A suspected hole from the melting of fuel in unit 1 has allowed water to leak in an unknown path from unit 1 which has exhibited radiation measurements \"as high as 1,120 mSv/h.\"  Radioactivity measurements of the water in the unit-3 spent-fuel pool were reported at 140 kBq of radioactive caesium-134 per cubic centimeter, 150 kBq of caesium-137 per cubic centimeter, and 11 kBq per cubic centimeter of iodine-131 on 10 May. In August 2011, several areas were spotted inside the reactor buildings where dose rates reach several sieverts per hour. TEPCO have reported at three sites 500 meters from the reactors that the caesium-134 and caesium-137 levels in the soil are between 7.1 kBq and 530 kBq per kilo of undried soil. Small traces of plutonium have been found in the soil near the stricken reactors: repeated examinations of the soil suggest that the plutonium level is similar to the background level caused by atomic bomb tests.  As the isotope signature of the plutonium is closer to that of power-reactor plutonium, TEPCO suggested that \"two samples out of five may be the direct result of the recent incident.\"  The more important thing to look at is the curium level in the soil; the soil does contain a short-lived isotope (curium-242) which shows that some alpha emitters have been released in small amounts by the accident.  The release of the beta/gamma emitters such as caesium-137 has been far greater.  In the short and medium term the effects of the iodine and the caesium release will dominate the effect of the accident on farming and the general public.  In common with almost all soils, the soil at the reactor site contains uranium, but the concentration of uranium and the isotope signature suggests that the uranium is the normal, natural uranium in the soil. Radioactive strontium-89 and strontium-90 were discovered in soil at the plant on 18 April, amounts detected in soil one-half kilometer from the facility ranging from 3.4 to 4400 Bq/kg of dry soil.  Strontium remains in soil from above-ground nuclear testing; however, the amounts measured at the facility are approximately 130 times greater than the amount typically associated with previous nuclear testing. The isotope signature of the release looks very different from that of the Chernobyl accident: the Japanese accident has released much less of the involatile plutonium, minor actinides and fission products than Chernobyl did. On 31 March, TEPCO reported that it had measured radioactivity in the plant-site groundwater which was 10,000 times the government limit.  The company did not think that this radioactivity had spread to drinking water.  NISA questioned the radioactivity measurement and TEPCO is re-evaluating it.  Some debris around the plant has been found to be highly radioactive, including a concrete fragment emanating 900 mSv/h. Air outside, but near, unit 3 was reported at 70 mSv/h on 26 April 2011.  This was down from radiation levels as high as 130 mSv/h near units 1 and 3 in late March.  Removal of debris reduced the radiation measurements from localized highs of up to 900 mSv/h to less than 100 mSv/h at all exterior locations near the reactors; however, readings of 160 mSv/h were still measured at the waste-treatment facility. Results revealed on 22 March from a sample taken by TEPCO about 100 m south of the discharge channel of units 1–4 showed elevated levels of Cs-137, caesium-134 (Cs-134) and I-131.  A sample of seawater taken on 22 March 330 m south of the discharge channel (30 kilometers off the coastline) had elevated levels of I-131 and Cs-137.  Also, north of the plant elevated levels of these isotopes were found on 22 March (as well as Cs-134, tellurium-129 and tellurium-129m (Te-129m)), although the levels were lower.  Samples taken on 23 and/or 24 March contained about 80 Bq/mL of iodine-131 (1850 times the statutory limit) and 26 Bq/mL and caesium-137, most likely caused by atmospheric deposition.  By 26 and 27 March this level had decreased to 50 Bq/mL (11) iodine-131 and 7 Bq/mL (2.9) caesium-137 (80 times the limit).  Hidehiko Nishiyama, a senior NISA official, stated that radionuclide contamination would \"be very diluted by the time it gets consumed by fish and seaweed.\"  Above the seawater, IAEA reported \"consistently low\" dose rates of 0.04–0.1 μSv/h on 27 March. By 29 March iodine-131 levels in seawater 330 m south of a key discharge outlet had reached 138 Bq/ml (3,355 times the legal limit), and by 30 March, iodine-131 concentrations had reached 180 Bq/ml at the same location near the Fukushima Daiichi nuclear plant, 4,385 times the legal limit.  The high levels could be linked to a feared overflow of highly radioactive water that appeared to have leaked from the unit -2 turbine building.  On 15 April, I-131 levels were 6,500 times the legal limits.  On 16 April, TEPCO began dumping zeolite, a mineral \"that absorbs radioactive substances, aiming to slow down contamination of the ocean.\" On 4 April, it was reported that the \"operators of Japan's crippled power plant say they will release more than 10,000 tons of contaminated water into the ocean to make room in their storage tanks for water that is even more radioactive.\"  Measurements taken on 21 April indicated 186 Bq/l measured 34 km from the Fukushima plant; Japanese media reported this level of seawater contamination second to the Sellafield nuclear accident. On 11 May, TEPCO announced it believed it had sealed a leak from unit 3 to the sea; TEPCO did not immediately announce the amount of radioactivity released by the leak.  On 13 May, Greenpeace announced that 10 of the 22 seaweed samples it had collected near the plant showed 10,000 Bq/Kg or higher, five times the Japanese standard for food of 2,000 Bq/Kg for iodine-131 and 500 Bq/kg for radioactive caesium. In addition to the large releases of contaminated water (520 tons and 4,700 TBq) believed to have leaked from unit 2 from mid-March until early April, another release of radioactive water is believed to have contaminated the sea from unit 3, because on 16 May TEPCO announced seawater measurements of 200 Bq per cubic centimeter of caesium-134, 220 Bq per cubic centimeter of caesium-137, and unspecified high levels of iodine shortly after discovering a unit-3 leak. At two locations 20 kilometers north and south and 3 kilometers from the coast, TEPCO found strontium-89 and strontium-90 in the seabed soil.  The samples were taken on 2 June.  Up to 44 becquerels per kilogram of strontium-90 were detected, which has a half-life of 29 years.  These isotopes were also found in soil and in seawater immediately after the accident.  Samples taken from fish and seafood caught off the coast of Ibaraki and Chiba did not contain radioactive stontium. As of October 2012, regular sampling of fish and other sea life off the coast of Fukushima showed that total cesium levels in bottom-dwelling fish were higher off Fukushima than elsewhere, with levels above regulatory limits, leading to a fishing ban for some species.  Cesium levels had not decreased 1 year after the accident. Continuous monitoring of radioactivity levels in seafood by the Japanese Ministry of Agriculture, Forestry and Fisheries (MAFF) shows that for the Fukushima prefecture the proportion of catches which exceed Japanese safety standards has been decreasing continuously, falling below 2% in the second half of 2013 and below 0.5% in the fourth quarter of 2014.  None of the fish caught in 2014 exceeded the less stringent pre-Fukushima standards.  For the rest of Japan, the peak figure using the post-Fukushima standards was 4.7% immediately after the catastrophe, falling below 0.5% by mid-2012, and below 0.1% by mid-2013. In February 2014, NHK reported that TEPCO was reviewing its radioactivity data, after finding much higher levels of radioactivity than was reported earlier.  TEPCO now says that levels of 5 million becquerels of strontium per liter were detected in groundwater collected in July 2013 and not 900,000 becquerels, as initially reported. Periodic overall reports of the situation in Japan are provided by the United States Department of Energy. In April 2011, the United States Department of Energy published projections of the radiation risks over the next year (that is, for the future) for people living in the neighborhood of the plant.  Potential exposure could exceed 20 mSv/year (2 rems/year) in some areas up to 50 kilometers from the plant.  That is the level at which relocation would be considered in the USA, and it is a level that could cause roughly one extra cancer case in 500 young adults.  However, natural radiation levels are higher in some parts of the world than the projected level mentioned above, and about 4 people out of 10 can be expected to develop cancer without exposure to radiation.  Further, the radiation exposure resulting from the incident for most people living in Fukushima is so small compared to background radiation that it may be impossible to find statistically significant evidence of increases in cancer. The highest detection of radiation outside of Fukushima peaked at 40 mSv.  This represents a much lower level then the amount required to increase a persons risk of cancer.  100 mSv represents the level at which a definitive increased risk of cancer occurs.  Radiation above this level increases the risk of cancer, and after 400 mSv radiation poisoning can occur, but is unlikely to be fatal. The zone within 20 km from the plant was evacuated on 12 March, while residents within a distance of up to 30 km were advised to stay indoors.  IAEA reported on 14 March that about 150 people in the vicinity of the plant \"received monitoring for radiation levels\"; 23 of these people were also decontaminated.  From 25 March, nearby residents were encouraged to participate in voluntary evacuation. At a distance of 30 km from the site, radiation of 3–170 μSv/h was measured to the north-west on 17 March, while it was 1–5 μSv/h in other directions.  Experts said exposure to this amount of radiation for 6 to 7 hours would result in absorption of the maximum level considered safe for one year.  On 16 March Japan's ministry of science measured radiation levels of up to 330 μSv/h 20 kilometers northwest of the power plant.  At some locations around 30 km from the Fukushima plant, the dose rates rose significantly in 24 hours on 16–17 March: in one location from 80 to 170 μSv/h and in another from 26 to 95 μSv/h.  The levels varied according to the direction from the plant.  In most locations, the levels remained well below the levels required to damage human health, as the recommended annual maximum limit is well below the level that would affect human health. Natural exposure varies from place to place but delivers a dose equivalent in the vicinity of 2.4 mSv/year, or about 0.3 µSv/h. For comparison, one chest x-ray is about 0.2 mSv and an abdominal CT scan is supposed to be less than 10 mSv (but it has been reported that some abdominal CT scans can deliver as much as 90 mSv).  People can mitigate their exposure to radiation through a variety of protection techniques. On 22 April 2011 a Japanese government report was presented by Minister of Trade Yukio Edano to leaders of the town Futaba.  In it predictions were made about radioactivity releases for the years 2012 up to 2132.  According to this report, in several parts of Fukushima Prefecture – including Futaba and Okuma – the air would remain dangerously radioactive at levels above 50 millisieverts a year.  This was all based on measurements done in November 2011. In August 2012, Japanese academic researchers announced that 10,000 people living near the plant in Minamisoma City at the time of the accident had been exposed to well less than 1 millisievert of radiation.  The researchers stated that the health dangers from such exposure was \"negligible\".  Said participating researcher Masaharu Tsubokura, \"Exposure levels were much lower than those reported in studies even several years after the Chernobyl incident.\" A detailed map was published by the Ministry of Education, Culture, Sports, Science and Technology, going online on 18 October 2011.  The map contains the cesium concentrations and radiation levels caused by the airborne radioactivity from the Fukushima nuclear reactor.  This website contains both web-based and PDF versions of the maps, providing information by municipality as had been the case previously, but also measurements by district.  The maps were intended to help the residents who had called for better information on contamination levels between areas of the same municipalities, using soil and air sample data already released.  A grid is laid over a map of most of eastern Japan.  Selecting a square in the grid zooms in on that area, at which point users can choose more detailed maps displaying airborne contamination levels, cesium-134 or -137 levels, or total caesium levels.  Radiation maps The unrecovered bodies of approximately 1,000 quake and tsunami victims within the plant's evacuation zone are believed to be inaccessible at the time of 1 April 2011 due to detectable levels of radiation. Radiation levels in Tokyo on 15 March were at one point measured at 0.809 μSv/hour although they were later reported to be at \"about twice the normal level\".  Later, on 15 March 2011, Edano reported that radiation levels were lower and the average radiation dose rate over the whole day was 0.109 μSv/h.  The wind direction on 15 March dispersed radioactivity away from the land and back over the Pacific Ocean.  On 16 March, the Japanese radiation warning system, SPEEDI, indicated high levels of radioactivity would spread further than 30 km from the plant, but Japanese authorities did not relay the information to citizens because \"the location or the amount of radioactive leakage was not specified at the time.\"  From 17 March, IAEA received regular updates on radiation from 46 cities and indicated that they had remained stable and were \"well below levels which are dangerous to human health\".  In hourly measurements of these cities until 20 March, no significant changes were reported. On 18 June 2012 it became known that from 17 to 19 March 2011 in the days directly after the explosions, American military aircraft gathered radiation data in an area with a radius of 45 kilometers around the plant for the U.S. Department of Energy.  The maps revealed radiation levels of more than 125 microsieverts per hour at 25 kilometers northwest of the plant, which means that people in these areas were exposed to the annual permissible dose within eight hours.  The maps were neither made public nor used for evacuation of residents. On 18 March 2011 the U.S. government sent the data through the Japanese Foreign Ministry to the NISA under the Ministry of Economy, Trade and Industry, and the Japanese Ministry of Education, Culture, Sports, Science and Technology got the data on 20 March. The data were not forwarded to the prime minister's office and the Nuclear Safety Commission, and subsequently not used to direct the evacuation of the people living around the plant.  Because a substantial portion of radioactive materials released from the plant went northwest and fell onto the ground, and some residents were \"evacuated\" in this direction, these people could have avoided unnecessary exposure to radiation had the data been published directly.  According to Tetsuya Yamamoto, chief nuclear safety officer of the Nuclear Safety Agency, \"It was very regrettable that we didn't share and utilize the information.\"  But an official of the Science and Technology Policy Bureau of the technology ministry, Itaru Watanabe, said it was more appropriate for the United States, rather than Japan, to release the data.  On 23 March - after the Americans - Japan released its own fallout maps, compiled by Japanese authorities from measurements and predictions from the computer simulations of SPEEDI.  On 19 June 2012 Minister of Science Hirofumi Hirano said that Japan would review the decision of the Science Ministry and the Nuclear-Safety Agency in 2011 to ignore the radiation maps provided by the United States.  He defended his ministry's handling of the matter with the remark that its task was to measure radiation levels on land.  But the government should reconsider its decision not to publish the maps or use the information.  Studies would be done by the authorities, whether the maps could have been a help with the evacuations. On 30 March 2011, the IAEA stated that its operational criteria for evacuation were exceeded in the village of Iitate, Fukushima, 39 km north-west of Fukushima I, outside the existing 30 km radiation exclusion zone.  The IAEA advised the Japanese authorities to carefully assess the situation there.  Experts from Kyoto University and Hiroshima University released a study of soil samples, on 11 April, that revealed that \"as much as 400 times the normal levels of radiation could remain in communities beyond a 30-kilometer radius from the Fukushima\" site. Urine samples taken from 10 children in the capital of Fukushima Prefecture were analyzed in a French laboratory.  All of them contained caesium-134.  The sample of an eight-year-old girl contained 1.13 becquerels/liter.  The children were living up to 60 kilometers away from the troubled nuclear power plant.  The Fukushima Network for Saving Children urged the Japanese government to check the children in Fukushima.  The Japanese non-profit Radiation Effects Research Foundation said that people should not overreact, because there are no reports known of health problems with these levels of radiation. On 31 October 2011 a scientist from the Worcester Polytechnic Institute, Marco Kaltofen, presented his findings on the releases of radioactive isotopes from the Fukushima accidents at the annual meeting of the American Public Health Association (APHA).  Airborne dust contaminated with radioactive particles was released from the reactors into the air.  This dust was found in Japanese car filters: they contained cesium-134 and cesium-137, and cobalt at levels as high as 3 nCi total activity per sample.  Materials collected during April 2011 from Japan also contained iodine-131.  Soil and settled dust were collected from outdoors and inside homes, and also from used children's shoes.  High levels of cesium were found on the shoelaces.  US air-filter and dust samples did not contain \"hot\" particles, except for air samples collected in Seattle, Washington in April 2011.  Dust particles contaminated with radioactive cesium were found more than 100 miles from the Fukushima site, and could be detected on the U.S. West Coast. Tests concluded between 10 and 20 April revealed radioactive caesium in amounts of 2.0 and 3.2 kBq/kg in soil from the Tokyo districts of Chiyoda and Koto, respectively.  On 5 May, government officials announced that radioactivity levels in Tokyo sewage had spiked in late March.  Simple-sum measurements of all radioactive isotopes in sewage burned at a Tokyo treatment plant measured 170,000 Bq/kg \"in the immediate wake of the Fukushima nuclear crisis\".  The government announced that the reason for the spike was unclear, but suspected rainwater.  The 5 May announcement further clarified that as of 28 April, the radioactivity level in Tokyo sewage was 16,000 Bq/kg. A detailed map of ground contamination within 80 kilometers of the plant, the joint product of the U.S. Department of Energy and the Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT), was released on 6 May.  The map showed that a belt of contamination, with radioactivity from 3 to 14.7 MBq caesium-137 per square meter, spread to the northwest of the nuclear plant.  For comparison, areas with activity levels with more than 0.55 MBq caesium-137 per square meter were abandoned after the 1986 Chernobyl accident.  The village of Iitate and the town of Namie are impacted.  Similar data was used to establish a map that would calculate the amount of radiation a person would be exposed to if a person were to stay outdoors for eight hours per day through 11 March 2012.  Scientists preparing this map, as well as earlier maps, targeted a 20 mSv/a dosage target for evacuation.  The government's 20 mSv/a target led to the resignation of Toshiso Kosako, Special Adviser on radiation safety issues to Japanese Prime Minister Naoto Kan, who stated \"I cannot allow this as a scholar\", and argued that the target is too high, especially for children; he also criticized the increased limit for plant workers.  In response, parents' groups and schools in some smaller towns and cities in Fukushima Prefecture have organized decontamination of soil surrounding schools, defying orders from Tokyo asserting that the schools are safe.  Eventually, the Fukushima education board plans to replace the soil at 26 schools with the highest radiation levels. Anomalous \"hot spots\" have been discovered in areas far beyond the adjacent region.  For example, experts cannot explain how radioactive caesium from the reactors at Fukushima ended up in Kanagawa more than 300 km to the south. In the first week of September the Ministry of Science published a new map showing radiation levels in Fukushima and four surrounding prefectures, based on the results of an aerial survey.  In the map, different colors were used to show the level of radiation at locations one meter above the ground. Up to 307,000 becquerels of cesium per kilogram of soil were detected during a survey held in Fukushima City, 60 kilometers away from the crippled reactors, on 14 September 2011.  This was triple the amount for contaminated soil that by Japanese governmental orders should be sealed into concrete.  According to \"Citizens Against Fukushima Aging Nuclear Power Plants\", these readings were comparable to the high levels in special regulated zones where evacuation was required after the Chernobyl accident.  They urged the government to designate the area as a hot spot, where residents would need to voluntarily evacuate and be eligible for state assistance.  Professor Tomoya Yamauchi of the University of Kobe, in charge of the study, in which soil samples were tested from five locations around the district, noted that the decontamination conducted in some of the areas tested has not yet reduced the radiation to pre-accident levels. On 18 October 2011 a hot-spot in a public square was found in the city of Kashiwa, Chiba in the Nedokoyadai district, by a resident walking with a dosimeter.  He informed the city council.  Their first readings were off the scale, as their Geiger-counter could measure up to 10 microsieverts per hour.  Later measurements by the Chiba environment foundation reported a final result of 57.5 microsieverts per hour.  On 21 October the roads around the place were sealed off, and the place was covered with sandbags three meters thick.  Further investigations and check-ups were planned on 24 October 2011.  These investigations showed on 23 October levels up to 276,000 becquerels radioactive cesium per kilogram of soil, 30 centimeters below the surface.  The first comments of town officials on the find of 57.7 microsieverts per hour were that there could not be a link with the Fukushima disaster, but after the find of this large amount of cesium, officials of the Science Ministry could not deny the possibility that the cause could be found at the Fukushima-site. In October 2011, radiation levels as high as those in the evacuation zone around Japan's Fukushima nuclear plant were detected in a Tokyo suburb.  Japanese officials said the contamination was linked to the Fukushima nuclear disaster.  Contamination levels \"as high as those inside Fukushima's no-go zone have been detected, with officials speculating that the hotspot was created after radioactive caesium carried in rain water became concentrated because of a broken gutter\". In October 2011 the Japanese Ministry of Science launched a phone hotline to deal with concerns about radiation exposure outside Fukushima Prefecture.  Concerned Japanese citizens had been walking with Geiger-counters through their locality in search of all places with raised radiation levels.  Whenever a site was found with a radiation dose at one meter above the ground more than one microsievert per hour and higher than nearby areas, this should be mentioned at the hotline.  One microsievert per hour is the limit above this topsoil at school playgrounds would be removed, subsidized by the state of Japan.  Local governments were asked to carry out simple decontamination works, such as clearing mud from ditches if necessary.  When radiation levels would remain more than one microsievert higher than nearby areas even after the cleaning, the ministry offered to help with further decontamination.  On the website of the ministry a guideline was posted on how to measure radiation levels in a proper way, how to hold the dosimeter and how long to wait for a proper reading. In October 2011 hotspots were reported on the grounds of two elementary schools in Abiko in Chiba: Radioactive cesium was found in waste water discharged into Tokyo Bay from a cement factory in the prefecture Chiba east of Tokio.  In September and October two water samples were taken, measuring 1,103 becquerels per liter and 1,054 becquerels per liter respectively.  These were 14 to 15 times higher than the limit set by NISA.  Ash from incinerators in the prefecture constituted the raw material to produce cement.  In this process toxic substances are filtered out of the ashes, and the water used to clean these filters was discharged into Tokyo Bay.  On 2 November 2011 this waste-water discharge was halted, and the Japanese authorities started a survey on the cesium contamination of the seawater of Tokyo Bay near the plant. On 12 November the Japanese government published a contamination map compiled by helicopter.  This map covered a much wider area than before.  Six new prefectures Iwate, Yamanashi, Nagano, Shizuoka, Gifu, and Toyama were included in this new map of the soil radioactivity of cesium-134 and cesium-137 in Japan.  Contamination between 30,000 and 100,000 becquerels per square meter was found in Ichinoseki and Oshu (prefecture Iwate), in Saku, Karuizawa and Sakuho (prefecture Nagano, in Tabayama (prefecture Yamanashi) and elsewhere. Based on radiation measurements made all over Japan between 20 March and 20 April 2011, and the atmospheric patterns in that period, computer simulations were performed by an international team of researchers, in cooperation with the University of Nagoya, in order to estimate the spread of radioactive materials like cesium-137.  Their results, published in two studies on 14 November 2011, suggested that cesium-137 reached up to the northernmost island of Hokkaido, and the regions of Chugoku and Shikoku in western Japan at more than 500 kilometers from the Fukushima plant.  Rain accumulated the cesium in the soil.  Measured radioactivity per kilogram reached 250 becquerels in eastern Hokkaido, and 25 becquerels in the mountains of western Japan.  According to the research group, these levels were not high enough to require decontamination.  Professor Tetsuzo Yasunari of the University of Nagoya called for a national soil-testing program because of the nationwide spread of radioactive material, and suggested identified hotspots, places with high radiation levels, should be marked with warning signs. The first study concentrated on cesium-137.  Around the nuclear plant, places were found containing up to 40.000 becquerels/kg, 8 times the governmental safety limit of 5.000 becquerels/kg.  Places further away were just below this maximum.  East and north-east from the plant the soil was contaminated the most.  North-west and westwards the soil was less contaminated, because of mountain protection. The second study had a wider scope, and was meant to study the geographic spread of more-radioactive isotopes, like tellurium and iodine.  Because these isotopes deposit themselves in the soil with rain, Norikazu Kinoshita and his colleagues observed the effect of two specific rain-showers on 15 and 21 March 2011.  The rainfall on 15 March contaminated the grounds around the plant; the second shower transported the radioactivity much further from the plant, in the direction of Tokyo.  According to the authors, the soil should be decontaminated, but when this is found impossible, farming should be limited. On 13 December 2011 extremely high readings of radioactive cesium – 90,600 becquerels per kilogram, 11 times the governmental limit of 8000 becquerels – were detected in a groundsheet at the Suginami Ward elementary school in Tokyo at a distance of 230 kilometers from Fukushima.  The sheet was used to protect the school lawn against frost from 18 March until 6 April 2011.  Until November this sheet was stored alongside a gymnasium.  In places near this storage area up to 3.95 microsieverts per hour were measured one centimeter above the ground.  The school planned to burn the sheet.  Further inspections were requested. All citizens of the town Fukushima received dosimeters to measure the precise dose of radiation to which they were exposed.  After September the city of Fukushima collected the 36,478 \"glass badges\" of dosimeters from all its citizens for analysis.  It turned out that 99 percent had not been exposed to more than 0.3 millisieverts in September 2011, except four young children from one family: a girl, in third year elementary school, had received 1.7 millisieverts, and her three brothers had been exposed to 1.4 to 1.6 millisieverts.  Their home was situated near a highly radioactive spot, and after this find the family moved out of Fukushima Prefecture.  A city official said that this kind of exposure would not affect their health. Similar results were obtained for a three-month period from September 2011: among a group of 36,767 residents in Fukushima city, 36,657 had been exposed to less than 1 millisievert, and the average dose was 0.26 millisieverts.  For 10 residents, the readings ranged from 1.8 to 2.7 millisieverts, but these values are mostly believed to be related to usage errors (dosimeters left outside or exposed to X-ray luggage screening). Due to objections from concerned residents it became more and more difficult to dispose of the ashes of burned household garbage in and around Tokyo.  The ashes of waste facilities in the Tohoku, Kanto and Kōshin'etsu regions were proven to be contaminated with radioactive cesium.  According to the guidelines of the Ministry of Environment, ashes radiating 8,000 becquerels per kilogram or lower could be buried.  Ashes with cesium levels between 8,000 and 100,000 becquerels should be secured, and buried in concrete vessels.  A survey was done on 410 sites of waste-disposal facilities, on how the ash disposal was proceeding.  At 22 sites, mainly in the Tokyo Metropolitan area, the ashes with levels under 8000 becquerels could not be buried due to the objections of concerned residents.  At 42 sites, ashes were found that contained over 8,000 becquerels of cesium, which could not be buried.  The ministry made plans to send officials to meetings in the municipalities to explain to the Japanese people that the waste disposal was done safely, and to demonstrate how the disposal of the ashes above 8000 becquerels was conducted. On 5 January 2012 the Nambu (south) Clean Center, a waste incinerator in Kashiwa, Chiba, was taken out of production by the city council because the storage room was completely filled with 200 metric tons of radioactive ash that could not disposed of in landfills.  Storage at the plant was full, with 1049 drums, and some 30 tons more were still to be taken out of the incinerator.  In September 2011, the factory was closed for two months for the same reason.  The Center's special advanced procedures were able to minimize the volume of the ash, but radioactive cesium was concentrated to levels above the national limit of 8.000 becquerels per kilogram for waste disposal in landfills.  It was not possible to secure new storage space for the radioactive ash.  Radiation levels in Kashiwa were higher than in surrounding areas, and ashes containing up to 70,800 becquerels of radioactive cesium per kilogram – higher than the national limit – were detected in the city.  Other cities around Kashiwa were facing the same problem: radioactive ash was piling up.  Chiba prefecture asked Abiko and Inzai to accept temporary storage at the Teganuma waste-disposal facility located at their border.  But this met strong opposition from their citizens. Radiation monitoring in all 47 prefectures showed wide variation, but an upward trend in 10 of them on 23 March.  No deposition could be determined in 28 of them until 25 March The highest value obtained was in Ibaraki (480 Bq/m on 25 March) and Yamagata (750 Bq/m on 26 March) for iodine-13.  For cesium-137, the highest values were in Yamagata at 150 and 1200 Bq/m respectively. Measurements made in Japan in a number of locations have shown the presence of radionuclides in the ground.  On 19 March, upland soil levels of 8,100 Bq/kg of Cs-137 and 300,000 Bq/kg of I-131 were reported.  One day later, the measured levels were 163,000 Bq/kg of Cs-137 and 1,170,000 Bq/kg of I-131. On 19 March, the Japanese Ministry of Health, Labour and Welfare announced that levels of radioactivity exceeding legal limits had been detected in milk produced in the Fukushima area and in certain vegetables in Ibaraki.  On 21 March, IAEA confirmed that \"in some areas, iodine-131 in milk and in freshly grown leafy vegetables, such as spinach and spring onions, is significantly above the levels set by Japan for restricting consumption\".  One day later, iodine-131 (sometimes above safe levels) and caesium-137 (always at safe levels) detection was reported in Ibaraki prefecture.  On 21 March, levels of radioactivity in spinach grown in the open air in Kitaibaraki city in Ibaraki, around 75 kilometers south of the nuclear plant, were 24,000 becquerel (Bq)/kg of iodine-131, 12 times more than the limit of 2,000 Bq/kg, and 690 Bq/kg of caesium, 190 Bq/kg above the limit.  In four Prefectures (Ibaraki, Totigi, Gunma, Fukushima), distribution of spinach and kakina was restricted as well as milk from Fukushima.  On 23 March, similar restrictions were placed on more leafy vegetables (komatsuna, cabbages) and all flowerheads brassicas (like cauliflower) in Fukushima, while parsley and milk distribution was restricted in Ibaraki.  On 24 March, IAEA reported that virtually all milk samples and vegetable samples taken in Fukushima and Ibaraki on 18–21 and 16–22 March respectively were above the limit.  Samples from Chiba, Ibaraki and Tochigi also had excessive levels in celery, parsley, spinach and other leafy vegetables.  In addition, certain samples of beef mainly taken on 27–show of 29 Marched concentrations of iodine-131 and/or caesium-134 and caesium-137 above the regulatory levels. After the detection of radioactive cesium above legal limits in Sand lances caught off the coast of Ibaraki Prefecture, the government of the prefecture banned such fishing.  On 11 May, cesium levels in tea leaves from a prefecture \"just south of Tokyo\" were reported to exceed government limits: this was the first agricultural product from Kanagawa Prefecture that exceeded safety limits.  In addition to Kanagawa Prefecture, agricultural products from Tochigi and Ibaraki prefectures have also been found to exceed the government limits, for example, pasture grass collected on 5 May, measured 3,480 Bq/kg of radioactive caesium, approximately 11 times the state limit of 300 becquerels.  Even into July radioactive beef was found on sale in eleven prefectures, as far away as Kōchi and Hokkaido.  Authorities explained that until that point testing had been performed on the skin and exterior of livestock.  Animal feed and meat cuts had not been checked for radioactivity previously. Hay and straw were found contaminated with cesium 80 km from the reactors and outside the evacuation zone.  The news of the contamination of foods with radioactive substances leaking from the Fukushima nuclear reactors damaged the mutual trust between local food producers, including farmers, and consumers.  The source of cesium was found to be rice straw that had been fed to the animals.  A notice from the Japanese government that was sent to cattle farmers after the nuclear accident made no mention of the possibility that rice straw could be contaminated with radioactive materials from the fallout.  Beef from Fukushima Prefecture was removed from the distribution channels.  Health minister Kohei Otsuka stated on 17 July 2011 that this removal might not be sufficient.  The urine of all cattle for sale was tested in order to return those cows that showed levels of radioactive substances higher than the government-set limit to farms so they could be decontaminated by feeding them safe hay.  The minister said that the government should try to buy uncontaminated straw and hay in other parts of the country and offer this to the farmers in the affected areas.  All transport of beef raised in the prefecture Fukushima was prohibited after 19 July.  The meat of some 132 cows was sold to at least 36 of the 47 prefectures of Japan.  In more and more places contaminated meat was found. In March 2012 up to 18,700 becquerels per kilogram radioactive cesium was detected in yamame, or landlocked masu salmon, caught in the Niida river near the town Iitate, which was over 37 times the legal limit of 500 becquerels/kg.  The fish was caught for testing purposes prior to the opening of the fishing season.  Fishing cooperatives were asked to refrain from catching and eating yamame fish from this river and all streams adjacent to it.  No fish was sold in local markets. No fishing was allowed in the river Nojiri in the region Okuaizu in Fukushima after-mid March 2012.  The fish caught in this river contained 119 to 139 becquerels of radioactive cesium per kilogram, although this river is located some 130 kilometers from the damaged reactors.  In 2011 at this place the fish measured about 50 becquerels per kilogram, and the fishing season was opened as usual.  But fishing was not popular in 2011.  Local people hoped it would be better in 2012.  After the new findings the fishing season was prosponed. On 28 March 2012 smelt caught in the Akagi Onuma lake near the city of Maebashi in the prefecture Gunma was found to be contaminated with 426 becquerels per kilogram of cesium. In April 2012 radioactive cesium concentrations of 110 becquerels per kilogram were found in silver crucian carp fish caught in the Tone River north of Tokyo, some 180 kilometers away from the Fukushima Daiichi Plant.  Six fishery cooperatives and 10 towns along the river were asked to stop all shipments of fish caught in the river.  In March 2012 fish and shellfish caught in a pond near the same river were found to contain levels above the new legal limits of 100 becquerels per kilogram. The Dutch bio-farming company Waterland International and a Japanese federation of farmers made an agreement in March 2012 to plant and grow camellia on 2000 to 3000 hectare.  The seeds will be used to produce bio-diesel, which could be used to produce electricity.  According to director William Nolten the region had a big potential for the production of clean energy.  Some 800,000 hectares in the region could not be used to produce food anymore, and after the disaster because of fears for contamination the Japanese people refused to buy food produced in the region anyway.  Experiments would be done to find out whether camelia was capable of extracting caesium from the soil.  An experiment with sunflowers had no success. High levels of radioactive cesium were found in 23 varieties of freshwater fish sampled at five rivers and lakes in Fukushima Prefecture between December 2011 and February 2012 and in 8 locations on the open sea.  On 2 July 2012 the Ministry of the Environment published that it had found radioactive cesium between 61 and 2,600 becquerels per kilogram.  2,600 becquerels were found in a kind of goby caught in Mano River, which flows from Iitate Village to the city of Minamisoma, north of the nuclear plant.  Water bugs, common food for freshwater fish, also showed high levels of 330 to 670 becquerels per kilogram.  Marine fish was found less contaminated and showed levels between 2.15 and 260 Bq/kg.  Marine fish might be more capable of excreting cesium from their bodies, because saltwater fish have the ability to excrete salt.  The Japanese Ministry of the Environment would closely monitor freshwater fish as radioactive cesium might remain for much longer periods in their bodies.  According to Japanese regulations, food is considered safe for consumption up to a maximum of 100 Bq/kg. In August 2012, the Health ministry found that cesium levels had dropped to undetectable levels in most cultivated vegetables from the affected area, while food sourced from forests, rivers or lakes in the Tohoku and northern Kanto regions are showing excessive contamination. In a 'murasoi'-fish (or rock-fish Sebastes pachycephalus) caught in January 2013 at the coast of Fukushima an enormous amount of radioactive cesium was found: 254.000 becquerel/kilogram, or 2540 times the legal limitm in Japan for seafood. On 21 February 2013 a greenling - 38 centimeters long and weighing 564 grams - was caught near a water intake of the reactor units.  It did set a new record: containing 740,000 becquerels radioactive cesium per kilogram, 7,400 times the Japanese limit deemed safe for human consumption.  The previous record of cesium concentration in fish was 510,000 Bq/kg detected in another greenling.  On the sea floor a net was installed by TEPCO, in order to prevent migrating fish to escape from the contaminated area. As of July 2011, the Japanese government has been unable to control the spread of radioactive material into the nation's food, and \"Japanese agricultural officials say meat from more than 500 cattle that were likely to have been contaminated with radioactive caesium has made its way to supermarkets and restaurants across Japan\".  On 22 July it became known that at least 1400 cows were shipped from 76 farms that were fed with contaminated hay and rice-straw that had been distributed by agents in Miyagi and farmers in the prefectures of Fukushima and Iwate, near the crippled Fukushima Daiichi nuclear power plant.  Supermarkets and other stores were asking their customers to return the meat.  Farmers were asking for help, and the Japanese government was considering whether it should buy and burn all this suspect meat.  Beef had 2% more Caesium then the governments strict limits. On 26 July more than 2,800 cattle carcasses, fed with cesium-contaminated food, had been shipped for public consumption to 46 of the 47 prefectures in Japan, with only Okinawa remaining free.  Part of this beef, which had reached the markets, still needed to be tested.  In an attempt to ease consumer concern the Japanese government promised to impose inspections on all this beef, and to buy the meat back when higher-than-permissible cesium levels were detected during the tests.  The government planned to eventually pass on the buy-back costs to TEPCO.  The same day the Japanese ministry of agriculture urged farmers and merchants to renounce the use and sale of compost made of manure from cows that may have been fed the contaminated straw.  The measure also applied to humus from leaves fallen from trees.  After developing guidelines for safety levels of radioactive caesium in compost and humus, this voluntary ban could be lifted. On 28 July a ban was imposed on all the shipments on cattle from the prefecture Miyagi.  Some 1,031 beasts had been shipped that probably were fed with contaminated rice-straw.  Measurements of 6 of them revealed 1,150 becquerels per kilogram, more than twice the governmental set safety level.  Because the origins were scattered all over the prefecture, Miyagi became the second prefecture with a ban on all beef-cattle shipments.  In the year before 11 March about 33,000 cattle were traded from Miyagi. On 1 August a ban was put on all cattle in the prefecture Iwate, after 6 cows from two villages were found with heavy levels of caesium.  Iwate was the third prefecture where this was decided.  Shipments of cattle and meat would only be allowed after examination, and when the level of caesium was below the regulatory standard.  In Iwate some 36,000 cattle were produced in a year.  All cattle would be checked for radioactive contamination before shipment, and the Japanese government asked the prefecture to temporarily reduce the number of shipments to match its inspection capability. On 3 August, the prefecture Shimane, in western Japan, conducted radiation checks on all beef cattle to ease consumer concerns about food safety.  Starting from the second week of August all cattle were tested.  Late July at one farm in this prefecture rice-straw was discovered with radioactive caesium levels exceeding the government safety guide.  Although all other tests of beef cattle found far lower levels of radioactivity than the government standard, prices of beef from Shimane plummeted and wholesalers avoided all cattle from the prefecture.  All processed beef would undergo preliminary screening, and meat registering 250 becquerels per kilogram or more of radioactive caesium – half the government safety level – would be tested further. The second week of August the prefecture of Fukushima Prefecture initiated a buy-out of all cattle that could not be sold because the high levels of caesium in the meat.  The prefecture decided to buy back all beef cattle that had become too old for shipment due to the shipping suspension in place since July. On 2 August a group of farmers agreed with the Fukushima prefectural government to set up a consultative body to regulate this process.  The prefectural government provided the subsidies needed.  There was some delay, because the farmers and the local government could not agree about the prices. The problems for the farmers were growing, because they did not know how to protect their cattle from contamination and did not know how to feed their cattle.  The farmers said that the buy-back plan needed to be implemented immediately. On 5 August 2011, in response to calls for more support by farmers, the Japanese government revealed a plan to buy up all beef contaminated with radioactive caesium, that had already reached the distribution chains, as an additional measurement to support beef cattle farmers. The plan included: Other measurements were the expansion of subsidies to beef cattle farmers: On 19 August 2011 was reported, the meat of 4 cows from one Fukushima farm had been found to be contaminated with radioactive caesium in excess of the government-set safety limits.  The day after the meat of 5 other cows from this farm was also found to contain radioactive caesium.  Because of this the central government delayed lifting a shipment ban on Fukushima beef.  The 9 cows were among a total of over 200 head of cattle shipped from the farm and slaughtered at a facility in Yokohama city between 11 March nuclear accident and April.  The beef had been stored by a food producer.  The farmer denied feeding the cows contaminated rice straw, instead he used imported hay that had been stored at another farm. Japan banned Fukushima beef.  These domestic animals were affected by the food supply.  It was reported that 136 cows consumed feed affected by radioactive caesium.  A number of cows were found to have consumed rice straw containing high levels of radioactive caesium.  This meat had already been distributed nationwide and that it \"could have already reached consumers.\"  They traced contaminated beef on farms near the Fukushima power plant, and on farms 100 km (70 miles) away.  \"The government has also acknowledged that the problem could be wider than just Fukushima.\" By August 2012, sampling of beef from affected areas revealed that 3 out of 58,460 beef samples contained radioactivity above regulatory limits.  Much of the radioactivity is believed to have come from contaminated feed.  Radioactivity infiltration into the beef supply has subsided with time, and is projected to continue decreasing. In August 2011, a group of 5 manufacturers of natto, or fermented soybeans, in Mito, Ibaraki planned to seek damages from TEPCO because their sales had fallen by almost 50 percent.  Natto is normally packed in rice-straw and after the discovery of caesium contamination, they had lost many customers.  The lost sales from April–August 2011 had risen to around 1.3 million dollars. On 3 September 2011 radioactive caesium exceeding the government's safety limit had been detected in tea leaves in Chiba and Saitama prefectures, near Tokyo.  This was the ministry's first discovery of radioactive substances beyond legal limits since the tests of food stuffs started in August.  These tests were conducted in order to verify local government data using different numbers and kinds of food samples.  Tea leaves of one type of tea from Chiba Prefecture contained 2,720 becquerels of radioactive caesium per kilogram, 5 times above the legal safety limit.  A maximum of 1,530 becquerels per kilogram was detected in 3 kinds of tea leaves from Saitama Prefecture.  Investigations were done to find out where the tea was grown, and to determine how much tea had already made its way to market.  Tea producers were asked to recall their products, when necessary.  As tea leaves are never directly consumed, tea produced from processed leaves are expected to contain no more than 1/35th the density of caesium (in the case of 2720bq/kg, the tea will show just 77bq/l, below the 200bq/l legal limit at the time) In the prefecture Shizuoka at the beginning of April 2012, tests done on tea-leaves grown inside a greenhouse were found to contain less than 10 becquerels per kilogram, below the new limit of 100 becquerels, The tests were done in a governmental laboratory in Kikugawa city, to probe cesium-concentrations before the at the end of April the tea-harvest season would start. The health ministry published in August 2012, that cesium levels in tea made from \"yacon\" leaves and in samples of Japanese tea \"shot through the ceiling\" this year. On 19 August radioactive caesium was found in a sample of rice.  This was in Ibaraki Prefecture, just north of Tokyo, in a sample of rice from the city of Hokota, about 100 miles south of the nuclear plant.  The prefecture said the radioactivity was well within safe levels: it measured 52 becquerels per kilogram, about one-tenth of the government-set limit for grains.  Two other samples tested at the same time showed no contamination.  The Agriculture Ministry said it was the first time that more than trace levels of cesium had been found in rice. On 16 September 2011 the results were published of the measurements of radioactive cesium in rice.  The results were known of around 60 percent of all test-locations.  Radioactive materials were detected in 94 locations, or 4.3 percent of the total.  But the highest level detected so far, in Fukushima prefecture, was 136 becquerels per kilogram, about a quarter of the government's safety limit of 500 Becquerel per kilogram.  Tests were conducted in 17 prefectures, and were completed in more than half of them.  In 22 locations radioactive materials were detected in harvested rice.  The highest level measured was 101.6 becquerels per kilogram, or one fifth of the safety limit.  Shipments of rice did start in 15 prefectures, including all 52 municipalities in the prefecture Chiba.  In Fukushima shipments of ordinary rice did start in 2 municipalities, and those of early-harvested rice in 20 municipalities. On 23 September 2011 radioactive caesium in concentrations above the governmental safety limit was found in rice samples collected in an area in the northeastern part of the prefecture Fukushima.  Rice-samples taken before the harvest showed 500 becquerels per kilogram in the city of Nihonmatsu.  The Japanese government ordered a two way testing procedure of samples taken before and after the harvest.  Pre-harvest tests were carried out in nine prefectures in the regions of Tohoku and Kanto.  After the find of this high level of cesium, the prefectural government dis increase the number of places to be tested within the city from 38 to about 300.  The city of Nihonmatsu held an emergency meeting on 24 September with officials from the prefecture government.  The farmers, that already had started harvesting, were ordered to store their crop until the post-harvest tests were available. On 16 November 630 becquerels per kilogram of radioactive cesium was detected in rice harvested in the Oonami district in Fukushima City. All rice of the fields nearby was stored and none of this rice had been sold to the market.  On 18 November all 154 farmers in the district were asked to suspend all shipments of rice.  Tests were ordered on rice samples from all 154 farms in the district.  The result of this testing was reported on 25 November: five more farms were found with cesium contaminated rice at a distance of 56 kilometers from the disaster reactors in the Oonami district of Fukushima City, The highest level of cesium detected was 1,270 becquerels per kilogram. On 28 November 2011 the prefecture of Fukushima reported the find of cesium-contaminated rice, up to 1050 Becquerels per kilogram, in samples of 3 farms in the city Date at a distance of 50 kilometers from the Fukushima Daiichi reactors.  Some 9 kilo's of this crops were already sold locally before this date.  Officials tried to find out who bought this rice.  Because of this and earlier finds the government of the prefecture Fukushima decided to control more than 2300 farms in the whole district on cesium-contamination.  A more precise number was mentioned by the Japanese newspaper The Mainichi Daily News: on 29 November orders were given to 2381 farms in Nihonmatsu and Motomiya to suspend part of their rice shipments.  This number added to the already halted shipments at 1941 farms in 4 other districts including Date, raised the total to 4322 farms. Rice exports from Japan to China became possible again after a bilateral governmental agreement in April 2012.  With government-issued certificates of origin Japanese rice produced outside the prefectures Chiba, Fukushima prefecture, Gunma, Ibaraki, Niigata, Nagano, Miyagi, Saitama, Tokyo, Tochigi and Saitama was allowed to be exported.  In the first shipment 140.000 tons of Hokkaido rice of the 2011 harvest was sold to China National Cereals, Oils and Foodstuffs Corporation. On 7 February 2012 noodles contaminated with radioactive cesium (258 becquerels of cesium per kilogram) were found in a restaurant in Okinawa.  The noodles, called \"Okinawa soba\", were apparently produced with water filtered through contaminated ashes from wood originating from the prefecture Fukushima.  On 10 February 2012 the Japanese Agency for Forestry set out a warning not to use ashes from wood or charcoal, even when the wood itself contained less than the governmental set maximum of 40 becquerels per kilo for wood or 280 becquerels for charcoal.  When the standards were set, nobody thought about the use of the ashes to be used for the production of foods.  But, in Japan it was a custom to use ashes when kneading noodles or to take away a bitter taste, or \"aku\" from \"devil's tongue\" and wild vegetables. On 13 October 2011 the city of Yokohama terminated the use of dried shiitake-mushrooms in school lunches after tests had found radioactive cesium in them up to 350 becquerels per kilogram.  In shiitake mushrooms grown outdoors on wood in a city in the prefecture Ibaraki, 170 kilometers from the nuclear plant, samples contained 830 becquerels per kilogram of radioactive cesium, exceeding the government's limit of 500 becquerels.  Radioactive contaminated shiitake mushrooms, above 500 becquerels per kilogram, were also found in two cities of prefecture Chiba, therefore restrictions were imposed on the shipments from these cities. On 29 October the government of the prefecture Fukushima Prefecture announced that shiitake mushrooms grown indoors at a farm in Soma, situated at the coast north from the Fukushima Daiichi plant, were contaminated with radioactive cesium: They contained 850 becquerels per kilogram, and exceeded the national safety-limit of 500-becquerel.  The mushrooms were grown on beds made of woodchips mixed with other nutrients.  The woodchips in the mushroom-beds sold by the agricultural cooperative of Soma were thought to have caused of the contamination.  Since 24 October 2011 this farm had shipped 1,070 100-gram packages of shiitake mushrooms to nine supermarkets.  Besides these no other shiitake mushrooms produced by the farm were sold to customers. In the city of Yokohama in March and October food was served to 800 people with dried shiitake-mushrooms that came from a farm near this town at a distance of 250 kilometer from Fukushima.  The test-results of these mushrooms showed 2,770 Becquerels per kilo in March and 955 Becquerels per kilo in October, far above the limit of 500 Becquerels per kilo set by the Japanese government.  The mushrooms were checked for contamination in the first week of November, after requests of concerned people with questions about possible contamination of the food served.  No mushrooms were sold elsewhere. On 10 November 2011 some 120 kilometers away southwest from the Fukushima-reactors in the prefecture Tochigi 649 becquerels of radioactive cesium per kilogram was measured in kuritake mushrooms.  Four other cities of Tochigi did already stop with the sales and shipments of the mushrooms grown there.  The farmers were asked to stop all shipments and to call back the mushrooms already on the market. The regulatory safe level for iodine-131 and caesium-137 in drinking water in Japan are 100 Bq/kg and 200 Bq/kg respectively.  The Japanese science ministry said on 20 March that radioactive substances were detected in tap water in Tokyo, as well as Tochigi, Gunma, Chiba and Saitama prefectures.  IAEA reported on 24 March that drinking water in Tokyo, Fukushima and Ibaraki had been above regulatory limits between 16 and 21 March.  On 26 March, IAEA reported that the values were now within legal limits.  On 23 March, Tokyo drinking water exceeded the safe level for infants, prompting the government to distribute bottled water to families with infants.  Measured levels were caused by iodine-131 (I-131) and were 103, 137 and 174 Bq/l.  On 24 March, iodine-131 was detected in 12 of 47 prefectures, of which the level in Tochigi was the highest at 110 Bq/kg.  Caesium-137 was detected in 6 prefectures but always below 10 Bq/kg.  On 25 March, tap water was reported to have reduced to 79 Bq/kg and to be safe for infants in Tokyo and Chiba but still exceeded limits in Hitachi and Tokaimura.  On 27 April, \"radiation in Tokyo's water supply fell to undetectable levels for the first time since 18 March.\" The following graphs show Iodine-131 water contaminations measured in water purifying plants From 16 March to 7 April: On 2 July samples of tapwater taken in Tokyo Shinjuku ward radioactive caesium-137 was detected for the first time since April.  The concentration was 0.14 becquerel per kilogram and none was discovered yesterday, which compares with 0.21 becquerel on 22 April, according to the Tokyo Metropolitan Institute of Public Health.  No caesium-134 or iodine-131 was detected.  The level was below the safety limit set by the government.  \"This is unlikely to be the result of new radioactive materials being introduced, because no other elements were detected, especially the more sensitive iodine\", into the water supply, were the comments of Hironobu Unesaki, a nuclear engineering professor at Kyoto University. Small amounts of radioactive iodine were found in the breast milk of women living east of Tokyo.  However, the levels were below the safety limits for tap water consumption by infants.  Regulatory limits for infants in Japan are several levels of magnitude beneath what is known to potentially affect human health.  Radiation protection standards in Japan are currently stricter than international recommendations and the standards of most other states, including those in North America and Europe .  By Nov 2012, no radioactivity was detected in Fukushimas mothers breast milk.  100% of samples contained no detectable amount of radioactivity. Mid November 2011 radioactive cesium was found in milk-powder for baby-food produced by the food company Meiji Co. Although this firm was warned about this matter three times, the matter was taken seriously by its consumer service after it was approached by Kyodo News.  Up to 30.8 becquerels per kilogram was found in Meiji Step milk powder.  While this is under the governmental safety-limit of 200 becquerels per kilogram, this could be more harmful for young children.  Because of this cesium-contaminated milk powder, the Japanese minister of health Yoko Komiyama said on 9 December 2011 at a press conference, that her ministry would start regularly tests on baby food products in connection with the Fukushima Daiichi nuclear plant crisis, every three months and more frequently when necessary.  Komiyama said: \"As mothers and other consumers are very concerned (about radiation), we want to carry out regular tests\", Test done by the government in July and August 2011 on 25 baby products did not reveal any contamination. In a survey by the local and central governments conducted on 1,080 children aged 0 to 15 in Iwaki, Kawamata and Iitate on 26–30 March, almost 45 percent of these children had experienced thyroid exposure to radiation with radioactive iodine, although in all cases the amounts of radiation did not warrant further examination, according to the Nuclear Safety Commission on Tuesday 5 July.  In October 2011, hormonal irregularies in 10 evacuated children were reported.  However, the organization responsible for the study said that no link had been established between the children's condition and exposure to radiation. On 9 October a survey started in the prefecture Fukushima: ultrasonic examinations were done of the thyroid glands of all 360,000 children between 0 and 18 years of age.  Follow-up tests will be done for the rest of their lives.  This was done in response to concerned parents, alarmed by the evidence showing increased incidence of thyroid cancer among children after the 1986 Chernobyl disaster.  The project was done by the Medical University of Fukushima.  The results of the tests will be mailed to the children within a month.  At the end of 2014 the initial testing of all children should be completed, after this the children will undergo a thyroid checkup every 2 years until they turn 20, and once every 5 years above that age. In November 2011 in urine-samples of 1500 pre-school-children (ages 6 years or younger) from the city of Minamisoma in the prefecture Fukushima radioactive cesium was found in 104 cases.  Most had levels between 20 and 30 becquerels per liter, just above the detection limit, but 187 becquerels was found in the urine of a one-year-old baby boy.  The parents had been concerned about internal exposure.  Local governments covered the tests for elementary schoolchildren and older students.  According to RHC JAPAN a medical consultancy firm in Tokyo, these levels could not harm the health of the children.  But director Makoto Akashi of the National Institute of Radiological Sciences said, that although those test results should be verified, this still proved the possibility of internal exposure in the children of Fukushima, but that the internal exposure would not increase, when all food was tested for radioactivity before consumption. Also in July citizens groups reported that a survey of soil at four places in the city of Fukushima taken on 26 June proved that all samples were contaminated with radioactive caesium, measuring 16,000 to 46,000 becquerels per kilogram and exceeding the legal limit of 10,000 becquerels per kg, A study published by the PNAS found that cesium 137 had \"strongly contaminated the soils in large areas of eastern and northeastern Japan.\" After the find of 8,000 becquerels of caesium per kilogram in wild mushrooms, and a wild boar that was found with radioactivity amounts about 6 times the safety limit, Professor Yasuyuki Muramatsu at the Gakushuin University urged detailed checks on wild plants and animals.  Radioactive caesium in soil and fallen leaves in forests in his opinion would be easily absorbed by mushrooms and edible plants.  He said that wild animals like boars were bound to accumulate high levels of radioactivity by eating contaminated mushrooms and plants.  The professor added that detailed studies were on wild plants and animals.  Across Europe the Chernobyl-incident had likewise effects on wild fauna and flora. The first study of the effects of radioactive contamination following the Fukushima Daiichi nuclear disaster suggested, through standard point count censuses that the abundance of birds was negatively correlated with radioactive contamination, and that among the 14 species in common between the Fukushima and the Chernobyl regions, the decline in abundance was presently steeper in Fukushima.  However criticism of this conclusion is that naturally there would be less bird species living on a smaller amount of land, that is, in the most contaminated areas, than the number one would find living in a larger body of land, that is, in the broader area. Scientists in Alaska are testing seals struck with an unknown illness to see if it is connected to radiation from Fukushima. Japanese bloggers are also reporting sick birds that cannot fly in the Fukushima area. About a year after the nuclear disaster some Japanese scientists found what they regarded was an increased number of mutated butterflies.  In their paper, they said, this was an unexpected finding, as \"insects are very resistant to radiation.\"  Since these are recent findings, the study suggests that these mutations have been passed down from older generations.  Timothy Jorgensen, of the Department of Radiation Medicine and the Health Physics Program of Georgetown University raised a number of issues with this \"simply not credible\" paper, in the journal \"Nature\" and concluded that the team's paper is \"highly suspect due to both their internal inconsistencies and their incompatibility with earlier and more comprehensive radiation biology research on insects\". Radioactive cesium was found in high concentration in plankton in the sea near the Fukushima Daiichi Nuclear Power Plant.  Samples were taken up to 60 kilometers from the coast of Iwaki city in July 2011 by scientists of the Tokyo University of Marine Science and Technology.  Up to 669 becquerels per kilogram of radioactive cesium was measured in samples of animal plankton taken 3 kilometers offshore.  The leader of the research-group Professor Takashi Ishimaru, said that the sea current continuously carried contaminated water southwards from the plant.  Further studies to determine the effect on the food-chain and fish would be needed. Detectable levels of radiation were found in an apartment building in Nihonmatsu, Fukushima, where the foundation was made using concrete containing crushed stone collected from a quarry near the troubled Fukushima Daiichi nuclear power plant, situated inside the evacuation-zone.  Of the 12 households living there were 10 households relocated after the quake.  After inspection at the quarry – situated inside the evacuation-zone around the nuclear plant—in the town of Namie, Fukushima between 11 and 40 microsieverts of radiation per hour were detected one meter above gravel held at eight storage sites in the open, while 16 to 21 microsieverts were detected in three locations covered by roofs.  From this place about 5,200 metric tons of gravel was shipped from this place and used as building material.  On 21 January 2012 the association of quarry agents in the prefecture Fukushima asked its members to voluntarily check their products for radioactivity to ease public concerns over radioactive contamination of building materials.  The minister of Industry Yukio Edano did instruct TEPCO to pay compensation for the economical damages.  Raised radiation levels were found on many buildings constructed after the quake.  Schools, private houses, roads.  Because of the public anger raised by these finds.  the government of Nihonmatsu, Fukushima decided to examine all 224 city construction projects started after the quake.  Some 200 construction companies received stone from the Namie-quarry, and the material was used in at least 1000 building-sites.  The contaminated stone was found in some 49 houses and apartments.  Radiation levels of 0.8 mSv per hour were found, almost as high as the radiation levels outside the homes.  None of these represents a potential danger to human health. On 22 January 2012, the Japanese government survey had identified around 60 houses built with the radioactive contaminated concrete.  Even after 12 April 2011, when the area was declared to be an evacuation zone, the shipments continued, and the stone was used for building purposes. In the first weeks of February 2012 up to 214,200 becquerels of radioactive caesium per kilogram was measured in samples gravel in the quarry near Namie, situated inside the evacuation zone.  The gravel stored outside showed about 60,000–210,000 becquerels of caesium in most samples.  From the 25 quarries in the evacuation zones, up to 122,400 becquerels of radioactive caesium was found at one that has been closed since the nuclear crisis broke out on 11 March 2011.  In one quarry, that is still operational 5,170 becquerels per kilogram was found.  Inspections were done at some 150 of the 1.100 construction sites, where the gravel form the Namie-quarry was suspected to be used.  At 27 locations the radioactivity levels were higher than the surrounding area. On 6 May 2012 it became known that according to documents of the municipal education board reports submitted by each school in Fukushima prefecture in April at least 14 elementary schools, 7 junior high and 5 nursery schools so called \"hot spots\" existed, where the radiation exposure was more than 3.8 microsieverts per hour, resulting in an annual cumulative dose above 20 millisieverts.  However all restrictions, that limited the maximum time to three hours for the children to play outside at the playgrounds of the schools, were lifted at the beginning of the new academic year in April by the education board.  The documents were obtained by a group of civilians after a formal request to disclose the information.  Tokiko Noguchi, the foreman of a group of civilians, insisted that the education board would restore the restrictions. On 22 December 2011 the Japanese government announced new limits for radioactive cesium in food.  The new norms would be enforced in April 2012. On 31 March 2012 the Ministry of Health, Labor and Welfare of Japan published a report on radioactive cesium found in food.  Between January and around 15 March 2012 at 421 occasions food was found containing more than 100 becquerels per kilogram cesium.  All was found within 8 prefectures: Chiba, Fukushima Prefecture (285 finds), Gunma, Ibaraki (36 finds), Iwate, Miyagi, Tochigi (29 finds) and Yamagata.  Most times it involved fish: landlocked salmon and flounder, seafood, after this: Shiitake-mushrooms or the meat of wild animals. In the first week of April 2012 cesium-contamination above legal limits was found in: In Gunma prefecture 106 becquerels/kg was found in beef.  Sharper limits for meat would be taken effect in October 2012, but in order to ease consumer concern the farmers were asked to refrain from shipping. In the last week of August Prime Minister Naoto Kan informed the Governor of Fukushima Prefecture about the plans to build a central storage facility to store and treat nuclear waste including contaminated soil in Fukushima.  On 27 August at a meeting in Fukushima City Governor Yuhei Sato spoke out his concern about the sudden proposals, and the implications that this would have for the prefecture and its inhabitants, that had already endured so much from the nuclear accident.  Kan said, that the government had no intention to make the plant a final facility, but the request was needed in order to make a start with decontamination. Radioactivity from the disaster was found in kelp off of Coastal California. According to a Professor at Stanford, there were some meteorological effects involved and that \"81 percent of all the emissions were deposited over the ocean\" instead of mainly being spread inland. Seawater containing measurable levels of iodine-131 and caesium-137 were collected by Japan Agency for Marine-Earth Science and Technology (JAMSTEC) on 22–23 March at several points 30 km from the coastline iodine concentrations were \"at or above Japanese regulatory limits\" while caesium was \"well below those limits\" according to an IAEA report on 24 March.  On 25 March, IAEA indicated that in the long term, caesium-137 (with a half-life of 30 years) would be the most relevant isotope as far as doses was concerned and indicated the possibility \"to follow this nuclide over long distances for several years.\"  The organization also said it could take months or years for the isotope to reach \"other shores of the Pacific\". The survey by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) reveals that radioactive cesium released from Fukushima I Nuclear Power Plant reached the ocean 2000 kilometers from the plant and 5000 meters deep one month after the accident.  It is considered that airborne cesium particles fell on the ocean surface, and sank as they were attached to the bodies of dead plankton.  The survey result was announced in a symposium held on 20 November in Tokyo.  From 18 to 30 April, JAMSTEC collected \"marine snow\", sub-millimeter particles made mostly of dead plankton and sand, off the coast of Kamchatka Peninsula, 2000 kilometers away from Fukushima, and off the coast of Ogasawara Islands, 1000 kilometers away, at 5000 meters below the ocean surface.  The Agency detected radioactive cesium in both locations, and from the ratio of cesium-137 and cesium-134 and other observations it was determined that it was from Fukushima I Nuclear Power Plant.  The density of radioactive cesium is still being analyzed, according to the Agency.  It has been thus confirmed that radioactive materials in the ocean are moving and spreading not just by ocean currents but by various other means. The United Nations predicted that the initial radioactivity plume from the stricken Japanese reactors would reach the United States by 18 March.  Health and nuclear experts emphasized that radioactivity in the plume would be diluted as it traveled and, at worst, would have extremely minor health consequences in the United States.  A simulation by the Belgian Institute for Space Aeronomy indicated that trace amounts of radioactivity would reach California and Mexico around 19 March.  These predictions were tested by a worldwide network of highly sensitive radiative isotope measuring equipment, with the resulting data used to assess any potential impact to human health as well as the status of the reactors in Japan.  Consequently, by 18 March radioactive fallout including isotopes of iodine-131, iodine-132, tellurium-132, iodine-133, caesium-134 and caesium-137 was detected in air filters at the University of Washington, Seattle, USA. Due to an anticyclone south of Japan , favorable westerly winds were dominant during most of the first week of the accident, depositing most of the radioactive material out to sea and away from population centers, with some unfavorable wind directions depositing radioactive material over Tokyo.  Low-pressure area over Eastern Japan gave less favorable wind directions 21–22 March.  Wind shift to north takes place Tuesday midnight.  After the shift, the plume would again be pushed out to the sea for the next becoming days.  Roughly similar prediction results are presented for the next 36 hours by the Finnish Meteorological Institute.  In spite of winds blowing towards Tokyo during 21–22 March, he comments, \"From what I've been able to gather from official reports of radioactivity releases from the Fukushima plant, Tokyo will not receive levels of radiation dangerous to human health in the coming days, should emissions continue at current levels.\" Norwegian Institute for Air Research have continuous forecasts of the radioactive cloud and its movement.  These are based on the FLEXPART model, originally designed for forecasting the spread of radioactivity from the Chernobyl disaster. As of 28 April, the Washington State Department of Health, located in the U.S state closest to Japan, reported that levels of radioactive material from the Fukushima plant had dropped significantly, and were now often below levels that could be detected with standard tests. Fear of radiation from Japan prompted a global rush for iodine pills, including in the United States, Canada, Russia, Korea, China, Malaysia and Finland.  There is a rush for iodized salt in China.  A rush for iodine antiseptic solution appeared in Malaysia.  WHO warned against consumption of iodine pills without consulting a doctor and also warned against drinking iodine antiseptic solution.  The United States Pentagon said troops are receiving potassium iodide before missions to areas where possible radiation exposure is likely. The World Health Organisation (WHO) says it has received reports of people being admitted to poison centres around the world after taking iodine tablets in response to fears about harmful levels of radiation coming out of the damaged nuclear power plant in Fukushima. In Operation Tomodachi, the United States Navy dispatched the aircraft carrier USS \"Ronald Reagan\" and other vessels in the Seventh Fleet to fly a series of helicopter operations.  A U.S. military spokesperson said that low-level radiation forced a change of course en route to Sendai.  The \"Reagan\" and sailors aboard were exposed to \"a month's worth of natural background radiation from the sun, rocks or soil\" in an hour and the carrier was repositioned.  Seventeen sailors were decontaminated after they and their three helicopters were found to have been exposed to low levels of radioactivity. The aircraft carrier USS \"George Washington\" was docked for maintenance at Yokosuka Naval Base, about 280 km from the plant, when instruments detected radiation at 07:00 JST on 15 March.  Rear Admiral Richard Wren stated that the nuclear crisis in Fukushima, 320 km from Yokosuka, was too distant to warrant a discussion about evacuating the base.  Daily monitoring and some precautionary measures were recommended for Yokosuka and Atsugi bases, such as limiting outdoor activities and securing external ventilation systems.  As a precaution, the \"Washington\" was pulled out of its Yokosuka port later in the week.  The Navy also temporarily stopped moving its personnel to Japan. The isotope iodine-131 is easily absorbed by the thyroid.  Persons exposed to releases of I-131 from any source have a higher risk for developing thyroid cancer or thyroid disease, or both.  Iodine-131 has a short half-life at approximately 8 days, and therefore is an issue mostly in the first weeks after the incident.  Children are more vulnerable to I-131 than adults.  Increased risk for thyroid neoplasm remains elevated for at least 40 years after exposure.  Potassium iodide tablets prevent iodine-131 absorption by saturating the thyroid with non-radioactive iodine.  Japan's Nuclear Safety Commission recommended local authorities to instruct evacuees leaving the 20-kilometre area to ingest stable (not radioactive) iodine.  CBS News reported that the number of doses of potassium iodide available to the public in Japan was inadequate to meet the perceived needs for an extensive radioactive contamination event. Caesium-137 is also a particular threat because it behaves like potassium and is taken up by cells throughout the body.  Additionally, it has a long, 30-year half-life.  Cs-137 can cause acute radiation sickness, and increase the risk for cancer because of exposure to high-energy gamma radiation.  Internal exposure to Cs-137, through ingestion or inhalation, allows the radioactive material to be distributed in the soft tissues, especially muscle tissue, exposing these tissues to the beta particles and gamma radiation and increasing cancer risk.  Prussian blue helps the body excrete caesium-137. Strontium-90 behaves like calcium, and tends to deposit in bone and blood-forming tissue (bone marrow).  20–30% of ingested Sr-90 is absorbed and deposited in the bone.  Internal exposure to Sr-90 is linked to bone cancer, cancer of the soft tissue near the bone, and leukemia.  Risk of cancer increases with increased exposure to Sr-90. Plutonium is also present in the MOX fuel of the Unit 3 reactor and in spent fuel rods.  Officials at the International Atomic Energy Agency say the presence of MOX fuel does not add significantly to the dangers.  Plutonium-239 is long-lived and potentially toxic with a half-life of 24,000 years.  Some hazard hazardous for tens of thousands of years.  Radioactive products with long half-lives release less radioactivity per unit time than products with a short half life, as isotopes with a longer half life emit particles much less frequently.  For example, one mole (131 grams) of I releases 6x10 decays 99.9% of them within three months, whilst one mole (238 grams) of U releases 6x10 decays 99.9% of them within 45 billion years, but only about 40 parts per trillion in the first three months.  Experts commented that the long-term risk associated with plutonium toxicity is \"highly dependent on the geochemistry of the particular site.\" An overview for regulatory levels in Japan is shown in the table below:\n\nUniversity of Scranton buildings and landmarks The University of Scranton’s 58-acre hillside campus is located in the heart of Scranton, a community of 75,000 within a greater metropolitan area of 750,000 people, located in northeast Pennsylvania.  Founded in 1888 as St. Thomas College and elevated to university status in 1938, the University has grown and changed over time.  Since 1984, the University has completed over 50 renovation projections.  Over the past decade alone, the University’s campus has undergone a dramatic transformation, as the school has invested more than $240 million towards campus improvements and construction, including the Loyola Science Center, the DeNaples Center, Pilarz and Montrone Halls, Condron Hall, Edward R. Leahy, Jr.  Hall, and the Dionne Green. Completed in 1992, the Harry and Jeanette Weinberg Memorial Library was designed to replace the Alumni Memorial Library, which proved unable to serve adequately the growing student population, to house the vast library collections, and lacked the necessary wiring for modernizing the library with new technological advances.  More than double the size of the Alumni Memorial Library, the Weinberg Memorial Library has five floors which can seat anywhere from 700 to 1000 users at cubicles, tables, group study rooms, and lounges.  It currently houses 473,830 volumes, over 15,500 electronic journals, 562,368 microform pieces and 1,709 periodical subscriptions, both current and archived.  It is also home to the University Archives and Special Collections, which features many rare books, as well as university records.  On the third floor, there are a number of administrative offices as well as two large classrooms which are used for classes based on learning about the library and the services it can provide.  The fourth floor has a large reading room featuring a stained glass window and a comfortable, quiet environment in which students can study at tables and couches.  The fifth floor is the Scranton Heritage Room which is a large open hall featuring beautiful views of the city, the surrounding mountains, and the Commons as well as thirty-nine panel paintings by Trevor Southey depict art, religion, and science in the Lackawanna Valley and in the world.  Throughout the year, the Heritage Room hosts various exhibits including displays of artifacts and documents from the university’s archives and special collections, showcases of faculty scholarship and university alumni authors, and the library's Environmental Art Show.  The Heritage Room also serves as the venue for many campus and community events such as lectures, receptions, student award presentations, Game Night, and the library's annual Book Sale. Since its completion, the Library has continued to adapt to the needs of its students and to update its resources to ensure that the students and faculty have access to new technological innovations.  Renovations at the Library include the opening of multiple 24-hour study rooms, including the Pro Deo Room, the Reilly Learning Commons, and, most recently, the entire second floor.  The Pro Deo Room contains a computer lab with networked PCs, two laser printers, a vending machine area, and a Java City Café.  The Pro Deo Room also features a 46-inch touchscreen table PC.  The Pro Deo Room is open 24 hours a day, seven days a week.  In order to accommodate the growing needs of students for more 24-hour study space in the library, the Library built a new space in 2010 which contained more than one hundred study spaces for students at cubicles, tables, and couches.  This study space was renovated in 2014.  Renamed the Reilly Learning Commons, the study room is now an interactive space with high end technology, group study rooms and areas designed to enhance collaboration.  The Learning Commons houses a lecture capture room to practice presentations and record them digitally, two writing center offices, technology support, and brand new iMacs.  In Fall 2015, in response to student feedback, the entire second floor of the library was opened for 24-hour use, allowing for more access to carrels, computers, and space for quiet study. In order to raise the $13.3 million dollars needed to build the Library, the University of Scranton launched the “Gateway to the Future” Fundraising Campaign.  During his speech at the Gateway to the Future Library Kickoff, Rev. Panuska underscored the importance of building a library which could adequately serve the needs of the university community, stating that \"a library is the essential non-human instrument which contributes to our meaning.  It touches the arts as well as business and science.  It focuses both faculty and students on the intellectual aspect of university life, where the focus must be.  It keeps us in touch with the knowledge of the past and with what is happening today, therefore allowing us intelligently to form the future.  It is a center for the transmission of knowledge, today involving the must [sic] advanced electronic technology.”  In late 1989, Harry Weinberg, a former Scranton businessman and long-time benefactor of the University of Scranton, made significant headway in the fundraising goal by announcing a six million dollar donation to the university from the Harry and Jeanette Weinberg Foundation, with five million dollars going to the library and the other one million going to the school’s Judaic Studies Institute.  In order to honor the significant contribution of Mr. Weinberg, the new library was named for him and his wife. Before becoming home to the Weinberg Memorial Library, the site had once belonged to Worthington Scranton where he lived until moving to the Estate in 1899, at which point the house was converted into the Hahneman Hospital until it relocated in 1906 to the current Community Medical Center site.  In 1941, Scranton donated the land to the university.  In the 1950s, the site held the A Building barracks, which were purchased by the university in order to accommodate increased enrollment due to the GI Bill which were used as classrooms and offices, until they were demolished in 1962.  Until the construction of the Weinberg Memorial Library in the 1990s, the site housed asphalt playing courts. On January 31, 2006, the University announced plans for the DeNaples Center, a new $30,000,000 campus center that would replace Gunster Memorial Student Center and mark the University’s most ambitious project in its 118-year history.  In the four decades since Gunster had been constructed in 1960, the University of Scranton, as University President Father Pilarz said, “has evolved into a broadly regional, comprehensive institution with students coming from more than 30 states and more than 35 countries,\" and thus “has simply outgrown the 77,000 square foot Gunster Center, which was built for a time when only 228 of our total student enrollment of 2,300 lived on campus.” The first floor of the building includes a grand lobby, the campus bookstore, the student mail center, commuter lockers, a Provisions on Demand (P.O.D.) convenience and the DeNaples Food Court, a retail dining option with seating for 250, which includes Starbucks Coffee, Chick-Fil-A, and Quizno’s among other options.  The second floor offers a fireplace lounge, offices for Student Affairs, University Ministries and the Student Forum.  The Student Forum contains a computer lab for students to use, as well as student space with couches and tables.  The Student Forum is home to the Center for Student Engagement, including the offices for the University of Scranton Programming Board (USPB), the Aquinas newspaper, the Windhover yearbook, the Jane Kopas Women's Center, the Multicultural Center, Student Government, and Community Outreach.  The third floor serves as the primary dining space in the building and has seating for 800.  Run by ARAMARK’s “Fresh Food Company,” the third floor dining hall contains several different stations, including Southern Kitchen, a fresh produce market, a pasta station, a brick oven, Mediterranean deli, Brazilian grill, round grill and charbroiler, where all meals are made fresh to order.  The fourth floor includes a subdividable 7,000 sq. ft. ballroom with dinner seating for 425 and lecture seating for more than 700 and three multipurpose meeting rooms as well as the Ann and Leo Moscovitz Theater with 260 fixed seats which boasts high-definition video, surround sound and comfortable, oversized theater seating.  On September 13, 2009, the fourth floor ballroom was dedicated in honor of Rev. Bernard R. McIllhenny, S.J., who served as headmaster at Scranton Prep from 1958-1966 and dean of admissions at the University from 1966-1997. The DeNaples Center was the first building on campus designed and constructed to achieve the LEED certification as part of the University’s Sustainability initiatives, which it received in February 2009.  LEED stands for Leadership in Energy and Environmental Design, a cutting-edge system for certifying design, construction and operations of “green” buildings, coordinated by the U.S. Green Building Council.  Some environmentally friendly highlights of the DeNaples Center include reduced light pollution, a drip irrigation system, and optimized energy performance.  The building also features products with recycled content, daylight harvesting, occupancy sensors for lighting control, water-saving fixtures, and a kitchen ventilation system that monitors heat and smoke and will adjust ventilation requirements according to need and not merely according to building occupancy schedule. The DeNaples Center is named in honor of the late Patrick and Margaret DeNaples, the parents of Louis DeNaples Sr., a local business owner, active community volunteer and philanthropist, former University board member, and reputed organized crime associate.  The DeNaples Center was dedicated in February 2008.  Upon the completion and opening of the DeNaples Center, the Gunster Memorial Student Center was demolished and was replaced by the Dionne Green, a large green space located directly in front of the DeNaples Center. For twenty-five years, there had been an effort by the University of Scranton to close the 900 and 1000 blocks of Linden Street which ran through the school’s campus.  In 1980, the improvement project was actualized.  The Commons project was intended to create a more attractive, park-like atmosphere on the campus and to eliminate the safety hazards associated with pedestrian and vehicle traffic.  With that new space, the university hoped to create a 20-foot-wide brick walkway, trees, benches, a water fountain, and patio area in addition to developing the area with landscaping. The University Commons proposal was approved by the Scranton City Council on December 20, 1978.  Construction on the project was begun on June 2, 1980, as parts of Linden Street were removed.  The project was completed around November 1980 and dedication ceremonies were held in December 1980.  Currently, it serves as the main walkway through the university's campus. In 1991, the University Commons was extended on the 300 block of Quincy Avenue between Linden and Mulberry Streets, which had been closed to vehicular traffic and owned by the university since 1987.  This pedestrian pathway, named Royal Way, serves as an official entrance to the university and the GLM (Gannon-Lavis-McCormick) student residences. At the time of its construction, the 24-foot-wide Royal Way was paved in z-brick and featured landscaping with trees and shrubs.  The Mulberry Street entrance to the Royal Way featured a campus gate, a gift from the University of Scranton Classes of 1985, 1990 and 1991, and the opposing terminus was Metanoia, the bronze sculpture of St. Ignatius by Gerard Baut.  The sculpture has since been moved to the opposite side of the University Commons, in front of the Long Center. Completed in 1960, the two-story building, formerly called Alumni Memorial Library, was designed to hold 150,000 volumes; the collection at the time numbered approximately 62,000 volumes.  It also had study space for approximately 500 students.  The split-level design also included conference rooms, a music room, a visual aid room, microfilm facilities, and a smoking lounge.  The buff iron-spot building was considered cutting edge at the time, with glare-reducing thermo-pane glass, noise-reducing solid brick walls, radiant heating and cooling, and humidity control.  Although originally estimated at $750,000, overall construction costs were approximately $806,000 after complications occurred when a massive mining cavity, complete with a network of surrounding tunnels, was discovered to lie only forty feet below the surface of the building site.  Using a digging rig brought in from Texas, contractors sunk 33 steel casings into the ground, each more than 40 feet long, and then poured concrete through them to form pillars in order to support the structure.  To raise money for the construction, a fundraising campaign led by Judge James F. Brady sought individual contributions from each of the university's alumni. The building was extensively renovated in 1993 after the completion of the new Weinberg Memorial Library.  No longer needed to house the university’s book collection or to serve as a study space for students, Alumni Memorial Hall was converted to house the Psychology Department on the second floor, which had formerly been located in O’Hara Hall, as well as the Division of Planning and Information Resources, which was formerly known as the University Computing and Data Services Center.  The new location in Alumni Memorial Hall “significantly enhance[d] educational and research facilities” for the Psychology Department, as John Norcross, chairman of the Psychology Department, remarked.  It offers faculty offices with adjoining labs, two classrooms designated for psychology, a conference room, open labs for students, facilities which better house experimental equipment, and state of the art animal facilities.  The space for the University Computing System included a main computer room, private study areas, several computer labs such as a DOS lab and a Macintosh lab, help desks, staff offices and areas for computer programmers.  Currently, the Division of Information Resources is responsible for providing technology-based services and solutions that support the work of the university, and guide the management of its information assets through the departments of Project Management, Network Infrastructure, Database Systems & Data Processing, Systems & Operations, Information Security Offices, IT Services, OIT Services, the Technology Support Center, and IT Development & Applications.  Originally, the renovations also provided space for the offices for Institutional Research, Learning Resources and Instructional Development, which was later renamed the Center for Teaching and Learning Excellence and relocated to the fifth floor of the Loyola Science Center. Completed in 2000, Brennan Hall houses the departments of the Arthur J. Kania School of Management, or KSOM.  The five-story, 71,000-square-foot building, located on the east side of Madison Avenue, features nine classrooms, seminar rooms, offices, a 140-seat auditorium, a quiet study area, an advising center, board rooms, and an Executive Education Center. The classrooms are located on the first two floors of Brennan Hall.  Two of the nine classrooms are two tiered case-study rooms equipped for video teleconferencing.  Two other classrooms are computers rooms, while the rest are traditional classrooms.  Each of the nine classrooms contains a principal teaching station, or the \"control center for the classroom.\"  The workstation allows teachers to control the classroom's lighting, display screens and computers.  Teachers will be able to connect to The university's video library and access satellite television.  Additionally, the building possesses a self-contained network accessible from each classroom seat.  In 2008, the university dedicated one of Brennan Hall’s classrooms.  The Jack and Jean Blackledge Sweeney Classroom on the first floor honors Jack Sweeney '61, the retired president and co-founder of Special Defense Systems in Dunmore, a member of the Pride, Passion, Promise Campaign Executive Committee, and an active University of Scranton alumni. The first floor also contains the Irwin E. Alperin Financial Center, which was opened in 2007.  The Alperin Center was designed to simulate a stock market trading floor, complete with an electronic ticker and data displays, 40 computers, a surround sound system, conference facilities, and a network of specialized software-designed to support the Kania School business curriculum with simulation capabilities and faculty-student research on financial and commodity markets.  Created to empower Kania faculty to use high-tech teaching tools to instruct students in concepts that will prepare them for the workforce, the Center provides a space where students could explore concepts like portfolio construction and risk management.  The Center is dedicated in honor of the late Irwin E. Alperin, a benefactor of the University of Scranton and a driving force in the economic growth of Scranton and Northeastern Pennsylvania through his company, Alperin Inc. which provided employment opportunities for local residents.  He had previously generously contributed to the University of Scranton, establishing the Alperin Chair, the first chair in the Kania School of Management, as well as a scholarship that provides financial assistance for outstanding students who otherwise might not be able to pursue a college education. The third and fourth floors house faculty offices, departmental offices, the dean’s office, and conference rooms.  There is a behavioral lab for teaching and research purposes, meeting and storage places for clubs and an MBA lounge that will include locker space for master’s degree students. The fifth floor houses the Executive Education Center.  The Executive Center includes five main areas: a dining room, a board room, a meeting room, a large reception area, and an auditorium on the second floor.  The center provides a businesslike environment for the university to host an expanded offering of continuing education programs, particularly in the areas of professional development and training in the use of new technologies.  The Executive Education Center provides technologically advanced conference space for the university, and businesses and organizations throughout northeastern Pennsylvania.  In 2005, it was named the Joseph M. McShane, S.J., Executive Center to commemorate not only Father McShane’s tenure as president of the University of Scranton from 1998 until 2003 but also his vision and drive to create a home for the Kania School of Management that “would serve as common ground for the university and the broader community.”  The Pearn Auditorium, which seats 140, serves as a gathering space for various lectures, presentations and community events.  The 148-seat auditorium is equipped with the latest technology, including a network connection at every seat.  The auditorium features Dolby surround sound, theater-style seating, a portable lectern with a touch screen control system and teleconference capabilities.  Dedicated in 2008, the James F. Pearn Auditorium on the second floor of Brennan Hall is named for the late father of Frank Pearn ‘83, the chief administrative officer of the Mergers and Acquisitions Division of Lehman Brothers, the chair of the university's Economic Strength Committee of the board of trustees, and a member of the Campaign Executive Committee.  Dedicated in 2008, the Rose Room, located on the fifth floor of Brennan Hall, is used for lectures, dinners, luncheons, seminars, and other campus events.  It can accommodate more than 200 people.  It honors Harry Rose '65, the president and chief executive officer of The Rose Group, a restaurant management company, a member of the university's board of trustees, and a member of the Campaign Executive Committee.  The Executive Center also contains a 50-seat board room which is used by various governing boards of the university, including the board of trustees, University Council and University Senate.  In 2003, the University of Scranton named the board room in honor of PNC Bank to recognize a significant grant from the PNC Foundation for the construction of Brennan Hall and to acknowledge the support PNC has consistently provided to the university, as \"PNC Bank has been a loyal friend of the University of Scranton and a generous supporter of our educational mission and commitment to serve those living in northeast Pennsylvania.”  Additional facilities of the Executive Center, which is available to organizations outside the university, include a lobby and reception area, and a meeting room accommodating 20 people. Brennan Hall also holds 38 photographic reproductions of stained glass windows from eight churches throughout northeastern Pennsylvania, photographed by Guy Cali Associates.  Churches represented include St. Luke's Episcopal Church, Nativity Church, Covenant Presbyterian Church, and St. Peter's Cathedral, Scranton; First Presbyterian Church and Memorial Presbyterian Church, Wilkes-Barre; Trinity Episcopal Church, Carbondale; and Grace Church, Honesdale.  The poster-size prints hang throughout the hallways and in the boardroom of Brennan Hall, which houses the Kania School of Management.  The Tiffany Glass Company produced several of the stained glass windows represented, including \"Triumphal Entry into Jerusalem\" and \"The Ascension\" from Covenant Presbyterian Church as well as “The Nativity” from St. Luke’s Episcopal Church.  For University of Scranton President Joseph M. McShane, S.J., Brennan Hall represented a place to highlight regional treasures that are rarely seen outside of each church's own congregation.  Additionally, the stained glass window reproductions “symbolize the University of Scranton's history of service to the people of Northeastern Pennsylvania and proclaim the university's continued commitment to this region for the 21st century.” Financed by the Campaign of Scranton, a $35 million capital fundraising effort, Brennan Hall cost $11.5 million dollars to construct.  The funds raised to build Brennan Hall included a $3.5 million gift from alumnus John E. Brennan ‘68 and $1 million of a $4 million gift from alumnus Arthur J. Kania ‘53, for whom the School of Management is named, with additional Campaign funds coming from alumni, friends of the university, corporations, and foundations.  In order to recognize Brennan’s generous contribution to the university, the new building was named in his honor.  John E. Brennan is the president of Activated Communications, New York City; a director and vice-chairman of the Board of Southern Union Company; a member of the board of directors for Spectrum Signal Processing; and a founder of Metro Mobile CTS, Inc., and served as its president and chief operating officer until its sale to Bell Atlantic Corp. Ciszek Hall, formerly known as the Center for Eastern Christian Studies, was built as an ecumenical and academic institute designed to promote knowledge about and understanding of the religious and cultural traditions of Eastern Christianity.  In addition to the Byzantine Rite chapel in the building, the Center was designed to house a 15,000-volume library, office, social area, and a cloister garden.  Construction was begun in 1987 and completed later that year.  The Center for Eastern Christian Studies was renamed Ciszek in 2005 in the memory of Fr.  Walter Ciszek, S.J., a native of northeastern Pennsylvania and a candidate for sainthood who spent twenty-three years ministering in Soviet prisons and the labor camps of Siberia.  Currently, Cisek Hall houses the university’s Office of Career Services, a chapel which celebrates service in the Byzantine Rite, and a library containing 15,000 books. In November 2013, the University broke ground on its newest building, the 111,500-square-foot, eight-story rehabilitation center designed to house the departments of Exercise Science, Occupational Therapy, and Physical Therapy.  Leahy Hall contains 25 interactive rehabilitation laboratories, 9 traditional and active-learning classrooms, research facilities, multiple simulation environments, more than 50 faculty offices, and 9 group study rooms.  A unified entrance for Leahy Hall and McGurrin Hall was also created, in order to promote and allow interaction between the various departments in the Panuska College of Professional Studies, the rest of which, including Nursing, Education, Counseling & Human Services, Health Administration and Human Resources, are housed in McGurrin Hall.  At 140 feet, it is now the tallest building on the University campus.  The building was designed for and was constructed in accordance with Leadership in Energy and Environmental Design (LEED) standards for certification. Leahy Hall contains 25 different laboratories, including three pediatric laboratories, focused on the physical, mental and emotional development of children.  Some of its laboratories are the Human Motion Laboratory, the Strength Laboratory, the Physiology Laboratory, the Human Anatomy Laboratory, the Active Learning Laboratory, the Body Composition Laboratory, the Therapeutic Modalities and Orthopedic Physical Therapy Laboratory, the Rehabilitation and Neurological Physical Therapy Laboratory, the Pediatrics Gross Motor Laboratory, the Kinesiology and Physical Rehabilitation Occupational Therapy Laboratory, the Occupational Performance Laboratory, the Hand and Rehabilitation Laboratory, and the Pediatric and Rehabilitation Suite containing the Gross Motor Rehabilitation Lab, Fine Motor Rehabilitation Lab, and the Sensory/Snoezelen Room.  The facility also includes a simulated neighborhood to help patients relearn daily living skills with an apartment, car and garage, bus stop, grocery store, and street as well as simulated hospital patient rooms for acute and long-term care designed to help students develop their patient-management skills.  These simulation rooms allow “students to get the experience of how, for example, people in wheelchairs navigate narrow spaces or place items on a conveyor belt in a supermarket,” says Julie Ann Nastasi, OTD, an occupational therapy faculty specialist at the University.  It includes a forum for lectures and events that can accommodate seating for 260 guests.  The new building includes a patient drop-off area in Kessler Court, an Einstein Bros Bagels Cafe, and a green roof and patio, designed to be used as an event space and to serve as an outdoor laboratory surfaced with different materials, garden beds of varying heights and work areas accessible to users with differing abilities. Leahy Hall is located on the former site of the Scranton chapter of the Young Women’s Christian Association, on the southwest corner of Jefferson Avenue and Linden Street.  Originally constructed in 1907 and purchased by the University of Scranton in 1976, the YWCA was transformed into Jefferson Hall, serving as an off-campus residence for University students until the building was converted into old Leahy Hall in 1984, used to house facilities for the University’s Physical Therapy and Occupational Therapy departments. Leahy Hall opened for the Fall 2015 semester and was dedicated on September 18, 2015 as Edward R. Leahy, Jr.  Hall, bearing the same name as the hall it replaced to recognize and honor the Leahy family for their service to the University, particularly in their endowment of health care education, dating back to the early 1990s.  The son of Edward and Patricia Leahy, Edward R. Leahy, Jr., was born in 1984 with cerebral palsy and several related disabilities.  He died shortly before his ninth birthday in 1993. In 1986, the University of Scranton acquired the former Immanuel Baptist Church at the corner of Jefferson Avenue and Mulberry Street in order to house the school’s Performance Music Program, which includes the university’s Orchestra, Bands, and Singers, as well as to serve as a site for musical and other arts performances, lectures, and special liturgies.  The church was built in 1909 in the Victorian Gothic style.  In 1984, the church was vacated when the congregation merged with the Bethany and Green Ridge Baptist churches before being acquired by the University of Scranton.  After its purchase by the university, the building underwent extensive renovations and restoration, including plaster repair and floor refinishing, painting and carpeting, extension of the stage, electrical re-wiring, new lighting, a new sound system, refurbishing the organ, pressure cleaning and restoration of the building’s masonry, and the installation of a new roof. The main floor of the building houses the Aula, a concert hall which can seat approximately 650 people; the Atrium, a large space which can be used as a recital, reception, or lecture hall that can seat 400 people and formerly served as the church's Sunday School; the Wycliffe A. Gordon Guest Artist Hospitality Suite, and the sound control room.  The ground floor of the building includes a large rehearsal hall, small ensembles areas, a musicians' lounge, practice rooms, offices, music library, and secure instrument storage and repair areas.  The Nelhybel Collection Research Room is on the top floor, along with the organ loft and organ chamber.  Houlihan-McLean features an historic 1910 Austin Opus 301 symphonic pipe organ, one of only a few surviving examples of early 20th-century organ building.  The 3,157 pipes, which include some as large as 17 feet long which weigh 200 pounds and others which are smaller than a pencil, were transported to Stowe, Pennsylvania to be cleaned and repaired by specialists at Patrick J. Murphy & Associates, Inc.  On January 30, 2005, university president Fr.  Joseph A. Panuska, S.J., celebrated the restoration by blessing the organ, which was then heard for the first time in decades as concert organist Thomas Murray performed selections by Vivaldi, Schumann, Grieg, Mendelssohn, and Elgar. The Houlihan-McLean Center also has a bell tower which holds a large bell, forged in 1883 by the Buckeye Bell Foundry and Van Duzen and Tift, Cincinnati, Ohio, and installed by the Immanuel Baptist congregation in the church when the Church moved into the current Houlihan-McLean Center from its former location.  The bell's inscription reads, \"Presented by the Choir in Memory of Mrs. C. F. Whittemore, Who Died July 7, 1883.\"  In 1991, the university installed an electronic bell ringer, programmed to ring the bell every hour using a motor and hammer manufactured in England. The building is named for Atty. Daniel J. Houlihan and Prof. John P. McLean, two dedicated, longtime faculty members at the university.  A former student of theirs was the benefactor whose contribution, made in their honor, enabled the university to acquire the structure in 1986. The Houlihan McLean Center is one of three churches the university acquired and preserved during the 1980s once their congregations were no longer able to maintain the buildings.  In 1985, the university converted the former Assembly of God Church at 419 Monroe Avenue into Rev. Joseph A. Rock, S.J., Hall.  It currently houses Madonna Della Strada Chapel, the principal campus setting for university liturgies, as well as the university’s Military Science department and ROTC program.  In 1986, the university acquired the Immanuel Baptist Church and converted it into the Houlihan McLean center.  Currently, it houses the university’s Performance Music Programs.  The university acquired the former John Raymond Memorial Church, Madison Avenue and Vine Street, in 1987.  It now serves as the Smurfit Arts Center, which houses studio space for the university’s Fine Arts department.  The university’s efforts were cited in a 1988 edition of “Inspired,” a bi-monthly publication devoted to the preservation of historic religious buildings. Completed in 1987, Kathryn and Bernard Hyland Hall is a four-story facility which contains sixteen classrooms and a 180-seat tiered lecture hall, in addition to a cafe and lounge.  Hyland Hall also housed the university’s bookstore until it was moved to the DeNaples Center in 2008.  The site of Hyland Hall was previously occupied by Lackawanna College, prior to its move to 901 Prospect Avenue.  Since 2001, Hyland has also been home to the university's Hope Horn Art Gallery.  Before moving to Hyland, the university’s Art Gallery had been located in The Gallery, which was demolished in 2001.  Hyland’s exhibit space is roughly double the size of the old art gallery with a wall of windows, a cathedral ceiling, and moveable walls to enhance the ambiance of the environment as well as an adjoining workshop and classroom space for lectures and workshops.  In 2004, the Art Gallery was named in honor of Hope Horn, a vibrant force in the arts community of Scranton who was a prolific painter and sculptor who bequeathed her estate to the University of Scranton to support art and music education at her death.  Currently, it mostly houses classes for the Departments of Political Science, Sociology, Criminal Justice, and World Languages and Cultures. Completed in August 1996, the Institute of Molecular Biology and Medicine was funded by a $7.5 million grant from the U.S. Air Force and the Department of Defense.  The 1,500 square-foot facility houses research laboratories, offices, and the Northeast Regional Cancer Institute.  The IMBM is dedicated to the molecular biological research, chiefly in the field of proteomics, or the study of the full set of proteins encoded by a genome.  The building was created to speed up the process of finding and treating viral diseases and cancer as well as to be able to engineer a patient’s immune system to avoid these diseases and to develop DNA probes that could possibly seek out a defective gene that is responsible for cancer.  The laboratories are equipped with technologically advanced systems used in the medical research field and each lab was designed for a specific purpose, such as genetic engineering, sequencing of DNA, and fluorescent microscopy.  It contains the most advanced proteomics laboratory in the region.  Additionally, the Institute has the capability to handle Level 3 pathogens. Completed in 2011, the Loyola Science Center, also known as the Unified Science Center, houses the university’s Biology, Chemistry, Computing Sciences, Mathematics, and Physics/Electrical Engineering departments as well as any programs currently associated with these departments.  In addition, it is designed to serve as a center for collaborative learning for all members of the campus and the community and to create a physical space that would deepen the university’s culture of engagement.  Father Quinn, the university’s president, remarked that the Center serves as the “academic heart of campus,” as a “place of research, scholarship, teaching and discovery, a place to find God in all things.”  Its design promotes effective intellectual collisions between and among faculty, both major and non-major students, and members of the community. The center includes a nearly 150,000-square-foot, four-story new structure on what was previously a parking lot along Monroe Avenue and Ridge Row which has been seamlessly integrated into nearly 50,000 square feet of renovated space in the Harper McGinnis Wing of St. Thomas Hall, which was built in 1987 to house the physics and electrical engineering departments.  The Harper-McGinnis Wing of St. Thomas Hall was extensively renovated in 2012 while the Science Center was being built.  It now houses the departments of Theology and Religious Studies, Communication, Philosophy, History as well as the office of LA/WS, or Latin American and Women’s Studies, and the university’s radio station, 99.5 WUSR.  Finally, the design includes a new entrance into St. Thomas Hall and the science center from the Commons.  The center’s layout provides a physical space that encourages integration among the traditional science, technology, engineering and mathematics programs, as well as the humanities, to drive the development of new teaching methods and engage students in practices that will prepare them for future challenges.  The building’s dynamic, modern design includes inviting spaces for student/faculty collaboration, visible glass-walled laboratories and the efficiencies of using shared instrumentation.  There are small “neighborhoods” of faculty offices, student learning spaces, laboratories and classrooms.  The application of the concept is evident from the exterior elevations which show the Science Center not as a single massive form, but rather, three smaller substructures. The Loyola Science Center contains 34 teaching and research laboratories, a rooftop greenhouse for teaching and research, a 180-seat lecture hall for symposia and seminars, numerous group study and research areas, 22 classrooms, 80 offices, a multi-story atrium, and a vivarium.  Additionally, the second floor of the Harper-McGinnis wing contains an area which highlights student, faculty, and community work and engages visitors.  It contains a large television which displays the University Twitter feeds, the science center’s energy usage, and videos featuring student and faculty research; glass exhibits which feature research projects and science displays; and aquariams which house fish for student study from a variety of different ecosystems.  The Loyola Science Center also contains Bleeker Street, a coffee shop and cafe.  The Center was designed to meet the Silver standard for Leadership in Energy and Environmental Design (LEED) certification, though it has not gone through the certification process. The $85 million, nearly 200,000-square foot building is the largest capital project in the history of the Jesuit university and the culmination of more than 15 years of planning and preparation.  After the Science Education Committee created the vision that would eventually become the Loyola Science Center in the fall of 1998, it took two years to complete a paper about the vision.  After seven years of programming meetings, the university broke ground May 14, 2009, for the facility’s construction. The Loyola Science Center was dedicated on September 28, 2012.  The Center was named in honor of St. Ignatius of Loyola, the founder of the Society of Jesus.  Additionally, three wings inside the building have been named to honor the contributions and service of members of the University of Scranton community.  On November 11, 2011, the first wing was dedicated as McDonald Hall.  Herbert McDonald served as president of the staff and chairman of the department of surgery at Hahnemann Hospital, now known as the Geisinger Community Medical Center of Scranton, and his wife Mary McDonald served on the university’s board of trustees and vice chair from 1989 to 1992.  Milani Hall was dedicated on March 24, 2012, in honor of Dr. Frank Milani '55, as a recognition of his family’s continued support of the university after he received his Bachelor of Science in biology from the university in 1955.  In recognition of Carl J. Keuhner and JoAnne M. Keuhner, Keuhner Hall was dedicated on August 5, 2012.  Carl Kuehner served on the board of trustees from 2003 to 2009 as well as chairman of the board from 2007 to 2009.  The fourth wing, Harper-McGinnis Hall, located in St. Thomas Hall, was built and dedicated in 1987 in recognition of physics professors Joseph P. Harper, Ph.D., the chairman of the physics department, and Eugene A. McGinnis, PhD, a long-time physics professor at the university.  Together, these men contributed more than 70 years of teaching service to the university. In 1968, the University of Scranton purchased the land where Loyola Science Center stands from the Scranton Redevelopment Authority for $25,221.60 as part of the city’s urban renewal project.  The 42,007 square foot lot, located at the eastern corner of Monroe Avenue and Ridge Row, had previously been occupied by Auto Express Company.  From the time of its purchase until construction began on the Loyola Science Center, the site served as a parking lot with sidewalks, landscaping, and lighting. The McDade Center for Literary and Performing Arts was constructed in 1992 on the former site of the Lackawanna County Juvenile Center.  Home to the university's English & Theatre department’s classrooms, offices, labs, meeting spaces, and a black box studio theatre, the McDade Center also houses the 300-seat Royal Theater where the University Players stage their productions.  The building's other features include a computer writing and instructions lab, a seminar room, a small screening room for film classes and an office for Esprit, the university's Review of Arts and Letters.  Additionally, the building contains stained glass in the lobby and an engraved quotation above the main entrance. The building's exterior features \"The Doorway to the Soul,\" a steel and wire sculpture by Pennsylvania artist Lisa Fedon.  “The Doorway” consists of 18 framed images fabricated variously of steel plate, perforated steel, round steel bars and wire cloth which each represent experiences in the human journey towards truth while the grid itself represented a matrix of inner-connectedness.  The individual panels within the grid are titled: The Thinker; Reaching Out To My Self; Natural and Curious Yearning of a Child; Eternal Bridge; Acceptance; A State of Calm, Peace, Knowing; Trials and Tribulation/The Ascent; The Void/God; The Writer; Father, Son, and Holy Spirit; Hope/Prayer; Christ; The Climb/The Worn Steps/The Invitation to Enter; The Written Word; Unconditional Love and Caring/Innocence of Children; The Self Exposed.  The two external panels are: The Self Observing and The Only Begotten Son.  Upon completion of the work, Panuska congratulated Fedon for her \"artistic insight and great sensitivity in fulfilling the terms of the commission.\"  He noted that in addition to bringing greater beauty to the campus, the works of art added to the campus in recent years are intended to help express the meaning and mission of the university to students, faculty, staff, and campus visitors.  The commissioned work was a gift of Patrick J. and Marie Connors Roche, generous benefactors of the university and the namesakes of the university’s Wellness Center. At the dedication ceremony in 1993, the building was named in honor of the Hon. Joseph M. McDade because of “his continuous support of this area and of the university and its academic mission,” Rev. Panuska noted. The McDade Center location was once the site of Crawford House, the 1898 Tudor Revival home of coal operator, baron, and Peoples Coal Company owner James L. Crawford.  In 1992, several years after Crawford’s wife passed away, Lackawanna County purchased the estate to serve as the Juvenile Detention Center.  In 1989, after four years of negotiations, the University of Scranton acquired Crawford House.  Originally, the university planned to renovate and restore the property, where it would relocate the Admissions and Financial Aid offices as well as a combinations switchboard and a visitors area.  However, the university discovered that the interior damage was too severe and that it would not be economically feasible to renovate it.  The university’s decision to demolish the Crawford House ignited fierce controversy because of strong opposition from local historical organizations, such as the Lackawanna Historical Society, the State Historic Preservation Office, and the Architectural Heritage Association who believed the house “represent[ed] the lifestyle of a coal baron of the late nineteenth century,” and was therefore significant for Scranton, a city founded on coal.  In an attempt to compromise with those upset by the potential demolition of Crawford House, the university proposed that the building be relocated in order to preserve its historical aspects but this too proved too costly so Crawford House was demolished in 1991.  Rather than using the site for administrative offices as originally planned, the university decided to build the Instructional Arts Facility which would be home to the English and Theater departments, as the need for performing arts space was identified back in 1983.  The Crawford House was subsequently delisted from the National Register in 1992. Completed in 1998, McGurrin Hall houses many of the departments in the J.A. Panuska College of Professional Studies, including Education, Nursing, Counseling and Human Services, and Health Administration and Human Resources.  The departments of Exercise Science, Occupational Therapy, and Physical Therapy, also part of the Panuska College, are housed in the adjacent Center for Rehabilitation Education, also known as Edward R. Leahy Jr.  Hall.  McGurrin's four stories include classrooms, laboratories, teaching instruction labs, and counseling suites as well as the Panuska College of Professional Studies’ advising center and administration offices.  When it was built, McGurrin was outfitted with the latest, most advanced technology in its labs and media-based equipment to deal with instruction in electronic media. McGurrin Hall is named in honor of Mary Eileen Patricia McGurrin, R.N., M.S.N., a former student at the University of Scranton and the daughter of Kathleen Hyland McGurrin and the late John F. McGurrin Sr.  Ms. McGurrin was an honors student at Abington Heights High School, earned her bachelor and master’s degrees in nursing from Thomas Jefferson College of Allied Health Services in Philadelphia.  A member of the American Nurses Association, she was a registered nurse who served on the staff of Wills Eye Hospital in Philadelphia following completion of her training.  She died of cancer in 1995 at the age of thirty-nine.  In loving memory of his niece, McGurrin’s uncle, Bernard V. Hyland, M.D., made a significant contribution to the Campaign for Scranton, which helped finance the building named in her memory.  Dr. Hyland hoped that all of the students who pass through the doors of McGurrin Hall will be filled with the same spirit of selfless service animated by Mary Eileen.  University President Rev. McShane noted that “it’s really appropriate and magnificent that the home of a professional studies is named for a nurse.” In 2003, the University of Scranton opened the Leahy Community Health & Family Center, which is located on the bottom floor of McGurrin Hall.  The Leahy Community Health & Family Center serves the dual purpose of identifying and meeting the health and wellness needs of underserved individuals in the greater Scranton community while providing a place where faculty guide students in a practical educational experience.  Faculty, graduate students, and undergraduate students from the Panuska College of Professional Studies, along with Pennsylvania licensed staff members, work together to fill gaps in health, wellness, and educational services offered to marginalized and underserved populations.  The Center provides a multitude of services to those with special needs, children and families, senior citizens, the homeless, and the uninsured, which include the university of Success, the Alice V. Leahy Food and Clothing Pantry, the Edward R. Leahy, Jr.  Center Clinic, “Peacemakers After School,” and “Growing Stronger.”  The University of Success is a pre-college program designed to provide academic, social, and cultural enrichment and experiences to high school students.  The ultimate goal of this program is to assist participating students to successfully complete high school and gain entrance into a college or university.  The Alice V. Leahy Food and Clothing Pantry is a student-run effort to provide homeless and at-risk people with clean, decent clothing and the most basic of human needs, food.  The Clinic provides free \"non-emergency\" health care to uninsured Lackawanna County residents who may otherwise forego health care due to cost or seek care in hospital emergency rooms.  The Clinic also provides physical therapy and counseling services.  “Peacemakers After School” is program for children between the ages of 9 and 13 and “Growing Stronger” is a program for area senior citizens.  The Leahy Community Health & Family Center also offers numerous educational programs, health fairs and special clinics throughout the year.  The Center includes a reception area, administrative offices, interview rooms with observation and closed circuit video capabilities, examination rooms, disabled access and restrooms, and a large group activity/conference area as well as sophisticated equipment as the region’s only motion analysis system, capable of analyzing movement and motor activity of people from infancy to advanced age. The Leahy Community Health & Family Center “blends so completely the unique quality of the University of Scranton with [its] Jesuit mission,” because it “provides a place for research, scholarship and practical experience for faculty and students alike while responding to the needs of children in our region who have special needs,” as University President Rev. McShane remarked.  It embodies the Jesuit ideals of faith in action and serving others. The Center is named for Edward J. Leahy, the late son of generous benefactors Patricia and Edward R. Leahy who passed away at the age of eight due to his significant disabilities.  The Center represents the Leahy’s tradition of donating to support individuals with disabilities both indirectly through funding research and directly through assistance and service programs.  In memory of Edward, the Leahys have “tried to engage in a continuous celebration of his life by helping others, particularly children, with special needs, but without sufficient resources to address those needs.”  Mr. Leahy noted that the Center “is a continuation of that celebration of Edward’s life, and it stands as proof that the youngest and the smallest among us can make a real difference.” Built in 1922, O’Hara Hall was originally called the Glen Alden building and served as the Scranton administrative headquarters for the Glen Alden Coal Company, which at one time had extensive anthracite operations in the Scranton area.  Located at the corner of Jefferson and Linden Avenues, the building was sold to the GA Building Corp in 1955 before being acquired by Alden Associates in 1958 before its title was transferred to the Prudential Savings Bank of Brooklyn.  During this time, it served as office space for a variety of local Scranton businesses and professional offices.  The Neoclassical, six-story building was sold to the University of Scranton in 1968 for $157,000. After renovations and improvements, the University of Scranton used the building to provide the school with more room for its facilities, particularly additional classrooms, faculty offices, supporting administrative services, and conference rooms.  O’Hara Hall also became the home of the Business Administration and Economics departments, including their accompanying statistics and accounting laboratories.  From 1978 until 2001, O’Hara Hall served as the headquarters for the University's School of Management. In 2001, after the Kania School of Management moved to the newly constructed Brennan Hall, O'Hara Hall was renovated and occupied by 11 other University departments, including both administrative offices and some programs for the College of Arts and Sciences such as the Dexter Hanley College (now the College of Graduate and Continuing Education), Alumni Relations, the Annual Fund, Continuing Education, Development, the World Languages and Cultures department, Instructional Development, the Learning Resource Center, the Political Science department, Public Relations, and the Sociology and Criminal Justice department.  The renovations include the construction of a foreign language laboratory with 25 computers for students taking courses in the World Languages and Cultures department.  In 2016, the office of the Registrar and Academic Services moved from St. Thomas Hall into O’Hara Hall. After polling the University community for suggestions, the University decided to rename the Glen Alden Building as O’Hara Hall, in honor of Frank J. O'Hara to recognize his tireless service and incredible contributions to the University.  Known as “Mr University,” Dr. O’Hara graduated from the University in 1925, served as the school’s registrar for 32 years, worked as the University’s Director of Alumni Relations from 1957 until 1970, received an honorary doctor of laws degree from the University, served as moderator of the University of Scranton Alumni Society. In 1960, the University of Scranton announced plans for a new classroom building, intended to replace the unsafe and overcrowded Barrack buildings, which had been purchased from the Navy in order to quickly accommodate the growing student body, which increased in the 1940s due to the G.I. Bill, a law which provided a range of benefits for returning World War II veterans, including paying for tuition and living expenses to attend college.  After holding a major fundraising campaign to raise $1,836,000 for the new building as well as to finish other expansion projects at the library and the student center, the University was ready to begin construction on the new building. Before building could commence, however, mine tunnels under the site needed to be backfilled.  Excavations underground showed that the proposed building site was directly above the oldest mine in Scranton, whose origins date back to the Civil War.  Supported only by wooden beams and decaying tree trunks, these huge mine chambers showed signs of extensive mining and “local” caving more than fifty feet below the surface.  In order to ensure that the new building would be constructed on a firm and strong foundation, mining experts created columns of debris more than 13 feet in circumference and flushed the open cavern with more than 18,000 cubic feet of concrete.  The total cost of construction was approximately $1,400,000. Constructed at the corner of Linden and Monroe Streets, St. Thomas Hall was completed in 1962.  Five stories tall, the modern L-shaped building contained 50 classrooms, 15 utility rooms, 11 equipment rooms, 10 corridors, and 128 offices, for both faculty and administrators.  In addition, the building housed ROTC offices, student lounges, the St. Ignatius Loyola Chapel with room for over two hundred participants, and four laboratories.  These laboratories included the education lab where students could practice lesson plans and make use of the latest model projectors and teaching aids in the audio-visual section of the lab; the psych and guidance laboratory with three interview rooms and a clinic complete with one way vision glass for observation and demonstration; the business department’s accounting and statistics labs containing modern business equipment; and the language laboratory where students can practice speaking and subsequently attain oral proficiency in foreign languages. During the dedication of St. Thomas, the original cornerstone from the University’s first building, Old Main, was built into the front corner of St. Thomas Hall.  Seventy five years after Old Main’s blessing in 1888, the University of Scranton transferred its cornerstone to the new campus, linking the University with its past and providing continuity from both the University's former name, St. Thomas College, and its old campus.  When the cornerstone was removed from its place in Old Main, it was discovered that it held a copper box, containing six newspapers published on the day of Old Main’s dedication and seven silver coins.  During the dedication of St. Thomas Hall, the 1888 newspapers were placed back into the cornerstone, along with letters from student body president Jack Kueny, Alumni Society president Atty. James A. Kelly, and alumnus and longtime administrator Frank O'Hara.  Also included was a letter from architect Robert P. Moran '25, addressed to the architect of a building that replaced St. Thomas in the future. Over the years, there have been numerous renovations and improvements of Saint Thomas Hall.  In 1965, the gas station at Linden Street and Monroe Avenue on the western end of the University of Scranton complex in front of St. Thomas Hall was razed in order to eliminate the cumbersome and dangerous curve at that intersection.  In its place, the island was built, allowing traffic onto campus to be routed around it.  In 1987, the Harper-McGinnis Wing was added to St. Thomas Hall to house the Physics and Electronics Engineering department.  Funded by the University's Second Cornerstone campaign, the Harper-McGinnis Wing is a two-floor addition that contained offices and laboratories for physics, electrical engineering, and computing sciences.  At the time of its opening, it contained several cutting edge research laboratories, including a modern and atomic physical lab, an optics and electronics lab, a microprocessor lab, an electricity and magnetism lab, a very large system integration (VLSI) lab, a microcomputer lab, and a computer assisted design lab.  The Harper-McGinnis Wing was dedicated in recognition of physics professors Joseph P. Harper, Ph.D., the chairman of the physics department, and Eugene A. McGinnis, Ph.D, a long-time physics professor at the University.  Together, these men contributed more than 70 years of teaching service to The University.  Then, in 2009, renovations during the summer targeted the first and fourth floors of St. Thomas Hall, converting the former St. Ignatius of Loyola Chapel space into offices for Human Resources and Financial Aid.  St. Thomas Hall was significantly renovated in 2011-2012 as part of the construction of the Loyola Science Center.  It now houses the departments of Theology and Religious Studies, Communications, Philosophy, History as well as the office of LA/WS, or Latin American and Women’s Studies, and the University’s radio station, 99.5 WUSR. St. Thomas Hall was named in honor of the namesake of St. Thomas College, now the University of Scranton.  St. Thomas Aquinas was a thirteenth-century philosopher and theologian, Dominican friar, Catholic priest, and Doctor of the Church.  Known best for his works Summa Theologica and Summa contra Gentiles, St. Thomas attempted to synthesize Aristotelian philosophy with the principles of Christianity.  He was one of the most influential thinkers of medieval times.  His work remains influential and continues to be the central point of reference for the philosophy and theology of the Catholic Church.  He is now a patron of Catholic universities, colleges, and schools. In January 1987, the University of Scranton under Rev. Panuska purchased the former John Raymond Memorial Church, Universalist, at Madison Avenue and Vine Street for $125,000.  Built in 1906, the Romanesque building contains one of the tallest bell towers in Scranton.  The main floor of the small but remarkably designed structure, which contains 7,200 square feet of floor space, is used as a studio-art facility for the Fine Arts program.  The basement is used for the department’s offices and classrooms.  During the renovations of the building, the university had to remove the stain glass windows and replace them with clear glass to provide the area with natural lighting.  The two stained glass windows from the Smurfit Arts Center, which were crafted by the Tiffany Glass Company, were moved to be displayed in Hyland Hall.  The Smurfit Arts Center was named for Michael W. J. Smurfit H'85, a generous Irish benefactor whose two sons, Anthony and Michael, attended the University of Scranton.  Smurfit was the chairman and chief executive officer of Jefferson Smurfit Group, Ltd., a multinational corporation with headquarters in Dublin, IReland; Alton, Illinois; and New York City. The Smurfit Arts Center is one of three churches the university acquired and preserved during the 1980s once their congregations were no longer able to maintain the buildings.  In 1985, the university converted the former Assembly of God Church at 419 Monroe Avenue into Rev. Joseph A. Rock, S.J., Hall.  It currently houses Madonna Della Strada Chapel, the principal campus setting for university liturgies, as well as the university’s Military Science department and ROTC program.  In 1986, the university acquired the Immanuel Baptist Church at the corner of Jefferson Avenue and Mulberry Street.  Currently, the Houlihan-McLean Center houses the university’s Performance Music Programs.  The university acquired the former John Raymond Memorial Church, Madison Avenue and Vine Street, in 1987.  It now serves as the Smurfit Arts Center, which houses studio space for the university’s Fine Arts department.  The university’s efforts were cited in a 1988 edition of “Inspired,” a bi-monthly publication devoted to the preservation of historic religious buildings. In 1984, the university completed construction on its very first athletic field in the school’s ninety-six history, which began in 1982 after the university acquired the land from the Scranton Redevelopment Authority.  The land had previously been used as a rail yard for the Lackawanna and Wyoming Valley Railroad.  The facility was designed as a multi-sports complex, complete with a regulation-size field for men’s and women’s soccer which also can be used for other sports such as softball, lacrosse, field hockey, and intramural athletics.  It also has bleachers which can seat 350 people, an electronic scoreboard, and a maintenance building containing restrooms, a storage area, and a parking lot.  Father Panuska noted that the building of the field was important because it fosters “the development of a total learning environment, an environment which supports a balanced life.” The university's board of trustees named the field in honor of Rev. John J. Fitzpatrick, S.J., a long-time booster of the university's athletic programs and dedicated member of the university community for twenty-two years.  Rev. Fitzpatrick founded the club football team (which played between 1967 and 1978) and offered pre-game prayers at many of the Royals' sporting events.  A dedication plaque on the field reads: \"Because he is an exemplary priest, long dedicated to the students of the university, and especially to its student athletes, this first athletic field of the University of Scranton is lovingly dedicated to a living proof that 'Reaching for the rising sun is surely worth the cost.'\" In 1997, a re-dedication ceremony celebrated the installation of new artificial turf and improved lighting for the field.  Currently, Fitzpatrick Field remains the university’s primary outdoor athletic facility and is used for the Royal’s varsity soccer, field hockey, and lacrosse teams.  The field is also used for intramural flag football, ultimate frisbee, soccer, and field hockey. Completed in 1967, the John J. Long Center contained the university’s first indoor athletic facilities, as well as instructional areas for physical education.  The Long Center is built into the slope of Linden Street, providing a single level on Linden Street and a three-story end of the building, overlooking Ridge Row.  The Long Center was built to enable the university to institute an academic program in physical education and provide a space for student assemblies, convocations, group meetings, and other large gatherings.  It was also created to give greater emphasis to intramural athletics and improve the school’s intercollegiate athletics. At the time of its construction, the top floor featured a large entrance foyer and a gymnasium, complete with movable bleacher seats that could accommodate up to 4,500 people.  The gymnasium contained three basketball courts, complete with a folding curtain in order to separate the gym, allowing multiple games or gym classes the occur at the same time.  It also contained two ticket rooms, court rooms and rest rooms, a sound control room, offices for the director and assistants of the physical education program, an equipment room, and storage rooms.  The second floor housed locker room facilities, rest rooms, and showers, in addition to saunas, whirpool baths, and a sun room.  It also had a training room, small offices for athletic coaches, a weight room, and an all-purpose room.  The bottom floor contained a wrestling room, a mechanical room, and laundry facilities. The Long Center was built on land, spanning 4.93 acres, that the university purchased from the Scranton Redevelopment Authority for $96,843, as part of the city’s urban renewal project.  Before handing over the title to the university, the Scranton Redevelopment Authority cleared the lot, located at the eastern corner of Linden Street and Catlin Court, by demolishing several existing structures. In order to pay for the $1.8 million facility, the University of Scranton acquired a $592,110 grant through the Higher Education Facilities Act and took out a $815,000 federal loan, made possible by the support of Congressman Joseph M. McDade and U.S. Senator Joseph Clark.  The university shouldered the remaining costs. In 2001, excavation under the Long Center provided a new home for the Department of Exercise Science and Sport.  The additional 10,000 square feet of space accommodated offices, classrooms, a fitness assessment center, and laboratories for sport biomechanics, body composition, cardio-metabolic analysis, biochemistry, and muscular skeletal fitness.  However, with the completion of the Center of Rehabilitation Education (also known as Edward R. Leahy, Jr.  Hall) in 2015, the Exercise Science Department relocated from the Long Center into the new building. After its completion in 1967, the university dedicated the athletic facility in honor of its former president, John J. Long, S.J., who served the university in that position from 1953 until 1963, to commemorate his dedication and tremendous contributions to the university.  After he stepped down from the presidency, Fr.  Long continued to serve the university in other positions, including assistant to the president, founder and moderator of the Alumni Society, and vice president for administrative affairs.  During his tenure as president, he led the university in its first major building campaign.  Starting in 1956, the campus was greatly expanded and modernized through the construction of fifteen new buildings, which included the Loyola Hall of Science, 10 student residence halls, St. Thomas Hall, Alumni Memorial Hall (formerly known as the Alumni Memorial Library) and Gunster Memorial Student Center (formerly known as the Student Union Building, and was demolished in 2008) as well as the Long Center.  He successfully led the university through two fundraising drives in order to finance these building projects, which also had the effect of incorporating the university into the Scranton community. In 1985, the university began construction on a physical education and recreation complex.  Completed in 1986, the William J. Byron, S.J. Recreation Complex is a three-level structure which connects to the Long Center, the facility for intercollegiate athletics.  The facility contains three multi-use courts for basketball, volleyball, tennis, and one-wall handball as well as a one-tenth mile indoor running track, a six-lane Olympic-sized swimming pool complete with diving boards and an electronic scoreboard, four 4-wall racquetball courts, a gallery which overlooks the swimming pool and the racquetball courts, two different aerobics/dance rooms, men’s and women’s locker rooms, saunas, and steam rooms.  Panuska spoke about the importance of the new recreation complex, stating that it would help the university offer more “health-related activities” and to serve the recreational needs of the student body, including the intramural program.  Panuska also noted that naming this facility for Fr.  Byron, the president of the University of Scranton from 1975 until 1982, “provides us with a marvelous opportunity to thank him for his leadership at the university and in the region.” Located at 600 Linden Street and Adams Avenue, Brown Hall, formerly named the Adlin Building, was acquired by the university in 2012 from Adlin Building Partnership.  Currently, Brown Hall houses the university’s Small Business Development Center, which provides educational programs and free, confidential consulting services to those starting and growing small businesses in the Scranton area, and Division of External Affairs, which handles some functions of undergraduate and graduate admissions, news and media relations, marketing communications, printing and mailing services, community and government relations.  On February 18, 2016, the University renamed Adlin Building as Louis Stanley Brown Hall, in memory of Louis Stanley Brown '19, the first black graduate of St. Thomas College. Campion Hall, opened in 1987, is the university’s residence building for the Jesuit community.  The faculty, named in honor of Saint Edmund Campion, S.J., a 16th-century Jesuit pastor and scholar who was martyred in England during the persecutions of Roman Catholics for defending his faith, provides living and working accommodations for thirty Jesuits.  The two-story building features thirty-one bedrooms, an interior garden, an office, kitchen and dining facilities, and a chapel in addition to a flexible design with four discrete sections, such that the building could adapt to the changing needs of the Jesuit Community at the university.  Before the construction of Campion Hall, the primary residence for the Jesuits at Scranton was the Estate, the former Scranton family residence which was given to the university by the family in 1941, which proved unable to meet their needs, as it only provided living accommodations for seventeen of the university’s thirty-six Jesuits in the 1980s.  The building of Campion Hall, estimated at $1.7 million, was financed entirely by the university’s Jesuit community.  Currently, Campion Hall provides housing for Jesuits who teach or hold administrative positions at the University of Scranton or at Scranton Preparatory School, a local Jesuit high school. Completed in 1928, the Chapel of the Sacred Heart, formerly the Alumni House and the Rupert Mayer House, was originally part of the Scranton Estate.  It was designed as a small athletic facility, containing a gym and a squash court.  While Worthington Scranton donated the Estate to the University in 1941, he reserved this building, the Quain Conservatory greenhouse, and Scranton Hall (the carriage house) for his own personal use.  In 1958, the remainder of the late Worthington Scranton’s Estate was acquired by the University, including the Chapel.  The University reportedly paid $48,000 for the title to the land. Over the years, the building has served the University in a variety of ways.  First, the facility was used as the center of athletics, complete with a weight facility and the Athletic Director's office.  In 1968, when the construction of the Long Center was completed, the athletic facilities were moved from the Chapel to the new building.  The building was then used as a print shop, which was moved to O’Hara Hall.  Then, the Chapel was used as the headquarters for the University’s Alumni Association, beginning in the 1970s until 2009.  In 2009, following improvements and changes in St. Thomas Hall, the Chapel moved from its location on first floor St. Thomas Hall to the newly renovated Rupert Mayer House.  Currently, the Chapel is used for daily masses, Eucharistic Adoration, and prayer by students, faculty, and staff of the University of Scranton. In 2008, with the completion of the DeNaples Center, the Gunster Memorial Student Center was functionally superseded.  As a result, it was demolished.  In its place, the University created the Dionne Green, a 25,000-square-foot green space roughly the size of a football field featuring a 3,600 sq ft outdoor amphitheater, a popular spot for classes during pleasant weather.  Located directly in front of the DeNaples Center, it serves as the gateway to the campus.  Dionne Green, along with the DeNaples Center and Condron Hall, was part of the University's Pride, Passion, Promise campaign, a $100 million effort to improve and update the campus. The Dionne Green was named for John Dionne '86 and Jacquelyn Rasieleski Dionne '89, University of Scranton alumni and benefactors.  Rev. Scott Pilarz, S.J., the University’s president, remarked that “John and Jacquelyn Dionne exemplify the Jesuit ideal of the magis, the restless desire for excellence grounded in gratitude” in their generosity to the University and to the larger community over the years.  Mrs. Dionne grew up in the Scranton area, attended the University of Scranton, worked as an intensive care nurse and actively participates in several charitable organizations.  A 1986 University graduate, Mr. Dionne is senior managing director of The Blackstone Group and founder and chief investment officer of the Blackstone Distressed Securities Advisors group, received his MBA from Harvard Business School, was a partner at Bennett Restructuring Funds, a $1 billion hedge fund, and has served on the board of directors of several nonprofit organizations.  He served as chair of the University’s Board of Trustees from 2004 to 2007, the co-chair of the President’s Business Council, and on the executive committee of Scranton’s Pride Passion Promise Campaign. In 1867, Joseph H. Scranton, one of the founders of the city of Scranton, commissioned the building of his family home.  Designed by New York architect Russell Sturgis, one of America’s most outstanding architects in the post Civil War era, the home was created in the French Second Empire style.  The house features stone masonry by William Sykes and detailed woodwork carvings designed by William F. Paris.  The twenty-five room, three story residence contained a billiards room, a ballroom, a library, a Tiffany glass skylight, and a solid mahogany staircase.  It is estimated that the cost of construction totaled $150,000. Throughout the years, a number of renovations and improvements were made on the estate.  The house initially featured a tower located on top of the front left corner of the Estate’s roof, which was later removed.  Additionally, while occupying the residence, William W. Scranton built the large granite wall surrounding the property in order to protect the estate and keep out rioting townspeople, upset about the tuberculosis epidemic that they felt had been spread through the city’s water supply, the rights of which were owned by the Scranton family.  Parts of the wall were later removed during the construction of Loyola Hall in 1956.  There used to be two open porches in the back of the Estate.  In the early '70's, both porches were enclosed and converted into a sitting room and a dining room. Construction commenced in 1867 and continued for four years, finishing in time for the Scranton family’s Thanksgiving celebrations in November 1871.  Less than a year later, Joseph H. Scranton died.  His son, William W. Scranton, then inherited the property.  After William W. Scranton’s passing, his wife, Katherine M. Scranton, used the home until her death in 1935, at which point it passed into the hands of their son, Worthington Scranton.  Because Worthington’s wife Marjorie was confined to a wheelchair, she had difficulty navigating around the Estate.  They built a new home outside of Scranton in Abington called “Marworth.”  Once construction on Marworth was completed in 1941, the Scrantons moved out of the Estate entirely, although that house had never been their primary residence. In 1941, Worthington Scranton donated his home and adjoining estate to Bishop Hafey, the bishop of the Diocese of Scranton and the University of Scranton's Board of Trustees President, for use by the University, because he felt that this land could be “most advantageously used for the development of an institution of higher learning so that the youth of this vicinity can get an education at a reasonable cost.”  However, he reserved the former carriage house, which he had converted into an office, the greenhouse, and the squash court for his own personal use.  Following Worthington's death in 1958, his son, William W. Scranton, gave the remainder of the Estate to the University of Scranton. In 1942, when the Christian Brothers transferred the title of the University to the Society of Jesus, the Jesuits decided to use the Estate as the Jesuit community residence.  After some renovations by the Jesuits, the first floor of the residence held a chapel, reception parlor, a 5,000-volume library, and recreation room, while the second and third floors served as private rooms for the Jesuits.  In 1957, a small fire broke out in the house’s main parlor.  During the 1960s, the Jesuit community restored the Estate.  Most of the interior woodwork was refinished for preservation purposes, and the ceiling frescoes were repainted and gold leafing was added to them.  In 1987, the Scranton Jesuit community moved from the Estate into the newly-completed Campion Hall, as the Estate proved to be insufficient for the community’s needs, as it could only accommodate 17 priests in the then-36 member community. In 2009, the Admissions Office moved its operations into the Estate. In 2001, after a period of significant campus expansion at the University, the Gallery Building was functionally superseded, as the departments it housed were moved to the newly remodeled O’Hara Hall and to Hyland Hall, including the University's Art Gallery, the Counseling Center, and the Department of Career Services as well as classrooms and lecture halls.  As a result, it was demolished.  In its place, the University created Founders Green, a large, open green space, which is located directly in front of Brennan Hall. Upon the completion of St. Thomas Hall in 1962, the Barracks buildings no longer needed to be used by the University for classrooms and lecture halls.  As a result, the buildings were demolished.  In place of the barrack called the Arts Building, the University created an outdoor recreation facility on the block bounded by Linden Street, Monroe Avenue, Mulberry Street, and Hitchcock Court.  The $86,000 project created four volleyball courts, three basketball courts, a grass practice field for football and soccer, and a faculty parking lot.  However, the fields were not lighted, so all activities had to be scheduled during the day. In the late 1970s, the University decided to renovate and improve these recreational facilities.  The school built the Galvin Terrace Sport and Recreation Complex, which contained six tennis courts, two combination basketball/volleyball courts capable of also accommodating street hockey, four handball/racquetball courts, and recreational and lounging space.  Completed in October 1978, the project was funded by the University’s Annual Fund Drive and its national capital campaign “Commitments to Excellence.”  It was used for intramural sports but also served as the home of the University's tennis team.  In the early 1990s, the recreational complex was demolished to make room for the Weinberg Memorial Library.  A small garden outside the Library is now known as Galvin Terrace. The Galvin Terrace was named for former University president Rev. Aloysius Galvin, S.J., who served as Scranton’s president from 1965 until 1970.  Born in Baltimore, Rev. Galvin served in the U.S. Navy from 1943 until 1946, and graduated from Loyola College, Baltimore in 1948.  He joined the Society of Jesus upon his college graduation in 1948, pursued philosophical and theological studies at Woodstock College, Maryland, and was ordained to the priesthood in 1957.  Rev. Galvin served as the Academic Vice President and Dean of Loyola College, Baltimore from 1959 to 1965.  After resigning from the University presidency, Rev. Galvin worked at Georgetown Preparatory School as a math teacher and student counselor for thirty-five years before his death in 2007. In 1996, the university community renovated a University-owned house at 317 North Webster Avenue into the Campus Mosque as a gift to the Muslim community of Scranton.  The university established the campus mosque in response to the growing need for a local mosque for the growing number of Muslim students, as there had not previously been any mosques in the city of Scranton.  Before the university created the mosque on its campus, the closest one had been in Wilkes-Barre, which had made it difficult for many Muslim students and faculty members to worship, as they were forced to recite prayers several times a day in their own apartments or dorms.  The Scranton Muslim community stated that “It [was] a spiritually uplifting experience to have our own mosque on campus.”  The mosque reflects the growing diversity of the university’s student body and the university’s commitment to meeting the spiritual needs of all members of the university community. The Mosque contained two large, spacious rooms as the women’s and men’s prayer rooms as well as a library housing countless reference books on the history of Islam and the Muslim religion, including translations of the interpretations of the Koran.  The Mosque was also equipped with an upstairs apartment where two members of the Muslim Student Association lived and served as caretakers of the facility. In 2007, the Mosque, along with several other properties, was razed in order to establish a site for the sophomore residence, Condron Hall.  The university then purchased and renovated a house at 306 Taylor Avenue for use as the new mosque, which is open to the public for prayer and reflection. When the University of Scranton acquired the Scranton family estate in the mid-1950s, the school received a garden, located next to the Chapel of the Sacred Heart at the corner of Linden Street and Monroe Avenue on the former grounds of the Estate.  Throughout the years, it was known by a couple of different names, including the Rose Garden and Alumni Garden.  In 2010, the University dedicated the Rose Garden to Rev. G. Donald Pantle, S.J. during the celebration of the 50th anniversary of his ordination, as a gift from James J. Knipper '81 and Teresa Poloney Knipper '82, in honor of their longtime friendship with Rev. Pantle. Rev. Pantle, a Scranton native, dedicated thirty-four years of service to the University of Scranton.  During his tenure at the University, he served as the Director of the Retreat Center at Chapman Lake, worked for Campus Ministries, taught German and Spanish for the World Languages and Cultures department.  He also served as the co-moderator of International Students Club, chaplain to the Women of the University Prayer Group and to the basketball, baseball, and soccer teams, and the Director of Fayette House, a residence that for many years was designated for students interested in learning Spanish to aid them in their careers.  Before working at the University of Scranton, Rev. Pantle entered the Society of Jesus in 1948, was ordained in 1960, served as the Jesuit Superior at Catholic University, and taught at Loyola High School, St. Isaac Jogues College, and Gonzaga High School.  Rev. Pantle received a bachelor’s degree at Bellarmine College, a master’s from Middlebury College/Mainz University, and studied at Woodstock College, American University, Berkeley College, Georgetown University, and Goethe-Instiutu in Murnau, Germany.  In 2014, Rev. Pantle retired to retired to St. Claude la Colombiere Jesuit Community Residence in Maryland. Completed in 1995, the Parking and Public Safety Pavilion accommodates 510 cars in its five stories, with one floor below ground, one floor at ground level, and three above ground.  It was constructed to expand the university’s on-campus parking capacity in order to meet the community’s need for additional places to park, with designated areas for students, faculty, staff, and guests.  Additionally, the parking garage contains the offices of the university's police and the offices of parking services.  The structure, which occupies 163,000 square feet, is located on the corner of Mulberry Street and Monroe Avenue.  The exterior complements the adjacent McDade Center for Literary and Performing Arts by mirroring its design.  The Monroe Avenue facade is also covered by a series of topiary planting screens on which climbing vines have grown. Located between the Chapel of the Sacred Heart and Scranton Hall on the grounds of the original Scranton family Estate, the Quain Conservatory was built in 1872.  The Scranton family used the greenhouse to grow and prepare cut flowers.  The glass building has a central square (20 ft by 20 ft) flanked by two 40 ft by 15 ft wings on either side.  At the time of its construction, each section had its own pool.  The Conservatory is one of few Victorian-style conservatories that remains essentially unaltered from its original design. In 1941, Worthington Scranton donated the Estate and its grounds to the University, but reserved a portion of the Estate for his own personal use, including the greenhouse.  In 1958, after the death of Worthington Scranton, the Scranton family donated the remainder of the Estate to the University, leading to the acquisition of the greenhouse. In the early 1970s, the student-led University Horticultural Society coordinated and organized an effort to renovate and restore the greenhouse.  In order to raise funds for their planned improvements, the Society organized field trips, plant sales, and a lecture series in addition to enrolling paid members to their group.  Additionally, Father Quain, the Acting President of the University, found out about the project and contributed additional funds to the project.  While restoring the greenhouse, the Society cleaned and painted the structures, refurbished the main pond and installed a new fountain pump, created a mushroom cellar in the greenhouse’s basement, and redesigned the plant beds.  The group installed a number of rare and exotic plants in the conservatory, including orchids, banana trees, mango trees, bougainvillia, tri-colored dracaena, star-shaped trees, bromeliads, Hawaiian wax flowers, night blooming cereus, bi-colored water lilies, the Rose of China (hibiscus), fig trees, pomegranates, pineapples, and grapefruit trees.  In September 1975, the University reopened and dedicated the greenhouse as the Edwin A. Quain Conservatory for his \"kindness, interest, and generosity to the Society.\"  Currently, the greenhouse is used for classes as well as faculty and personal research projects. The Roche Wellness Center, located at the corner of Mulberry Street and North Webster Avenue, was acquired by the university in 1992 and opened for student use in 1996.  Originally built in 1986 by pharmacist Alex Hazzouri, the Wellness Center previously housed Hazzouri’s pharmacy and drugstore as well as a restaurant named Babe’s Place.  In 1989, Alex Hazzouri was arrested and arraigned on drug-trafficking charges, and his pharmacy was closed indefinitely, as the government seized the building.  After the investigation was closed, the government auctioned off the building in 1992.  It was purchased by the university for $500,000.  Beginning on August 2, 1993, the building served as a home to the Scranton Police Department's Hill Section precinct station.  A new Student Health and Wellness Center was soon moved in, along with the university's Drug and Alcohol Information Center and Educators (DICE) Office.  In 1996, the Roche Wellness Center opened, housing the Student Health Services department.  The building holds a reception area, four exam rooms, a laboratory, an assessment room, an observation room, and storage space. On December 15, 1983, the University of Scranton purchased the Assembly of God Church from the Reformed Episcopalian congregation who could no longer properly maintain the facility as the costs and utilities were too high.  Once it was acquired by the university.  the Assembly of God Church was renamed to Rock Hall to honor the late Rev. Joseph A. Rock, S.J., a well-known and respected educator at the University of Scranton.  The university’s president, Rev. Panuska, noted that “the growth of the university, both in terms of the beauty of its campus and the achievements of its students, was in no small measure due to the efforts of Father Rock,” as “his contributions to the university as a teacher, administrator, counselor, and friend were exceptional.”  Originally, the university intended to use the first floor of the facility for administrative offices which had previously occupied space in St. Thomas and Jefferson Halls, including the Department of Central Services, the Maintenance Department, and the Security Department while the assembly area of the new hall was supposed to provide a needed alternative for smaller social and cultural affairs, including lectures, dinners, and dances, now held in the over-scheduled Jefferson and Eagen Auditoriums.  During the renovations of Rock Hall, however, the need for a new chapel was identified, as the St. Ignatius chapel in St. Thomas Hall did not provide adequate seating and contained structural limitations which were not conducive to acoustics or the aesthetics of the liturgies.  Named Madonna della Strada, or \"Our Lady of the Way\", in reference to an image of the Virgin Mary enshrined in the Church of the Gesu in Rome, the Chapel serves as the primary site for the university’s major liturgical services, including the regular Sunday masses.  Rev. Panuska commented that the building and chapel are important additions to the school, particularly because the chapel “provides the university and the surrounding community with a beautiful setting for liturgical celebrations.”  The chapel was consecrated on February 15, 1985, by Bishop James C. Timlin, D.D. Currently, the first floor of Rock Hall is the home of the university's Military Science department and ROTC program. Rock Hall is one of three churches the university acquired and preserved during the 1980s once their congregations were no longer able to maintain the buildings.  In 1985, the university converted the former Assembly of God Church at 419 Monroe Avenue into Rev. Joseph A. Rock, S.J., Hall.  It currently houses Madonna Della Strada Chapel, the principal campus setting for university liturgies, as well as the university’s Military Science department and ROTC program.  In 1986, the university acquired the Immanuel Baptist Church at the corner of Jefferson Avenue and Mulberry Street.  Currently, the Houlihan-McLean Center houses the university’s Performance Music Programs.  The university acquired the former John Raymond Memorial Church, Madison Avenue and Vine Street, in 1987.  It now serves as the Smurfit Arts Center, which houses studio space for the university’s Fine Arts department.  The university’s efforts were cited in a 1988 edition of “Inspired,” a bi-monthly publication devoted to the preservation of historic religious buildings. Constructed in 1871, Scranton Hall was built as a one-story carriage house and stable on the Scranton family Estate by Joseph H. Scranton.  In 1928 and continuing into 1929, Worthington Scranton and his wife added an additional story, renovating the building and converting it into an office space. In 1941, Worthington Scranton donated his home and adjoining estate to Bishop Hafey, the bishop of the Diocese of Scranton and the University of Scranton's Board of Trustees President, for use by the University, because he felt that this land could be “most advantageously used for the development of an institution of higher learning so that the youth of this vicinity can get an education at a reasonable cost.”  However, he reserved the former carriage house, the greenhouse, and the squash court for his own personal use.  Following Worthington's death in 1958, the University acquired the rest of the Estate from his son, William W. Scranton, for $48,000.  The former carriage house was of particular interest to the University because it would allow them to centralize the scattered administrative offices on campus.  Since it was acquired in 1958, the building has been used to house the President’s Office and other administrative offices. From 1958 until 1984, the building was known simply as the President’s Office Building.  In 1984, the University’s President Rev. J.A. Panuska, S.J. renamed the building as Scranton Hall to honor the contributions of the Scranton family.  He stated that: \"Ever since the Scrantons began migrating to Northeastern Pennsylvania in the late 1830s, their vision has touched the life of this region to such an extent that this city and this University bear their name.\" In 1961, the University of Scranton purchased a nine-acre tract of lakefront property containing three buildings on Chapman Lake, about 30 minutes away from the University.  Originally known as the Bosak Summer Estate, the land was owned by Walter Bloes, a tax collector, and was briefly converted into a restaurant-tavern before being sold to the University.  The University bought the property for $65,000. When the University acquired the Estate, they named it Lakeside Pines.  For several years, it was chiefly used as a place for relaxation by the Jesuits and for conferences with faculty members and student leaders.  As time progressed, the University’s Office of Campus Ministries began using the Chapman Lake property as a Retreat Center.  The site originally had one old retreat house, featuring several bedrooms equipped with bunkbeds, a small chapel, a main room with a fireplace, a kitchen, and dining area. In 1998, the University expanded the lakeside Conference and Retreat Center.  Doubling the size of the center, the new 16,000 square-foot facility contained a dining room, kitchen, a large meeting room nicknamed the Lake Room, five small meeting rooms, and a residential wing with 11 bedrooms.  In 2005, in order to meet the growing demand for retreats, the University expanded the Retreat Center again.  The University built an 11,584 square-foot facility adjacent to building constructed in 1998.  The old retreat center was demolished over safety concerns in 2004, making room for the expansion.  The new addition contained a lounge, 21 more bedrooms, and a 65-seat modern chapel with large window views of the Lake.  On November 7, 2006, the University dedicated the Retreat Center chapel, naming it in honor of Blessed Peter Faber, an early Jesuit who, together with St. Francis Xavier and St. Ignatius Loyola, served as the nucleus of the Society of Jesus.  Born April 7, 1506, Peter Faber was the first of the companions to be ordained a priest. Retreats offered at Chapman Lake are usually offered and run by staff and students from the University of Scranton's Office of Campus Ministries.  More than 1,400 people participate annually in about 50 retreats and other spiritual programs conducted at Chapman Lake by the university.  Retreats are offered virtually every weekend, including retreats for seniors as they prepare to end their college careers, for students interested in learning about Ignatian spirituality, for students who have never experienced a retreat before, for students seeking a better understanding of faith and Christian living, and for participants searching for answers to help them through the challenges they face as students. First-year students are offered traditional double rooms that share a community restroom.  All freshmen dorms are located near the center of the university’s campus.  Freshmen housing does not have air-conditioned or carpeted rooms.  Each building has washers and dryers on the first floor for student use as well as light housekeeping services provided to all rooms and bathrooms. Sophomore students are offered suite-style housing, in which two double rooms share a shower and toilet, with each room having its own sink.  Sophomore housing is air conditioned.  All of the buildings have kitchens.  Each building has washers and dryers on the first floor for student use as well as light housekeeping services provided to all rooms and bathrooms.  The three buildings are located together in a cluster on the university’s campus to replicate the close housing arrangement experienced by first-year residential students. Upperclassmen and graduate students are offered apartments and houses. In 1945, with the end of World War II and the creation of the G.I. Bill enrollment exploded at the University of Scranton.  In order to accommodate this dramatic increase in enrollment, the University acquired three “barracks” buildings from the government in 1947 which they placed on the 900 block of Linden Street, part of the former Scranton Estate.  They were named A or Arts Building, B or Business Building, and E or Engineering Building, and each housed classrooms and offices pertaining to those specific subjects.  Purchased from the Navy for one dollar plus transportation and remodeling costs, the Barracks were naval buildings being used in Portsmouth, Virginia that were dismantled and then reassembled in Scranton.  Originally intended as temporary measures to accommodate the larger student body, the Barracks buildings were used for nearly fifteen years before being replaced by permanent structures. The Arts Building featured the \"Pennant Room,\" a lounge that was decorated with pennants from other Jesuit colleges and universities.  It was demolished in 1962 after the completion of St. Thomas Hall. In 1973, the University of Scranton acquired the Bradford House, formerly the Rose Apartment Building.  Bradford House was named for Bradford County in Pennsylvania.  Located in the Hill Section, the Bradford House was three-story apartment building which was converted into student apartments as part of an effort to accommodate the growing number of boarding students.  Equipped with a kitchen and bathroom, each apartment housed about six students.  In 1978, as part of a major accessibility initiative in order to comply with new federal regulations, the first floor of the Bradford House was converted into a wheelchair-accessible apartment for women students.  In 1998, Bradford House was razed in order to make room for the Kania School of Management’s Brennan Hall. In 1945, with the end of World War II and the creation of the G.I. Bill enrollment exploded at the University of Scranton.  In order to accommodate this dramatic increase in enrollment, the University acquired three “barracks” buildings from the government in 1947 which they placed on the 900 block of Linden Street, part of the former Scranton Estate.  They were named A or Arts Building, B or Business Building, and E or Engineering Building, and each housed classrooms and offices pertaining to those specific subjects.  Purchased from the Navy for one dollar plus transportation and remodeling costs, the Barracks were naval buildings being used in Portsmouth, Virginia that were dismantled and then reassembled in Scranton.  Originally intended as temporary measures to accommodate the larger student body, the Barracks buildings were used for nearly fifteen years before being replaced by permanent structures. The University's first cafeteria opened in the Business Building in February 1948, with accommodations for 250 students.  The cafeteria later moved to the Engineering Building.  The Business Building was demolished in 1961 to make room for the construction of St. Thomas Hall. Claver Hall served as an administrative building, housing the Physical Plant and Purchasing departments.  It was demolished in 2010 to make room for Pilarz Hall and Montrone Hall.  It was named for St. Peter Claver, S.J., a Spanish Jesuit priest and missionary. In 1945, with the end of World War II and the creation of the G.I. Bill enrollment exploded at the University of Scranton.  In order to accommodate this dramatic increase in enrollment, the University acquired three “barracks” buildings from the government in 1947 which they placed on the 900 block of Linden Street, part of the former Scranton Estate.  They were named A or Arts Building, B or Business Building, and E or Engineering Building, and each housed classrooms and offices pertaining to those specific subjects.  Purchased from the Navy for one dollar plus transportation and remodeling costs, the Barracks were naval buildings being used in Portsmouth, Virginia that were dismantled and then reassembled in Scranton.  Originally intended as temporary measures to accommodate the larger student body, the Barracks buildings were used for nearly fifteen years before being replaced by permanent structures. E Building held the University's physics laboratories, with special equipment for experimentation in optics, electricity and magnetism.  The building also housed a lecture hall, physics department offices, and a photography dark room.  After Loyola Hall of Science was completed in 1956, most of the science classrooms and laboratories were moved there and the Engineering Building underwent renovations.  The school constructed the St. Ignatius Chapel, a cafeteria, and lounges. In 1960, E Building was dismantled in order to make room for St. Thomas Hall.  It was then shipped to St. Jude's Parish in Mountaintop, Pennsylvania to be used as a grade school annex.  It was subsequently demolished in the 1980s. In 1979, the University of Scranton purchased the Pennsylvania Drug Warehouse for $150,000 from Kay Wholesale Drugs of Wilkes-Barre.  After significant renovations, the three-story building was dedicated as the Gallery Building on April 28, 1982 in honor of former University President J. Eugene Gallery, S.J. who served as the second Jesuit president from 1947 until 1953.  When the building was opened, it housed a Media Resource Center, two large multipurpose lecture rooms, Career Services, a Counseling Center, the Audio Visual department, the Computer Science department, computer laboratories, an Art Gallery, and a study area.  A ten-foot wide wire mesh satellite dish on the roof served as a receiver for educational programs, part of the University's participation in the National University Teleconference Network.  In the 1990s, the Gallery also housed the Dexter Hanley College and the Office of Annual Giving. The Gallery was demolished in October 2001 to make room for Founders' Green, outside of Brennan Hall.  Many of the departments housed in the Gallery were moved into the remodeled O’Hara Hall, while the Art Gallery was moved into the fourth floor of Hyland Hall. In 1955, the University of Scranton announced an ambitious $5,000,000 campus expansion plan, which proposed constructing ten new buildings over the course of the next ten years so that the school’s physical plant would be concentrated at the former Scranton family Estate, the temporary barrack structures would be replaced with safer and more permanent buildings, and its facilities would be expanded to better serve its growing student body.  One of these proposed buildings was a Student Center.  Construction began in 1959 on the $1,030,000 three-story brick, steel, and concrete building.  When it was completed in 1960, the Student Center housed a cafeteria, bookstore, student activities offices, staff and student lounges, a snack bar, game room, a rifle room, and a large ballroom/auditorium.  The cafeteria was designed to seat 600 and to provide between 1200 and 1500 lunches each day (for a full-time student body of 1,358), along with 400 to 600 breakfasts and dinners for resident students (then numbering less than 250). Over the years, the Student Center was renovated and expanded to fit the needs of the growing student body.  In 1974, a $228,000 renovation converted the third floor patio into a grill room, providing another dining area for students with a seating capacity of 300.  In December 1980, during the dedication of the University Commons, the Student Center was also rededicated and renamed as the Joseph F. Gunster Memorial Student Center, in memory of Joseph F. Gunster, a St. Thomas College alumnus, generous University benefactor, and Florida attorney who had previously practiced law in Lackawanna County for over twenty years, whose $1,150,000 estate gift had been the largest in the University's history.  The completion of the University Commons closed the 900 and 1000 blocks of Linden Street to vehicular traffic, creating a campus and making it easier and safer for students to walk between Gunster, Galvin Terrace, the Long Center, St. Thomas Hall, and their residence halls.  During the construction, the entrance to Gunster from the Commons was updated to include a gathering area and speaker's forum.  In 1989, Gunster was renovated, as one of its rooms named the Archives was modernized.  In the Archives, seating was added for an additional 75 to 125 people, an 800-square-foot dance with a disc jockey booth was created, and a retractable video screen was installed.  The snack bar was also expanded, offering a wider variety of food choices.  Because the bookstore was moved to Hyland Hall from Gunster Memorial Student Center, they also expanded the third floor cafeteria.  A summer 1993 expansion of Gunster by architects Leung, Hemmler, and Camayd created a 19,000 sq. ft. addition for dining services, increasing the seating capacity from 630 to 1,000.  The addition included a new food court located on the third floor and created more space for food preparation as well as student activities and organizations.  In summer 2004, the staircase and brick patio outside of Gunster Student Center were replaced due to safety concerns, and the second floor dining room was renovated. By 2001, with the realization that Gunster could not be effectively renovated any further and was unable to adequately meet the needs of the University student body which had grown dramatically since its completion in 1960, the University initiated plans for a new student center, which culminated in the construction of the DeNaples Center in 2008.  Upon the completion and opening of the DeNaples Center, the Gunster Memorial Student Center was demolished.  In its place, the University created the Dionne Green, a 25,000-square-foot green space roughly the size of a football field featuring a 3,600 sq ft outdoor amphitheater, located directly in front of the DeNaples Center. Donated in 1984 to the University by an anonymous faculty member, the Hill House was located on the corner of North Webster and Linden Street.  It was used as a faculty residence, a guest house, and a facility for meetings and social gatherings.  Hill House was named for Rev. William B. Hill, S.J., who in 1984 was marking his 15th year of service to the University of Scranton, having served as having served as an English professor, the academic vice president from 1975 until 1978, the chair of the English department from 1973 until 1975, special assistant to the president from 1987 until 2002, the chaplain of the board of trustees, and the chaplain of the Pro Deo et Universitate Society.  Hill House was razed in the summer of 2007 to make way for Condron Hall. In 1985, the University of Scranton acquired the Hopkins House, located at 1119 Linden Street.  It originally served as the home for the university's student publication offices, which included the Aquinas student newspaper, the Windhover yearbook, and the literary magazine Esprit.  The House was named in honor of Gerard Manley Hopkins, a leading English poet, convert to Catholicism, and Jesuit priest. In 1988, because of a shortage of available on-campus beds, the university converted Hopkins House into a student residence.  The housing crunch resulted from the city’s crackdown on illegal rooming houses, as well as concerns about security and the conditions of off-campus houses, which all lead to an increasing demand for on-campus housing.  In 1990, the university converted the Hopkins House into the Service House, a themed house meant to bring together students, faculty, and staff with an interest in community service to act as a catalyst to expand the university’s involvement in volunteer work through getting as many people involved as possible and coordinating the volunteer activities of the other student residences. Before it was acquired by the university, Hopkins House was the home of Terry Connors, the university photographer for over four decades.  In 2007, Hopkins House was demolished in order to make room for the construction of Condron Hall, a sophomore residence hall. Acquired by the University in 1977, the Jerrett House, a converted apartment building, was the first campus house dedicated to study and reserved for women students.  Opened in 1978, it housed approximately 20 students.  In 1980, Jerrett House made history as the first co-ed student residence on campus, when male students moved into the first floor apartments in order to increases protection around the female residences on Madison Avenue.  Jerrett was retired as a student residence in Fall 2008 due to the construction of Condron Hall, as it had become dilapidated over the years. In 1973, the University acquired Lackawanna House, named after Lackawanna County, Pennsylvania, as part of an effort to accommodate the growing number of residential students.  The building cost $32,500 and required several thousand dollars of renovation to be converted into a residence for 25 students.  It also housed offices for the Aquinas and the Hill Neighborhood Association.  After the completion of Redington Hall, a large dormitory on campus with a capacity of approximately 240 students, in 1985 the University closed Lackawanna House because of its dilapidated condition and sold it. Acquired in 1973 by the University, Lancaster House was a house located on Clay Avenue and converted to a student residence as part of an effort to accommodate the growing number of residential students.  Named for Lancaster County, Pennsylvania, it housed upperclassmen.  In the 1980s, Lancaster House was demolished to make room for Gavigan Hall, a four-story residence hall that houses approximately 240 students in four-person suites. In 1908, construction was completed on the three-story residence for the Christian Brothers, which would later be named La Salle Hall, adjacent to Old Main, St. Thomas College’s main academic building.  It was constructed on a site that had been purchased by the Diocese of Scranton in 1888. In 1942, the incoming Jesuits dedicated the building as La Salle Hall as a tribute to the departing Christian Brothers.  Because the building was too small to house the large Jesuit community, who chose instead to live in the Scranton Estate, which had been donated by Worthington Scranton in 1941, the Jesuits renovated the building.  The first floor housed the office of the University President, the second floor contained a small chapel for daily mass and devotions, and the third floor was converted into offices for the Jesuit faculty.  The chapel was dedicated to the Sacred Heart of Jesus and featured a large painting of the apparition of the Lord to St. Margaret Mary from the cloister of the Georgetown Visitation Convent. After the end of World War II, enrollment at the University exploded as veterans went back to college.  In order to accommodate these larger numbers, the University acquired three former Navy barracks in 1947 which they constructed on the 900 block of Linden Street, part of the former Scranton Estate in the lower Hill section, as the University was unable to expand any further on Wyoming Avenue.  Over the next fifteen years, the University embarked on an ambitious building project to move its entire campus to the Scranton Estate.  Thus, in 1962, after the completion of St. Thomas Hall, the University no longer needed to use Old Main or La Salle Hall and they were vacated.  In 1964, the University of Scranton donated La Salle Hall to the city of Scranton as a contribution to the Central City Redevelopment Project.  The property was worth approximately $360,000 at the time.  In 1970, the building was converted into Cathedral Convent for the Sisters of the Immaculate Heart of Mary, who staffed Bishop Hannan High School across the street. Leahy Hall was originally the home of the Scranton Chapter of the Young Women's Christian Association (YWCA).  The building was constructed in 1907 in the Colonial Revival style.  The three-story, red brick building, designed by Scranton architect Edward Langley, featured offices, lounges, meeting rooms, a gymnasium, and a cafeteria.  The YWCA held classes in physical culture, sewing, music and singing, arithmetic, grammar, Bible study, cooking, and English.  It also created a debate club, served lunch to the public in its cafeteria, and became a meeting spot for local women's civic organizations.  In 1927, the Platt-Woolworth building was constructed as an addition to meet the growing needs of the YWCA.  Funded by Frederick J. Platt and C. S. Woolworth, the new wing to the YWCA headquarters provided housing for 100 women, as well as kitchens, laundry facilities, an auditorium, and a basement swimming pool. As the University of Scranton expanded and began to accept women students, its women students also benefited from the YWCA's services and programs.  Several women resided at the YWCA while studying at the University, including international students.  Over time, however, the YWCA found it increasingly difficult to maintain the property.  In 1976, the University of Scranton purchased the YWCA building for $500,000.  The University of Scranton initially named the building Jefferson Hall.  After the YWCA vacated the building in June 1978, moving to a new location on Stafford Avenue, significant renovations converted the structure into an off-campus residence for 91 students.  The building also contained a gym for recreational athletic activities, conference rooms, a dark room, and offices for student organizations, including the Aquinas, Windhover, Hanley College Council, Debate Club, Esprit, T.V. and radio stations.  A student lounge and snack bar were added in February 1979, although they were removed during later renovations in September 1984.  In November 1983, the gym was transformed into a facility for the Physical Therapy department, housing three laboratories, a dark room, classrooms, and faculty offices. In 1995, The University renamed Jefferson Hall as Edward R. Leahy, Jr., Hall in gratitude to the Leahy family for their endowment of health care education at Scranton.  The son of Edward and Patricia Leahy, Edward R. Leahy, Jr., was born in 1984 with cerebral palsy and several related disabilities.  He died shortly before his ninth birthday in 1993.  Until the fall of 2013, Leahy Hall housed facilities, offices, classrooms, and laboratories for the departments of Physical Therapy and Occupational Therapy.  Demolition of Leahy Hall began September 16, 2013, and revealed a time capsule, which held a 1907 almanac and a wealth of YWCA papers, pamphlets, and clippings dating back to the 1890s.  Construction was completed on the new Edward R. Leahy, Jr.  Hall in 2015 and it opened for use for the Fall 2015 semester. Loyola Hall was constructed in 1956, as part of a major campus expansion.  Built at a cost of $1,205,000, the reinforced concrete structure featured a porcelain enameled steel \"skin\" brickwork as well as aluminum mullions along its exterior.  At the time of its opening, the ground floor was dedicated to engineering, the first floor to physics, the second floor to biology, and the third floor to chemistry.  The penthouse housed the university's radio station (WUSV) and its equipment, including a steel radio tower, which was subsequently dismantled in 1974.  When the building was first constructed, its ultra modern design, technologically advanced features, and ability to house all of the science departments in one building made it a vital part of the University of Scranton’s campus.  Before the construction of Loyola Hall, engineering students had been forced to go elsewhere for the final two years of their education because the university lacked the proper equipment to teach them. As part of the “Second Cornerstone” campaign, a fifteen million dollar expansion and improvement project, the university extensively renovated Loyola Hall in 1987.  In the $2,750,000 expansion of Loyola Hall, the existing building was remodeled and an expansion towards Monroe Avenue was added, in order to accommodate the growing student body and the expanding science programs.  An additional floor and a twenty-foot extension of Loyola’s east wall expanded the floor space of the facility by more than 14,000 feet.  The new space provided room for additional chemistry laboratories, classrooms, research areas, and computer facilities for faculty and students. With the construction of the Loyola Science Center in 2011, Loyola Hall was functionally superseded.  The science departments, classrooms, and laboratories formerly housed in Loyola Hall were moved to the more modern, more technologically advanced, more energy-efficient, and safer The Loyola Science Center.  Before being demolished, it served as “swing space,” or a housing site for classes or offices whose buildings are undergoing renovations.  The building provided housing for the Panuska College of Professional Studies Academic Advising Center and the departments of Physical Therapy and Occupational Therapy, all displaced by the demolition of Leahy Hall and the construction of the new Center for Rehabilitation Education.  In the summer of 2016, Loyola Hall was demolished. In 1978, the University acquired Luzerne House, located at 308 Clay Avenue, for $60,000.  It was then converted into a residency for women students with a capacity of 32 occupants.  Luzerne House, named for Luzerne County in Pennsylvania, was demolished in 2010 as part of the University of Scranton's restoration project on Clay Avenue.  The site now features green space and a graded sidewalk. Acquired in 1974 by the University, Mercer House was converted to a student residence as part of an effort to accommodate the growing number of residential students.  Named for Mercer County, Pennsylvania, it housed upperclassmen.  In the 1990s, the University stopped using Mercer House as a residence for students after the completion of several larger on-campus residence halls. Acquired in 1974 by the University, Montgomery House, named for Montgomery County in Pennsylvania, was converted to a student residence as part of an effort to accommodate the growing number of residential students.  Montgomery House was retired as a student residence in Fall 2008 following the construction of Condron Hall. Old Main, also known as College Hall, was the first building constructed for St. Thomas College and served as the center of the school's campus for many years. In 1883, Bishop O'Hara purchased the Wyoming Avenue property near St. Peter's Cathedral from William H. Pier.  On August 12, 1888, he blessed and placed a cornerstone as the foundation for St. Thomas College.  The laying of the cornerstone was a city-wide celebration, featuring a parade, musical performances by the Cathedral choir and a local orchestra, and a sermon by Bishop O’Hara, attracting residents from the city of Scranton as well as the surrounding area, as far as Wilkes-Barre and Carbondale.  The cornerstone held a copper box, in which were placed six newspapers from the day of the dedication and seven silver coins.  After four years of intense fundraising, the construction of Old Main was completed.  The three-story red brick building, located on Wyoming Avenue next to St. Peter’s Cathedral and the Bishop’s residence, had three floors and a basement.  Originally, there were eight classrooms on the first and second floors, the third floor was an auditorium/gymnasium, and the basement held a chapel dedicated to St. Aloysius.  In September 1892, the College opened for classes. By the 1920s, Old Main could no longer fully accommodate the growing institution and underwent a number of renovations.  In 1926, the College’s first library was established after part of the gymnasium on the third floor of Old Main was converted.  Originally, the library’s collection consisted of 300 books that Bishop Hoban had donated from his personal collection.  Over the next ten years, the College continued to make changes to Old Main to meet the needs of the expanding student population.  The library’s collections continued to grow, its existing laboratories were modernized, and the building was repainted and repaired.  The gymnasium on the third floor was converted into three laboratories, a lecture hall, and faculty offices.  Without a gym on campus, physical education classes moved to the Knights of Columbus gym on North Washington & Olive Streets, and basketball practice moved to Watres Armory.  Additional renovations in 1949 transformed the basement of Old Main into a student lounge.  Known as Anthracite Hall, it contained a snack bar and could be converted into an auditorium/ballroom with a seating capacity of 600.  The library was also expanded to occupy the entire third floor of Old Main, as classrooms and laboratories moved into the Navy barracks, which had been acquired by the University in 1947. In 1941, Worthington Scranton donated his home and adjoining estate, located on Linden Street in the lower Hill section about seven blocks away from Old Main on Wyoming Avenue, to the University of Scranton.  As the school continued to grow, additional buildings were acquired and constructed on the former Scranton Estate, as there was no room for expansion on Wyoming Avenue.  Gradually, all operations moved from Old Main to the new campus.  The arts and sciences, business, and engineering divisions were moved from Old Main to the naval barracks when they were purchased in 1947.  During the 1950s, the University embarked on an ambitious $5,000,000 campus expansion plan, building Loyola Science Hall, Alumni Memorial Library, Gunster Memorial Student Center, and St. Thomas Hall, which allowed the school to vacate its Wyoming Avenue properties, including Old Main in 1962. During the dedication ceremony for the new classroom building, the original cornerstone from Old Main transferred to the front corner of St. Thomas Hall.  Seventy four years after Old Main’s blessing in 1888, the University of Scranton transferred its cornerstone to the new campus, linking the University with its past and providing continuity from both the University's former name, St. Thomas College, and its old campus. After the University vacated Old Main, it was used by Scranton Preparatory School for two years after its previous home, the former Thomson Hospital, was purchased and demolished by the Scranton Redevelopment Authority as part of an effort to widen Mulberry Street.  In 1964, Scranton Prep moved to its permanent location, the former Women’s Institute Building of the International Correspondence Schools, at 1000 Wyoming Avenue.  After Scranton Prep moved locations, the University of Scranton transferred the title of the building back to St. Peter's Cathedral parish.  In 1968, Old Main was demolished.  Currently, the land serves as the Cathedral Prayer Garden. Acquired in 1974 by the University, Somerset House was converted to a student residence as part of an effort to accommodate the growing number of residential students.  Originally housing male students, it was converted to a female residence in 1980 and later became coed.  Named for Somerset County in Pennsylvania, Somerset House was razed in the 1990s to make way for Brennan Hall. Constructed in 1895, Thomson Hall was originally the private hospital of Dr. Charles E. Thomson.  When it was first built, the structure was four stories tall, but a later expansion added two additional stories for a total of 24,000 square feet.  In 1941, Bishop Hafey purchased the Hospital, which had by then ceased operation, for $60,000 for use by the University of Scranton.  Called the Annex, the building was not used by the Christian Brothers before they relinquished control of the University to the Society of Jesus. When the Jesuits arrived at Scranton, renovations were made to the building, including the creation of additional classrooms, faculty offices, and living quarters for out-of-town students and aviation cadets training at the University.  It opened in 1942, after the Jesuits took ownership of the University of Scranton.  However, two days before Christmas in December 1943, the Annex was severely damaged by a fire.  Since the University was in recess for the Christmas holiday, no one was in the building to be injured, although a firefighter died later that night of a heart attack, believed to have been brought on by exhaustion and smoke inhalation.  The upper two floors of the Annex were gutted and subsequently eliminated, before the rest of the building was repaired. The sharp decline in enrollment caused by World War II had reduced the University's space requirements, so when the Annex was repaired, it was decided that the University of Scranton did not need the building.  Instead, the Jesuits decided to open a high school housed in the Annex.  Since the Jesuits had arrived in Scranton, the Scranton diocese and the Catholic community had requested they establish a college preparatory school.  The availability of a reconstructed Annex made such a step possible.  Thus, the Scranton Preparatory School was born in 1944.  In 1961, Scranton Prep moved from the Annex to the recently vacated Old Main because the Annex had been purchased by the Scranton Redevelopment Authority.  It was demolished later that year in order to widen Mulberry Street. Throop House was originally the private home of Dr. Benjamin H. Throop, a pioneer Scranton physician.  It was constructed in 1880, and owned by Dr. Throop until his death in 1897.  The structure, located between the Thomson Hospital and LaSalle Hall, was owned by the Throop Estate until 1922, when it was purchased by the Diocese of Scranton in order to accommodate the growing student body of St. Thomas College and provide additional classroom space.  The two-story barn behind the Throop House, later called C Building, was converted into a chemistry laboratory.  During the 1920s and 1930s, the Throop House mainly held freshman classes but also served as a meeting space for the local Scranton Catholic Club and classrooms for the high school division of St. Thomas.  Though its former barn was used by the University until 1956, Throop House was demolished in January 1943, because it was considered a fire hazard. Acquired by the University in 1982 for $115,000 to be used as a student residence, Wyoming House was constructed in 1901 and originally known as Jefferson Towne House.  The three-floor, Colonial Revival-style Towne House had been used over the years as a medical office, a music conservatory, a funeral parlor, and a catering business.  In 2000, Wyoming House was demolished to make room for the construction of Mulberry Plaza.\n\nChristchurch Transport Board The Christchurch Transport Board was an autonomous special-purpose municipal authority responsible for the construction, acquisition, and ownership of local transport assets and the operation of public transport services in the Christchurch region of New Zealand’s South Island.  Constituted as the Christchurch Tramway Board in 1902, it operated trams and buses to Christchurch's outer suburbs and satellite towns for 84 years until being disestablished in 1989 by local government reforms. The Board assumed control of the existing network of privately run tramways and converted these to electric operation whilst also extending the network.  Economy measures resulted in several tram routes being converted to trolley bus and later diesel bus operation from the 1930s.  The remaining tram routes were progressively closed in the decade following the end of World War II as the infrastructure required renewal or replacement.  The last of the Board's trams were withdrawn in 1954, followed by its trolley buses in 1956.  Its operations were fully converted to diesel buses in 1964 when the last of its older vehicles were replaced, including its petrol buses. Subsequent to its demise, the Board's bus operation was transferred to a Local Authority Trading Enterprise (LATE) and was just one of several service providers in Christchurch following deregulation in mid-1991.  The remnants of the Board survive today as the Christchurch City Council-owned Red Bus, the name by which it has been known since 1999. From 1880 a series of tramway routes had been constructed and operated by private companies using both horse and steam as motive power.  While these services heralded a major improvement to local transport at the time, at around the turn of the century there was a mood for change.  Other towns and cities around the country had either already established electric tram networks or were considering their introduction including Auckland (1902), Dunedin (1903), and Wellington (1904). The concessions under which the Christchurch Tramway Company was operating were due to expire at various times between September 1899 and 1919.  With this in mind it sought to negotiate with the various local bodies concerned for an extension of these concessions, both to enable it to continue its business and to provide some certainty for future investment.  With public sentiment in favour of electrification, and the parlous financial state of the private tramway companies making it unlikely that they would be able to provide such a service, the councils were opposed to renewing the concessions. By the turn of the century the Christchurch City Council was strongly in favour of bringing the tramways under municipal control but only on its own terms.  This ignited suspicion amongst Christchurch City's neighbouring boroughs – through whose territory such a system would have to be built even if only to replace the existing private network – that they would have no control over the running of such a system.  A series of tramway conferences were held from the mid-1890s for the local bodies involved to advance the cause of municipalisation but it was not until March 1902 that the City Council acquiesced to the demands of its neighbours for a tramway board with elected members.  The Tramway Conference had received an offer to construct and operate an electric tramway system from the British Electric Traction Company (the same company that had the contract for the Auckland system) in February but, despite public support for the proposal, declined as they were already in favour of municipal control. Despite generally favourable public opinion on bringing electric trams to Christchurch there were some objectors.  These included the Royal Society who believed an electric tramway system would cause magnetic disturbances that would interfere with readings made at its Christchurch observatory; the municipal utilities who feared leakage from the return current of an electric tramway would corrode underground pipes through electrolysis; and the telephone authority who believed that the flow of electricity through such a system would interfere with earth currents in telephone circuits and thus render them useless.  Solutions were found for all of the utilities concerns. The tramway conferences culminated in June 1902 with a decision to create a separate Tramway Board with elected representation from each affected district.  To this end the Christchurch Tramway District Act was passed, effective from 26 September 1902, and the Christchurch Tramway Board was born. The Board's district included the City of Christchurch; the Boroughs of Sydenham, St. Albans, Linwood, Woolston, New Brighton, and Sumner; and all or part of the Road Districts of Spreydon, Avon, Heathcote, Riccarton, and Halswell, with provision made for the future addition of other areas.  It was authorised to levy rates in its district for tramway purposes and to raise loans (initially up to £250,000 and later an additional £100,000 if required) subject to approval by plebiscite. An election was held the following year on 22 January 1903 to elect the first Tramway Board.  Fifteen candidates vied for eight positions in a contest that resulted in the election of well-known local politicians and businessmen.  The following were elected: The Board's first meeting was held on 29 January at which various other officers were appointed.  A second meeting was held on 13 February and shortly thereafter the Board was able to hire Hulbert Chamberlain, an American consultant with expertise in electric tramways. Voters were also asked on the day of the Board election to decide on the issue of amalgamating the boroughs of Sydenham, Linwood, and St. Albans with the City of Christchurch.  This proposal was easily carried and formally took effect from 1 April 1903.  This was to prove fortuitous for the Tramway Board by making it easier to gain concessions for the construction of tramway lines. The proposal for the first loan was put before voters on 16 June 1903 and was carried with 89.4% in favour and 10.6% against.  Later that year the tramway district was expanded to include the Riccarton - Sockburn sub-district, resulting in a ninth board position being created, which was filled by John Joseph Dougall.  A new loan of £55,000 was also approved. As the private tramway companies were operating under concessions that were due to expire after the Board intended to begin operation, it realised it would have to compensate these companies to become the sole operator in Christchurch.  Despite the significant capital investment required for infrastructure renewal in the private tramway lines, and the limited utility of other assets for the Board’s operation, none of the parties could reach agreement on a suitable price.  In all three cases the amount of compensation was decided through arbitration. The Christchurch Tramway Company remained in operation until 16 May 1905, the day the Board commenced operation.  It had been running at its own risk by special arrangement with the Board since September 1904 when many of its concessions had expired.  The Board paid £23,910 for the company's plant. The Board entered into an agreement with the New Brighton Tramway Company whereby the company would remain in service until the Board was ready to assume control of the line.  The company ceased operations on 1 August 1905 at which point it handed its assets over to the Board and was compensated with £7,267.  However, construction was behind schedule so the Board contracted William Hayward and Company to supply drivers and horses so it could start its own service.  This arrangement lasted until the following year. The City and Suburban Tramway Company initially indicated to the Board that it would rather continue to provide its horse tram service to which the Board was not agreeable.  It ceased operations in November 1906 when it was taken over by the Board and compensated with £7,982. The City Council received compensation of £1,200 from the Board for its Corporation Line. The Tramway Board officially commenced operations on 5 June 1905 when an official party departed the Falsgrave Street car depot aboard a convoy of seven trams with double-decker trailers at front and rear bound for Papanui.  After an accident en route necessitated the withdrawal of the fifth and sixth cars in the procession, the remaining cars proceeded to their destination where an opening day function was held at the Royal Café. Timetabled services began the following day with Board engineer Hulbert Chamberlain serving as motorman on the first tram.  Their novelty quickly made the trams popular with the public causing patronage to rise to \"unprecedented and unexpected\" levels.  As with the earlier private tramway operators, the Board derived much of its income from the carriage of passengers attending major social events or on excursions to popular locations like the beach or racetrack. From the beginning it was the Board's intention to eventually operate all lines with electric trams and to retire the use of the horse and steam trams from public service as soon as practicable.  Horses continued to be used for a short while on the lines formerly operated by the New Brighton and City and Suburban tramway companies: in the former case because completion of the electrification on that line had been delayed and in the latter case because of pressures on the Board with the construction of lines elsewhere.  Other lines were initially operated with the steam motors once the track had been laid but before the installation of the electrical infrastructure had been completed.  Steam motors were also used for shunting, to haul special purpose cars used for maintenance, and to haul special services on particularly busy peak days. The Board’s early performance, while not outstanding, was still impressive.  The first year of operation generated revenue of £43,109 with the sale of 5,000,000 fares.  For the 1907–1908 fiscal year this had risen to £91,083 with 10,500,000 fares sold and for the 1909–1910 period revenue was £105,024 with 12,500,000 fares sold. The organised labour movement had its first dealings with the Board following the formation of the Christchurch Tramway Worker's Union in 1906.  The Board welcomed this move as a way of constructively resolving disputes and other small employment matters.  Unlike the unions in Auckland and Wellington, the Christchurch union maintained a cordial relationship with the Board and it was many years later before they called for major industrial action. The conservative influence on the Board faced its first challenge at the 1912 elections.  The preceding year, voting rights in municipal elections had been opened up to all eligible adults, an opportunity various socialist groups sought to take advantage of by fielding their own candidates for election to the Board.  Some of the incumbent members warned voters about the possibility of a left-leaning Board putting the concerns of workers ahead of those of the travelling public, and succeeded in having only a single candidate from the opposing camp elected. All major tramline construction was completed by 1914 with the opening of the St. Martins line, after which no new lines were built.  This was partly as a result of the First World War, which restricted the supply of materials and thus meant extensions to the network were not possible.  Patronage, which had held up well through the war years, was constrained after war's end by economic conditions restricting the growth of the tramway below that which had been desired by the Board. The Board adopted a patriotic stance during the war years as seen in measures it adopted at the time including supplementing the military pay of its staff on active duty and expressing a preference in hiring for returned servicemen and those who had attempted to enlist but were deemed unsuitable for military service. In 1917 the Board purchased its first bus and commenced a feeder service from the end of the Sockburn tram route to Templeton.  The bus was a battery-powered vehicle with solid rubber tyres but proved to be so unreliable that it was rebuilt and converted to use a petrol engine.  It was later joined by another petrol-powered bus. A grand plan for the modernisation and improvement of the tramway system was presented to the Board in 1919.  It proposed borrowing £340,000 for a series of improvements including the acquisition of new rolling stock, upgraded workshops, double-tracking and new passing loops, and new routes and route extensions.  The loan was raised and many of these works were carried out but by the time the Board could seriously consider the issue of the new routes it discovered that post-war inflation had dramatically increased the cost of laying tramlines.  As a consequence only two of the proposed seven routes were actually constructed, those being a line along Lichfield Street to connect Oxford Terrace to Colombo Street and an extension of the Spreydon Line. The Christchurch Transport Board, like most other organisations around the country, was hit hard by the influenza epidemic of 1918.  Their employees were affected to the extent that services were cancelled on Sundays from 17 November and evening services after 20:15 were withdrawn from 19 November.  These changes remained in effect until 8 December.  The lost business during this time is estimated to have cost the Board £3,000. Additional measures were taken by the Board to assist with the response to the health crisis.  Twenty-three of its electric tramcars were converted to \"inhalation chambers\" and placed around the city; public health workers were given free passes; and all services were operated as open conveyances. A recession in 1921 slowed economic activity and caused patronage of the Board's trams, which had in recent years generated healthy returns, to plateau at around 25,000,000 per annum, whereabouts it remained for much of the 1920s. It was during this period that advances were made in the development of buses that improved their reliability and introduced innovations such as the pneumatic tyre.  The Board, which by this time had come to the realisation that it would not be able to afford to significantly extend the tramway network, purchased some additional buses to provide services in areas not served by the trams.  These developments also led to the rise of the private bus operators, which were able to provide services in areas not covered by the Board, and later to directly challenge the Board itself.  After introducing a number of measures to directly combat the private operators, legislative means were employed to deal with them. After the Board purchased the assets and business of most of its private competitors following legislative changes in the mid-1920s, it acquired a motley collection of buses with which it was able to start services on new routes.  Despite the high maintenance and inventory costs associated with these vehicles, they were used to establish permanent routes to Bryndwr (originally planned to be a tramline), Springfield Road, Pleasant Point (feeder service), Shirley, and Avonside. Inter City Motors, the one competitor that survived the Board's attempts to eliminate competition, found success in challenging the Board on the New Brighton route.  This was a serious problem for the Board's North Beach tramline, one of the least profitable in the system, and whose tortuous path was longer than Inter City Motors' more direct route.  The poor state of the track meant that increasing the speed of trams running on the line to combat the Inter City Motors service was not an option so the Board closed the line in 1927, but public pressure resulted in its reopening a short time later.  The line continued to deteriorate to the point where replacement or closure was unavoidable.  As the former could not be justified, the line was closed permanently beyond Marshland Road and, as the Board had not been impressed with petrol buses, services were replaced with trolley buses in 1931.  The trolley buses followed a different route to Marshland Road, enabling the trams to continue running as far as Marshland Road. Having formed a favourable opinion of trolley buses, the Board decided to use them on a new line to Richmond.  However, it realised that its troubles with Inter City Motors on the North Beach route would soon be resolved and given the vehicles that would become available, an extension of the trolley bus system would not be needed.  An attempt to cancel the order failed, so the second trolley bus line was opened in 1934 to Marshland Road via Richmond.  This enabled trams to be withdrawn from the remaining portion of the North Beach tramline, which followed the same route as the new Richmond trolley bus line. The diesel engine had become the power plant of choice for commercial transport operators by the mid-1930s, supplanting and effectively making obsolete the petrol engine for such purposes.  The advantages of diesel buses prompted a policy change by the Board whereby it would use diesel buses on more lightly patronised routes while remaining committed to trams on major routes.  To this end a loan of £20,000 was raised, enabling the Board to purchase 10 AEC diesel-powered chassis and acquire the business of Inter City Motors.  The motor bodies were constructed on the chassis by the Board in two configurations, six with half-cabs and four with full-front windscreens.  Once completed, they replaced trams on the Dallington route, and petrol buses on the Bryndwr and Springfield Road routes. With the diesel buses having proved to be a success, it was intended next to replace tram services on the Opawa–Fendalton and St. Martins routes, being the next two least viable tramlines.  Though bus services did run to St. Martins for a while, the intervention of World War 2 meant that it was several more years before these tramlines were closed permanently. Patronage of the trams, which had risen steadily up to the 1920s before levelling out, started to fall in 1928–1930.  The Great Depression, brought about by the share market crash in October 1929, only exacerbated the Board's problems.  A loss of £1,092 was recorded for 1931 and a projected loss of around £10,000 was expected in 1932 unless there were further measures to reign in costs. To address its financial woes, the Board implemented a policy of staff reduction through attrition but as economic conditions worsened, fewer people wanted to leave their jobs with the Board, reducing the effectiveness of the policy.  Wages remained the biggest expense, so the Board announced that there would be an 11% wage cut and reduction in working conditions.  When the union refused to agree to these terms, the Board exploited a loophole in the industrial award, dismissing its entire traffic staff and then inviting them to apply for new positions with the new conditions.  The matter went to arbitration at which the magistrate sided with the union and set aside the changes for a period of 12 months. Seeking other ways to save money, the Board obtained permission in November 1931 to have only a single conductor on duty per tram regardless of the number of trailers.  By mutual agreement with the union in February 1932, it was decided on a temporary system of a reduction in hours for all traffic staff rather than terminating the employment of 11 men that had become redundant.  When the men refused to agree to a renewal of this arrangement the Board dismissed 12 of its staff.  A rally in support of the tramway men was held on May Day at which it was decided to strike from the following Wednesday if the notices of dismissal were not rescinded.  The Board warned its staff that if they refused to go to work on Wednesday that they would be dismissed, and also advertised for \"volunteers\" to apply for permanent positions with the Board.  Despite offers of negotiation from the union, the Board remained implacable in its position. The strike started peacefully but soon turned violent.  Over the next several days there were numerous skirmishes in which trams and tram staff were attacked.  Tramcars and the depot were fortified, police accompanied trams on their rounds, and special constables were sworn in to assist the regular police.  The strike ended on 10 May after public opinion turned against the striking workers.  A special tribunal was established to deal with the matter.  It eventually ruled largely in favour of the Board, upholding the dismissal of 40 of the striking union workers with the remainder of the available positions to be filled by the \"volunteers\" taken on to work during the strike.  The result of the incident left much bitterness for many years to come. Following New Zealand's declaration of war in 1939, many changes to everyday life were brought about that affected the Board's business.  Petrol rationing was immediately introduced, and this combined with shortages of materials such as rubber significantly affected the use of one of the trams greatest competitors: the private motorcar.  This prompted an unprecedented boost to patronage of the tramways, often leading to severe overcrowding, and significantly increased the Board's revenue. Another factor in the Board's favour during this time was the large number of military personnel stationed in and around Christchurch, from the Army at Burnham, the Airforce at Wigram, and a contingent of American soldiers based in the city.  Special tickets for soldiers were introduced and remained popular for many years. The Board also faced several challenges during the war years.  Operating costs increased, including staff wages, repair and maintenance, electricity, and standing charges.  Supplies of materials and spare parts were restricted, owing to the difficulty of importing them.  Staff availability and retention became an issue with many away on active military service, resulting in the hiring of more women. Wartime regulations required the Board to reduce its bus services to conserve resources, and thus services were cut by a quarter.  One of the more obvious effects of this decision was the St. Martins route, which had been converted to bus operation in January 1941, was reactivated as a tram route in July 1942, despite the poor condition of the track. The end of the war heralded one of the most turbulent periods in the Board's history.  Years of deferred maintenance, heavy usage during the war years, and a lack of skilled staff, materials, and spare parts had left the tramway in poor condition.  A significant investment was required to renew the system but uncertainty was growing over the future of the trams.  The Board also had to contend with a more aggressive union movement, and a council that was seeking legislative authority to enable it to assume control of the tramway.  This was to be one of many occasions on which it was suggested, or an attempt was made, to have the Board assimilated by the City Council or controlled by a Council-affiliated Metropolitan Board of Works. A committee was formed to investigate the future of the tramway system and reported back in July 1946 that as each line reached the end of its useful life it should be closed and replaced with buses.  Providing services to new areas had become a priority, especially as the city was growing beyond the extent of the tramway network. As an earlier attempt to acquire diesel buses had been thwarted by the war, the Board had to settle for purchasing petrol buses after the war owing to the shortage of diesel vehicles.  This enabled the commencement of new services in 1946 to areas not covered by the tramway network including Mount Pleasant, Somerfield, Creyke Road, Huntsbury, Wharenui, and Sanatorium on the Cashmere Hills.  Also, buses replaced trams for the second – and final – time on the St. Martins route. The resignation of general manager Hedley Jarman in October 1946 was followed by the appointment of John Fardell as his replacement.  Fardell, having previously been employed at Reading Corporation Transport where he was involved in the replacement of trams with trolley buses, wasted little time in making his views known on the direction of public transport in Christchurch.  He was in favour of the replacement of trams and trolley buses as they reached the end of their useful lives with diesel buses, with the exception of the Papanui–Cashmere route for which he favoured trolley buses. Accepting that significant capital investment was required to replace the worn-out tramway with buses, the Citizen's Board was granted permission in February 1948 to raise a loan of £1,350,000, subject to approval by plebiscite.  As elections for the Board were due later that year, it was decided to run the poll for the loan at the same time.  While there was debate as to what mix of diesel and trolley buses would be appropriate in any new system, the Labour Party questioned whether the loan was even affordable: a strategy which saw the loan rejected and a slim Labour majority returned to the Board. While Fardell was still strongly in favour of a majority diesel-powered bus fleet, this was at odds with the views of the Labour members, which had campaigned on a majority trolley bus fleet.  The new board proceeded to order 40 new trolley buses to replace trams on major routes, but the Loans Board rejected the application for a loan to cover the cost of the buses and stabling facilities.  Replacement of the tramway had become a matter of some urgency when, in April 1950, the Board decided it had to act and ordered the first 39 of an intended fleet of 105 trolley buses.  Permission for a loan of £950,000 was granted, subject to approval by plebiscite.  This time, an aggressive advertising campaign preceding the poll succeeded in winning approval for the loan.  Despite the move to introduce buses on many tram routes, trams were still favoured for the Papanui–Cashmere route so it was decided to relay track on this line to keep the service going. During the campaign for the 1951 Board elections, much was made of the fact that the Labour-controlled Board had failed to purchase any new buses.  The Labour candidates were seen as lacking credibility having achieved little during the past term with the result that the party was trounced at the poll: no Labour Party members were elected.  The new Board, at Fardell's direction, wasted little time in cancelling the trolley bus order that had been placed by the previous board and instead ordered 57 diesel buses to replace trams on several routes. With the end of the Christchurch tramway system in sight it was decided to re-brand the Christchurch Tramway Board to reflect its move away from a purely tram-based operation.  The name of the Board was changed by the Local Legislation Act 1951 which amended the Board’s constitutional legislation, the Christchurch Tramway District Act, to officially rename the organisation to Christchurch Transport Board effective from 5 December 1951. What followed over the next three years was the wholesale closure of what was left of the tramway.  As more buses became available they were quickly put to work on routes formerly served by trams.  The remaining lines were closed in quick succession starting with Fendalton–Opawa in 1950 and ending with Papanui–Cashmere in 1954.  Over the life of the tram network, 1,605,932,516 fares had been sold. As well as replacing the trams, the new buses were also deployed on new routes in areas of significant residential development including Riccarton, Ilam, Harewood Road, Shirley, Aranui, and Fendalton.  Additional buses were ordered in 1956 and 1958.  The first batch was to enable the retirement of the last of the trolley buses on the Richmond and Marshland Road route (May 1956) and North Beach (November 1956). Modernisation of the Board's bus fleet was complete in 1964 when, after an earlier purchase of 24 AEC Reliance chassis in 1961, the Board was able to retire the last of its older buses.  This meant that the fleet was now composed entirely of AEC vehicles. The Board's base of operations was relocated to the north side of Moorhouse Avenue in 1969 (site of the present-day Red Bus depot).  The first block of land for this site had been purchased in 1920, with later purchases including a neighbouring brewery in 1940.  New workshops had already been established on the site in 1961, with other features including parking for 200 buses and administration and servicing facilities.  New headquarters were established in Carruca House when the Board opened the building in July 1973. John Fardell, who had been at the helm of the Board for nearly three decades, retired in 1973.  Max Taylor, who had previously held senior management positions with the Board including Chief Engineer from 1965 and Assistant general manager from 1971, succeeded him. Patronage of the Board's services, which had been trending downwards since the early 1960s, was boosted in the 1970s by several major events.  First, the 1974 Commonwealth Games proved to be a boon for public transport, with an additional 1,525 services provided of which 1,195 were to or from Queen Elizabeth II Park, the main games facility.  Second, the oil crises of 1973 and 1979 restricted the use of private motor vehicles leading to an increase in patronage of the buses.  This effect was not, however, to last beyond the end of the \"car-less days\" programme in 1980. Innovative attempts at new services were made from time to time, occasionally with some success.  A central city loop route was started in 1964 for which a small fare was charged, but failed to catch on.  The \"City Clipper\" was introduced in the 1970s on a new central city circuit using buses with a special livery and charging no fare.  It was initially backed by local business until 1976, after which it was subsidised until poor patronage led to its demise in the early 1980s.  Several services linking shopping malls were tried, largely without success.  In 1984 a short-lived service called the \"Seaside Special\" was started to bring people from southern and northern suburbs to New Brighton on Saturdays for shopping.  One of the more successful acquisitions for the Board was the Airport route from Midland Coachlines in 1976. A Government grant of $50,000,000 to municipal transport authorities in 1977 enabled the Board to purchase 96 Bristol buses that entered service between 1978 and 1981.  Nearly a decade later the last major fleet upgrade was made when the Board, in conjunction with several other transport authorities around the country, ordered 90 MAN buses as part of a bulk order in 1986.  Though an additional 9 were ordered later, they did not arrive until after the Board was disestablished. The passing of the Urban Transport Act 1980, based on the recommendations of the Carter Report from 1970, stripped municipal authorities such as the Christchurch Transport Board of their authority to levy rates within their districts.  Responsibility for funding public transport was bestowed upon regional authorities – in the Board's case, the Canterbury United (later Regional) Council — in keeping with the view that transport needed to be planned and managed in a co-ordinated way on a regional basis, though this change did not actually take effect until 1 April 1988. Following the sale and break-up of Midland Motors, the Board obtained its licenses to operate public transport services and assumed control of its rural bus routes in November 1982.  These routes were integrated into the Board's existing operations and, because Midland had not been allowed to carry short-haul passengers within the Board's rating district, services to Christchurch's satellite settlements were improved under the Board's control, as it did not have these restrictions. A series of marketing campaigns in the early 1980s was successful in increasing patronage of the Board's buses, culminating in an improvement of 10.3% being reported in the 1984 Annual Report.  This was also a time of increasing economic troubles for the country, typified by a dramatic rise in inflation.  Following the end of a price and wage freeze, inflation again began to rise sharply, prompting the Tramways Union to campaign for an increase in allowances to cover cost-of-living increases for its members.  This resulted in a series of 24- and 48-hour strikes from 8 November 1985 when the Board was not agreeable to their demands, to which the Board responded with a lock-out on 3 December.  Fifteen days of industrial action followed, at the conclusion of which the striking staff returned to work having failed to secure any concessions from the Board.  Some conditions were curtailed, and the liberal approach to various employment matters that workers had enjoyed was replaced by a hard-liner stance.  To make up for revenue forgone during the strikes, fares were raised by 33%.  This did not bode well with passengers, and when combined with the loss of goodwill from the recent industrial action, they deserted the buses to the extent that the Board lost around half of the improvement in its patronage of recent years.  With the general economic malaise that followed, patronage of the Board's services never recovered to previous levels. The Fourth Labour Government implemented a programme of economic reform to stabilise the economy and improve productivity that included the corporatisation and deregulation of state-sector agencies.  One area to receive special attention was transport, which under state and municipal control had little or no effective competition.  It was felt that this was inefficient and delivered \"a poor transport system\" as these providers had no incentive to offer a better service or improve their operation. The Board, which had been aware for some time of the government's intention for organisations such as itself to operate in a more commercial manner, had been making changes with this in mind.  The Christchurch City Council had investigated the idea of transforming the Board into a limited liability company in the late 1980s, fully held by the Council, but found that the legislation on which the Board was founded had no provision to allow this.  An Establishment Unit was created in March 1988, under the auspices of the Local Government Act 1974, to manage the transition of the Board to a commercial entity.  The Unit was replaced in October with a Transitional Committee that had wider representation from local authorities to decide the nature of the company that the Board would become. The rating district, which had been used to subsidise the Board's operations, disappeared with the transfer of responsibility for funding public transport to the Canterbury United Council in April 1988.  In its place, they recovered the funds from the local authorities in whose districts the Board operated.  A uniform fare structure was introduced, charging passengers by the distance travelled, except those cases where the full subsidy was not forthcoming from the relevant local council in which case higher fares were charged in those areas to compensate. The local government reforms of 1989 significantly reduced the number of local bodies through amalgamation and disestablished most of the special-purpose bodies then in existence.  Regional councils were assigned responsibility for planning and funding of public transport but were not permitted to be involved as a supplier of services.  The law required transport assets under municipal control to be either divested or corporatised as Local Authority Trading Enterprises.  The Local Government Amendment Act that implemented these reforms became law on 1 November 1989 and marked the end of the Christchurch Transport Board.  The Board held its last meeting on 11 October 1989, followed by a large social function at the Ferrymead Heritage Park on 29 October attended by current and former staff. The Board's operations briefly became the responsibility of the Christchurch City Council before being transferred to a new company, Christchurch Transport Limited.  The regional council introduced a competitive tendering system in early 1991 for which Christchurch Transport was one of several bidders.  The last vestiges of the Board's monopoly position disappeared when the deregulated transport market took effect from 1 July 1991. To commemorate what would have been the centenary of the Board’s founding, several former Transport Board staff members resolved in March 2002 to organise a reunion of those who had worked for the Board.  It was decided to hold the event on Labour Weekend 2003 in recognition of the first meeting of the Board having been held in January 1903.  A charitable society was formed, the CTB 100th Anniversary Society Incorporated, and it was agreed to organise five events to celebrate the occasion as well as the publication of the book \"CTB: A Brief Social History of the Christchurch Transport Board 1903–1989\" and the production of memorabilia. The Board’s first competitor actually began operation before the first tram ran.  The Christchurch Motor Omnibus Company commenced its first service from Cathedral Square to the railway station in April 1904.  After the trams started running in June 1905 the bus was switched to a Riccarton route.  Several other private bus companies were also operating at this time but were of little concern to the Board as the nature of bus technology at the time and the state of the roads meant that trams still offered a superior ride quality. Several private bus operators began offering services to areas not well served by trams from the early 1920s following advancements in the design and reliability of buses.  The Board, perceiving this development to be a threat to its business and the significant investment ratepayers had made in it, reluctantly investigated the idea of running its own bus services where it was not economical to run trams.  There were six of these companies hoping to pick up business primarily in newer areas of the city that were not covered by or near the tram network. The problem escalated to be a matter of serious concern for the Board by 1925 when the operators decided to challenge the Board on its key tram routes.  Not having the same obligations as the Board enabled the bus companies to run their services ahead of the trams at peak times to capture some of the Board's most lucrative business.  Several measures were implemented by the Board to combat this menace including hiring buses to run competing services against the \"pirate\" operators, adding more express trams, tuning the trams and power supply to improve acceleration characteristics, purchasing additional buses, and stationing roving trams at key points to immediately commence service on the appearance of a rival company's bus.  This period, known colloquially as the \"bus wars\", resulted in many minor traffic accidents, altercations between staff of rival companies and the Board, and buses and trams appearing to race each other on many occasions to be the first to collect passengers.  For a select few it also resulted in a much-improved service both in terms of timeliness and frequency as the companies competed with the Board for their custom.  The public generally viewed the private operators favourably because they provided either a better service or a service that the Board did not and one for which they had grown tired of waiting for the Board to organise. Realising that the \"bus wars\" would not end well the Board colluded with its Auckland counterpart in early 1925 to support the introduction of legislation to control the problem.  The Motor Omnibus Traffic Act became law on 1 November 1926 and empowered municipal transport authorities such as the Board to compulsorily acquire their competitors.  The only option available to a private company wanting to continue its own service was to charge 2d more per section than the equivalent tram fare.  Most of the private operators were unable to compete on these terms and quickly sold out to the Board with the exception of Inter City Motor Service Limited, which was able to continue operations with the higher fares until 1935. From the outset the Board did have one consistent competitor in the form of the government-controlled Railways Department.  Initially the Department and the Board competed with each other for patrons between Papanui and Christchurch railway stations and from the city to the Riccarton Racecourse.  The Board held the advantage on the former route, as its line between these two points was more direct (shorter) and ran via the city centre, making it popular with patrons including those who entered the city by train.  On the latter route the Board enjoyed superiority patronage-wise, despite the protestations of the Railways Department that its service could get patrons to and from the racecourse more quickly.  It was believed that there was a \"psychological advantage\" in the Board's favour in that its line connected directly with Cathedral Square whereas passengers of the Railways Department heading the same way faced a walk up Colombo Street from Moorhouse Avenue.  In both cases the routes terminated at sidings and passenger landings inside the racecourse grounds.  The Department's service to the racecourse was terminated in 1954 by which time the Board had already converted its competing tram route to diesel bus operation.  Diesel buses replaced the tram routes to Christchurch and Papanui railway stations in 1936 and 1954 respectively, though the Department did not finally cease suburban passenger operations to these stations until 1976. While the Lyttelton road tunnel was still under construction, the Board sought and was granted a license to operate a passenger service to Lyttelton via Ferry Road, which commenced as soon as the tunnel opened in 1964.  This route competed for passengers with the Railways Department's Christchurch–Lyttelton suburban passenger trains.  Though the Department had held out hope that both the Board's and the Department's services could coexist following the opening of the road tunnel, and even opened a new railway station at Lyttelton in 1963, the losses mounted due to competition from buses and private motorcars forcing the Department to terminate the passenger rail service in 1972.  The Board then had its license for the Lyttelton route altered so it could run its buses roughly paralleling the railway line to collect passengers who had been using the rail service. In the decades preceding World War 2, the bicycle amounted to one of the Board's biggest competitors thanks largely to Christchurch's flat terrain, numbering as many as 50,000.  They were popular for commuting and for many were the only form of transport a household had.  The Board maintained a hostile attitude to them, and they were not popular with the tramway men for the major fluctuations in patronage they could cause depending on the weather. Private motorcars were starting to become more common in the 1920s, but were initially more of a danger to tram passengers than to the business of the Board.  As many tramlines ran down the middle of a road, it was common to hail a tram by walking out into the road, which became more dangerous with the faster moving motorcars.  Also, traffic behind a tram that had stopped to allow for the embarkation or disembarkation of passengers was only able to pass between the tram and curb, right into the path of passengers moving between the tram and footpath. A significant portion of the Board's business was derived from the evening fares of people going out to social functions or events at places such as cinemas, dance halls, clubs, and the like.  The introduction of television in the early 1960s gave people a new and more convenient way to spend their leisure time, significantly impacting on the popularity of these types of activities.  A fall of 19% in the Board's revenue between 1962 and 1968 was largely down to the sale of fewer off-peak fares. The Christchurch Transport Board utilised a variety of transport modes to provide public transport services.  Starting with trams when it began operations in 1905, it later experimented with petrol buses in the 1920s, trolley buses from the early 1930s, and introduced its first diesel buses in the mid-1930s. All of the Board's tram routes and the majority of its bus routes were designed around a radial model where services either terminated at, or ran through, a central interchange.  There were some special bus services operating on point-to-point or orbital routes but these tended to be either ephemeral in nature or designed for a specific clientele (e.g. shoppers). Some routes included short workings, which were typically extra services run to intermediary termini to cope with high loadings at peak times.  A wide-ranging review of the bus route network instigated in the late 1970s resulted in the termination of most of these short workings, the amalgamation of shorter routes into longer ones, and the re-timing of many services to make arrival times more realistic. Assets acquired by the Board from the old private tramway companies included lines serving the localities of Papanui, North Beach, New Brighton, Sumner, Cashmere, and Hillmorton.  These lines, along with a new line to Riccarton, were included in the first tranche of construction to be undertaken on the network.  Electric trams commenced services as electrification works were completed to Papanui and Christchurch Railway Station (6 June 1905); Sumner (25 April 1907); Cashmere (16 August 1905); Riccarton (12 March 1906); Lincoln Road (8 February 1906); and New Brighton (6 August 1906).  Some of these lines had opened earlier using steam traction while electrification was still underway.  Trams were stabled at the depot on Falsgrave Street and, using lines along Manchester and Colombo Streets, started their runs from a central hub at Cathedral Square. Over the first decade of operation, new lines or extensions were added to Edgeware Road (24 December 1906); Opawa (21 September 1909); Fendalton (20 November 1909); Cranford Street (1 July 1910); North Beach (1 October 1914); Spreydon (3 August 1911); Cashmere Hills (1 May 1912); Dallington (1 November 1912); from Papanui to Northcote (28 February 1913); St. Martins (7 April 1914); from Edgeware Road to St. Albans Park (19 July 1915); and from Riccarton to Plumpton Park (January 1916).  A temporary loop line to North Hagley Park also operated for five months to serve the Christchurch International Exhibition from November 1906 – March 1907. The early 1920s saw the last of the major construction to be undertaken on the tramlines with extensions to Barrington Street for the Spreydon line (August 1922) and the Lichfield Street connection (July 1922).  It was also at around this time that the Board had proposed several other new lines to the Northeast Central City, South Brighton, and Bryndwr as well as a link between the St. Albans Line and North Beach Line, and a deviation in the Burwood area.  These ideas never proceeded because the prevailing economic conditions made it impossible to justify the cost of them, with buses eventually being used as a cheaper alternative. The first tram services to be withdrawn and replaced by buses were those to Northcote (30 September 1930), Marshland Road–North Beach (5 July 1931), and Richmond (17 December 1934) when the cost of keeping these lines open could not be justified.  Other lines to be closed later that same decade due to economic constraints included Worcester Street, Dallington, and Wainoni in 1936.  When the Board considered the future of the tram network after World War 2, a policy shift led to the progressive closure of the network and its replacement with buses as they became available.  The first to go was the line to St. Martins (20 May 1946), followed by Fendalton and Opawa (6 February 1950), New Brighton (18 October 1952), Sumner (6 December 1952), Riccarton (14 June 1953), St. Albans Park and Spreydon (21 June 1953), Cranford Street and Lincoln Road (26 July 1953), and Papanui and Cashmere (11 September 1954). The first service, to Shirley, commenced on 1 April 1931 and was later opened as far as the Brighton Pier via North Beach on 5 July 1931.  This first route, though it was intended to replace the North Beach trams, actually followed a different path between Fitzgerald Avenue and Marshland Road.  Patronage of the Marine Parade section was poor, leading to the truncation of services at North Beach from 31 May 1933. When it came time to close the remainder of the North Beach tramline, the Board decided to replace the trams with trolley buses.  The second line, to Marshland Road via Richmond, opened on 17 December 1934. All inbound services entered the Cathedral Square terminus from Worcester Street and outbound services ran along High and Cashel Streets.  The lines on Fitzgerald Avenue extended as far south as Moorhouse Avenue, running past and providing access to the Board's workshop (between Ferry Road and Moorhouse Avenue) for servicing and to the depot on Falsgrave Street for stabling.  None of the trolley buses ever displayed route numbers, though route numbers were later assigned to some trolley bus routes and short workings for when diesel buses were used to assist. The inflexibility of the trolley bus system and a desire to standardise on diesel buses led to the decision to withdraw the trolley buses in the 1950s.  The short working to Shirley was discontinued on 16 July 1951 with the remaining services to Marshland Road via Richmond closing on 31 May 1956 and to North Beach on 8 November 1956. Opened: 6 June 1905 (electric tram); Opened, exhibition loop: November 1906 (electric tram); Closed, exhibition loop: 16 April 1907 (electric tram); Opened, Northcote extension: 28 February 1913 (electric tram); Closed, Northcote extension: 30 September 1930 (electric tram); Opened, Northcote extension: 1 October 1930 (petrol bus); Abandoned, Northcote extension: 10 May 1932 (petrol bus); Closed: 11 September 1954 (electric tram); Opened: 11 September 1954 (diesel bus) The Papanui Township was the first area in Christchurch to receive an electric tram service when the system was inaugurated in 1905.  The line quickly proved its worth, becoming one of the most profitable lines in the tram network. Buses first appeared on this route in the mid-1920s during the so-called \"bus wars\" when private bus operators decided to take on one of the Board's most lucrative services.  Once the Board had seen off the competition on this route, it was not until 1930 that buses again made an appearance.  The local council wanted to re-lay Main North Road, along which the tram line ran down one side, and gave the Board several options: to re-lay the tramline down the middle of the road, to double-track the line, or to remove the line.  The Board could not countenance the re-laying of the line because of the cost and opted to abandon the tram service to Northcote.  When permission to terminate the tram service without a plan for a replacement bus service was denied, the Board sought expressions of interest from private bus operators to run a feeder service to the terminus of its tramline in Papanui after rejecting other suggestions including that it should establish a trolley bus service on the route.  None of the responses received were to the Board's liking, so it decided to run the service itself.  On this basis, permission was granted to terminate the Northcote tram service and the Board's replacement bus service commenced the following day.  It was never popular, with the necessity of having to transfer between buses and trams at Papanui being a common complaint, and after several service reductions were made in line with falling demand (including a short-lived Sunday service), the Board abandoned the route altogether in 1932 after coming to an arrangement with a private operator to take over the service. Midland Motorways acquired the operator of the Northcote run in 1936 and maintained its own service there until the Board bought it out in 1982.  When the Board withdrew trams from Papanui in 1954, it also tried to claim the right to operate the Northcote bus services.  Midland was successful in retaining the right to operate its own service there, forcing the Board to run its Northcote service via a different route wholly within its own rating district.  Once the Board acquired Midland Motorways it was able to run its own Northcote service via the original route along Northcote Road. The first diesel buses on the Papanui route were introduced on 10 September 1951 as a temporary measure when work commenced on re-laying the top end of the Papanui tramline, ironically with the intention of keeping the Papanui–Cashmere tram service running.  Though the intention had been to re-lay the tramline as far as Leinster Road, work was stopped at Blighs Road once the Board realised the futility of investing more money in the tramway.  After the decision had been made in January 1953 to use diesel buses on this route they were used in weekends, and on the day of the \"last tram\" buses had already taken over the route before the final tram returned to the depot.  The route was extended several times after the trams were withdrawn, finally terminating at Trafford Street. Opened, to south end of Colombo Street: August 1905 (electric tram); Opened, to start of Hackthorne Road: December 1911 (electric tram); Opened, to Dyers Pass Road: February 1912 (electric tram); Opened, to Hills terminus: 1 May 1912 (electric tram); Opened, Barrington Street–Hills terminus: 10 April 1953 (diesel bus); Closed, Barrington Street–Hills terminus: 13 April 1953 (electric tram); Closed: 11 September 1954 (electric tram); Opened: 11 September 1954 (diesel bus) Services to the Cashmere Hills were progressively converted to electric tram operation after the Board assumed control of the route from the Christchurch Tramway Company.  Though trams were used to provide services on this route until the 1950s, there were several occasions on which buses were used. Soon after the Board took delivery of a Ransomes trolley bus in 1931, they conducted trials with it on the Hills portion of the Cashmere route.  The objective was to determine how trolley buses would perform on such terrain, though not intending to actually convert the route to trolley bus operation.  The following year, buses temporarily replaced trams on this route during the tramway strike because of concerns about the replacement motormen the Board had taken on to operate the trams were not sufficiently skilled to safely operate a tram on the Hills section, and those supporting the strike had taken to sabotage to disrupt tram operation on the Hills section.  Tram services were again temporarily replaced with buses during a power supply crisis from June to August 1947. Following the conversion of the Spreydon route to bus operation on 22 June 1953, the Cashmere route temporarily became a through route with the Spreydon service in which some of the buses from Spreydon continued on along Barrington Street before proceeding to the Hills terminus.  Once trams were also removed from the Cashmere route in 1954, these bus services reverted to the old tram routes though a new service was established that terminated at Macmillan Avenue via Dyers Pass Road. With the opening of Princess Margaret Hospital on 31 August 1959 the route was altered to make the hospital a stop during visiting hours.  The service's increasing popularity with visitors to the hospital ensured that by August 1965 all buses ran past the hospital. Opened, to Heathcote Bridge: November 1905 (electric tram); Opened, to Head Street terminus: 29 April 1907 (electric tram); Closed: 6 December 1952 (electric tram); Opened: 6 December 1952 (diesel bus) Upon assuming control of the Sumner line in 1905, the Board continued to operate the steam tram service it inherited from the Christchurch Tramway Company while the initial contract for electrification was still underway.  From November of that year it ran electric tram services until conversion to bus operation in 1952. Buses did make an appearance on this route while the trams were still running during the so-called \"bus wars\" when from 1925 both Inter City Motors and Suburban Motors ran their own private bus services along the route in an attempt to capture some of the Board's patronage.  The Board responded by using its own buses to head off the competition and eventually acquired the operation of Suburban Motors in 1926. The Sumner route was one of a few occasions on which a public ceremony was held to mark the conversion of one of the Board's tramway lines to bus operation.  The first bus travelled from Cathedral Square to Sumner on the afternoon of 6 December 1952 where a crowd had gathered to witness several local dignitaries officiate the commencement of the new bus service.  At the conclusion of the official function, the last tram made its way back to town. Many improvements were made following the introduction of buses to the Sumner route.  New destinations were served, catering for the travel requirements of local residents, students for several schools near the route, and visitors to the Jubilee Home in Woolston. The day after trams ceased running on the Riccarton line, 15 June 1953, the Sumner bus service was extended to include the Riccarton route.  This included the introduction of short workings to Smith Street (3S), Woolston (3W), and Redcliffs (3R), and peak-time express services stopping after Smith Street or Bamford Street. Woolston bus services were rerouted in 1982 to terminate at Rutherford Street, replacing the 13R portion of the Opawa service that had also terminated there, over the earnest but unheeded objections of users of the 13R service.  The separate Woolston service was discontinued in the late 1980s when the Mt. Pleasant service was rerouted along the end portion of that route. Opened: 1 July 1910 (electric tram); Closed: 26 July 1953 (electric tram); Opened: 27 July 1953 (diesel bus) The Cranford Street route was one of the new tramlines constructed and opened by the Board after it had electrified the lines it inherited from the private tramway companies.  It remained a tram-only service until conversion to bus operation in 1953. Initially, buses ran the route of the old tramline, terminating at Westminster Street.  Some services ran an extended route through to Weston Road, replacing the previous Weston Road via Springfield Road service.  A further extension of the route was implemented on 26 May 1958, with a new terminus at McFaddens Road.  The unsuitable nature of the land beyond McFaddens Road for residential development meant that the terminus remained unchanged for the next two decades. The acquisition of the remaining bus services of Midland Motorways by the Board in November 1982 gave it access to several towns and settlements north of Christchurch, which it apportioned to both the Papanui Road and Cranford Street routes.  Cranford Street became such a busy thoroughfare for these services that it ceased to be a destination in its own right after 1982. Opened, via Worcester Street: 4 August 1906 (electric tram); Opened, via Cashel Street: 1 November 1910 (electric tram); Closed: 18 October 1952 (electric tram); Opened: 18 October 1952 (diesel bus) The Board acquired the privately operated tramline from the city to New Brighton in 1906 at which time it proceeded to electrify the line and commence its own service. The first regular bus service to Brighton was provided by Inter City Motors during the \"bus wars\" of the mid-1920s.  This was the first real competition to trams on the Brighton route and soon became a problem that demanded a response.  In addition to improvements to the tram service, the Board decided to run its own bus service to counter that of Inter City Motors. Services between Brighton and North Beach proved to be problematic for the Board and following the withdrawal of trams from this section in 1931 a replacement trolley bus service was introduced.  It failed to attract a sufficient number of passengers and was withdrawn two years later.  The Board had already tried running a trial bus service along this route, in September 1927, by extending the Pleasant Point service but the trial was unsuccessful.  The next attempt in 1938, which had been designed to cater for shoppers and evening cinema patrons, also failed and like the 1927 trial was cancelled after less than one month in operation.  However, the evening service continued on to 1946 at which time it was replaced by a private bus service provided by the cinema.  The cinema service, including routes and times, was advertised with upcoming movies and lasted to 1964. When a speedway opened on Rowan Avenue, Aranui in January 1949 its Saturday evening race meetings over the summer months were a popular fixture for many years.  The number of trams required to convey patrons to the racecourse caused operational difficulties to such an extent that it proved to be impossible to maintain the regular timetable and consequently buses were typically used to replace trams on the Brighton line when race meetings were held.  Race meetings also meant a busy time for the tram crews and because they didn't have time for their usual meal break, a special bus was provided by the Board to bring these crews into the city for a meal before returning them to the racecourse to bring the patrons home. The end for trams on the Brighton line came on 18 October 1952 when the first bus of the replacement service departed Cathedral Square at 15:00.  At the Brighton Post Office a ribbon had been strung across Seaview Road where the first bus and the last tram waited on either side.  Once the formalities were concluded and the ribbon cut, the tram returned to the city and the bus completed its run to the pier, to be followed by another 13 of the fleet.  The tram timetable remained in effect until the following Monday at which time a new timetable was introduced.  The new service included short workings to Aldwins Road (5A) and Breezes Road (5B), and incorporated the old Pleasant Point service that later became known as South Brighton (5S). One problem that had been the bane of the old tram service was the need to run through (semi-) rural areas where there were few, if any, fare-paying passengers.  The residential development of Aranui and industrial development near Brighton resulted in increased patronage for the buses.  To cater to demand, the Breezes Road bus route was altered to terminate in Rowan Avenue. Brighton services were connected with other routes to provide cross-town links.  From 25 May 1953 Brighton buses ran a combined route with the Mays Road service, which, while not as successful as the Board had hoped, remained in effect for nearly two decades.  Next, Brighton services were connected to the Fendalton route from 1972, which proved to be more successful, and they remained this way until deregulation. Trams on the Worcester Street and Dallington lines were some of the first to be withdrawn by the Board, being the first tram routes to be replaced by diesel buses on 2 November 1936.  It was also at this time that the Wainoni route operated by Inter City, a company the Board had acquired the previous year, was incorporated into the Board's own Worcester Street and Dallington routes. It was with the introduction of these routes that the Board also changed its policy on route numbering.  Previously, an unadorned route number had indicated that a conveyance would run the full length of the route, with the addition of letters to indicate short workings.  Now, the reverse applied, with the number six used for the shortest part of the service and 6D and 6W used for extended routes, conflicting with the previous use of number 6 for the Dallington tram and 6W for the Worcester Street tram.  This, combined with the ambiguity of \"6D\" as a route number (to many, 6D was thought to be the fare), caused much confusion amongst passengers. Unlike most other services run by the Board, the number 6 routes ran through Cathedral Square to either the railway station or Lichfield Street.  This arrangement lasted until October 1942 when wartime restrictions resulted in most trips to the railway station terminating at Lichfield Street instead. Opened: March 1906 (electric tram); Closed: 1 November 1936 (electric tram); Opened: 2 November 1936 (diesel bus) Much of the Worcester Street route was shared with other services but despite this, it was sufficiently well patronised to justify a half-hourly weekday service and less frequent Saturday services.  Originally terminated at the corner of Linwood Avenue and Buckleys Road, it was extended to serve post-war development in Bromley by establishing a new terminus along Linwood Avenue at Hargood Street on 14 June 1948.  Buses running to this new terminus were designated Bromley (6B). The Bromley service's Railway Station link became obsolete with the decline in passenger rail traffic and the fact that many other bus routes ran past the Railway Station.  With a dedicated Railway Station service no longer required, the Bromley and Beckenham routes were linked in the 1980s. Opened: 1 November 1912 (electric tram); Closed: 1 November 1936 (electric tram); Opened: 2 November 1936 (diesel bus) Trams first served Dallington when the Board opened an extension of the Worcester Street line to the suburb in 1912.  It was one of the last tramlines to be built in Christchurch and also one of the first to be closed by the Board. At first, patronage was light enough that express services were not required.  When peak loadings on the Wainoni route were particularly heavy, Dallington buses were used to transfer passengers from their own route to Wainoni services at Woodham Road, an arrangement that lasted until August 1946. The bus route initially followed the old tramline up Gloucester Street to terminate at Avonside Drive.  While the tramline was in operation it was always worked with traditional double cab tramcars, as there was no room at the terminus to install a turning facility for one-man tram operation.  The same issue caused problems for bus drivers for many years.  The original wooden Dallington Bridge at the end of Gloucester Street had a severe weight limit and drivers were advised that they were not permitted to encroach onto the bridge.  This made it difficult to turn buses for the return journey and it was not until improvements were made in Avonside Drive near the bridge that the manoeuvre could be carried out without reversing. Route extensions were not possible until 1954 when the old bridge was replaced at which time the route was extended to McBratneys Road.  Further extensions included Ferner Street (5 August 1957), Locksley Avenue (29 September 1958), and select peak-time services to the intersection of Avonside Drive and Torlesse Street (December 1968).  The Dallington route was also linked to cross-town services including Bryndwr from October 1955 and to Avonside from 1980/81 with an extension to Liggins Street and later to Bampton Street from 1984. Opened: 2 November 1936 (diesel bus) Wainoni first received a bus service in January 1926 during the \"bus wars\" when Inter City established a through service to North Beach.  This service was later taken over by the Board when it acquired the company in October 1935, and became a dedicated Wainoni service when the portion of the old North Beach run between the Bower Hotel and North Beach was abandoned on 10 February 1936.  On 2 November 1936 the route was altered to use Worcester Street (abandoning the Inter City route via Hereford Street) and a new timetable was adopted, combining its services with those of the Worcester Street and Dallington routes. Weight restrictions on the old Bower Bridge limited the number of passengers that buses could carry over the bridge.  Work on a new bridge started in August 1941 with construction of pre-cast concrete piles and from 8 December 1941 to 17 August 1942 the bridge was closed to all but pedestrian traffic to allow for the old bridge to be dismantled as the new bridge was erected.  The opening of the new bridge allowed for all previous loading restrictions to be rescinded. At first, loadings in the Wainoni area were light enough that an hourly service frequency sufficed, even at peak times.  The route's link with the Railway Station, however, ensured that buses were typically at capacity by Cathedral Square, whereupon Dallington buses would be used to transfer passengers to Woodham Road to catch a Wainoni service.  There were still no evening services, but not for lack of trying, as an unpopular Friday night service introduced in 1937 was withdrawn after about 3 months, and a service designed for cinema patrons in 1941 had to be withdrawn the following year for wartime fuel rationing.  From mid-1949 until 1952 the evening services on Fridays and Saturdays continued on to South Brighton.  It was not until September 1951 that Sunday services were introduced. Route extensions included moving the terminus to the intersection of New Brighton Road and Palmers Road on 1 January 1962 and later into the suburb of Parklands, and on 4 September 1967 to Niven Street via Breezes Road and Avondale Road.  This route was again extended once the old Avondale Road Bridge was replaced, terminating at Burwood Hospital. Cross-town links included the Wharenui route from October 1955 to 4 September 1967 at which point the service terminated at Cathedral Square, and with the Mays Road route from 1972. Opened: 8 February 1906 (electric tram); Closed: 26 July 1953 (electric tram); Opened: 27 July 1953 (diesel bus) The Lincoln Road route was inherited from the Christchurch Tramway Company and services initially provided by the Board using steam trams until electric trams were introduced on the line in 1906.  This remained a tram service until conversion to bus operation in 1953. With the introduction of bus services, the old tram route was retained for most services but some ran an extended route to a new terminus at the intersection of Hoon Hay Road and Lewis Street to serve a rapidly growing residential area.  The route was split on 30 May 1960 when it was extended to a new terminus at Sparks Road/Victors Road and a terminus added at Halswell Road/Rowley Avenue.  As with the preceding tram service, this route was always combined with the Cranford Street route. Opened, to Clyde Road: November 1905 (electric tram); Opened, to Riccarton Racecourse: 12 March 1906 (electric tram); Closed: 14 June 1953 (electric tram); Opened: 15 June 1953 (diesel bus) Riccarton was one of the first new tramlines to open, initially using steam traction from 2 November 1905, and later electric trams as electrification proceeded to the terminus.  The Board's Riccarton bus service did not come until nearly five decades later when they replaced the trams on 15 June 1953.  This new service formed part of a cross-town service with the Sumner route.  Space constraints in Cathedral Square meant that the central city terminus of the route was located on Worcester Street outside the Avon cinema. The bus route only followed the route of the tramline as far as the western end of Riccarton Road where it split into two legs.  Some buses carried on to the Racecourse via Yaldhurst and Racecourse Roads whilst the others ran on to Buchanans Road.  Residential development to the west of the city soon ensued and to meet the demand for public transport services the Yaldhurst leg was extended to terminate at Cutts Road.  Further extensions included to Fovant Street on 4 June 1962 and Bentley Street on 4 June 1966.  A new leg was created along Avonhead Road to Maidstone Road on 4 December 1959 but later cancelled in favour of extending the Creyke Road route.  The Buchanans Road leg was also extended to match residential development, to Wycola Avenue from 30 September 1957, and later down Gilberthorpes Road to connect with Templeton buses in Islington. There were two short workings on the Riccarton route, to Clyde Road (8C) and Church Corner (8B).  These were linked to the short workings on the Sumner route and were popular throughout the day, and complemented the express services at peak times. 8H Ran to Hornby, terminating at Denton Park where there was an off-road turning loop. Opened, to Holmwood Road: November 1909 (electric tram); Opened, to Clyde Road: 18 December 1912 (electric tram); Closed: 5 February 1950 (electric tram); Opened: 6 February 1950 (diesel bus) Trams provided public transport services to Fendalton for four decades from 1909 until being withdrawn in 1950.  Following the successful introduction of diesel buses on the Worcester Street, Dallington, and Railway Station routes in 1936, the Board made plans in 1938 to introduce them to other lightly patronised tramlines, specifically St. Martins and Fendalton/Opawa.  Though buses did replace trams for a short time on the St. Martins route, the intervention of World War 2 scuttled the Board's plans to withdraw the Fendalton and Opawa tram service due to restrictions on supplies and the unavailability of new vehicles. When the Board was next able to consider the issue of bus replacement of tram services following World War 2, the Fendalton/Opawa tram service was one of the first to go.  Though the trams were not withdrawn until 5 February 1950, buses had been used on the Fendalton route for a number of years.  Sunday services had already been switched to bus operation from 10 April 1949, and in December a route to Clyde Road via Ilam Road had started to serve State housing developments.  This latter service became part of the Fendalton route when the new bus timetable was introduced on 6 February 1950. Extensions to the Fendalton service included to Truman Road (mid-1950s), to Grahams Road (30 June 1958), to Kendal Avenue (2 December 1963), and finally Burnside Crescent (7 December 1964).  This portion of the route mostly served State housing and consequently, buses had to cater for the large number of young children living in the area. Midland Motorways held a license to operate bus services on Memorial Avenue out to the Airport, a situation that prevented the Board from extending the Fendalton service along this route.  When it obtained its own license for Memorial Avenue, services were extended to Ilam Road from 6 January 1958.  Further extensions included Grahams Road (30 June 1958), Avonhead Road (2 December 1963), and Withells Road (1977).  The Fendalton route's cross-town link was changed to Brighton in 1972, ending its association with Opawa. Opened: 27 December 1906 (steam tram); Opened: 15 August 1910 (electric tram); Closed: 16 August 1934 (electric tram); Opened: 17 December 1934 (trolley bus); Closed: 30 May 1956 (trolley bus); Opened: 31 May 1956 (diesel bus) Christchurch's second trolley bus line opened on 17 December 1934.  It followed the North Beach tramline through Richmond as far as Marshland Road where it met but was not physically joined to the existing trolley bus line to North Beach.  There was a short working to the intersection of North Avon Road and Tweed Street. As with trolley buses on the North Beach service, those used on the Richmond run were not equipped to display route numbers.  This caused confusion when, in the late 1930s, diesel buses were used on this route to augment capacity and did show route numbers.  In this case, buses used the route number 10, with 10T used for the short working to Tweed Street. For a short time following the commencement of a new Shirley service on 15 January 1951, North Beach trolley buses ran via the Richmond route, replacing the Marshland Road via Richmond service.  The two separate trolley bus routes were reinstated on 16 July 1951. As the Board's post-war programme of fleet upgrades and modernisation neared its zenith in the 1950s, it was decided to withdraw the trolley bus system and focus on diesel buses.  Consequently, the last trolley bus service to Marshland Road ran on 30 May 1956. The replacement diesel bus service was extended several times, first to Joy Street from 25 August 1958, then to Briggs Road, and throughout the 1980s it followed the development of the north-eastern suburbs through Burwood, Parklands, Waimairi Beach, and North Beach to end up at New Brighton.  A cross-town link with the Opawa route was established in December 1972 that lasted to 1980/81 when it was switched to a combined Marshland Road–Avonhead route. Opened: 13 January 1926 (petrol bus) A motor omnibus service run by C. R. Brown to Avonside commenced operation in the early 1920s.  Its route followed Armagh Street, Brittan Street, and Patten Street to terminate on Retreat Road.  When Brown tried to renew his license in June 1926, the Board used its authority under the Motor Omnibus Traffic Act 1926 to compulsorily acquire his business.  After performing due diligence on the operation, the Board decided not to continue the service, in part because they were hoping to also acquire the business of Inter City Motors and thereby establish a new route serving North Beach, Wainoni, and Avonside. When their attempt to purchase Inter City Motors was rebuffed, they acquiesced to local pressure and decided to run their own Avonside service, which started on 19 December 1927.  It never distinguished itself financially and in 1929 it was decided to put the service out for tender.  The tender was awarded to W. C. Sanders who took over the running of the service on 11 July 1929 using a White bus he had agreed to purchase from the Board.  This also coincided with an extension of the route along Kellar Street (previously Leighton Street) and Morris Street west to terminate at Avonside Drive.  A special route via England Street (previously Rolleston Street) and Woodham Road was used when events were held at Wilding Park. Sanders' contract expired on 11 July 1933 and the Board, being wary of his financial position, decided not to renew it and instead offered it to two of his drivers, Ted Pycroft and Jim Henderson.  They assumed responsibility for the service and continued operations using a Republic bus provided by the Board.  Over the next two decades there were a succession of private operators and vehicles including the introduction of a Thornycroft in 1937 and a Ford V8 in 1942. With the post-war easing of restrictions on the supply of fuel, additional services were provided on the Avonside route.  The route was also extended along Robson Avenue, Galbraith Avenue, and Morris Street from 18 October 1948 to serve new residential development.  New patronage required additional capacity and so to cater for the demand an A.E.C. \"Q\" bus was used on Saturdays.  As the \"Q\"s were only equipped to display route numbers and not destinations, the route number 11 was assigned to Avonside. The Board resumed control of the Avonside service on 18 June 1951 at which time the drivers that had been operating the service became employees of the Board.  When the Dallington route assimilated the Avonside service in 1980/81, Avonside ceased to be a destination in its own right though the route number 11 remained in use for buses travelling through the area. Opened: 7 April 1914 (electric tram); Closed: 5 January 1941 (electric tram); Opened: 6 January 1941 (diesel bus); Re-opened: 6 July 1942 (electric tram); Closed: 19 May 1946 (electric tram); Re-opened: 20 May 1946 (diesel and petrol bus) Despite being the last tramline constructed and opened by the Board, St. Martins was never particularly busy and by the 1930s had deteriorated to the point where it was being considered for closure.  Following the successful introduction of diesel buses in 1936, the Board made plans in 1938 to use them to replace tram services on other routes, including St. Martins.  When a sufficient number of vehicles were available, the tram service was withdrawn on 5 January 1941 and the bus service commenced the next day, following the tram route and using the same stops as the trams.  New services were soon added to the timetable including express services at peak times. The imposition of restrictions on the Board's operations due to World War 2 and the difficulty in obtaining parts and supplies for its buses led to the withdrawal of the St. Martins bus service and the reintroduction of trams on 6 July 1942, despite the poor condition of the tram track.  This situation lasted until the trams were withdrawn for the second time on 19 May 1946 and the reintroduction of the bus service the next day using both petrol and diesel vehicles. Heading out to St. Martins, the new bus route used Manchester Street instead of Colombo Street as previously, and at Moorhouse Avenue buses either turned left to go down Waltham Road past the Railway Station, or right to head down Colombo Street and Brougham Street through the Sydenham shopping precinct.  Bus drivers were tasked with counting passengers using the Sydenham route and, following analysis of the data obtained, it was decided to discontinue the Sydenham deviation, with all buses using Waltham Road from 12 December 1949. From 19 May 1947 to 20 February 1949 the St. Martins route was linked with the Somerfield route on weekdays only.  A new route was created in December 1984 combining the St. Martins and Huntsbury routes. Opened: 21 September 1909 (electric tram); Closed: 5 February 1950 (electric tram); Opened: 6 February 1950 (diesel bus) Services to Opawa commenced on 14 March 1907 using steam-hauled trams.  Electric trams followed in 1909 and, despite an attempt to replace them with buses in the 1930s, remained in service for four decades.  Bus services began with Sunday services from 10 April 1949 and concluded with complete bus replacement on 6 February 1950.  Two extensions of the Opawa route were established only months after the new bus service started: to Hillsborough, terminating at Grange Street from Opawa Road (13H); and to the Radley area (Woolston), terminating on Rutherford Street from Garlands Road (13R).  Both of these changes, effective 28 August 1950, were intended to serve areas of new residential development as well as the industrial complex between Rutherford Street and Bamford Street.  A further extension of the Hillsborough service was implemented on 30 June 1969, terminating at Brabourne Road on Opawa Road. Coinciding with the new cross-town link established between Fendalton and Brighton in December 1972, the Woolston portion of the Opawa service was joined to the Marshland Road route.  Also, as the Lyttelton suburban rail services had recently ceased operation, the Lyttelton bus service was rerouted to follow the railway line and collect passengers from around the railway stations, and in so doing largely followed the Hillsborough section of the Opawa service. The Marshland Road and Opawa routes were de-linked in 1980–81, and the Woolston section of the Opawa service was terminated when the area became served by a change to the Woolston service.  The similarity of the Opawa and Lyttelton routes had resulted in their amalgamation by December 1984 and the route number 13 was no longer used. Opened, to Strickland Street: 3 August 1911 (electric tram); Opened, to Coronation Street: September 1915 (electric tram); Opened, to Barrington Street: August 1922 (electric tram); Closed: 21 June 1953 (electric tram); Opened: 22 June 1953 (diesel bus) Spreydon was one of the later tram routes to open, and later had the distinction of being the last tram route on which a section of new tramline was constructed (the Barrington Street extension in 1922).  Trams were withdrawn from the Spreydon route on 21 June 1953 and replaced with buses the following day.  The bus route ran to a new terminus beyond the end of the tramline, at the intersection of Barrington and Somerfield Streets.  The Somerfield bus route, which had terminated here, was redirected. A new leg to the Spreydon route was created, down Selwyn Street to the intersection with Somerfield Street.  This was joined on 15 September 1958 with the first route by using Somerfield Street to create a loop, with buses travelling in both directions.  Simeon Street and Coronation Street were removed from the route on 17 December 1984 and a new terminus established at the intersection of Barrington and Athelstan Streets. Opened, via Manchester Street: 6 June 1905 (electric tram); Opened, via Colombo Street: 4 July 1905 (electric tram); Closed, via Manchester Street: 6 April 1932 (electric tram); Closed, via Colombo Street: 1 November 1936 (electric tram); Opened: 2 November 1936 (diesel bus) Trams operated a dedicated service between the central city and Railway Station until they were withdrawn from this route, along with the number 6 trams, on 1 November 1936.  The replacement was the city's first diesel bus service, which was introduced as an economy measure to reduce expenditure on lightly trafficked routes.  The tram route via Manchester Street had closed 2½ years earlier and since then trams to the Railway Station had used Colombo Street. The Board's bus service, introduced on 2 November 1936, continued to be complemented by trams, which provided an irregular service to the Railway Station as they travelled past on their way to or from the Falsgrave Street depot. Being a dedicated link to the Railway Station, the number 6 buses had been timed to meet arriving trains from various locations around Christchurch.  As more routes were converted to bus operation, some were routed past the Railway Station and were able to alleviate some of this load.  The Worcester Street route remained linked to the Railway Station after the Dallington and Wainoni routes were assigned to new cross-town services with the Bryndwr and Wharenui routes respectively in October 1955. The number of people needing to access the Railway Station declined along with the use of passenger rail around Christchurch.  The last suburban rail service ran on 30 April 1976 and the final local service followed a few months later on 14 September.  By the early 1980s the dedicated Railway Station bus service had become obsolete and was withdrawn, leaving only those buses that frequented the railway station in passing. Opened, to Edgeware Road: 24 December 1906 (electric tram); Opened, to terminus: 19 July 1915 (electric tram); Closed: 21 June 1953 (electric tram); Opened: 22 June 1953 (diesel bus) The Board's bus service to St. Albans Park started on 22 June 1953, a day after the last St. Albans tram.  It ran to a new route that followed the tramline, but continued on to a new terminus at Ranger Street via Flockton Street, Kensington Avenue, and Philpotts Road.  An existing service that had commenced in 1949 along Springfield Road and Weston Road was truncated at Cranford Street.  In the central city, buses initially followed the one-way streets but from the mid-1960s they were rerouted to Manchester Street and Peterborough Street. The withdrawal of the trams afforded the City Council an opportunity to reconfigure Bealey Avenue.  Traffic had previously travelled in both directions either side of the median strip with trams running on double track in the northern carriageway.  When buses were introduced on the route the Avenue was changed to have eastbound traffic on the northern side and westbound traffic on the southern side. A new service commenced on 5 August 1957 that continued down Westminster Street to terminate at Hills Road. The Ranger Street terminus of the original route was changed in January 1959 by turning it into a loop around some local streets to avoid problems that drivers had had with turning their vehicles for the return journey. Opened: 12 January 1925 (petrol bus), 2 November 1936 (diesel bus) Bryndwr was one of the areas the Board had hoped to construct a new tramline for in the 1920s, but was prevented from doing so by economic conditions.  When the Board failed to provide its own service to the suburb, the private operator W. C. Sanders filled the gap with a point-to-point service linking Bryndwr and Harewood from 1924.  The Board was desirous of running its own Bryndwr service for which it had ordered Tilling-Stevens buses.  In the meantime, it commenced operations using a recently acquired White bus on 12 January 1925.  When this bus had to be deployed elsewhere during the \"bus wars,\" the service had to be continued with vehicles from the disparate collection still in the Board's possession until they were able to take delivery of the Tilling-Stevens buses in late 1925. The original route started from Cathedral Square and ran along Colombo Street, Armagh Street, Park Terrace, through the Carlton Mill area, and up Rossall Street to the intersection of Glandovey Road and Strowan Road.  From here, buses alternated between travelling in a clockwise or anti-clockwise direction around the circuit formed by Glandovey Road, Idris Road, Wairakei Road (previously Wairarapa Road), and Strowan Road.  Inbound services entered Cathedral Square from Worcester Street rather than Colombo Street.  Timetabled services ran Monday to Saturday, no Sunday service. Pre-war extensions to the Bryndwr route included a trial of services to Windermere Road from Idris Road in November 1927 and in July 1931 having select services continuing on down Wairarapa Road to either Ilam Road or Blighs Road. Bryndwr was one of several routes to switch to diesel bus operation on 2 November 1936 when A.E.C. \"Q\" buses commenced operation.  As the \"Q\"s required a route number to indicate their destination, the number 17 was used for buses going as far as Idris Road and Wairarapa Road, and those travelling on used the route number 17E.  Route number 17 had previously been assigned to the proposed Bryndwr tram route when new destination roll blinds were produced in 1921.  Later fleet upgrades included the introduction of A.E.C. Mark IV Regal to the Bryndwr route to replace the \"Q\"s, and the replacement of the Mark IVs with the Long Reliance in 1958.  It was not until the early 1980s that Long Reliances were retired after which a variety of Bristols and MANs were used. A cross-town service between Bryndwr and Dallington was established in October 1955. Extensions to the Wairakei Road leg continued apace with the terminus moving to Greers Road on 1 May 1950 and further expanded in March 1951, Grahams Road (September 1958), Kendall Avenue (February 1960), Charlcott Street (May 1962), Breens Road (February 1967), and finally to Sheffield Crescent where industrial development had taken place in the late 1970s/early 1980s.  Services to Aorangi Road were not able to proceed on to Greers Road until February 1960 following the completion of Condell Avenue.  Further extensions to this leg followed on 14 September 1964 to Leacroft Street by way of Kilburn Street and Isleworth Street and in 1982 to the intersection of Sawyers Arms Road and Crofton Road by way of Breens Road and Gardiners Road. Opened: 28 January 1925 (petrol bus), 2 November 1936 (diesel bus) In January 1925 Inter City Motor Service started a bus service to Springfield Road, having recently discontinued its service to Papanui.  The Board viewed this move as a threat to its business and responded by starting its own bus service to the area on 28 January 1925 using hired vehicles.  The single bus on offer from Inter City Motors was unable to compete against the two deployed by the Board and so the Inter City Motors service was quickly withdrawn.  It was not until later in the year that the Board was able to take delivery of additional buses for its own fleet and relieve itself of the considerable expense of hiring buses from private operators.  Two of these contractors later established their own service to Springfield Road in competition with the Board along the route that had been used by Inter City Motors.  The Board acquired their business the following year. Services actually ran as far as Mays Road, and Springfield Road was one of the main thoroughfares.  At first, buses started from Cathedral Square and traversed Victoria Street, Durham Street, Springfield Road, Somme Street, Hawkesbury Avenue, Browns Road and Bretts Road on their way to Mays Road.  The Mays Road destination was removed in May 1926, with services looping around the block of Bretts Road, Knowles Street, and Rutland Street.  Mays Road was reinstated in March 1927 as an extended circuit whilst retaining Knowles Street for some services, but from September the Mays Road circuit was used by all services, alternating between Bretts Road and Rutland Street for outbound journeys. A variety of petrol buses from the Board's fleet were used on this route.  They were supplanted on 2 November 1936 by A.E.C. Regal half-cab diesel buses.  It was also at this time that route numbers were allocated to the Springfield Road services, with those using Rutland Street known as 18R and via Bretts Road as 18B. Apart from a short-lived service to Weston Road, this route remained unaltered after the 1920s.  It was part of two cross-town links, from 25 May 1953 with the Brighton route and from late 1972 with the Wainoni route.  By the 1950s the route had become known as the Mays Road service. Opened: 14 February 1949 (diesel bus); Closed: 27 July 1953 One of many areas to experience residential development in post-World War 2 Christchurch was a strip of land running southwest – northeast between Innes Road and McFaddens Road.  This area was not well served by public transport, with the nearest tramlines being those to Cranford Street and St. Albans Park.  There was no prospect of these lines being extended, as an earlier proposal from the 1920s to link the St. Albans Park line to the North Beach line had been scrapped.  For either of these routes to connect to this area the trams would first have to be withdrawn. As a temporary solution, it was decided to extend the Springfield Road service into this area.  Starting from 14 February 1949, every other Springfield Road bus turned off Rutland Street into Weston Road, ending its run at Cranford Street.  The terminus of this route followed the growth of residential development eastwards, moving to Jameson Avenue by 20 June 1949 and to Philpotts Road via Nancy Avenue and Ranger Street on 8 December 1952.  Patronage was regularly assessed to ensure the service's continued viability. Despite the indirect nature of the route to the central city, it maintained its popularity with local residents.  When the St. Albans Park tramline closed, the replacement bus service followed an extended route to the intersection of Philpotts Road and Ranger Street from 22 June 1953.  As this was also the terminus of the 18W service, its route was truncated at Cranford Street.  Just one month later following the replacement of the Cranford Street tramline with an extended bus route terminating at Weston Road on 27 July 1953, the 18W service was cancelled.  The number 18 buses resumed their original routes to Mays Road. Opened, to Burwood: 15 August 1910 (electric tram); Opened, Burwood–North Beach and Pier: 1 October 1914 (electric tram); Opened, to Shirley: 15 January 1925 (petrol bus); Closed, Marshland Road–terminus: 22 August 1927 (electric tram); Re-opened, Marshland Road–terminus: 1 October 1928 (electric tram); Opened, to Shirley: 1 April 1931 (trolley bus); Closed, Marshland Road–terminus: 4 July 1931 (electric tram); Opened, Shirley–North Beach and Brighton Pier: 5 July 1931 (trolley bus); Closed, North Beach–Brighton Pier: 13 May 1933 (trolley bus); Closed: 8 November 1956 (trolley bus); Opened: 8 November 1956 (diesel bus) Public demand for improved transport services to the suburb of Shirley prompted the Board to consider its options in the 1920s.  It had proposed a new tramline connecting the St. Albans Park line to the North Beach line along Warrington Street and Shirley Road but, like other proposed tramlines at the time, the idea had to be dropped due to economic conditions.  It therefore decided to start its own bus service and, using a vehicle hired from private operator W. C. Sanders, commenced services on 15 January 1925.  The route initially started from a terminus near the intersection of Colombo Street and Gloucester Street, from where it followed Manchester Street, Oxford Terrace, Kilmore Street, Fitzgerald Avenue, Hills Road, and Warden Street, where it terminated in a circuit around Chancellor Street, Shirley Road, and Slater Street.  Inbound services returned along Armagh Street.  Within the first two months the City terminus had been relocated to The Square.  In using Manchester Street, Oxford Terrace, and Kilmore Street, the service followed the route of the proposed Northeast City tramline.  Services ran on Mondays to Saturdays, no Sunday service. Christchurch's first trolley bus service commenced operation to Shirley on 1 April 1931, later extended to North Beach and the Brighton Pier on 5 July 1931.  The route, starting from Cathedral Square, ran along High Street, Cashel Street, Fitzgerald Avenue, Hills Road, and Shirley Road where the trolley bus line met the old North Beach tramline at Marshland Road and followed it to the terminus.  Inbound services entered The Square from Worcester Street.  Short workings to the intersection of Shirley Road and Stapletons Road, Burwood, and eventually at the New Brighton Racecourse were provided for with loops in the overhead lines.  When the second trolley bus service to Richmond opened in 1934, both routes shared the line down Cashel Street to Fitzgerald Avenue. At its greatest extent, the trolley bus line ran down Marine Parade, like the North Beach tramline before it, to connect with the Brighton tramline at the Pier.  This part of the route was never successful so the line was truncated at North Beach on 13 May 1933.  The tramline at the North Beach end of the route along Marine Parade and as far back as Bassett Street was, however, left in place to be used for special events at the Racecourse.  On such occasions the trolley buses were not expected to cope with the crowds along the regular North Beach route so capacity was augmented by running extra trams up the Brighton tramline then back along the eastern end of the North Beach tramline to a point where they were met by trolley buses running shuttles to the Racecourse. As on Bealey Avenue, traffic ran in both directions on either side of the median strip on Fitzgerald Avenue.  Trolley buses used the western carriageway between the Moorhouse Avenue depot and Kilmore Street, and the eastern carriageway between Heywood Terrace and Bealey Avenue.  It was not until the withdrawal of the trolley buses that Fitzgerald Avenue could be converted to one direction per carriageway. A short-lived extension was started on 25 May 1936 using a petrol bus that was run out to North Beach from the City and back on scheduled services every day.  During the day, it connected with trolley buses at the Ozone Corner and ran up Marine Parade to Effingham Street (previously Berry Street).  The trial failed to attract sufficient interest and the service was cancelled a couple of months later on 31 July. Trolley buses were identified only by their destinations, as they were not equipped to display route numbers.  Only when diesel buses were called upon to augment capacity did route numbers become necessary and so the number 19 was assigned to the North Beach service.  Short workings to Shirley and Burwood were assigned 19S and 19B respectively. A new residential development north of Shirley Road attracted the attention of the Board in 1951.  It was decided to establish a new bus service to the area and, as the Board was not interested in extending the trolley bus network, it was started on 15 January 1951 using petrol and diesel buses.  The route ran to Acheson Avenue via Emmett Street, travelling under trolley bus lines much of the way.  As the point at which this new service turned off Shirley Road into Emmett Street was close to where the Marshland Road via Richmond trolley bus route met the North Beach line, North Beach trolley buses were diverted at the same time to run via Richmond and the Richmond trolley bus service was suspended.  A few months later on 16 July the trolley buses reverted to their previous routes but the short working to Shirley was cancelled. Further development in the Emmett Block area and neighbouring suburbs led to extensions of the Shirley bus service, including to Voss Street via Orcades Street and Quinns Road (28 April 1958), up Olivine Street (December 1959), and to the intersection of Marshland Road and Briggs Road (3 December 1962). The North Beach service remained largely unaltered until the trolley buses were replaced with their diesel counterparts on 8 November 1956 and it was only then that modifications to the route could be considered.  The first was to relocate the Burwood terminus to the intersection of Bassett Street and Parnwell Street (24 August 1957) followed by a deviation around Lake Terrace Road, Burwood Road, and Travis Road before reaching the terminus (5 October 1964), and the inclusion of Burwood Hospital from Burwood Road and Mairehau Road on 13 October 1966. A cross-town service between Burwood and Springs Road (previously Wharenui) was established on 4 September 1967. There were also alterations at the North Beach end of the route, including: around Bower Avenue and Marriotts Road from December 1958; along Marine Parade to Pacific Street from December 1959; and an extended circuit for select services only around Bower Avenue, Beach Road, and Marine Parade to serve the suburb of Waimairi Beach from 3 July 1961.  The growing importance of New Brighton as a weekend shopping destination led to the extension of these services from North Beach to New Brighton on Saturdays from 1 June 1963. Opened: 9 September 1946 (petrol bus) The Somerfield route connected an area of housing development near Lyttelton Street to the central city and commenced service on 9 September 1946.  At first buses ended their run in Rose Street but the terminus was moved on 25 November to the intersection of Barrington and Somerfield Streets.  The central city terminus was located on Gloucester Street near Chancery Lane, a stop that was shared with several other routes started at around this time due to space constraints in Cathedral Square.  It was not until 19 April 1952 that a stop in the Square could be allocated to the Somerfield route. Outbound services ran non-stop to Christchurch West High School to encourage those not travelling out of the central city to use other services on account of the limited capacity of the Ford V8 buses used on this route.  This measure remained in effect until the service frequency had sufficiently increased. A cross-town service was created on 19 May 1947 involving the Somerfield and St. Martins routes.  Initially only in effect on weekdays because of the limited weekend services provided to Somerfield, it was increased to a full-time arrangement from 20 February 1949. The route was extended several times: first, to Hoon Hay Road from 22 June 1953 to avoid a clash with changes to the Spreydon route; next, to the intersection of Cashmere Road and Worsleys Road on 31 August 1962; and lastly, to Princess Margaret Hospital on 17 December 1984. Opened: 9 September 1946 (diesel bus) The Creyke Road bus service served residential areas between the Fendalton and Riccarton tram routes.  Starting from Gloucester Street, buses followed much of the Riccarton tramline, turning off Riccarton Road at Straven Road and eventually terminating at the intersection of Creyke Road and Ilam Road.  The first stop for outbound services and the last stop for inbound services were at Christchurch Boys High School meaning there was no connection with the tram service. At first the timetable provided only for a daytime weekday service that did not cater for commuters.  This changed when the route was linked with the Beckenham service on 12 July 1948, adding both commuter services and a late night Friday service.  The starting point of the service moved from Gloucester Street to Cathedral Square in December 1958. Disappointing returns from the Creyke Road service prompted an instruction to drivers to pick up passengers along the Riccarton tram route, and later, the route was changed to use Deans Avenue and Kilmarnock Street where it was hoped to appeal to more potential passengers. Maidstone Road was constructed in 1963–64 and enabled the extension of the bus route past the University of Canterbury and the Teachers College.  Having exceeded Creyke Road as a destination, the route was renamed Avonhead to more accurately describe the area served.  Further extensions were made as the community grew, eventually terminating on Woodbury Street at Mirfield Place and returning in a loop around Ansonby Street. The Avonhead and Marshland Road routes were combined during 1980–81. Opened: 18 June 1918 (battery-electric bus), Q3/Q4 1920 (petrol bus), 15 June 1953 (diesel bus) The Board's first, and only, experiment with a battery-electric bus constituted their first service to Templeton, which commenced service on 18 June 1918.  It was a feeder service that connected with the No. 8 Riccarton trams at the tram terminus in Sockburn via Islington and ran seven days a week.  The vehicle, a Walker electric, was unreliable and often failed in service, leading to its replacement on the Templeton run in 1920 by a Garford petrol bus.  The ride quality and amenities of the Garford were little better than the Walker it replaced, so in 1923 the body of the Walker was rebuilt on a Leyland chassis, increasing its capacity and providing a much improved ride for the passengers. The Templeton route became a lucrative business for several private operators during the \"bus wars\" of the mid-1920s.  They were able to provide a more appealing service for many passengers over those provided by the Board and the Railways Department with a direct connection between the central city and Templeton.  With the passage of the Motor Omnibus Traffic Act in 1926, these operators were either driven out of business or purchased by the Board, upsetting many travellers who did not appreciate the mode change at Sockburn or the fact that through fares were not available. In October 1926 the service was put out to tender.  The successful bidder took over the service on 15 November 1926, but soon discovered that a significant portion of their patronage was derived from workers at the Islington freezing works which, when out of season, considerably affected the revenue they earned.  The Board therefore agreed to increase the subsidy. Weekdays through services from Templeton to Cathedral Square were trialled for a three-month period from August 1926, after which the timetable reverted to the previous feeder service.  The Board reinstated them from 1 August 1930 on a select few services through to the Bridge of Remembrance as it had remained in control of the timetable. When the contract came up for renewal in October 1928, it was re-let to the same operator.  W. C. Sanders took over the service in October 1930 when the contract next came up for renewal.  This was not the only business Sanders had with the Board and he quickly discovered that he was unable to meet all of his obligations.  The Board cancelled all of the contracts it had with him, save for his operation of the Avonside service.  Under the Board's control once again, the through services were discontinued and the feeder operation was reinstated from May 1931 using two drivers previously in the employ of Sanders.  After a dispute with the union, the drivers became contracted operators of the service in April 1932, being compensated on a mileage basis.  The contact was renewed in June 1933. A variety of vehicles made an appearance on the Templeton service after the Walker and Garford buses.  When the service was let to private operators, they used a Graham-Dodge and later a Republic, both hired from the Board.  A Minerva was assigned to the route when through services were introduced to provide additional capacity.  A Thornycroft, formerly in service to Inter City Motors, replaced the Minerva in 1937.  When the Thornycroft had to be withdrawn in 1939 due to deficiencies identified by the Transport Department, a later model Minerva was assigned to the Templeton run.  A Ford V8, the first in the Board's fleet, replaced it in 1942.  As the Ford was equipped to display route numbers, it was the first to use the number 22, which had been assigned to Templeton. Direct connections to Templeton were re-established on 6 October 1947 when most weekday services terminated in the central city.  Most weekend services, including all those on Sundays, remained feeders to the Riccarton trams.  The through-runs operated as quasi-express services between Sockburn and the City.  This remained the case until the replacement of trams on the Riccarton route with diesel buses on 15 June 1953, at which point all services terminated in the central city. Expansion of the city in the suburbs of Hornby, Islington, and Templeton in the 1950s required several modifications to the route to ensure its greatest utility to the area.  For several years from April 1957 some buses ran via Blenheim Road until the Wharenui buses were extended through this area.  Selected Islington services used a modified route along Waterloo and Carmen Roads from September 1957 including stops at several industrial installations for the benefit of workers in the suburbs of Sockburn and Hornby.  Special services were also provided for patrons of events at the Ruapuna Speedway, and during the 1974 Commonwealth Games to Denton Park. Opened: 27 November 1944 (petrol bus) The genesis of the public bus service to Mount Pleasant was a bus service that the Board was already providing for workers at the Marathon Rubber Footwear factory in Woolston.  From 27 November 1944 this service was extended from the factory to the Mount Pleasant school on Major Hornbrook Road.  Morning and evening return services were provided, catering for factory workers travelling from the central city and mainly shoppers during the day.  Inbound services followed a route along Major Hornbrook Road, St. Andrews Hill Road, Ferry Road, Dyers Road, Linwood Avenue, and Worcester Street. Services were provided using a \"square\" Ford V8 that displayed the route number 3P the whole time it was assigned to the Mount Pleasant route, despite the route number 23 having been \"reserved\" for Mount Pleasant as far back as 1948.  The AEC Mark IV Regals were the first to use route number 23. Changes to the route included an extension to Mount Pleasant Road via Belleview Terrace from 2 June 1958.  It became an extension of the Sumner service in 1982, eventually adopting the route number 3G.  Following the cancellation of the Woolston service in the late 1980s, Mount Pleasant buses were rerouted to serve the area with a diversion along Rutherford Street, Garlands Road, and Radley Street. Opened: 19 January 1925 (petrol bus), 18 October 1952 (diesel bus) Public agitation around 1919 for improved transport links to South Brighton prompted the Board to investigate its options for the area.  A tramline, as an extension of the Brighton service, was considered but rejected, as was the idea of using the Walker battery-electric bus that had recently been removed from the Templeton run. The Board did undertake a one-month trial in 1923 using a Garford bus and local driver, but it made a loss and was not renewed.  It was another two years before they tried again, and on 19 January 1925 a contracted feeder service connecting with the Brighton trams commenced operation using a local driver, Ern Smith.  He used his own vehicle, a converted International truck, for which he was compensated on a mileage basis. The service remained a contracted operation for the next 2½ decades.  Arch Lawson took over in December 1932 and, with a Willys-Knight vehicle, ran the service for the next four years.  Jim McGregor successfully tendered for the service in February 1936 and used his own Packard except when demand necessitated the use of a larger vehicle on loan from the Board.  By 1939 overcrowding had become a problem so the Board increased the number of services and constructed a new bus on a Ford V8 chassis with a capacity for 16 seats.  It was delivered to McGregor on 11 August 1939 and he paid for it over time by not receiving a subsidy for operation of the service. Wartime restrictions on petrol resulted in the removal of all Sunday services from the timetable from March 1942.  They were not reinstated until November 1943. Overloading had again become a problem by 1943 resulting in the increased use of a larger vehicle to handle the demand.  McGregor had his contract extended in October 1944, giving him the certainty he needed to obtain a larger vehicle.  His Ford V8 was stretched, enabling it to carry 25 passengers, and was ready in September 1945.  The Board occasionally borrowed this vehicle to trial new services, and in these cases McGregor had use of one of the Board's \"Square\" Ford V8 buses, as he had done while his own Ford was being modified. Following concerns raised about the dearth of Sunday and evening services by the South Brighton Progress League in August 1946, the Board increased the subsidy it paid for operation of the service.  Despite this, Sunday services continued to run to a summer timetable, and evening services were withdrawn after failing to attract sufficient interest.  Evening services did make an appearance again in July 1949, though not as a Pleasant Point service, but instead as an extension of one of the late Wainoni services on Fridays and Saturdays. McGregor retired in January 1950, leaving the operation to his business partner, Wilf Read.  On 18 June 1951 the Board assumed control of the service, having terminated the contract with Read.  As the vehicle that had been used on this service belonged to the private operator, the Board decided to use a 29-seat Ford V8 from its own fleet on the route. Through services were introduced on 26 November 1951 at the behest of the Progress League, comprising morning and evening commuter services and a select few daytime services for shoppers.  The Fords soon had to be replaced by Leyland Tiger buses to cope with the demand, though the Fords continued to provide the feeder services. The withdrawal of trams from the Brighton tramline and the introduction of diesel buses to the route saw Pleasant Point services combined with those to Brighton in a new timetable, effective from 18 October 1952.  As connections with trams were no longer required, the new timetable also eliminated the feeder services. Several route modifications followed to extend services to areas of new development including to the intersection of Caspian Street and Rockinghorse Road from 3 October 1960, and eventually, to the end of Rockinghorse Road.  When the terminus moved beyond the Pleasant Point Domain, the route number 5S and destination South Brighton were assigned to the route.  The previous route number, 24, appears to have been allocated after 1945, the route prior to this time being identified only by its destination, Pleasant Point.  The route was re-designated Southshore in 1984. Opened: 5 January 1976 (diesel bus) Canterbury's centennial celebrations in 1950 were the first occasion on which the Board was called upon to provide services to the Airport.  Demand for travel to the official function on 18 December 1950 at which the former Harewood Airport became Christchurch International Airport exceeded the capacity of the regular airport service provided by Midland Motorways so the Board was asked to assist for the occasion. The Board's own regular service to the airport began on 5 January 1976 following the Board's acquisition of the license for the route after Midland Motorways discontinued its own airport service.  It quickly became one of the more profitable routes for the Board, justifying an increase in service frequency to 30-minute intervals after only two years. Besides local residents along the route and airport workers, air passengers were one of the biggest sources of patronage for the service.  The large amount of luggage they normally had with them necessitated the use of specially adapted buses with space for luggage racks and bays.  When \"New\" Reliance buses proved inadequate to the task, they were replaced on this route with three Swifts and a Leyland with more generous accommodation for luggage.  The University of Canterbury also became an important source of passengers as the route included Riccarton and Ilam Roads. Opened: 31 December 1946 (diesel bus) New housing estates in Riccarton between Blenheim and Riccarton Roads prompted the creation of the Wharenui route, which commenced service on 30 December 1946.  Buses also provided access to the industrial area off Blenheim Road and to the Addington Railway Workshops.  A small change was made in 1947 to provide better access to an area of new State housing off Wharenui Road. Initially, Wharenui was a standalone route that started from Gloucester Street until such time as it could be accommodated in Cathedral Square.  From October 1955 it was linked with the Wainoni route and then from 4 September 1967 with the Burwood route. Though Riccarton Road was part of the route, the first stop for outbound services and the last stop for inbound services were in Picton Avenue meaning there was no connection with the Riccarton tramline.  This issue became moot on 1 August 1960 when a route alteration diverted Wharenui buses down Deans Avenue and Blenheim Road before rejoining the old route.  There were many changes to the route over the next three decades including extensions that took it well beyond Wharenui.  New destinations included Curletts Road and Springs Road and after the route ended up connecting with Hornby Mall in the 1980s it was combined with existing Templeton, Lincoln, and Southbridge routes. Opened: 13 May 1946 (diesel bus) Services to Huntsbury commenced on 13 May 1946 departing from Cathedral Square and terminating in Huntsbury Avenue.  The hill section of the route included two sharp turns that the Ford V8 buses used on the route had difficulty in negotiating. Light loadings on this route only justified the provision of three commuter and a few casual traveller return services per weekday.  An evening service was not added to the timetable until April 1949, and another five years before a Friday night service for shoppers was included. Numerous route changes were made as alterations to the nearby St. Martins and Beckenham routes necessitated modifications to ensure the best coverage for residents in the area.  In the year from August 1957 eight trips were added to the timetable but all were eliminated in September 1958. Similarity between the Huntsbury and St. Martins routes led to the Huntsbury route becoming part of the St. Martins service on 17 December 1984.  Two new route numbers were created: route 12E terminating on Centaurus Road at Vernon Terrace, and route 12F which was an extension of the 12E route by running back down Centaurus Road to Ramahana Road to join the old Huntsbury route. Opened: 12 July 1948 (diesel bus) The Beckenham service from Cathedral Square to its terminus at the south end of Birdwood Avenue commenced on 12 July 1948 as a weekday-only service to cater for those not served by either the Cashmere or St. Martins routes.  It was linked with the Creyke Road route. Route alterations included a switch from Carlyle Street to Byron Street after the Waltham Road railway over bridge was built; a change in the route used by off-peak services on 16 September 1957 to follow part of the St. Martins route before rejoining the Beckenham route at Norwood Street; and a change on 26 September 1958 to the start of the route to make it run through Sydenham. Residential development in the Bowenvale Valley from the mid-1970s resulted in an extension to the Beckenham route over the bridge at the bottom of Bowenvale Avenue and including Landsdowne Terrace and Wedgewood Avenue.  Weight restrictions on the bridge meant that only the Board's \"Short\" Reliance buses could be used and three were retained for this purpose after others in the class had been disposed of.  Following the withdrawal of the remaining \"Short\" Reliance buses in 1981, permission was obtained to use \"New\" Reliance vehicles with seated passengers only.  Strengthening work carried out on the bridge in 1986 enabled the restrictions to be rescinded. When the link with the Creyke Road service was removed, the Beckenham route was linked for a short time with an Avonside service before being combined with the Bromley service in the 1980s. Opened: 28 February 1964 (diesel bus) Prior to its regular Lyttelton services, the Board ran tourist services over the Port Hills via Sumner.  In 1950, it also provided special services to transport people to Lyttelton for the Canterbury Centennial celebrations that included a re-enactment of the arrival of the first settlers.  Buses were required to augment the capacity of the trains that were also used to bring revellers to the festivities. The Lyttelton road tunnel was opened on 27 February 1964 and the Board's buses were used to convey visitors to the official ceremony.  Timetabled services began the following day. The more direct route of the bus service from Cathedral Square along High Street and Ferry Road proved too popular for the Railways Department's Lyttelton passenger service to compete, leading to the cancellation of these trains in 1972.  This enabled the Board to have its license for the Lyttelton route changed to follow a path more closely aligned to the rail line and to serve areas near the railway stations.  The similarity of the new Lyttelton route to the existing Opawa route was such that the two routes were eventually combined. In Lyttelton, buses were always run around the block of Dublin, London, and Oxford Streets to Norwich Quay to avoid the need to reverse buses on Lyttelton's steep streets.  Some buses connected with the Diamond Harbour ferry at the jetty. Tolls were payable by all vehicles using the road tunnel, including the Board's buses, and this cost was passed on to passengers.  Drivers handed over pre-paid tickets at the tollbooth to use the tunnel and passengers were issued with a separate ticket on purchase of a fare for a Lyttelton bus to cover the toll.  This extra charge had been incorporated into the regular fares by June 1965 and the toll tickets were thereafter only used with special tickets. After the commencement of the Board's Airport service in 1976, the Lyttelton and Airport routes were linked to form a combined service.  The Board acquired the Governors Bay service from Railways Road Services in November 1982.  In keeping with the existing Lyttelton service, it became a commuter service with a once-a-week extra service for casual travellers. The Parklands service was created to cater for travellers to Christchurch's northeastern suburbs following a redesign of the bus route network in 1984.  It replaced services that previously terminated in, or passed through, Wainoni. Opened: 22 December 1984 (diesel bus) New Brighton used to attract people from across the city on Saturdays to take advantage of the fact that it was the only place in Christchurch where shops traded in the weekend.  To cater for the demand, two services were run on Saturdays in addition to the regular Brighton service: route 30, bringing in people from the suburbs of Bishopdale, Papanui, St. Albans, Mairehau, Shirley, Burwood, and North Beach, and route 31, for people from Spreydon, Sydenham, St. Martins, Linwood, Wainoni, and Bexley. As weekend trading was extended, and later included Sunday trading, Brighton lost its appeal as a shopping destination and the services were discontinued. The rolling stock available to the Board at the commencement of its services in June 1905 consisted of both electric trams it had commissioned and vehicles acquired from the private tramway companies that had previously served Christchurch.  In keeping with its desire to modernise the tramway, the Board had taken delivery of 27 electric tramcars constructed by John Stephenson and Company, New York (22) and Boon and Company, Christchurch (5).  Also added to the fleet were 7 Kitson steam trams and 42 trailers from the Christchurch Tramway Company, and 22 trailers from both the New Brighton Tramway Company and the City and Suburban Tramway Company when the sale of the assets of these companies to the Board was finalised. Horse trams, a legacy of the old private tramway companies, were phased out as soon as they could be replaced by steam or electric traction.  The steam trams continued in service for many decades, initially to continue services while installation of electrical infrastructure was still underway, but eventually relegated to non-revenue operations and providing additional capacity at times of heavy loadings. The trams supplied by John Stephenson and Company were the first and last trams to be imported by the Board.  Thereafter all trams and trailers were manufactured in Christchurch, two of which were made by the Board itself and the remainder supplied by Boon and Company.  Over the life of the tramway, Boon constructed 69 electric trams and 44 trailers for use in Christchurch including custom models such as the \"Hills\" cars for use on the Cashmere line. The manner in which buses were added to the Board's fleet in its early years typically involved the purchase of a chassis and running gear from a supplier, agent, or manufacturer followed by assembly and construction of a body either in its own workshops or at a local firm.  Several were also acquired in operational condition from private operators following the \"bus wars\" of the mid-1920s. By the 1930s, the Board had become dissatisfied with petrol buses and, after investigating a new \"trackless tram\" technology from Britain, decided to use trolley buses to replace trams on the North Beach tramline.  English Electric won the contract to supply the buses for the first route, but after evaluating a Ransomes trolley bus, the Board decided to go with Ransomes when purchasing trolley buses for the second route.  They proved to be popular with the public and resulted in operational cost savings over the trams, giving the Board little desire to ever rely on petrol buses again. Diesel technology had matured to a point where, in the mid-1930s, the Board was able to purchase diesel buses to replace trams on several of its least lucrative tramlines.  Though plans were made to introduce them on other routes, the arrival of World War 2 made them difficult to obtain.  Still needing to increase capacity and retire older vehicles in the fleet, the Board had to turn back to petrol buses and reluctantly ended up with one of the largest fleets of Ford V8 buses in the country. After the war, the Board wasted little time in modernising its fleet with diesel buses as they became available.  They were used to replace all remaining tram services by years end 1954, and replace the trolley buses in 1956, as well as enabling the creation of new routes.  The fleet became all-diesel in the 1960s and thereafter underwent occasional fleet upgrades, culminating in the Board's last purchase of M.A.N. buses in the mid-1980s. A.E.C. buses first joined the Board's fleet as a result of the \"bus wars\" of the mid-1920s when they purchased two A.E.C. petrol-powered buses from private operators Dixon and Munnerley.  The first of these buses was sold in 1931 to the Waimakariri River Trust.  The second bus, after being dismantled, was sold in 1935 with both the body and chassis going to separate new owners. When it came time to consider further tramline closures in the mid-1930s, the Board decided to go with A.E.C. diesel buses as they felt the technology had matured to the point where they were a reliable option.  Though the trolley buses introduced only a few years earlier had performed well and found favour with passengers, their dependence on fixed infrastructure did not suit the Board's preference for operational flexibility.  The first order for ten chassis was placed with A.E.C. in 1935–1936. Once the Regal Mark IVs had been used to retire the last of the trams, the Board sought to acquire additional diesel buses to replace its first generation diesel buses from the 1930s, the trolley bus fleet, and the Ford V8s.  They settled on the A.E.C. Reliance, of which three different models were purchased: the \"short\", \"long\", and \"new\" Reliances. The Board's fleet of A.E.C. \"Q\"s was delivered in two stages.  The first batch were assembled in the Board's workshops from four chassis of the initial order with A.E.C. and finished with wooden framing and aluminium panelling.  They commenced their duties as nos. 225–228 on 2 November 1936, followed two years later by the second batch of two \"Q\"s, numbered 235–236, on 1 May 1938. These diesel buses were the first to feature a new livery consisting of cream with a red band around the bottom half and Tramway Board decals on the side.  This was a departure from the Board's traditional use of white and green on its trams. Nos. 225 and 226 were sold in 1960 to Transport Units and Trailers.  The other four \"Q\"s were still on the Board's fleet register in March 1963.  The Tramway Historical Society acquired the remains of \"Q\"s nos. 225, 227, and 228 and plans to restore no. 228 to working order. The other six chassis from the initial order placed with A.E.C. were for Regal-class buses, which, as with the \"Q\"s, were also assembled in the Board's workshops with wooden framing and aluminium panelling.  The design of the Regals was in the half-cab style, in which the driver was isolated from the passengers in a small compartment to the right of the centrally mounted engine at the front of the vehicle.  This first batch of Regals commenced their duties on 2 November 1936 as numbers 229–234. A second batch of six Regals was ordered in 1940 and entered service as numbers 237–242.  Satisfied with the performance of the diesels now in its fleet, the Board attempted to order an additional 13 Regal chassis with which it intended to construct buses for the replacement of tram services to Fendalton and Opawa.  The order was never completed, as the vehicles could not be supplied after the commencement of World War 2. As with the \"Q\"s, the Regals started out life in the new cream and red livery, though some were later repainted in the revised livery of red with the new logo on the sides. Collins Brothers of Kurow purchased nos. 238, 240, and 242 between 1958 and 1960 for use as staff transport on public works projects.  Numbers 229 and 232 also became staff transport, for contractors Wilkins and Davies, and Otago Motors purchased no. 233.  Five of the Regals were still in the Board's possession as at March 1963.  The Tramway Historical Society later acquired both 233 and 240, and intends to restore no. 240 to working order. In what was to become the biggest addition to the Board's fleet thus far, they decided in the early 1950s to order A.E.C. Regal Mark IV buses.  The first batch of 72 vehicles arrived in 1952, having been manufactured by Crossley (numbers 290–328) and Park Royal (numbers 329–361).  Another 23 buses arrived in 1954, also from Park Royal, and entered service as numbers 362–384.  Concerns were raised about the buses being assembled in England rather than supporting local manufacturers, but the Board considered its decision justified in light of its need for vehicles to replace trams that were being withdrawn from service at about the same time. All Mark IVs were delivered in a new livery, consisting of red with cream around the windows.  As the Board's name had changed in 1951 to the Christchurch Transport Board, the old Tramway Board crest was dropped and in its place the buses featured a new Transport Board logo on their sides. After serving the Board for a quarter of a century, the buses were withdrawn from the 1970s, starting with no. 343 that was scrapped after being involved in an accident on 2 September 1974, and later, nos. 305 and 367, which were sold in March 1975.  From October 1978 the remaining Mark IVs were disposed of, with the final 12 in the Board's possession last being listed in the fleet register on 31 March 1981.  A special last service for the Mark IVs was run on 29 July 1981 using class leader no. 290. As second hand vehicles, the Mark IVs found little favour with other operators, only ten of which ended up with new transport service providers.  New private owners repurposed a few, but most were scrapped.  No. 290 was loaned to the Tramway Historical Society from 1978, an arrangement that was made permanent when it was given to the society in 1989. The first batch of 17 \"short\" Reliances were assembled by Park Royal, England and introduced as nos. 385–401 in 1956–1957 to replace trolley buses on both of the electrified routes.  A second batch of 20 \"short\" Reliances was delivered in 1958 as part of an order for 30 vehicles from Park Royal.  They entered service as nos. 402–421. Most of the \"short\" Reliances were sold on to other operators around the country when retired by the Board between 1975 and 1981.  Nos. 385–386, 388–389, 393, 397, and 399 went to Nelson Suburban Bus Lines; nos. 403–405, 407–409, and 412 went to Ritchies; and nos. 390, 392, 395, 398, and 401 went to Waitaki Transport.  There were several other sales in small lots to other smaller operators. The last \"short\" Reliances in service to the Board were nos. 415, 417, and 418, which were used on the City Clipper service from 1975 to June 1981, when the service was withdrawn.  These buses were then sold to Newmans for use as staff transport at Marsden Point. Number 410 ended up with Wyldes Motors, Runanga who, in 1991, gave it to the Tramway Historical Society in working order. Ten \"long\" Reliance buses were delivered in 1958 as part of a 30-vehicle order that also included 20 \"short\" Reliances.  They entered service as nos. 422–431. They primarily served on the Dallington and Bryndwr routes as it was found that their lighter steering was advantageous on the narrow roads and corners typical of these routes.  The arrival of these buses gave the Board the capacity it needed to be able to dispose of the last of the petrol buses in its fleet, making the whole fleet diesel only. When retired, no. 426 was sold to Timaru City Council and no. 430 to Southey, Northland.  The last of these buses was gone by 1982. In 1963 the Board decided to acquire more Reliance buses, for which it purchased 24 of the same ten-metre chassis used in the earlier \"long\" Reliance fleet.  New Zealand Motor Bodies was commissioned to assemble the vehicles, which commenced their duties as nos. 432–455 from September 1963.  They became known as the \"new\" Reliances, the last of which was delivered in 1964. Initially, they were assigned to the Fendalton/Opawa route.  In 1966, they were moved on to the Papanui/Cashmere route, as originally intended, as their lighter construction made them more appropriate for the hill section than the Mark IVs they replaced. Twenty-two of the \"new\" Reliance fleet received new motors as they were overhauled after the Board took delivery of 25 new engines in 1979.  The original motors had proved to be unreliable, experiencing a high rate of failure in the head gasket. Unlike the other Reliances, the \"new\" Reliances ended their days with the Board in the new red-and-white livery, which first made an appearance on the Bristol \"Hess\" buses, rather than the original in-service all-red livery. Ten of the \"new\" Reliances were sold to Charlie Dobson for use on a school contract.  Eleven of the fleet were still in service at the time of deregulation in 1989, and were sold the following year to a private operator.  Also in 1989, the Board gave no. 452 in working order to the Tramway Historical Society. New Zealand Motor Bodies assembled the first Swift, no. 456, for the agents to use as a demonstration.  It was purchased by the Board and entered service in 1973.  Board general manager Fardell had hoped to purchase additional Swifts but was unable to obtain approval to do so.  The local agent, in anticipation of further business with the Board, had imported a further two chassis, but these ended up in storage when not purchased.  When the Board finally purchased them in 1977, they were assembled by Hawke Brothers and commenced their duties as nos. 457 and 458, with the same internal configuration as no. 456. In service, they were beset by a litany of problems that made them unpopular with both the Board and its drivers.  The engine and transmission in no. 456 had to be replaced, and there was an ongoing problem with the fuel supply that made them unsuitable for use on the hills.  Also, the rear-engine nature of this vehicle caused problems with weight distribution, which was resolved by changing the seating configuration to 35 seated, 34 standing. After the latter two buses were added to the fleet they were mostly used on the Airport route.  Though they offered good ride quality, the limited seating provision made them unpopular with passengers at peak times. The increasing unreliability of the engines led to the withdrawal of the Swifts, the last of which, no. 458, was retired from the Bromley, North Beach, and Bowenvale routes in 1989.  No. 456 eventually ended up in Edgecumbe with a private owner, while no. 458 was converted for use as a pie cart in Latimer Square before being transferred to the West Coast. Bristols were the first new buses to be added to the Board's fleet as part of its fleet upgrade programme commenced in the mid-1970s.  They had hoped to purchase the RELL6G with the Gardner engine, but when informed that they were not available, opted to go with the RELL6L (Rear Engine, Long, Low, 6 cylinder, Leyland 510 engine) instead. The first batch of 25 vehicles came with kitset bodies designed by Eastern Coachworks, England, and were assembled by Hawke Brothers, Auckland.  Originally numbered 457–481, nos. 457–458 later became 475–476 when the former numbers were allocated to two of the Swifts, leaving the Bristols numbered 459–483.  They commenced their duties between 1974 and 1975. After experiencing problems with the English motor bodies, the Board decided to purchase only the chassis when ordering the next batch of 29 vehicles.  Once landed in New Zealand, the bodies were constructed and fitted to the chassis by Hawke Brothers, giving them the same outward appearance as the earlier ECW-designed buses.  Numbered 484–512, these vehicles commenced their duties between 1977 and 1978. The new C.T.B. logo first made an appearance on the Bristol buses, which were also the last of the fleet to feature the all-red livery first seen on the Mark IVs.  A select few buses from the second batch had fold-down seats to provide additional space for luggage on the Airport service. These buses remained in service to the Board until deregulation, after which they were \"retired\" by the Christchurch City Council in an attempt to hinder the operations of companies competing with its Christchurch Transport Limited, the successor to the Board.  When this failed, the Council was eventually able to dispose of the buses to other operators around the country.  Two of these vehicles were given to the Tramway Historical Society in working order: no. 480 (Mark I), and no. 510 (Mark II) in 1990.  A second RELL Mk II, No. 505, was donated to the Society by Good Times Tours in 2013 as a source of spare parts. The next round of bus acquisitions for the fleet modernisation programme consisted of Bristol RELL6L chassis and motor bodies licensed from Carrosserie Hess A.G., Switzerland.  Once arrived in New Zealand, all of the chassis were assembled in Nelson.  Next, they were fitted with small fibreglass cabs and driven by Transport Board staff to New Zealand Motor Bodies factories in Christchurch, and later Palmerston North, where the bodies were constructed.  Nos. 513–578 were assembled in Christchurch up to 1980 when that factory closed, after which nos. 580–599 were assembled in Palmerston North.  No. 579 was framed in Christchurch and finished in Palmerston North.  The remainder of the \"Hess\" buses, nos. 600–608, were also assembled in Christchurch before the factory closed, but received a different fitout from the other buses for their duties on charter work. The Board's last livery change, red with a white, central horizontal stripe, first made an appearance on the \"Hess\" buses.  This livery was also applied to some other buses when they received major overhauls.  These buses also featured triple-digit route number displays beside the destination sign at the front, as well as a destination sign at the side next to the front door. As with the earlier batches of Bristols, the Board had been keen to avoid the Leyland engine, so after a favourable evaluation from a 1987–1988 trial, all of the buses assembled in Palmerston North were refitted with M.A.N. engines. After deregulation drastically cut the number of routes Christchurch Transport was responsible for, the \"Hess\" buses were relegated to providing additional capacity at peak times, as the M.A.N. buses were sufficient for regular duties.  When these buses were no longer required, the Council was eventually able to sell them to other operators around the country.  They gave no. 538 in working order to the Tramway Historical Society in 1990. After successfully introducing diesel buses to its operation in the mid-1930s, the Board had hoped to phase out its older petrol-powered vehicles and focus on diesels, both for new routes and as required to replace trams.  With the onset of World War 2, the supply of diesel-powered vehicles dried up and the only option for augmenting the fleet was the Fords, which eventually numbered 41 in service to the Board. The Board's first order for Ford V8 chassis from Canada was for 20 units, of which it took delivery of 10 and then sold 3 to Dunedin City Tramway.  The seven remaining chassis were completed with wooden framed, aluminium-panelled motor bodies as follows: nos. 243–244 at Modern Motor Bodies, Christchurch; nos. 245–246 at Crawley Ridley, Wellington; nos. 247–249 were started by the Board but finished by Modern Motor Bodies.  Because of their chiselled appearance, they became known as the \"Square\" Fords. After entering service between 1942 and 1945, these vehicles made an appearance on a variety of routes: Templeton (no. 243); Avonside (no. 244); Mount Pleasant (no. 245); and the Farmers Free Bus (no. 247).  Number 248 was occasionally used for relief duties on the Pleasant Point service. Numbers 244 and 246 ended their days with the Board, once withdrawn from revenue service in September 1954, as shuttles for staff bicycles between the central city terminus and the Moorhouse Avenue depot, until sold in 1958. As production of the Board's preferred vehicles after the war was focussed on war recovery efforts, they decided to order an additional 20 Ford V8s after being offered 10 by New Zealand Motor Bodies in June 1945.  These vehicles were manufactured in Petone to a New Zealand Motor Bodies design using steel framing and panelling, and commenced duties in 1946 as numbers 250–269.  Though they were painted in the same cream and red colours used on earlier Transport Board vehicles, the livery of all-over cream with a red band on the sides along the bottom earned them the nickname \"Cream\" Fords. When nos. 244 and 246 were retired from their duties as bicycle shuttles, the job was taken over by nos. 251 and 254 until 1969 when the Ferry Road depot was opened, eliminating the need to transport bicycles. After determining the suitability of the Ford V8s for its Sanatorium service, the Board purchased a Ford V8 chassis and commissioned Johnson and Smith to construct a body for it.  Their unique (to the Board's fleet) design, using wooden framing and aluminium panels, featured a short front hood and a seating capacity of 25. In addition to the Sanitoria service, it was also assigned to the Huntsbury route, and was later used for driver training before being retired from service in 1955. The last batch of 13 Ford V8s to be added to the Board's fleet were assembled in the Board's own workshops and commenced their duties from 1947 to 1950, numbered 271–283.  With wooden framing and aluminium bodywork, their outline was similar to that of the previous batch of Fords.  Painted in a style more reminiscent of the livery used on earlier Board vehicles, they became known as the \"Red\" Fords. The Fords proved useful for the post-war expansion of the Board's route network, and in replacing trams as they were retired. Besides their suburban duties, they were also popular for charter work, including staff social functions.  Midland Motorways and Days Motors occasionally hired them to cover their regular services if their own buses were out on other work, and the Railways Department used them for relief services to replace trains when they were non-operational. The United States Air Force hired nos. 243, 248, and 274 for use as staff transport for Operation Deep Freeze in 1958.  They hired a range of buses from the Board each summer for many years until eventually purchasing their own vehicles. The North Canterbury Catchment Board hired no. 243 in July 1959, after which it was sold in November, along with nos. 248 and 274 in June. Throughout the mid-to-late 1950s, Nelson Suburban Bus Lines purchased the largest number of the Board's Fords, including nos. 245, 247, and 270 (1955); nos. 253, 255–259, 261, 264, and 266–268 (1957–1958); and nos. 273 and 275–278 (1955–1958).  The remaining Fords were sold over the same time frame to a variety of private operators.  No. 263 ended up with the Timaru Harbour Board, and was later acquired by the Tramway Historical Society around 1990. A Garford bus was purchased from New Zealand Farmers Co-op in June 1920 with the intention that it would replace the Walker electric bus on the Templeton service.  Fitted with weather blinds and solid rubber tyres, its ride quality was no more improved than that of the vehicle it replaced. When a Leyland bus was assigned to the Templeton route in 1923, the Garford was put on stand-by for relief duties.  Later that year, it was used on a short-lived trial service to Pleasant Point. A report in August 1925 on the performance of the Board's bus fleet mentioned that the Garford had a fuel efficiency of 5.72 miles per gallon.  In 1927, the vehicle was scrapped: the body was sold and the chassis was repurposed. When private operator W. C. Sanders sold his business to the Board following the enactment of the \"Motor Omnibus Traffic Act\" in 1926, his two Leyland buses were added to the fleet.  One of these vehicles was already suitably configured for use as an omnibus and was put to work in place of the Board's older 1923 Leyland.  It only remained in revenue service a short time before being converted into a tower truck for maintenance of the trolley bus overhead electrical infrastructure.  Sanders' second Leyland, which he was using for his sightseeing tours, was converted into a tip-truck for maintenance of the permanent way along with the Board's older Leyland. In 1928 the Board sought to add more buses to its fleet to handle an increasing workload.  They requested tenders for the supply of chassis and after rejecting several bids decided on two options, including a Leyland Tigress.  The Tigress consisted of a six-cylinder engine on a Leyland Lioness chassis, and was the only vehicle of its type in the country.  Local company Johnson and Smith constructed a 29-seat capacity body on the chassis.  After some initial problems, it served the Board well. In 1942 the bus was sold to Hobbs Motors of Dunedin. As diesel buses became available again after the restrictions on their supply following World War 2 eased, the Board decided to go with the Leyland Tiger and purchased six chassis in 1950.  McWhinnie completed the first bus, numbered 284, and the remaining vehicles had their bodies constructed by New Zealand Motor Bodies.  In all cases, the buses had the half-cab style front end with a 37-seat capacity.  Though capable of displaying destination signs, they were not fitted to show route numbers. They were unpopular with the drivers for a variety of reasons, including: the synchromesh gearboxes required a lot more work to operate compared to the pre-selector gearboxes in the older buses; the separation of the driver's cab from the passenger compartment made visibility and fare collection difficult; and it was often necessary to exit the vehicle many times per shift using a small side door into the path of oncoming traffic to perform routine tasks.  However, compared to the Ford V8s, they were more powerful, had better brakes, and were more reliable. When replaced by the Regal Mark IV buses later that decade on suburban routes, they were assigned to sightseeing and charter duties and later used on tours. No. 284 was eventually sold to M. L. Paynter Limited, a building contractor, for use as staff transport.  Nos. 285–289 were sold in 1964 to Nelson Suburban Bus Lines.  The Tramway Historical Society acquired no. 284 in 1978 from Paynter and it is currently in storage awaiting restoration. The Leyland Lion was out-shopped from Hawke Brothers, who had built the body, in 1978.  It was designed to accommodate both seated and standing passengers and was fitted out for the Airport service, to which it was initially assigned. Three years later it was assigned to the Board's \"Be Mobile\" service for which it was modified, including the installation of a wheelchair hoist in place of the rear exit.  In 1983 it was relieved of these duties and resumed its former role in suburban revenue service. Two Leyland Cubs were purchased in 1984 for the price of one standard-sized bus to provide much needed additional capacity.  They, like the Lion before them, were designed to accommodate both seated and standing passengers and were assembled by Modern Motor Bodies Limited.  They were evaluated to find the most suitable duties for them, and were found to be most useful on routes where the operation of a full-sized bus was difficult or only light loadings were expected. The design of the rear suspension made for poor ride quality and, as the Board discovered, made the vehicles liable to experience rear wheel spin.  This resulted in an accident in May 1986 when the driver of Cub no. 611 lost control and the vehicle crashed into a tree, with four of the occupants ending up in hospital. Though number 611 was repaired, both of the Cubs lost favour with the public and they were gradually phased out.  When retired from service with the Board, no. 610 was sold to Hamilton City Buses and no. 611 went to Queenstown. The last buses to enter service with the Board, the M.A.N. SL202, were ordered in 1984.  Coachwork International was commissioned to assemble the buses to M.A.N. designs, which were built at their Takanini, Auckland (nos. 612–649) and Palmerston North (nos. 650–669) sites.  The Board took delivery of the first vehicle, no. 612, on 5 June 1986.  After minor fitout changes and staff training, it commenced its duties with three others of the class on 1 August 1986. Designline of Ashburton also supplied M.A.N. buses to the Board, including no. 670, built to the original German specification, and no. 671, a so-called \"midi\" bus with a 34-seat capacity.  These were intended for suburban routes, while nos. 672–680, based on a three-axle chassis, were intended for use on rural and charter duties.  All entered service in 1990 with the Board's successor, Christchurch Transport. No. 670, while introducing a number of innovations that would later become standard, was not authorised to carry standing passengers, and thus lacked the necessary capacity for use on busy suburban routes.  It was to have been deployed on a Rangiora service for which its generous luggage accommodation would be useful, but deregulation resulted in the license for the Rangiora service being awarded to another operator before no. 670 was delivered.  Consequently, it was never used in revenue service, and was sold after being used for a series of demonstration trips. Red Bus gave the class leader, no. 612, to the Tramway Historical Society in working order on 10 November 2010. The first Minerva was built in 1928 on an imported chassis with a 29-seat capacity by local firm Johnson and Smith.  A second Minerva was added to the fleet in 1931 after the Board was offered a chassis and engine for £750 following the failure of the local Minerva dealer.  Johnson and Smith also won the contract for the second vehicle, and constructed a 30-seat capacity body on the chassis with several improvements over the first Minerva. The first bus was sold in 1942 to Peninsula Motors, Dunedin and in May 1943 Nelson Suburban Bus Lines purchased the second bus. The first Republics to see service with the Board were three model 81s, built on chassis ordered in December 1925 from local supplier, Inglis Brothers.  Two of these chassis were delivered later that month.  Bodies for these vehicles were constructed in the Board's own workshops with capacity for 23 seats.  Initial problems with regulations governing the use of omnibuses saw the maximum loading decreased to 21 seats, but after some modifications the 23-seat capacity was restored. A fourth model 81 Republic was added to the fleet from private operator C. R. Brown after the Board acquired his business following the enactment of the \"Motor Omnibus Traffic Act\" in 1926. After being involved in an accident in 1935, the first Republic was scrapped.  The second was sold in 1931 to the Waimakariri River Trust and the remaining two were retired in 1937. The Board purchased two Tilling-Stevens chassis in early 1925, on which it built bodies with a 25-seat capacity.  They entered service shortly thereafter but by November of that year serious maintenance issues were evident.  In May 1926 they were retired from regular service with the Board and then used occasionally until October to fend off a private operator plying the Sumner route.  The following month they were converted into tower trucks for the maintenance of overhead electrical infrastructure.  The bodies were sold in 1931 and 1932. Designed in their native England to operate in cities with a trolley bus network, the Tilling-Stevens buses employed an electric motor to provide motive power.  The vehicles were fitted with trolley poles and where available, trolley overhead lines would be used to provide power to the electric motors.  The vehicle could also operate away from a trolley network by using its petrol engine to run a generator that powered the electric motors. Having decided to use trolley buses on the North Beach route, six chassis were ordered from English Electric through local agent Cory-Wright and Salmon in February 1930.  Boon and Company won the contract to build the bodies for the six vehicles at £719 each.  As with the body they had constructed for the Walker electric bus, their design for the trolley buses was influenced by their experience as tram builders, giving the vehicles a rather utilitarian appearance.  They were the first vehicles in the Board's fleet to have three axles. One of the biggest frustrations both drivers and passengers alike had with the design of these buses was the narrow front door which allowed for only one stream of people to use it at a time.  Drivers had to wait for disembarking passengers to exit before waiting passengers could board and pay their fares.  The buses were fitted with rear doors but they remained non-operational, a situation that was not remedied for most of the buses until after they were refurbished between 1948 and 1950. Most of the vehicles were dismantled and sold to private individuals following the withdrawal of the trolley buses in 1956.  Some parts such as the axles typically ended up in other vehicles, e.g. trailers, while the bodies were often used as small buildings, such as sheds.  After a period of being used as holiday accommodation, no. 209 was donated to the Tramway Historical Society without its running gear and is currently in storage awaiting restoration.  No. 210, obtained by the Tramway Historical Society from Transport North Canterbury in 1965, was restored around 1970.  It was overhauled in 2013 and is now currently used at Ferrymead Heritage Park giving heritage trolley-bus rides on special occasions. Though the English firm of Ransomes, Sims, and Jefferies had not been successful in its bid for the supply of vehicles for the first trolley bus service in Christchurch, they had built a vehicle of the type they had intended to supply to Christchurch and offered it to the Board to evaluate, free of charge, for a year.  It arrived at Lyttelton, ready for use, on 21 April 1931 and was numbered 213. It quickly proved to be a useful addition to the fleet.  The first, and only, English Electric trolley bus in service at the time for the Shirley route, opened on 1 April 1931, had to be temporarily withdrawn after a month for upgrades to its bodywork, during which time no. 213 was able to provide relief cover.  It was also used for experiments on the Cashmere Hills tram route to test performance on steep terrain.  On the morning of 24 May 1931 and the afternoon of 1 June 1931 no. 213 was run along the Cashmere tramline to Dyers Pass Road with one pole on the overhead tram wire and towing a contact line attached to a skate on the tram rail.  Though the Board was satisfied with the results, it ultimately decided against conversion of the Cashmere tram route to trolley bus operation. At the conclusion of the trial of no. 213, the Board decided to purchase the vehicle.  When trams were withdrawn from the remainder of the North Beach tramline in 1934, it was decided that that route also would be converted to trolley bus operation, for which it was decided to go with an additional four Ransomes trolley buses.  Ransomes supplied the chassis and running gear and the bodies were constructed in the Board's own workshops to a slightly different design than that of no. 213. The newer Ransomes buses, nos. 214–217, were typically used on the Marshland Road via Richmond route while no. 213 continued to be used on the North Beach route with the English Electrics.  To this end, they had separate lanes at the central city depot so the appropriate vehicles were always available. Most of the vehicles were dismantled and sold to private individuals following the withdrawal of the trolley buses in 1956.  After a period of being used as holiday accommodation, no. 216 was donated to the Tramway Historical Society without its running gear and is currently in storage awaiting restoration.  The body of demonstrator no. 213 was also preserved by the THS but was destroyed by arsonists in the 1970s. The Board purchased its first bus, a battery-electric Walker vehicle, in 1918.  Importer A. R. Harris supplied the chassis, and local firm Boon and Company constructed the body.  They chose a design reminiscent of a tramcar, which gave the vehicle an appearance that saw it nicknamed \"Beetle\". The \"Beetle\" bus commenced service on 3 June 1918 but was never popular with the travelling public.  The entrance to the vehicle was by way of narrow, steep steps, making ingress and egress difficult; the bus was initially fitted with weather blinds rather than windows, which offered little protection from the elements; solid rubber tyres on rough roads made for poor ride quality; and despite frequent recharging, it occasionally failed to complete a shift without running out of power. It was initially put to work on the Templeton route, running a shuttle from the end of the Riccarton tramline to the terminus via Hornby and Islington, until being taken out of service in June 1920.  Its next assignment was to run an afternoon shuttle service, starting on 2 October 1920, between the Brighton tramline at Linwood Avenue and the Bromley Cemetery during weekends.  This proved to be its last public service, ending on 1 January 1922, after which it was dismantled.  The body was later lengthened and mounted on another chassis while the Walker chassis was cut up for parts.  The Tramway Historical Society have two similar Walker chassis built in 1918 as part of their collection, and intend to restore one of these with a replica \"Beetle\" body. A.E.C. Regal Mark IV no. 290, formerly in service with the Transport Board, was used in the filming of scenes for the movie \"Heavenly Creatures\" in 1993. A Transport Board bus appears in a scene from episode 2 of the children's television serial \"The Games Affair\", based around the 1974 Commonwealth Games. N.B.  There were numerous amendment acts to the Board's founding legislation in addition to the principal acts listed above.\n\nReduced muscle mass, strength and performance in space There is a growing research database which suggests that skeletal muscles, particularly postural muscles of the lower limb, undergo atrophy and structural and metabolic alterations during space flight.  However, the relationships between in-flight exercise, muscle changes and performance are not well understood.  Efforts should be made to try to understand the current status of in-flight and post-flight exercise performance capacity and what the goals/target areas for protection are with the current in flight exercise program. From the very beginning of the U.S. human space program, serious and reasonable concern has been expressed about exposure of humans to the microgravity of space due to the potential systemic effects on terrestrially-evolved life forms that are so suitably adapted to Earth gravity.  Humans in the microgravity environment of space, within our current space vehicles, are exposed to various mission-specific periods of skeletal muscle unloading (unweighting).  Unloading of skeletal muscle, both on Earth and during spaceflight, results in remodeling of muscle (atrophic response) as an adaptation to the reduced loads placed upon it.  As a result, decrements occur in skeletal muscle strength, fatigue resistance, motor performance, and connective tissue integrity.  In addition, there are cardiopulmonary and vascular changes, including a significant decrease in red blood cell mass, that affect skeletal muscle function.  This normal adaptive response to the microgravity environment is, for the most part, of little consequence within the space vehicle \"per se\", but may become a liability resulting in increased risk of an inability or decreased efficiency in crewmember performance of physically demanding tasks during extravehicular activity (EVA) or abrupt transitions to environments of increased gravity (return to Earth, landing on the surface of another planetary body). In the U.S. human space program, the only in-flight countermeasure to skeletal muscle functional deficits that has been utilized thus far is physical exercise.  In-flight exercise hardware and protocols have varied from mission to mission, somewhat dependent on mission duration and the volume of the spacecraft available for performing countermeasures.  Collective knowledge gained from these mission has aided in the evolution of exercise hardware and protocols in attempts to refine the approach to prevention of spaceflight-induced muscle atrophy and the concomitant deficits in skeletal muscle function. Long duration missions and exploration missions with several transitions between gravitational environments present the greatest challenges to risk mitigation and to development of countermeasures of proven efficacy. Russian scientists have utilized a variety of exercise hardware and in-flight exercise protocols during long-duration spaceflight (up to and beyond one year) aboard the Mir space station.  On the International Space Station (ISS), a combination of resistive and aerobic exercise has been used.  Outcomes have been acceptable according to current expectations for crewmember performance on return to Earth.  However, for missions to the Moon, establishment of a lunar base, and interplanetary travel to Mars, the functional requirements for human performance during each specific phase of these missions have not been sufficiently defined to determine whether currently developed countermeasures are adequate to meet physical performance requirements. Access to human crewmembers during both short- and long-duration mission for the study of skeletal muscle adaptation to microgravity and the efficacy of countermeasures has been, and continues to be, limited.  Consequently, a more complete understanding of physiologic models for conduct of both fundamental and applied skeletal muscle research.  Various models for which sufficient data have been collected have been concisely reviewed.  Such models include horizontal or head-down bed rest, dry immersion bed rest, limb immobilization, and unilateral lower-limb suspension.  While none of these ground-based analogs provides a perfect simulation of human microgravity exposure during spaceflight, each is useful for study of particular aspects of muscle unloading as well as for investigation of sensorimotor alterations. Due to limitations in the number of spaceflights and crewmembers in which novel countermeasures can be tested, future development, evaluation and validation of new countermeasures to the effects of skeletal muscle unloading will likely employ variations of these same basic ground-based models.  Prospective countermeasures may include pharmacologic and/or dietary interventions, innovative exercise hardware providing improved loading modalities, locomotor training devices, passive exercise devices, and artificial gravity either as an integral component of the spacecraft or as a discrete device contained within it.  With respect to the latter, the hemodynamic and metabolic responses to increased loading provided by a human-powered centrifuge have been described recently.  Even more recently, an approach to provide both aerobic and resistive exercise by incorporating a cage-like platform into the design has been developed by the same investigator group. Animal studies, conducted both during spaceflight and in ground-based simulations of the skeletal muscle unloading associated with spaceflight, have contributed to the scientific knowledge base in a manner not totally achievable by means of human spaceflight and ground-based analog studies alone.  This is because many of the variables present with human subject investigations can be more tightly controlled in animal studies, and the much larger number of animals typical of such experiments contributes to a greater statistical power to detect differences.  A major advantage in use of rodent models is that the adaptive changes to both spaceflight and hind-limb suspension occur in a much shorter time frame than they do in humans (hours to days versus days to weeks).  This enables prediction of long-term changes in human skeletal muscle based on the shorter absolute time frame of the rodent investigations.  Additionally, it is possible to perform a highly controlled, straightforward experiment in rodents without a requirement to provide some type of countermeasure intervention that introduces a confounding variable.  In human studies, it is not possible on ethical grounds to withhold countermeasures known to have some degree of effectiveness to provide a population of true control subjects, in which only the effects of spaceflight are seen, for comparison to subjects utilizing countermeasures modalities.  Animal studies do not suffer from such restrictions.  Further work is needed to provide a better understanding of the problem, which will allow novel approaches to countering loss of skeletal muscle function associated with spaceflight in humans.  Relevant animal spaceflight studies, as well as investigations using muscle unloading paradigms that contribute to our current knowledge base, are presented. Prior to launch of the first American astronaut, suborbital flights of non-human primates (chimpanzees) demonstrated that launch and entry, as well as short-duration microgravity exposure, were all survivable events. The initial biomedical problem faced by Project Mercury (which ran from 1959 - 1963) was establishment of selection criteria for the first group of astronauts.  Medical requirements for the Mercury astronauts were formulated by the NASA Life Sciences Committee, an advisory group of distinguished physicians and life scientists.  Final selection criteria included results of medical testing as well as the candidates' technical expertise and experience.  Aeromedical personnel and facilities of the Department of Defense (DoD) were summoned to provide the stress and psychological testing of astronaut candidates.  The screening and testing procedures defined for the selection of Mercury astronauts served as the basis for subsequent selection of Gemini and Apollo astronauts when those programs were initiated. While the Mercury flights were largely demonstration flights, the longest Mercury mission being only about 34 hours, Project Mercury clearly demonstrated that humans could tolerate the spaceflight environment without major acute physiological effects and some useful biomedical information was obtained, which included the following: Because of the short mission durations of Project Mercury, there was little concern about loss of musculoskeletal function; hence no exercise hardware or protocols were developed for use during flight.  However, the selection criteria ensured that astronauts were in excellent physical condition before flight. Biomedical information acquired during the Mercury flights provided a positive basis to proceed with the next step, the Gemini Program, which took place during the 20 months from March 1965 to November 1966.  The major stated objective of the Gemini Program was to achieve a high level of operational confidence with human spaceflight.  To prepare for a lunar landing mission, three major goals had to be realized.  These were: Thus, Project Gemini provided a much better opportunity to study the effects of the microgravity of spaceflight on humans.  In the 14-day Gemini 7 flight, salient observations were undertaken to more carefully examine the physiological and psychological responses of astronauts as a result of exposure to spaceflight and the associated microgravity environment. The Gemini Program resulted in about 2000 man-hours of weightless exposure of U.S. astronauts.  Additional observations included the presence of postflight orthostatic intolerance that was still present for up to 50 hours after landing in soe crewmembers, a decrease in red cell mass of 5 - 20% from preflight levels, and radiographic indications of bone demineralization in the calcaneus.  No significant decrements in performance of mission objectives were noted and no specific measurements of muscle strength or endurance were obtained that compared preflight, in-flight and postflight levels. The major objective of the Apollo Program was the landing of astronauts on the lunar surface and their subsequent safe return to Earth.  The Apollo (1968–1973) biomedical results were collected from 11 crewed missions that were completed within the five-year span of the Apollo Program, from pre-lunar flights (missions 7 through 10); the first lunar landing (mission 11), and five subsequent lunar exploratory flights (mission 12 through 17).  Apollo 13 did not complete its intended lunar landing mission because of a pressure vessel explosion in the Service Module.  Instead, it returned safely to Earth after attaining a partial lunar orbit. Essential to the successful completion of the Apollo Program was the requirement for some crew members to undertake long and strenuous periods of extravehicular activity (EVA) on the lunar surface.  There was concern about the capability of crew members to accomplish the lunar surface excursions planned for some of the Apollo missions.  Although reduced lunar gravity was expected to make some tasks less strenuous, reduced suit mobility coupled with a complex and ambitious timeline led to the prediction that metabolic activity would exceed resulting levels for extended periods.  Since the nature and magnitude of physiological dysfunction resulting from microgravity exposure had not yet been established (and is still not concisely defined), suitable physiological testing was completed within the constraints of the Apollo Program to determine if crewmember physiological responses to exercise were altered as a consequence of spaceflight. Initial planning for the Apollo Program included provisions for in-flight measurements of salient parameters of concern including physiological responses to exercise.  However, the fire in the Apollo 204 spacecraft (also known as Apollo 1), fatal to astronauts Grissom, White, and Chaffee, resulted in NASA management initiating changes in the program that eliminated such prospects.  This, investigators were left with only the possibility to conduct pre-flight and post-flight exercise response studies and to assume that these findings reflected alterations of cardiopulmonary and skeletal muscle function secondary to microgravity exposure.  It was realized early on that within the context and constraints imposed by the realities of the Apollo missions, the inability to control certain experiment variables would present challenges to many biomedical investigations.  Firstly, re-adaption to Earth gravity procedures introduced additional challenges to a well-controlled experiment design since Apollo crew members spent variable amounts of time in an uncomfortably warm spacecraft bobbing in the ocean and additionally, orbital mechanics constraints on re-entry times imposed crew recovery times that prevented the possibility of conducting pre- and post-flight testing within a similar circadian schedule.  The effect of these uncontrollable conditions and that of other physical and psychological stresses could not be separated from responses attributable to microgravity exposure alone.  Thus, data relating to the physiological responses to exercise stress in Apollo astronauts must be interpreted within this overall context. No standardized in-flight exercise program was planned for any of the Apollo flights; however, an exercise device (Figure 6-1) was provided on some missions.  Crewmembers, when situated in the Command Module (CM), typically used the exerciser several time per day for periods of 15–20 minutes. The pre- and post-flight testing consisted of graded exercise tests conducted on a bicycle ergometer.  Heart rate was used for determining stress levels, and the same heart rate levels were used for pre- and postflight testing. Although the exact duration of each stress level was adjusted slightly (1–2 minutes) for the later Apollo missions to obtain additional measurements, the graded stress protocol included exercise levels of 120, 140 and 160 beats per minute, corresponding to the light, medium, and heavy work respectively for each individual.  For the Apollo 9 and 10 missions, a stress level of 180 beats per minute was added.  The entire test protocol was conducted three times within a 30-day period before lift-off.  Postflight tests were conducted on recovery (landing) day and once more at 24 to 36 hours after recovery. During each test, workload, heart rate, blood pressure, and respiratory gas exchange (O consumption, CO production, and minute volume) measurements were made.  For Apollo 15 to 17 missions, cardiac output measurements were obtained by the single-breath technique.  Arteriovenous oxygen differences were calculated from the measured oxygen consumption and cardiac output data. The data collected were voluminous and are summarized in tabular form by Rummel et al.  Dietlein has provided a concise synopsis of the findings.  In brief, reduced work capacity and oxygen consumption of significant degree was noted in 67% (18 of 27) of the Apollo crewmembers tested on recovery.  This decrement was transient, and 85% of those tested (23 of 27) returned to preflight baseline levels within 24–36 hours.  A significant decrement in cardiac stroke volume was associated with diminished exercise tolerance.  It was not clear whether the exercise decrement had its onset during flight.  If it did, the Apollo data did not reveal the precise in-flight time course because of lack of in-flight measurement capabilities.  The astronauts' performance on the lunar surface provided no reason to believe that any serious exercise tolerance decrement occurred during flight, except that related to lack of regular exercise and muscle disuse atrophy. The studies completed during Apollo, although less than optimal, left no doubt that a decrement in exercise tolerance occurred in the period immediately after landing, although it is believed that such decrements were not present during surface EVA.  It seems likely that multiple factors are responsible for the observed decrements.  Lack of sufficient exercise and development of muscle disuse atrophy probably contributed.  Catabolic tissue processes may have been accentuated by increased cortisol secretion as a consequence of mission stress and individual crew member reaction to such stress.  Additional factors associated with the return to Earth's gravity may also be implicated.  This, the observed diminished stroke volume (cardiac output) is certainly contributory and, in turn, is a reflection of diminished venous return and contracted effective circulating blood volume induced by spaceflight factors.  Skeletal muscle atrophy is mentioned with respect to its possible contribution to exercise intolerance, and in some of the later Apollo flights lower limb girth measurements were completed (data not published) that provided the first evidence for loss of muscle mass in the legs. The Skylab program (May 1973 - November 1974) was from the onset, intended to provide a life sciences laboratory in space.  A significant number of experiments were conducted to provide physiologic data from humans exposed to long-duration stays in a microgravity environment. A 56-day ground-based simulation of many of the Skylab experiments, conducted in an environmentally-controlled, enclosed chamber, was termed the Skylab Medical Experiments Altitude Test (SMEAT) and represented the first mission.  The three subsequent orbital missions were termed Skylab 2, 3 and 4.  These three long-duration mission were 28, 56 and 84 days in duration, respectively.  Collectively, the Skylab missions achieved a milestone in providing a vast array of human spaceflight biomedical information during missions of longer duration than any previous mission. With respect to the current issue of loss of muscle mass and function, two key studies were performed during the course of the three Skylab orbital missions.  First, leg and arm volumes were calculated by measuring the girth (circumference) of contiguous 3-centimeter arm and leg segments, treating all the segments as a short tapered cylinder, and then summing the segment volumes to obtain the volume of each extremity. The second study included the first muscle strength measurements by means of a dynamometer.  In addition to measurements relating directly to skeletal muscle strength and mass, indirect measurements were made that demonstrated that all Skylab crewmembers had a negative nitrogen balance indicative of skeletal muscle attrition.  This was also observed 10 years later in short-duration Space Shuttle crewmembers. Upper and lower limb volumes obtained on the three crewmembers of Skylab 4 are shown in figure 6-2.  Fluid shifts contributed the largest changes to lower limb volumes, but loss of leg tissue mass is clearly evident, particularly in the Commander.  As shown in the graphs, significant loss of leg volume occurs within the first few days of microgravity exposure while changes in the upper limbs are less remarkable.  Upon return to Earth, much of the loss of leg volume is corrected and there is often a short over-correction or overshoot.  Once this fluid shift resolves, the true loss of muscle mass remaining in the legs is revealed that more slowly returns to the baseline or preflight level (see figure 6-2, leg during recovery on right side of graph for all three crewmembers). In the Skylab 4 Commander, the loss in leg volume appears to be nearly 300 cc.  (figure 6-2, topmost graph).  Because the complement of exercise equipment for this mission was the largest (consisting of a cycle ergometer, passive treadmill, and the \"Mini gym\", modified commercial devices that provided the capability for low-load resistive exercises) losses in muscle mass and strength were less than in the previous two missions of shorter duration. During the Skylab program, exercises and exercise devices were added incrementally and the testing expanded with each mission.  This produced a different exercise environment for each flight so that in reality, there were three separate but related orbital experiments, each with N=3.  The results from each mission significantly affected the next. Preflight and postflight evaluation of muscle strength was performed on the right arm and leg of each crewmember for all three Skylab orbital missions by means of a Cybex isokinetic dynamometer.  The protocol completed on each crewmember included a thorough warm-up, and 10 maximum-effort full flexions and extensions of the arm at the elbow and of the hip and knee at an angular rate of 45° per second.  The isokinetic leg strength from all three missions, as well as body weights and leg volumes, are presented in Figure 6-3. On Skylab 2, only the bicycle ergometer was available for the in-flight exercise, with testing performed 18 days before launch and 5 days after landing.  While it was realized that these times were too temporally remote from the flight, this was the best that could be achieved due to schedule constraints.  By the time day 5 muscle testing was completed, some recovery in function had likely occurred; however, a marked decrement still remained.  The decrement in leg extensor strength was nearly 25%; the arms suffered less but also exhibited marked losses (data not shown).  The Commander's arm extensors showed no loss, since he used these muscles in hand-pedaling the bicycle, being the only Skylab crewmember to adopt this mode of arm exercise.  This illustrated a fundamental point in muscle conditioning: to maintain the strength of a muscle, it must be stressed to or near the level at which it will have to function.  Leg extensor muscles, important in standing and providing propulsive forces during walking, are capable of generating forces of hundreds of pounds, while the arm extensor forces are measured in tens of pounds.  Forces developed in pedaling a bicycle ergometer are typically tens of pounds and are thus incapable of maintaining leg strength.  The bicycle ergometer proved to be an excellent machine for aerobic exercise and cardiovascular conditioning, but it was not capable of developing either the type or level of forces needed to maintain strength for walking under 1G. Immediately after Skylab 2, work was started on devices to provide adequate exercise to arms, trunk, and legs.  A commercial device, termed \"Mini Gym\", was modified extensively and designated \"MK-I\".  Only exercises that primarily benefited arms and trunk were achievable with this device.  While forces transmitted to the legs were greater than those from the cycle ergometer, they were still limited to an inadequate level, since this level could not exceed the maximum strength of the arms, which represents a fraction of leg strength. A second device, designated \"MK-II\", consisted of a pair of handles between which up to five extension springs could be attached, allowing development of maximum forces of 25 pounds per foot.  These two devices were flown on Skylab 3, and in-flight nutrition support and exercise time and food were increased.  The crew performed many repetitions per day of their favorite maneuvers on the MK-I and to a lesser extent, on the MK-II.  Also, the average amount of work done on the bicycle ergometer was more than doubled on Skylab 3, with all crewmembers participating actively. It was perceived by Skylb life scientists that a device that allowed walking and running under forces equivalent to Earth gravity would provide more strenuous exercise.  Immediately after completion of Skylab 2, work was begun on a treadmill for Skylab 4.  As mission preparation progressed, the launch weight of Skylab 4 escalated so much that the final design of the treadmill was constrained by weight limitations.  The final weight for the device was a mere 3.5 pounds.  This passive device (figure 6-4) consisted of a Teflon-coated aluminum walking surface attached to the Skylab iso-grid floor.  Four rubber bungee cords provided an equivalent weight of about 80 kilograms (175 lbs) and were attached to a shoulder and waist harness worn by crewmembers during use.  By angling the bungee cords to that the user was pulled slightly forward, an equivalent to a slippery hill was created.  High loads were placed on some leg muscles, especially the calf, and fatigue was so rapid that the device could not be used for significant aerobic work because of the bungee/harness design.  It was absolutely necessary to wear socks and no shoes to provide a low-friction interface to the Teflon surface. On Skylab 4, the crew used the bicycle ergometer at essentially the same rate as on Skylab 3, as well as the MK-I and MK-II Mini Gym exercisers.  In addition, they typically performed 10 minutes per day of walking, jumping and jogging on the treadmill.  Food intake had again been increased. Upon their return to Earth and even before muscle testing, it was apparent that the Skylab 4 crewmembers were in very good physical condition.  They were able to stand and walk for long periods without apparent difficulty on the day after landing (R+1), in contrast to the crewmembers from the earlier two missions.  Results of strength testing confirmed a surprisingly small loss in leg strength even after nearly 3 months of microgravity exposure (figure 6-3).  In fact, knee extensor strength increased over the pre-flight level (figure 6-13). A variety of investigations related to skeletal muscle function have been completed during the course of the Space Shuttle Program (1981 - 2011).  The most comprehensive of these was a suite of investigations accomplished during the Extended Duration Orbiter Medical Project (EDOMP), which was carried out during 1989 - 1995 with missions of up to 16 days.  Studies most relevant to the risk on which this report focuses include the following: The collective specific aim of DSO 477 and DSO 617 was to evaluate functional changes in concentric and eccentric strength (peak torque) and endurance (fatigue index) of the trunk, arms, and legs of crewmembers before and after flight.  LIDO® dynamometer located at the Johnson Space Center and at both the prime and contingency landing sites were used to evaluate concentric and eccentric contractions before and after flight. Test subjects in this study exercised during flight for various durations, intensities and numbers of days on the original Shuttle treadmill (figure 6-5) (as opposed to the EDO treadmill, which flew on later Shuttle missions and was the bases for the ISS treadmill) as part of separate in-flight investigations.  Exercise protocols included continuous and interval training, with prescriptions varying from 60% to 85% of preflight VO as estimated from heart rate (HR) Some subjects had difficulty in achieving or maintaining their target HR during flight.  The brake (figure 6-5).  A harness and bungee/tether system was used to simulate body weight by providing forces equivalent to an approximate 1-G body mass.  Subjects on this non-motorized treadmill were required to walk and run at a positive percentage grade to overcome mechanical friction.  Study participants were familiarized with the LIDO® test protocol and procedures about 30 days before launch (L-30), after which six test sessions were conducted.  Three sessions were completed before launch (L-21, L-14 and L-8 days) and three after landing (R+0, R+2 and R+7 to R+10 days). The muscle groups tested are shown in table 6-1.  Torque and work data were extracted from force-position curves.  Peak-torque, total work, and fatigue index measured in the three preflight test sessions were compared; when no differences were found between sessions, values from the three preflight sessions were averaged and this average was used to compare preflight values with those on landing day and during the postflight period. Skeletal-muscle strength was defined as the peak torque generated throughout a range of motion from three consecutive voluntary contractions for flexion and extension.  Eccentric contractions are actions of the muscle in which force is generated while the muscle is lengthening, as opposed to the concentric actions in which the muscle is shortening (contracting) while generating force.  Skeletal-muscle endurance was defined as the total work generated during 25 repetitions of concentric knee exercise, as determined from the area under the torque curve for a complete exercise set.  Work also was compared between the first 8 and last 8 repetitions.  Endurance parameters were measured during concentric knee flexion and extension activity only.  On R+0, significant decreases in concentric and eccentric strength were shown in the back and abdomen when compared to the preflight means (table 6-1). Concentric back extension and eccentric dorsiflexion remained significantly less than preflight values on R+7.  Recovery (an increase in peak torque from R+0 to R+7) was demonstrated for the eccentric abdomen and the concentric and eccentric back extensors. However, the data depicted in table 6-1 may be somewhat misleading because in some cases there were tremendous differences in strength between crewmembers who exercised during flight versus those who did not.  For example, some crewmembers who exercised during flight actually gained in isokinetically measured strength in the ankle extensor/flexor muscles (anterior versus posterior calf muscles, that is \"m. tibialis anterior\" versus the gastrocnemius/soleus complex) compared to crewmembers who did not exercise and who actually showed a decrease in isokinetically measured strength in these muscles (figure 6-6). With respect to endurance, a majority of the decrease in the total quadriceps work occurred on R+0.  This likely reflects significant loss in the first third of the exercise bout (-11%).  The declines in peak torque at the faster endurance test velocities are consistent with changes seen at the slower angular velocity used during the strength tests.  Torque for the quadriceps at 75° per second was 15% less than preflight values but for the hamstrings was 12% less than the preflight mean at 60° per second.  Endurance data showed little difference between preflight and R+7 tests, suggesting that crewmembers had returned to baseline by 1 week after landing. Additionally, subjects who did exercise during flight compared to those who did not had significantly greater (\"p\" < 0.05) losses within 5 hours of landing in concentric strength of the back, concentric and eccentric strength of the quadriceps (30° per second), and eccentric strength of the hamstrings, relative to the respective preflight values (data not shown here).  According to Greenisen et al., non-exercisers also had significantly less concentric strength of the quadriceps at 75° per second and lower total work extension, work first-third flexion, and work last-third extension, immediately after landing, than before flight.  The conclusions reached by the investigators were that the data indicate that muscles are less able to maintain endurance and resist fatigue after spaceflight, and that exercise may avert decrements in these aspects of endurance. Conversely, crewmembers who exercised during flight had greater losses in trunk muscles strength as measured at landing than did the non-exercising group (figure 6-7).  However, preflight strength in trunk flexion and extension was substantially greater in the exercising group than in the non-exercising group.  Apparently treadmill exercise did not prevent decrements in trunk strength after 9 to 11 days of spaceflight, and the investigators proffered the explanation that preservation of muscle function may be limited only to those muscles that are effectively used as part of the exercise regimen. The specific aim of DSO 475, \"Direct Assessment of Muscle Atrophy Before and After Short Spaceflight\" was to define the morphologic and biochemical effects of spaceflight on skeletal fibers.  To obtain myofiber biomechanical and morphological data from Space Shuttle crewmembers, biopsies were conducted once before flight (L - > 21 days) and again on landing day (R+0).  The subjects were eight crewmembers, three from a 5-day mission and five from an 11-day mission.  Biopsies of the mid-portion of the \"m. vastus lateralis\" were obtained by means of a 6-mm biopsy needle with suction assist.  A one-tailed paired \"t\"-test was used to identify significant differences (\"p\" < 0.05) between the mean values of fiber cross-sectional area (CSA), fiber distribution, and number of capillaries of all crewmembers before flight and the mean values for all crewmembers after flight. According to this report, CSA of slow-twitch (Type I) fibers in postflight biopsies was 15% less than in preflight biopsies; the CSA of fast-twitch (Type II) fibers was 22% less after flight than before (figure 6-8).  Mean values did not reflect the considerable variation seen in the biopsies from the eight astronauts who participated.  At least some of this variation likely resulted from differences in the types and quantities of preflight and in-flight countermeasures (exercise or LBNP) used by the different crewmembers.  The relative proportions of Type I and Type II fibers were different before and after the 11 day mission: the fiber distribution also seemed to follow the same trend after the 5 day mission (more Type II and fewer Type I fibers after than before), but the sample size was too small to reach statistical significance.  The number of capillaries per fiber was significantly reduced after 11 days of spaceflight. However, since the mean fiber size was also reduced, the number of capillaries per unit of CSA of skeletal muscle tissue remained the same. The purpose of DSO 606, \"Quantifying Skeletal Muscle SIze by Magnetic Resonance Imaging (MRI),\" was to non-invasively quantify changes in size, water, and lipid composition in antigravity (leg) muscles after spaceflight.  This experiment was the first attempt to measure limb volumes before and after flight since the less sophisticated methods of measuring limb girths during Apollo and SKylab programs were used.  The subjects included a total of eight Space Shuttle crewmembers, five from a 7-day flight and three from a 9-day flight.  All subjects completed one preflight and two postflight tests on either L-30 or L-16 and on R+2 and R+7.  Testing involved obtaining an MRI scan of the leg (soleus and gastrocnemius) at The University of Texas - Houston Health Science Center, Hermann Hospital.  Multi-slice axial images of the leg were obtained to identify and locate various muscle groups.  Changes in water and lipid content were measured, in addition to CSA, to distinguish changes in fluid versus tissue volumes.  Multiple slices were measured by computerized planimetry. CSA and volume of the total leg compartment, soleus, and gastrocnemius were evaluated to assess the degree of skeletal muscle atrophy.  The volumes of all 3 compartments were significantly smaller (\"p\" < 0.05) after both the 7 and 9 day Shuttle flights than they were before flight.  Volume decreased by 5.8% in the soleus, 4.0% in the gastrocnemius, and 4.3% in the total compartment.  These losses were stated to represent the true level of skeletal muscle tissue atrophy and not changes associated with fluid shifts.  No recovery was apparent by 7 days after landing (data not shown).  This was an interesting finding and certainly indicates that the losses were not due to fluid shifts, but the delay in recovery after these rather short flights is contrary to what was observed and documented during the Skylab program of flights much longer in duration, albeit by less sophisticated methods during Skylab. The Space Shuttle Program and, in particular, EDOMP has provided a great deal of knowledge about the effects of spaceflight on human physiology and specifically on alterations in skeletal muscle mass, strength, and function.  Once again, losses of skeletal muscle mass, strength, and endurance were documented, in some cases in spite of exercise countermeasures.  But some findings were encouraging, particularly indications that in-flight exercise does have a positive effect in countering losses in muscle strength at least in the legs (see table 6-1 and figure 6-6), as predicted from the results of the 84-day Skylab 4 mission when multiple modesof exercise were used including a unique \"treadmill\" device (see figure 6-4).  This unusual treadmill provided loads of sufficient magnitude to the legs in a fashion approaching resistance exercise.  However, the data provided by MRI volume studies indicate that not all crewmembers, despite utilization of various exercise countermeasures, escape the loss in muscle mass that has been documented during most of the history of U.S. human spaceflight since Project Mercury.  This, additional research is needed to continue the development of countermeasures and equipment that will eventually provide a successful solution for all human space travelers. During the seven NASA-Mir flights, seven U.S. astronauts trained and flew jointly with 12 Russian cosmonauts over a total period of 977 days (the average stay was 140 days) of spaceflight, which occurred during the period from March 1995 to June 1998.  The major contribution of the joint U.S./Russian effort on the Mir space station relevant to the current risk topic was the first use of MRI to investigate volume changes in the skeletal muscles of astronauts and cosmonauts exposed to long-duration spaceflight.  This began with the first joint mission, Mir-18, and continued until the final Mir-25 mission.  The data indicated that loss of muscle volume, particularly in the legs and back, was greater than with short-duration spaceflight but not as great as the data from short-duration flight might have predicted.  A comparison between volume losses in the selected muscle groups in short-duration spaceflight on the Space Shuttle,long-duration (119 d) bed rest, and a (115 d) Shuttle-Mir mission demonstrates the relative time course of the losses (figure 6-9). There is good correlation between long-duration bed rest and spaceflight of similar duration except that losses in the back muscles are much less with bed rest.  This likely reflects use of these muscles during bed rest to adjust body position and to reduce the potential for vascular compression and tissue injury.  During spaceflight the back muscles are apparently less used because they do not have to support the upright body against Earth gravity and are not used with great force to make positional adjustments of the body as they are during the recumbency of bed rest. The International Space Station’s (ISS) first crew (Expedition 1) arrived in October 2000; since then there have been 15 additional Increments.  The data presented here were collected during the first 11 of the ISS Expeditions. The complexities and shortcomings of collecting scientific data from a laboratory orbiting more than 300 miles above the Earth and completing 18 orbits per day at a speed of more than 17,000 mph with discontinuous voice and data communications, combined with the constraints and limitations of up mass, crew time, and on-board logistics, cannot be overstated. Another problem was exercise hardware that was built and launched but failed to meet science requirements.  (The Resistive Exercise Device [RED] science requirement was to provide a load of up to an equivalent of 600 lbs., but the interim RED [iRED] provides only half of that amount.  Ground-based studies have shown that it does produce a positive training effect similar to equivalent free weights when used in a high-intensity program, but it will likely not provide sufficient load in a zero-gravity environment to prevent loss of muscle and bone tissue, as determined from parabolic flight studies.)  Other problems were failure at one time or another of each piece of onboard exercise hardware with reduced utilization at other times, and other limitations imposed because transmission of forces to the space frame have confounded inflight exercise sessions.  In fact, during the first eleven ISS Expeditions, only for 2 short periods during Expeditions 3 and 4 were all three U.S. onboard exercise devices (Cycle Egometer with Vibration Isolation System [CEVIS], Treadmill with Vibration Isolation System [TVIS], and iRED) capable of being used under nominal conditions (Figure 6-10).  The almost continuously suboptimal availability of exercise equipment likely has reduced maintenance of crew physical fitness. Despite these shortcomings, lean tissue mass data collected by means of dual-energy x-ray absorptiometry (DEXA) before and after flight compares favorably with data from NASAMir, and the total body and leg losses are in fact less than seen during NASA-Mir or during three separate bed rest studies of similar durations in the range of 20-170 d (Figure 6-11).  However, the news is not entirely good since knee extensor and knee flexor strength losses in long-duration crewmembers after flights aboard Mir and ISS were ~23% and ~25%, respectively (Figure 6-12), indicating that strength losses in the quadriceps and hamstring muscle groups were significant and similar for NASA-Mir and early ISS missions, despite apparent slightly increased preservation of muscle mass (lean tissue) in the legs of ISS crewmembers compared to crewmembers on NASA-Mir missions (also Figure 6-11).  These near equivalent losses occurred in spite of iRED being present on the ISS.  Unfortunately, MRI data collected by Fitts and colleagues to assess skeletal muscle volumes in ISS crewmembers are not yet available to allow comparison with those from NASA-Mir.  With respect to endurance, the following comparison (Figure 6-13) shows a trend for improved maintenance of muscle endurance on ISS with respect to NASA-Mir although the loss of endurance on ISS was greater than that documented during short-duration Space Shuttle missions (for ISS, n = 2). ISS crewmembers, under the supervision of their crew surgeons, participate in a postflight exercise program implemented by certified trainers who comprise the Astronaut Strength, Conditioning and Rehabilitation (ASCR) group at Johnson Space Center.  A portion of this program includes physical fitness testing on an individual basis.  The results of these “functional” tests, which consist of six exercises, reveal that crewmembers return with less physical capability than when they launch but that most of the decrements are reversed by postflight day 30 secondary to the ground-based exercises the crewmembers complete in the days after their return to Earth (Figures 14 and 15). In this section, only the historical highlights of some highly relevant skeletal muscle investigations have been included and discussed.  A complete treatment of all data would cover several volumes.  However, from this brief historical overview it is possible to see how initial indications of losses in skeletal muscle function led to attempts to provide exercise countermeasures.  Such countermeasures were utilized during spaceflight, crewmembers were tested upon return, and exercise regimens and equipment were modified for use in future missions.  In the subsequent sections, human spaceflight and ground-based analog studies and experimental animal studies are reviewed that contribute to the evidence base on the alterations in skeletal muscle form and function that occur with the muscle unloading associated with the microgravity environment.  It is this knowledge base on which future operational countermeasures and investigations into the fundamental changes in muscle physiology will be based. The responses of the human body to microgravity exposure during spaceflight involve adaptations at numerous levels.  It is believed that skeletal muscle adaptations to microgravity, which affect both muscle mass and function, involve structural alterations in the neural as well as the myofibrillar components of skeletal muscle.  It is well accepted that the muscles involved in the maintenance of an upright position in terrestrial gravity (the antigravity muscles) are the most susceptible to spaceflight-induced adaptations.  This susceptibility may reflect the almost continuous levels of self-generated (active) and environmentally generated (reactive) mechanical loading to which these muscles are exposed under normal Earth gravity.  Thus, effects related to the decrease in the level of mechanical loading that occurs during microgravity exposure logically would be reflected most acutely in these muscles.  Changes at the structural level within skeletal muscle after spaceflight are paralleled by spaceflight-induced changes at the functional level such as decreased muscle strength and increased muscle fatigability.  This summary addresses nearly exclusively those investigations in which the effects of mechanical unloading on antigravity muscles were examined, and the consequent tissue remodeling at the structural and biochemical levels.  Additionally, the relative success of various countermeasures is examined. Decreases in skeletal muscle size and function have been reported since humans first began to explore space.  Spaceflight results in the loss of lean body mass as determined by body composition measurements.  Urinary amino acid and nitrogen excretion, both indirect measures of catabolism of lean body mass, are elevated during both brief and long spaceflights.  Direct measurement of protein synthesis during spaceflight using 15N-glycine incorporation as a marker revealed an increase in whole-body protein synthesis rates.  These results indicated that the significant decrease in lean body mass observed after spaceflight must be associated with a significant increase in protein degradation rates rather than an inhibition of protein synthesis.  Decreases in lower-limb muscle circumference and calculated muscle volumes were detected in Apollo and Spacelab astronauts.  Decreases in muscle strength, circumference, and tone have also been reported in cosmonauts.  More recently, these findings have been confirmed by direct volume measurements (by magnetic resonance imaging [MRI] of astronauts on the Space Shuttle and of Russian cosmonauts and U.S. astronauts after tours of duty on the Mir space station. Changes in lean body mass and muscle volume are paralleled by a concomitant decrease in myofiber cross-sectional area (CSA).  To date, preflight and postflight muscle biopsy samples have been obtained from only a few crewmembers.  In U.S. studies, muscle biopsies were obtained before and after flight from the m. vastus lateralis of 8 astronauts after 5- and 11-day missions.  Notably, postflight muscle sampling was carried out within 2–3 hours of landing, which minimized the effects of reambulation on the muscle.  Analysis of the muscle biopsy samples with a variety of morphologic, histochemical, and biochemical techniques indicated that the myofiber CSA was significantly decreased after spaceflight; that atrophy was greatest in Type IIB myofibers, followed by Type IIA and then Type I myofibers; that expression of Type II myosin heavy chain (MHC) protein was significantly increased, with an apparent decrease in the amount of Type I MHC protein expressed; and that the number of myonuclei per mm of myofiber length was significantly decreased in Type II myofibers after 11 days of spaceflight.  In contrast to these findings, analysis of needle biopsy samples from cosmonauts, conducted by the Institute for Biomedical Problems after 76- and 180-day flights, indicated a large degree of individual variation in the extent of myofiber atrophy, with the decrease in myofiber CSA ranging from about 4% to 20%.  This variation was attributed to variations in compliance with exercise countermeasures by individual cosmonauts during the flights. More recent muscle biopsy studies have indicated that despite consistent decreases in myofiber CSA in the m. soleus and m. gastrocnemius after spaceflight, MHC expression does not seem to shift, as was previously described by Zhou et al.  This discrepancy may reflect the effects of exercise countermeasure protocols carried out by the astronauts during the later flight and the examination of muscles different from those studied in the earlier flight (gastrocnemius and soleus vs. vastus lateralis). Decrements in the aerobic capacity of crewmembers after spaceflight, coupled with a reduction in muscle oxidative capacity, indicate that the vascular supply to skeletal muscle may also be affected by spaceflight.  However, at present no consistent relationship is apparent between the degree of muscle atrophy (measured by MRI or myofiber CSA determination after muscle biopsy) and the reported changes in muscle strength and function, although typically loss in muscle strength exceeds the loss in muscle volume.  The reasons for these counter-intuitive results are unclear and will probably remain so until resources become available for long-term, on-orbit study of the skeletal muscle atrophic response to spaceflight. In addition to the effects of spaceflight on the myofibrillar component of skeletal muscle, the role of the neural components of skeletal muscle atrophy must not be understated.  A functional disruption of neuronal control at the neuromuscular level, which seems to be paralleled by a reduction in the overall electrical activity of the muscle after spaceflight, raises the possibility that neuron-derived factors that play a role in the growth or maintenance of skeletal muscle may be disrupted.  The hypothesis that microgravity causes a fundamental alteration in motor control has also been suggested.  Studies conducted at JSC by the Exercise Physiology Laboratory showed that two-legged muscle power declines considerably more than can be explained by the loss in muscle mass alone.  Additionally, the loss of explosive leg power was associated with a substantial reduction in the electromyography (EMG) activity of the m. rectus femoris, m. vastus lateralis, and m. vastus medialis.  These investigators concluded that microgravity induced a basic change in motor control and coordination such that motor activation of extensor muscles was reduced.  Similar observations have been made after long-duration spaceflight on Mir and ISS. Evidence exists that exercise strategies are effective in attenuating muscle strength loss in bed rest.  Bamman et al. preserved pre-bed rest muscle strength of the thigh and calf in subjects who performed resistive exercise with loads equivalent to 80-85% of their pre-bed rest strength (1-RM).  Protection of muscle volume occurred through the maintenance of protein synthesis, which also likely influenced muscle strength.  Similarly, Akima et al. were able to maintain isometric peak torque in subjects who performed daily maximal isometric contractions of the knee extensors during 20 days of bed rest.  Using an aggressive resistive exercise training protocol, Shackelford et al. preserved isokinetic muscle strength and observed substantial increases in isotonic muscle strength over the course of 89 days of bed rest in exercising subjects.  Using a flywheel resistive exercise device, Alkner and Tesch prevented the loss of muscle mass and strength in the thigh and attenuated the losses in the calf. The similarity in skeletal muscle responses during spaceflight and bed rest were elegantly demonstrated by Trappe and colleagues in a combined 17-day spaceflight study of 4 crewmembers and a 17-day bed rest study of 8 test subjects.  In all of these subjects, assessment of muscle fiber size, composition, and in vivo contractile characteristics of the calf muscle were completed.  Protocols and timelines for the two studies were identical, which allowed direct comparisons between a spaceflight and a bed rest study of equivalent duration.  Calf muscle strength was measured before and on days 2, 8, and 12 of spaceflight and bed rest as well as on days 2 and 8 after spaceflight and bed rest in the two investigations.  Muscle biopsies were obtained before and within 3 hours after spaceflight (m. gastrocnemius and m. soleus) and bed rest (m. soleus) just before reloading.  After 17 days of spaceflight or bed rest, no significant measurable changes occurred in maximal isometric calf strength, force-velocity characteristics, myofiber composition, or volume in the calf muscles studied.  Since loss of skeletal muscle strength is an expected finding in both spaceflight and bed rest, the investigators concluded that the testing protocol utilized during both studies must have provided sufficient resistance exercise to prevent losses in muscle strength and changes in morphology. Some general conclusions that can be drawn from the data gathered from astronaut/cosmonaut studies are as follows.  First, loss of muscle mass is most prevalent in the antigravity muscles such as the soleus; second, the atrophic response to short-term spaceflight does not seem to be specific to myofiber type; and third, myosin heavy chain (MHC) isoform expression does not seem to shift from Type I MHC to Type II during short (< 18-day) spaceflights. Several ground-based paradigms have been used to emulate the effects of microgravity unloading on human skeletal muscle, including complete horizontal or 6° head-down-tilt bed rest, dry immersion, and unilateral upper- and lower-limb unloading with or without joint immobilization.  In general, skeletal muscle responses to unloading have been similar in all of these models.  Although no perfect simulation of crew activities and the microgravity environment can be adequately achieved, Adams and colleagues have suggested that bed rest is an appropriate model of spaceflight for studying skeletal muscle physiologic adaptations and countermeasures. Bed rest unloading causes a significant loss of body nitrogen and lean body mass.  A reduction in the size or volume of the ambulatory muscles accounts for most of the decrease in lean body mass after bed rest.  This decrease correlates with a significant reduction in muscle protein synthesis.  Horizontal and 6° head-down-tilt bed rest protocols of various durations (7 days, 14 days, 30 days, 5 weeks, or 17 weeks) have resulted in significant reductions in lower-limb muscle volume as measured by MRI, ranging from a 30% loss in the ankle extensor muscles to a 12% loss in the plantar flexors (gastrocnemius and soleus).  Decreases in muscle volume after bed rest were paralleled by decreases in muscle strength and endurance, as evidenced by significant decreases in angle-specific torque, isokinetic muscle strength, and fatigability.  Similar losses in muscle volume, paralleled by decreases in muscle strength and endurance, have been observed after unilateral lower-limb suspension.  Dry immersion, a whole-body-unloading paradigm with the added advantage of mimicking the reduced proprioceptive input encountered during spaceflight, also brings about reductions in muscle volume, strength, endurance, electrical activity, and tone. At the structural level, the loss of muscle volume in these models correlates with a significant decrease in CSA of both Type I and Type II myofibers.  In general, Type II myofibers seem to be more likely to atrophy than do Type I myofibers during short-term unloading, with no significant myofiber type shifting being observed, although alterations in total muscle MHC protein isoform expression have been reported.  However, prolonged bed rest (greater than 80 days) does significantly change the number of MHC hybrid fibers observed in the soleus muscle.  Immobilization by limb casting does not seem to reduce the relative proportions of muscle-specific proteins, such as carbonic anhydrase II and myoglobin, over that predicted by the overall decrease in muscle protein synthesis.  In contrast, experimental evidence suggests that the specific activity of muscle enzymes involved in oxidative metabolism, such as pyruvate dehydrogenase, is decreased by cast immobilization.  A similar reduction in the activity of citrate synthase, but not phosphofructokinase, has been detected in the vastus lateralis, indicating a significant impairment of the oxidative capacity in this muscle after unilateral limb suspension.  The differences observed between cast immobilization and unilateral limb suspension or bed rest protocols may reflect the former being a better model of muscle atrophy induced by hypokinesia and the latter two being better models of muscle atrophy induced by muscle hypodynamia.  The latter situation more closely resembles the actual conditions experienced by crewmembers during spaceflight, namely removal of mechanical loading without a reduction in limb mobility.  However, it is apparent that although ground-based unloading models are useful in studying the effects of microgravity on skeletal muscle, no single terrestrial model system produces all the physiological adaptations in skeletal muscle observed as a consequence of spaceflight.  Absent from human analog studies are the unique operational and psychological stressors associated with spaceflight that exacerbate the physiological changes resulting from muscle unloading. Again, the decreases in muscle volume and myofiber CSA observed in these ground-based analogs of spaceflight bring about changes in the neuronal-activation patterns of the unloaded muscles, including decreased electrically evoked maximal force, reduced maximal integrated electromyography, and neuromuscular junction dysfunction.  Certainly such decreases in the neural drive in unloaded muscle play a role in the atrophic response. As in spaceflight, adaptations to unloading can be observed after short-duration bed rest.  For example, after 20 d of bed rest, volume of quadriceps muscle decreased by 8%, hamstrings decreased by 10%, and plantar flexor muscles were reduced by 14%.  During a longer, 89-d bed rest, greater reductions in muscle volume in the quadriceps (-15%), hamstrings (-13%), soleus (-29%), and gastrocnemius (-28%) were reported.  In a 90-day bed rest trial, a 26% ± 7 decline in the CSA of the calf muscle was observed.  This rate of decline is consistent with earlier measurements in which after 90 days of bed rest, a roughly 15% decline in quadriceps and hamstring muscle volume measured by MRI scans were noted in two subjects.  Reductions in muscle strength were also demonstrated in these studies. Bamman and colleagues observed losses of 18, 17, and 13% in concentric, eccentric, and isometric plantar flexor peak torque, respectively, after 14 d of bed rest, and Akima and his co-investigators observed a 16% decrease in knee extensor isometric torque after 20 days of bed rest.  Although not specifically reported, subjects in an 89-day bed rest trial experienced significant reductions in isokinetic torque in the lower body, with the greatest losses in the knee extensors (-35%).  This study also used isotonic testing (1RM), and mean losses ranging from -6 to -37% were observed; reductions in adductor, abductor, and leg press strength were on the order of ~25-30%.  In an earlier 90-day bed rest trial, LeBlanc and colleagues observed losses of 31% in knee extension strength and 15% in knee flexion strength.  Few studies have reported changes in the ab/adductor or the flexor/extensor muscles of the hip.  Shackelford et al. reported that isotonic strength decreased by about 25% in the adductors, but only a 6% decrease in the hip flexors was demonstrated after 17 weeks of bed rest.  After 55 days of bed rest, Berg et al. reported that a 22% reduction in isometric hip extension occurred, although the extensor muscles in the gluteal region decreased in volume by only 2%.  The authors reported no explanation for this discrepancy between the proportion of reduced strength relative to the loss of mass, and also stated that no previous studies in the literature had made these concurrent strength/volume measurements in the hip musculature. Some general conclusions that can be drawn from the above human studies are as follows.  First, terrestrial unloading models produce selective atrophy in the muscles of the lower limbs, especially the anti-gravity muscles; second, this response is greater in the extensor muscles than in the flexor muscles; third, muscle atrophy occurs quickly (within 7–14 days) in response to unloading; fourth, loss of muscle mass is paralleled by decrements in muscle strength and endurance, but strength losses typically are greater than volume losses; fifth, if atrophy is specific to a myofiber type within these muscles, it seems to be Type II myofibers; and sixth, terrestrial unloading does not seem to produce a slow-to-fast shift in absolute myofiber characteristics but does alter the expression of MHC isoforms in human muscle so that an increase in MHC hybrid myofibers is observed, resulting in a faster phenotype. Other research findings exist that relate peripherally to this risk description that should remain associated with it.  The physical inactivity and muscle unloading occurring in association with spaceflight can result in a decrease in muscle mass, which in turn may be associated with an increased susceptibility to insulin resistance (glucose intolerance).  While this association is quite clearly documented in bed rest studies, the association is not yet solidified for spaceflight.  Additionally, the major countermeasure to muscle atrophy is exercise, and it should be appreciated that crewmembers chronically exposed to the microgravity environment may develop impaired body temperature regulation during rest and exercise that may lead to heat strain and injury.  These are discussed more fully in the paragraphs that follow. After short-duration spaceflights, Soviet cosmonauts were observed to have elevated serum insulin levels that persisted up to 7 d after landing.  In the first 28 U.S. Space Shuttle flights (2-11 d duration), serum insulin levels (n = 129) were elevated by 55% on landing day compared to before flight.  Russian space life science investigators reported two-fold or greater increases in insulin levels in three cosmonauts within 1 day after they returned from a 237-d flight.  The associated finding of elevations in both insulin and blood glucose (12% on landing day compared to preflight levels in 129 Space Shuttle crewmembers on flights of 2-11 d duration) may indicate an acquired decreased tissue sensitivity to insulin associated with spaceflight.  Ground-based bed rest studies simulating weightlessness in humans have shown an increased insulin response to glucose tolerance tests.  In such studies, plasma insulin levels have increased up to four-fold compared to those of control subjects, and blood glucose levels exceeded those of the controls 2 h after glucose loading.  In a well-designed 7-d bed rest study, insulin action on both whole-body glucose uptake rate and leg glucose uptake rate was investigated.  It was concluded that the inactive muscle of bed rested subjects was less sensitive to circulating insulin.  However, in a study of four Space Shuttle astronauts by the same investigators, in which glucose tolerance tests were performed 15 d before launch, on flight day 7, and on postflight days 2 and 15, increases in the concentrations of insulin, glucose, and Cpeptide in in-flight samples were observed, but the changes were not significantly different from the preflight and postflight values.  The investigators concluded that 7 d of spaceflight did not confirm the assumption that microgravity exposure leads to impaired glucose tolerance.  However, the brief (7 d) exposure to microgravity may have been insufficient in duration to induce statistically significant changes, and thus additional studies on crewmembers from long duration missions are needed to confirm these findings. Human expenditure of energy results in the generation of heat.  The body heat generated by normal activities, and particularly by exercise, triggers homeostatic regulatory mechanisms with the goal of maintaining body core temperature within its relatively narrow, safe physiologic range by means of vasoregulation and diaphoresis.  The weightlessness environment of spaceflight may impair heat dissipation by reducing evaporative and conductive heat exchange.  Microgravity and spaceflight may perturb the body’s thermoregulatory mechanisms by altering the work efficiency, metabolic rate, or circadian rhythms of heat production.  Additionally, human space travelers are often not well hydrated, have a 10-15% decrease in intravascular fluid (plasma) volume, and may lose both their preflight muscular and cardiovascular fitness levels as well as their thermoregulatory capabilities.  As a result, they may become less heat-acclimated or may acquire an altered thermal sensitivity. Alterations in thermoregulation in association with spaceflight could significantly affect a variety of spaceflight-associated activities including exercise as a countermeasure to muscle atrophy, cardiac deconditioning, and bone loss; extravehicular activity (EVA); and vehicle landing and egress.  EVA suits and launch and entry or advanced crew escape suits (ACES) worn by ISS and Shuttle crewmembers are designed to provide an impermeable barrier between the wearer and the external environment.  To compensate for lack of heat exchange through the fabrics of these suits, the EVA suit provides both liquid (conductive) and air (convective) cooling, while a liquid cooling garment is worn under the ACES in addition to a hose connection to forced orbiter cabin air.  Thus, crewmembers with altered thermoregulatory capabilities are at even greater risk should failure of the cooling systems of these garments occur.  Manifestations of altered thermoregulation include increased heart rate and body temperature during exercise, decreased work capacity and endurance, decreased postflight orthostatic tolerance, decreased cognitive ability, and a delay in recovery of exercise capacity and endurance after flight. Thermoregulation has been studied in association with both spaceflight and 6° head-down-tilt bed rest.  To date, there have been no direct measurements of heat balance during in-flight exercise sessions.  In the only spaceflight study, submaximal exercise and thermoregulatory responses were recorded before flight and at 5 d after landing in two crewmembers who completed a 115-d mission.  Normal heart rates were observed for both crewmembers during supine exercise for 20 min each at 20% and 65% of VO2max.  However, during postflight (five days after landing) testing, exercise was voluntarily discontinued after only 8-9 min of supine exercise at the 65% of VO2max level for the two crewmembers when they both experienced difficulty in maintaining pedaling frequency and complained of leg fatigue, and their heart rates exceeded the highest recorded preflight levels.  Both crewmembers exhibited a more rapid increase in body core temperature during the shorter postflight exercise session than during the preflight session; it was concluded that heat production was not altered but that impairment of heat dissipation due to altered vasodilatory and sweating responses were responsible for the increased rate of rise in the core body temperature. Adequate energy (caloric) intake is a necessary requirement for humans living and working in space, and much attention has been focused on this requirement.  Less effort has been spent on understanding how the caloric heat generated by energy expenditure is handled by humans whose physiologic responses to heat may be altered in the unique physical environment of spaceflight.  Such studies should be considered at a higher level of priority for future human space missions.  Recently applied models may be of use in providing a better understanding of the magnitude of this associated risk. This section summarizes the studies that have been conducted on animal subjects (such as rodents and non-human primates) that have been exposed either to spaceflight or (in the case of rodents) to the well accepted ground-based analog of hind-limb suspension (HS) to ascertain the effects of unloading states on the properties of muscle mass, strength, and endurance.  The results presented herein overwhelmingly corroborate the body of evidence that has been reported on human subjects in the preceding sections of this report.  Importantly, through the use of more cellular and molecular analyses, greater insights have been obtained into the underlying mechanisms associated with these alterations in muscle structure and function.  Since the majority of evidence concerning the effects of spaceflight on mammalian skeletal muscle has been derived from rodent studies, the information provided here is focused mostly on the rodent model.  It is important to point out that the structure and function of rodent skeletal muscle are nearly identical to those of human skeletal muscle.  For example, rodent muscle is composed of the same general fiber-type profile and is sensitive to the same environmental (mechanical, hormonal, metabolic) cues observed for human muscle.  Thus, the information summarized below provides credence to the data base derived from human subjects.  However, it is important to point out that one primary advantage of the rodent model is that adaptive changes that occur in both species unfold in a much shorter time frame in rodents than in humans (hours to days versus days to weeks), making it possible to predict long-term changes in human skeletal muscle based on the shorter absolute time frame of the studies performed on rodents.  Another important consideration in the context of animal research during spaceflight is that one can perform a straightforward experiment in which there is no requirement to provide some type of countermeasure intervention as there is for humans, and can thereby avoid the introduction of a confounding variable in ascertaining the true effects of spaceflight on a wide range of physiological variables.  Also, given the remarkable agreement in the quantitative and qualitative nature of the findings observed in the spaceflight studies versus those obtained from ground-based HS studies, we have chosen to combine and integrate significant portions of the data that have been gathered in the last 25 years.  This rodent data base in space life sciences research includes 14 flight experiments with 8 sponsored by the Russian Cosmos Program and 6 sponsored by NASA Space Life Sciences (SLS) and Space Transportation System (STS) missions.  These flight experiments are complemented by numerous ground-based research studies that focused collectively on the topics described below.  Most importantly, all of the data reported in this summary are derived from animal cohorts in which the control animals were studied from a synchronous vivarium group of the same age, strain, and gender, and the analyses were performed at the same time as that of the experimental groups.  The provided information is based entirely on peer-reviewed experiments as detailed in the bibliography provided. While recorded observations during spaceflight are less extensive in rodents (due to fewer flight missions with opportunities for astronauts or payload specialists to observe them), the available data suggest that rodents rely less on the hindlimbs for executing most movement patterns (as is the case for humans).  During spaceflight, their ankles appear to assume a plantar flexed position that may reduce the passive tension (force) imposed on the triceps surae group, of which the anti-gravity slow-twitch soleus muscle is a chief component.  A similar posture has been observed in the ground-based analog of HS.  This posture is thought to affect the residual tension placed on this muscle group in the absence of a normal weight-bearing state, that is, the ankle plantar flexor muscle group becomes truly unloaded.  While electromyographic studies on adult rodents have not been conducted during spaceflight, studies performed on rodents during chronic HS indicate that only a transient reduction occurs in electrical activity of the ankle plantar flexor muscles (soleus and medial gastrocnemius).  This pattern of activity is consistent with the posture of the muscle and the maintenance of muscle mass during the 28-day time frame of the experiment.  That is, the EMG activity was well maintained, while the ongoing atrophy was maintained.  These findings reinforce the notion that it is the mechanical activity rather than the electrical activity imposed on the muscle that is essential to maintaining physiological homeostasis. When animals return from spaceflight of even short duration (days), their basic activity patterns are altered.  The center of gravity in rats is much lower than normal.  They no longer support their body weight and initiate movement off the balls of their feet, and the ankle joint assumes an exaggerated dorsiflexed position.  Movement for most voluntary activities is much slower and more deliberate (the animals cover smaller distances per unit time), and the animals spend significantly less time in bipedal stances.  Furthermore, the rodents use their tails for basic support to a greater degree, based on observations by the investigators.  Thus, rodent motor skills and basic locomotor capability have less fidelity and capacity during posture maintenance and locomotion during the early stages of recovery; however, by 9 days after flight the activity properties return to those seen in normal conditions. Considerable information has accumulated covering a large number of spaceflight and HS experiments that span a time frame of ~4 to 22 days for spaceflight and from 1 to 56 days for HS.  These experiments have primarily focused on extensor muscles used extensively for postural support and locomotor activity.  The review by Roy, Baldwin, and Edgerton provides one of the most comprehensive reviews on rodents in the space environment.  Additional reviews on this topic have been published.  The collective observations clearly show that these types of muscle undergo significant reductions in muscle mass (muscle weight) along with a concomitant loss in total protein and myofibrillar (the fraction that is composed of the contractile machinery of structural proteins) protein content of the targeted muscles.  In some experiments, it has been reported that the myofibrillar fraction can be degraded to a greater extent than other muscle fractions.  The general pattern demonstrates that a rapid loss in muscle weight and net total and myofibrillar protein content (concentration (mg/g X muscle weight) occurs during the first 7–10 days of unloading and this is followed by a more gradual loss in these constituents.  The net result is that between 25 and 46% of the muscle mass can be lost in antigravity muscles of the lower extremity such as the soleus (Sol; a calf muscle) and vastus intermedius (VI; a deep layered quadriceps muscle), which are composed mostly of the slow Type I myofibers containing the slow myosin heavy chain (MHC) protein.  MHC is the most abundant protein expressed in striated muscle; and this structural / regulatory protein serves as the motor protein that regulates, in synergy with its companion protein actin, the contraction process that derives the force, work, and power generation that is necessary for the muscle groups to bring about both movement and stabilizing types of activity (posture).  It is also important to point out that fast-twitch synergistic muscles (expressing fast isoforms of MHC) are also targeted, but these muscles and their fibers are apparently not as sensitive to the unloading stimulus as the slower types of muscle are.  Compared to both the slow and fast types of muscle, atrophy of the corresponding joint flexors, for example the tibialis anterior and extensor digitorum longus muscles in the leg, is markedly less. Histochemical and immunohistochemical analyses at the single-fiber level clearly show that the atrophic process seen at the gross level is due to a reduction in the diameter of the affected myofibers of which the individual muscles are composed.  These observations show that the slow type of fiber is more sensitive than the faster types of fiber, which is consistent with the gross muscle mass determinations.  As a rule, regardless of the muscle, the larger fibers, whether fast or slow, are more sensitive to the unloading stimulus than their smaller counterparts. Accompanying the atrophy process noted above are the important observations that many (but not all) of the slow fibers in primarily antigravity-type muscles (such as SOL and VI) are also induced to express fast myosin isoforms.  This transformation is largely manifested in the expression of hybrid fibers, in which both slow MHC and either fast type IIx or fast type IIa MHC become simultaneously co-expressed.  These observations suggest that the slow MHC is targeted for degradation, evidenced by the net loss in slow MHC in the atrophying muscle (fibers), while at the same time, according to premRNA and mRNA analyses, up-regulation of the faster MHC genes by transcriptional and/or pretranslational processes occurs.  More recent studies on this topic clearly suggest that the type IIx MHC, which is a faster isoform than the IIa type, is more abundantly expressed.  From these observations it is apparent that the myofibrillar fraction, which is a key component of the muscle, is targeted for net degradation (as noted above) for two reasons: Providing further insight is the observation that the unloading state of spaceflight and of HS also increases the expression of fast type II sarcoplasmic reticulum (SR) ATPase-driven calcium pumps (SERCA II) while repressing the slower type I SERCA calcium pump.  Since calcium cycling is used to regulate fiber activation and relaxation, the SR component of the muscle fiber controls the synchrony of contraction-relaxation processes.  Since calcium cycling and crossbridge cycling are the two major systems that account for the vast majority of the energy expended during muscle contraction to support movement, when this property of the muscle is switched to a faster system the muscle can function more effectively in the unloaded environment.  However, when the muscle encounters environments with a high gravitational stimulus, the faster properties are inherently less economical in opposing gravity and thus the muscle fibers become more fatigable when contracting against a load for long durations. In contrast to the contractile apparatus, studies on various rodent skeletal muscle metabolic enzymes have revealed a variety of responses with no clear-cut adaptive changes in oxidative enzyme expression.  These observations are consistent with the results of studies focusing on mitochondrial function after 9 days of spaceflight in which no reduction in the capacity of skeletal muscle mitochondria to metabolize pyruvate (a carbohydrate derivative) was observed.  These analyses were carried out under state 3 metabolic conditions, that is, non-limiting amounts of substrate and cofactors to simulate an energy turnover demand similar to that of high-intensity exercise.  However, when a fatty acid substrate was tested, a reduction in the capacity of different muscle types to oxidize the long-chain fatty acid, palmitate, was observed.  This latter finding is in agreement with the observation that muscles exposed to spaceflight increase the level of stored lipid within their myofibers.  Additionally, use of the metabolic pathway for glucose uptake is increased in muscles undergoing HS.  Thus, while the enzyme data are equivocal, it appears that in response to states of unloading, some shift in substrate preference may occur whereby carbohydrates are preferentially utilized based on utilization capability.  If this is indeed the case, it could result in a greater tendency for muscle fatigue, should the carbohydrate stores become limited during prolonged bouts of EVA activity. Stevens and associates reported that in isolated single-fiber analyses, deficits in force generation capacity were found along with a reduced sensitivity to calcium stimulation.  Similar observations occurred for both slow and fast ankle extensor fibers after 14 days of spaceflight.  This study focused on the force-generating aspects of muscle fibers.  It appears that only two additional studies have been conducted to examine the effects of spaceflight on rodent skeletal muscle functional properties using a more comprehensive set of analyses.  One project was carried out for 6 days while the other involved a 2-week flight (SLS-2).  In both studies, the measurements focused on the force-velocity properties, which define the limits of functional capacity of the muscle.  These studies were conducted on the soleus skeletal muscle, in which slow-twitch myofibers predominate, because of the dynamic changes in fiber morphology and phenotype that were observed in the other studies summarized above.  Analyses on the animals were initiated within 6 hours of return from spaceflight.  The findings showed that the maximal strength of the muscle, as studied in situ using a computer-programmed ergometer system, was reduced by 24% after the 6-day flight and 37% after the 14-day flight.  These changes were consistent with the degree of atrophy observed at both the gross and single-myofiber level.  Also, shifts occurred in the force-frequency response of the soleus in the flight animals, suggesting a switch to a faster contractile phenotype.  Maximal shortening velocities were increased by 14% and 24% in the 6-day and 14-day spaceflight groups, respectively.  These intrinsic increases in shortening speed were attributed, in part, to the de novo expression of the fast type IIx MHC in many of the slow muscle fibers.  On the other hand, both work- and power-generating capacities of the flight-induced atrophied muscles were significantly decreased.  Additionally, the resistance to fatigue was significantly decreased as well as the ability to sustain work and power output in response to a paradigm involving repetitive contraction output.  Similar findings have been observed using comparable analytical approaches involving the HS model.  Taken together, the findings clearly indicate that when skeletal muscles, especially those having a large proportion of slow myofibers, undergo both atrophy and remodeling of the contractile phenotype, the functional capacity of the muscle is reduced along with its ability to sustain work output.  If a sufficient mass of muscle tissue across several key muscle groups were similarly affected, this would most likely impair the fitness of the individual when challenged with moderate-intensity exercise scenarios. Riley and associates have provided an excellent synopsis of the structural integrity of mammalian muscle during the early stages after return from spaceflight.  Their findings suggest that in atrophied slow types of skeletal muscle, there is no evidence of fiber damage when the muscles are taken from animals euthanized and processed during spaceflight.  However, observations suggest that during the first 5–6 hours after spaceflight (the earliest time point at which the animals can be accessed), edema occurs in the target anti-gravity muscles such as the soleus and the adductor longus (AL).  This is thought to occur by increased blood flow to the muscles when they become initially reloaded in opposition to gravity.  In addition, in certain regions of the AL, there is some indication of fiber damage based on histological analyses of the myofibril integrity and protein alignment in the sarcomere.  While these observations were noted in ~2.5% of the fibers of the AL, they were not present in the soleus.  Riley has proposed that the reason for the differential response between the two muscle groups is that weakened animals have altered their posture and gait so that eccentric stress is placed on the AL, resulting in some fiber damage.  Edema and fiber damage were not noted in another cohort of animals studied 9 days after landing.  However, in additional studies performed on both spaceflight and HS rodents in which 12 to 48 hours were allowed to pass before the muscles were analyzed, observations indicated that the normal cage activity induced significant lesions in the muscles after sufficient reambulation was allowed.  These included eccentric-like lesioned sarcomeres, myofibrillar disruptions, edema, and evidence of macrophage activation and monocyte infiltration (known markers of injury-repair processes in the muscle) within target myofibers. The inference of these findings is that there is indeed a propensity for muscle injury secondary to the atrophic process that weakens the muscle, and given the instability of the animal after spaceflight as described above, there is most likely a potential for injury if stressful stimuli are imposed on the muscle system before it can regain its proper structural and functional capability. As presented above, skeletal muscle atrophy involves an imbalance between the processes that control protein synthesis (also known as protein translation) and those that control protein breakdown.  When the two processes are in synchrony, muscle mass is stable.  However, if there is an imbalance such that the protein synthetic pathway is decreased relative to that of the rate of degradation, muscle atrophy will occur.  In the case of skeletal muscle atrophy in response to spaceflight or HS, a decrease in the capacity for synthesis as well as an increase in the processes that regulate degradation seem to occur, creating a rapid net degradation response to the unloading stimulus.  On the basis of the available information, such a scenario is thought to involve the following chain of events.  At the onset of unloading involving a wide range of models including spaceflight, a decrease in transcriptional and/or pre-translational activity occurs in skeletal muscle that affects the type I and IIa MHC genes as well as the actin gene.  This results in a reduced level of both pre-mRNA and mRNA pools (the latter being a substrate for protein translation) for these three proteins.  Together, MHC and actin provide the bulk of the myofibril fraction that accounts for most of the protein in the muscle cell.  Concomitantly, a decrease occurs in the activity of key protein kinase enzyme systems (constituting the PI3kinase/akt/mTOR pathway), which regulates the protein synthetic apparatus controlling protein translation.  This alteration, in combination with a smaller amount of mRNA substrate, collectively contributes to a reduction in the net capacity for protein synthesis.  Occurring simultaneously with this process is the up-regulation of a set of genes that encode proteins that play a regulatory role in augmenting protein degradation.  These include the myostatin gene, the atrogin 1 gene, and a gene called muscle ring finger protein, referred to as MURF.  Myostatin is an antigrowth transcription factor, which is thought to negatively modulate the genes that promote growth.  Atrogin and MURF are E3 ligases that are responsible for ubiquinating target proteins to mark them for degradation in a system designated as the proteasome.  Interestingly, this MURF protein has been reported to be a key regulator for specifically targeting breakdown of the type I and type IIa MHC proteins. As a result of the reduction in net capacity for protein synthesis and the augmentation of protein degradation, a net loss of muscle protein in the muscle fiber occurs along with a change in the relative proportion of the MHC protein content, since available findings show that the faster MHC genes are up-regulated during muscle atrophy.  Hence, this results in a smaller, faster muscle phenotype, which is apparently more suitable for muscle performance in states of unloading.  The chain of events described above must be blunted or reversed if the muscle is to perform optimally when faced with an increased gravitational stimulus in returning to Earth or transitioning from low gravity (microgravity) to higher gravitational environments such as landing on the Moon or Mars.  It is apparent that the best strategy to accomplish this task is via a vigorous countermeasure program that provides a high level of mechanical stress to prevent the imbalance in protein expression that occurs when the muscle is insufficiently loaded for significant periods without an intervening anabolic stimulus. To our knowledge the only other species besides the rat that has been involved in spaceflight studies on skeletal muscle is the rhesus monkey.  Two monkeys were flown in space for 14 days on the Bion 11 satellite.  They were compared to ground-based vivarium control animals as well as a chair- restricted group that involved immobilization of the upper arm and shoulder.  The results from these studies provided the following insights.  Individual fibers (slow and fast) of the monkey displayed functional properties that were more closely aligned to those of human fibers than to those of rodents, in that the fibers were larger but less powerful per unit cross-sectional area than rodent fibers.  However, in pre- versus postflight analyses of single fibers, slow fibers in both the slow-twitch soleus and triceps muscles underwent greater atrophy and reductions in force and power production than fast-twitch fibers.  Also, transformations in the myosin heavy chain profile indicated that there was a greater level of hybrid slow/fast fibers in the two different muscle groups.  Immobilization of the triceps muscle group produced similar responses, but the magnitude of change was much less than that in the spaceflight animals. Additional experiments performed on these same animals, involving locomotor activity before and after spaceflight via muscle electromagnetic and tendon force recordings, respectively, demonstrated that postural and locomotor control was compromised by spaceflight as has been observed in humans.  These alterations were chiefly manifested in modified load-related cues as reflected in the altered relative recruitment bias of flexor muscles versus extensors and fast versus slow motor unit pools.  In an additional flight study (Cosmos Flight 2229) involving two rhesus monkeys, EMG recordings were obtained before, during, and after spaceflight.  These experiments were unique in that recordings obtained during spaceflight revealed a preferential shift in recruitment patterns favoring the fast medial gastrocnemius versus its synergistic slow soleus muscle, that is, the normal recruitment pattern was reversed; and this alteration was maintained well into the recovery stage after spaceflight, further suggesting a reorganization of the neuromotor system during and immediately after exposure to microgravity. Thus, it is apparent that skeletal muscle fibers of humans, monkeys, and rodents share similar patterns of myofiber alterations that, in the case of monkeys and humans, are also linked to altered motor performance in response to different states of unloading, reduced usage, and return to an Earth gravitational environment. To our knowledge, no relevant citations exist in the literature to date in which a computer-based or digital simulation has been used to predict the loss of skeletal muscle mass and function in a microgravity environment or to predict the efficacy of countermeasures in experimental animals or humans.  However, there is an effort to develop such a simulation that has been named “The Digital Astronaut”.  The Digital Astronaut is described as “an integrated, modular modeling and database system that will support space biomedical research and operations, enabling the identification and meaningful interpretation of the medical and physiological research required for human space exploration, and determining the effectiveness of specific individual human countermeasures in reducing risk and meeting health and performance goals on challenging exploration missions”.  Because of the difficulties in developing such a mathematical model based on the complexities and variables of human physiology operating in the unusual environment of microgravity, the utility of this approach, although reasonable, remains to be proven. It must be stated at the outset that the risk(s) related to loss of skeletal muscle mass, strength, and endurance depends not only on the level of loss but also on the starting point and the relative physiologic expense required to successfully complete a requisite set of tasks within a fixed period.  Thus, a crewmember must be capable of completing a task before being exposed to microgravity, the amount of functional loss cannot be allowed to fall below the level needed to successfully complete all assigned tasks, and the physical performance requirements for completion of the tasks should be known.  Without information relating to the physical performance requirements of tasks, it is not possible to determine the risk of failure. Additionally, if a task could not be completed by a crewmember before microgravity exposure, it can reasonably be stated that the risk of failure during a mission is 100%.  However, even if the crewmember has the capability to complete every possible task, a composite of the tasks to be completed over a finite period presents an entirely different requirement because it might be possible to select a composite of tasks to be completed within a work period that exceeds the capabilities of a single crewmember or perhaps every crewmember.  Additionally, all possible contingencies that might arise must be considered, so that a crewmember will be able to deal with such off-nominal scenarios even near the end of a duty day.  Thus, even an approach as basic as thoughtful scheduling of daily tasks could serve to help mitigate risk. From the above discussion, several important items emerge that must be known with respect to the risks related to loss of skeletal muscle mass, strength, and endurance.  These include: An indication of the importance of individual baseline performance is obtained from an illustrative example from the EDOMP program.  Losses in trunk flexor and extensor strength were greater for the crewmembers who exercised on the Shuttle treadmill during flight than for the crewmembers who did not exercise during their mission (see Figure 6-7).  Although at first this seems counterintuitive, simple logic provides the explanation.  Crewmembers who chose to exercise during flight did so because they exercised regularly as part of their daily routine before flight.  Since they were at a higher level of fitness than their non-exercising crewmember cohorts, they lost more strength during flight.  However, what cannot be ascertained from % change data are absolute strength levels.  For instance, exercising crewmembers who lost twice as much abdominal and back muscle strength as their non-exercising counterparts could still have greater strength in those muscles if they started off three times stronger than their non-exercising colleagues. With respect to future missions involving humans, lunar sortie missions will probably represent the lowest risk of the currently planned missions and will likely be no greater in risk than the Apollo missions (at least with respect to skeletal muscle performance) unless unusual surface operations are planned that differ markedly from Apollo lunar surface operations.  The longest cumulative time of lunar surface EVA by a crew during the Apollo Program was about 22 hours.  (combined from 3 separate days) and the longest total duration of the crew on the lunar surface was about 75 hours during the sixth and final Apollo mission (Apollo 17). The answer to the question of whether exercise equipment should be available to crewmembers for short missions to the Moon and back is actually an easy one and the answer is a resounding “Yes.”  During some of the Apollo missions, a small, lightweight device called the “Exer-Genie,” which required no external power, was made available to crewmembers (see Figure 6-1) and they were encouraged to use it.  Specific comments from the Apollo crewmembers collected during the recent “Apollo Summit” are particularly relevant <ref name=\"Scheuring et al 2007/8 (139)\"> </ref> and can be summarized as follows: Lunar outpost missions will present a greater challenge than shorter “sortie” missions, but with respect to the current risk topic they probably represent risks similar to those experienced on the ISS.  Lunar gravity, although about 1/6 that of Earth gravity, is still more conducive to providing sufficient loading to maintain muscle mass and function than is microgravity.  Certainly exercise regimens and hardware will be required, not only for countering muscle atrophy but for the reasons stated by Apollo astronauts above as well.  How much exercise is needed and the proper way to implement it are certainly knowledge gaps that will require innovative research to fill.  Part of this research will unquestionably help to define the level of risks to which crews will be exposed but will also be helpful in properly mitigating those risks. Without doubt, transport between the Earth and Mars as well as the return trip represent the greatest risks to humans encountered in the history of human spaceflight.  Notwithstanding the risks of radiation exposure, deterioration of the musculoskeletal system must be prevented or a mission to Mars (and back) will not be successful.  Highly refined exercise protocols and robust exercise equipment and methods to monitor functional capacity are mandatory for mitigation of the risks inherent in long-duration exposure of humans to microgravity.  A huge challenge will be to provide the above within the current design of the crew exploration vehicle (CEV), which provides trivial space for equipment and crew.  The cramped confines will afford little room for stretching or exercise.  Modest or no power for equipment and a human life support system whose design may be marginal to support a full complement of exercise by efficiently dealing with the heat, water vapor, and carbon dioxide that are byproducts of human exercise are additional challenges that must be overcome. Knowledge gained during lunar outpost missions will be highly relevant to successful establishment of a Martian outpost.  If the challenges posed by the long transit to Mars and the extended period of microgravity exposure can be met, the outpost phase should represent a much lower risk by comparison, since lunar outpost experience will have allowed significant opportunity to develop risk-mitigation strategies for this phase.  The gravitational environments are similar; in fact, the Martian gravity field, being greater than that of the Moon, will provide a less formidable setting.  However, capability to provide sufficient exercise capacity during the Martian outpost phase is essential in preparing the crew for a long-duration exposure to microgravity on the transit back to Earth.  This probably represents the greatest challenge with respect to maintaining a safe level of skeletal muscle performance for exploration-class missions. Despite four decades of effort, success in prevention of spaceflight muscle atrophy and skeletal muscle functional deficits has not yet been achieved in every case although progress has been made.  Gaps in our knowledge have prevented us from implementing a countermeasures program that will fully mitigate the risks of losing muscle mass, function, and endurance during exposure to the microgravity of spaceflight, particularly during long-duration missions.  There are also gaps in our knowledge about working and living in partial-G environments and the effect that wearing an EVA suit has on human performance in such an environment. The major knowledge gaps that must be addressed by future research to mitigate this risk of loss of skeletal muscle mass, function, and endurance include the following: A mission to Mars or another planet or asteroid within the Solar System is not beyond possibility within the next two decades.  Extended transit times to and from distant planetary bodies within the context of current CEV designs represents a formidable challenge to the life sciences community.  Knowledge drawn from experience and research during long-duration microgravity exposure on the ISS will be beneficial in mitigating risks to humans during this phase.  Many gaps in our current knowledge about living and working for long periods on planetary surfaces in partial G environments should be filled during lunar outpost missions.  This article incorporates public domain material from the National Aeronautics and Space Administration document \"Human Health and Performance Risks of Space Exploration Missions\" (NASA SP-2009-3405).\n\nNorthern goshawk The northern goshawk (Old English: \"gōsheafoc\", \"goose-hawk\"), Accipiter gentilis, is a medium-large raptor in the family Accipitridae, which also includes other extant diurnal raptors, such as eagles, buzzards and harriers.  As a species in the \"Accipiter\" genus, the goshawk is often considered a \"true hawk\".  The scientific name is Latin; \"Accipiter\" is \"hawk\", from \"accipere\", \"to grasp\", and \"gentilis\" is \"noble\" or \"gentle\" because in the Middle Ages only the nobility were permitted to fly goshawks for falconry. This species was first described under its current scientific name by Linnaeus in his \"Systema naturae\" in 1758. It is a widespread species that inhabits many of the temperate parts of the Northern Hemisphere.  The northern goshawk is the only species in the \"Accipiter\" genus found in both Eurasia and North America.  It may have the widest distribution of any true member of the Accipitridae family, behind arguably only the hen harrier (\"Circus cyaenus\") (which is often considered two species between North America and Eurasia) and occurring over a slightly wider range than either golden eagles (\"Aquila chrysaetos\") or rough-legged buzzards (\"Buteo lagopus\").  Except in a small portion of southern Asia, it is the only species of \"goshawk\" in its range and it is thus often referred to, both officially and unofficially, as simply the \"goshawk\".  It is mainly resident, but birds from colder regions migrate south for the winter.  In North America, migratory goshawks are often seen migrating south along mountain ridge tops at nearly any time of the fall depending on latitude. The northern goshawk has a large circumpolar distribution.  In Eurasia, it is found in most areas of Europe excluding Ireland and Iceland.  It also has a fairly spotty distribution in western Europe (i.e. Great Britain, Spain, France) but is more or less found continuously through the rest of the continent.  Their Eurasian distribution sweeps continuously across most of Russia, excluding the fully treeless tundra in the northern stretches, to the western limits of Siberia as far as Anadyr and Kamchatka.  In the Eastern Hemisphere, they are found in their southern limits in extreme northwestern Morocco, Corsica and Sardinia, the \"toe\" of Italy, southern Greece, Turkey, the Caucasus, Sinkiang's Tien Shan, in some parts of Tibet and the Himalayas, western China and Japan.  In winter, northern goshawks may be found rarely as far south as to Taif in Saudi Arabia and perhaps Tonkin, Vietnam. In North America, they are most broadly found in the western United States, including Alaska, and provinces in western Canada.  Their breeding range in the western contiguous United States largely consists of the wooded foothills of the Rocky Mountains and many other large mountain ranges from Washington to southern California extending east to central Colorado and westernmost Texas.  Somewhat discontinuous breeding populations are found in southeastern Arizona and southwestern New Mexico, thence also somewhat spottily into western Mexico down through Sonora and Chihuahua along the Sierra Madre Occidental as far as Jalisco and Guerrero, their worldwide southern limit as a breeding species. The goshawk continues eastbound through much of Canada as a native species, but is rarer in most of the eastern United States, especially the Midwest where they are not typically found outside of the Great Lakes region (a very small population persists in the extreme northeastern corner of North Dakota), where a good-sized breeding population occurs in the northern parts of Minnesota, Illinois, Michigan and somewhat into Ohio.  They breed also in mountainous areas of New England, New York, central Pennsylvania and northwestern New Jersey, sporadically down to extreme northwestern Maryland and northeastern West Virginia.  Vagrants have been reported in Ireland, central Morocco, northern Algeria, Tunisia, Libya, Egypt, Israel, Jordan, Saudi Arabia, southern Iran, Pakistan, western India (Gujarat) and on Izu-shoto (south of Japan) and the Commander Islands, not to mention most of the United States where they do not breed. Northern goshawks can be found in both deciduous and coniferous forests.  While the species might show strong regional preferences for certain trees, they seem to have no strong overall preferences nor even a preference between deciduous or coniferous trees despite claims to the contrary.  More important than the type of trees are the composition of a given tree stand, which should be tall, old-growth with intermediate to heavy canopy coverage (often more than 40%) and minimal density undergrowth, both of which are favorable for hunting conditions.  Also, goshawks typically require close proximity to openings in which to execute additional hunting.  More so than in North America, the goshawks of Eurasia, especially central Europe, may live in fairly urbanized patchworks of small woods, shelter-belts and copses and even use largely isolated trees in central parts of Eurasian cities.  Access to waterways and riparian zones of any kind is not uncommon in goshawk home ranges but seems to not be a requirement.  Narrow tree-lined riparian zones in otherwise relatively open habitats can provide suitable wintering habitat in the absence of more extensive woodlands.  The northern goshawk can be found at almost any altitude, but recently is typically found at high elevations due to a paucity of extensive forests remaining in lowlands across much of its range.  Altitudinally, goshawks may live anywhere up to a given mountain range’s tree line, which is usually 3000 m in elevation or less.  The northern limit of their distribution also coincides with the tree line and here may adapt to dwarf tree communities, often along drainages of the lower tundra.  In winter months, the northernmost or high mountain populations move down to warmer forests with lower elevations, often continuing to avoid detection except while migrating.  A majority of goshawks around the world remain sedentary throughout the year. The northern goshawk has relatively short, broad wings and a long tail, typical for \"Accipiter\" species and common to raptors that require maneuverability within forest habitats.  For an \"Accipiter\", it has a relatively sizeable bill, relatively long wings, a relatively short tail, robust and fairly short legs and particularly thick toes.  Across most of the species' range, it is blue-grey above or brownish-grey with dark barring or streaking over a grey or white base color below, but Asian subspecies in particular range from nearly white overall to nearly black above.  Goshawks tend to show clinal variation in color, with most goshawks further north being paler and those in warmer areas being darker but individuals can be either dark in the north or pale in the south.  Individuals that live a long life may gradually become paler as they age, manifesting in mottling and a lightening of the back from a darker shade to a bluer pale color.  It plumage is more variable than that of the Eurasian sparrowhawk (\"Accipiter nisus\"), which is probably due to higher genetic variability in the larger goshawk.  The juvenile northern goshawk is usually a solid to mildly streaky brown above, with much variation in underside color from nearly pure white to almost entirely overlaid with broad dark cinnamon-brown striping.  Both juveniles and adults have a barred tail, with 3 to 5 dark brown or black bars.  Adults always have a white eye stripe or supercilia, which tends to be broader in northern Eurasian and North American birds.  In North America, juveniles have pale-yellow eyes, and adults develop dark red eyes usually after their second year, although nutrition and genetics may affect eye color as well.  In Europe and Asia, juveniles also have pale-yellow eyes while adults typically develop orange-colored eyes.  Moulting starts between late March and late May, the male tends to moult later and faster than the female.  Moulting results in the female being especially likely to have a gap in its wing feathers while incubating and this may cause some risk, especially if the male is lost, as it inhibits her hunting abilities and may hamper her defensive capabilities, putting both herself and the nestlings in potential danger of predation.  The moult takes a total of 4-6 months, with tail feathers following the wings then lastly the contour and body feathers, which may not be completely moulted even as late as October. Although existing wing size and body mass measurements indicate that the Henst's goshawk (\"Accipiter henstii\") and Meyer's goshawk (\"Accipiter meyerianus\") broadly overlap in size with this species, the northern goshawk is on average the largest member of the genus \"Accipiter\", especially outsizing its tropic cousins in the larger Eurasian races.  The northern goshawk, like all \"Accipiters\", exhibits sexual dimorphism, where females are significantly larger than males, with the dimorphism notably greater in most parts of Eurasia.  Linearly, males average about 8% smaller in North America and 13% smaller than females in Eurasia, but in the latter landmass can range up to a very noticeable 28% difference in extreme cases.  Male northern goshawks are 46 to long and have a 89 to wingspan.  The female is much larger, 58 to long with a 108 to wingspan.  In a study of North American goshawks (\"A. g. atricapillus\"), males were found to average 56 cm in total length, against females which averaged 61 cm .  Males from six subspecies average around 762 g in body mass, with a range from all races of 357 to .  The female can be up to more than twice as heavy, averaging from the same races 1150 g with an overall range of 758 to .  Among standard measurements, the most oft-measured is wing chord which can range from 286 to in males and from 324 to in females.  Additional, the tail is 200 - , the culmen is 20 - and the tarsus is 68 - . Northern goshawks normally only vocalize during courtship or the nesting season.  Adult goshawks may chatter a repeated note, varying in speed and volume based on the context.  When calling from a perch, birds often turn their heads slowly from side to side, producing a ventriloquial effect.  The male calls a fast, high-pitched \"kew-kew-kew\" when delivering food or else a very different croaking \"guck\" or \"chup\".  The latter sound has been considered by some authors similar to that of a person snapping the tongue away from the roof the mouth; the males produce it by holding the beak wide open, thrusting the head up and forward, than bringing it down as the sound is emitted, repeated at intervals of five seconds.  This call is uttered when the male encounters a female.  Two calls have been recorded mainly from brooding females in the race \"A. g. atricapillus\": a recognition scream of short, intense notes (\"whee-o\" or \"hee-ya\") which ends in harsh, falsetto tone; then a dismissal call given when the male lingers after delivering food, consisting of a choked, cut-off scream.  Meanwhile, the adult female's rapid strident \"kek-kek-kek\" expresses alarm or intent to mob towards threatening intruders.  This is often done when mobbing a predator such as a great horned owl (\"Bubo virginianus\") and as it progresses the female's voice may lower slightly in pitch and becomes harsh and rasping.  As the intensity of her attacks increases, her kakking becomes more rapid and can attain a constant screaming quality.  Females often withdraw into the treetops when fatigued, and their calls are then spaced at longer intervals.  Males respond to interlopers or predators with a quieter, slower \"gek gek gek\" or \"ep ep ep\".  A call consisting of \"kek…kek.kekk kek kek-kek-kek\" is used mainly by females in advertisement and during pre-laying mutual calling.  Both sexes also may engage in kakking during copulation.  Vocalizations mainly peak in late courtship/early nesting around late March to April, can begin up to 45 minutes before sunrise, and are more than twice in as frequent in the first three hours of daylight as in the rest of the day.  Occasionally hunting northern goshawks may make shrill screams when pursuing prey, especially if a lengthy chase is undertaken and the prey is already aware of its intended predator. The genus \"Accipiter\" contains nearly 50 known living species and is the most diverse genus of diurnal raptors in the world.  This group of agile, smallish, forest-dwelling hawks has been in existence for possibly tens of millions of years, probably as an adaptation to the explosive numbers of small birds that began to occupy the world’s forest in the last few eras.  The harriers are the only group of extant diurnal raptors that seem to bear remotely close relation to this genus, whereas buteonines, Old World kites, sea eagles and chanting-goshawks are much more distantly related and all other modern accipitrids are not directly related. Within the \"Accipiter\" genus, the northern goshawk seems to belong to a superspecies with other larger goshawks from different portions of the world.  Meyer's goshawk, found in the South Pacific, has been posited as the most likely to be most close related living cousin to the northern goshawk, the somewhat puzzling gap in their respective ranges explained by other Palearctic raptors such as Bonelli's eagles (\"Aquila fasciata\") and short-toed eagles (\"Circaetus gallicus\") that have extant isolated tropical island populations and were probably part of the same southwest Pacific radiation that led to the Meyer's goshawk.  A presumably older radiation of this group occurred in Africa, where it led to both the Henst's goshawk of Madagascar and the black sparrowhawk (\"Accipiter melanoleucus\") of the mainland.  While the Henst's goshawk quite resembles the northern goshawks, the black sparrowhawk is superficially described as a “sparrowhawk” due to its relatively much longer and finer legs than those of typical goshawks but overall its size and plumage (especially that of juveniles) is much more goshawk than sparrowhawk-like. Outside of the presumed superspecies, the genus \"Erythrotriorchis\" may be part of an Australasian radiation of basal goshawks based largely on their similar morphology to northern goshawks.  Genetic studies have indicated that the Cooper's hawk of North America is also fairly closely related to the northern goshawk, having been present in North America before either of the other two North American \"Accipiters\".  However, the much smaller sharp-shinned hawk, which has similar plumage to the Cooper's hawk and seems to be most closely related to the Eurasian sparrowhawk, appears to have occupied North America the latest of the three North American species, despite having the broadest current distribution of any \"Accipiter\" in the Americas (extending down through much of South America). The northern goshawk appears to have diversified in northern, central Eurasia and spread both westwards to occupy Europe and, later on, eastwards to spread into North America across the Bering Land Bridge.  Fossil remains show that goshawks were present in California by the Pleistocene era. Two non-exclusive processes could have occurred to cause the notably color and size variation of northern goshawks throughout its range: isolation in the past enabled gene combinations to assort as distinct morphs that suited conditions in different geographical areas, followed by a remixing of these genotypes to result in clines, or subtle variation in modern selection pressures led to a diversity of hues and patterns.  As a result of the high variation of individual goshawks in plumage characteristics and typical trends in clinal variation and size variations that largely follow Bergmann's rule and Gloger's rule, an excessive number of subspecies have been described for the northern goshawk in the past.  In Europe (including European Russia) alone, 12 subspecies were described between 1758 and 1990.  Most modern authorities agree on listing nine to ten subspecies of northern goshawks from throughout its range. The juvenile plumage of the species may cause some confusion, especially with other \"Accipiter\" juveniles.  Unlike other northern \"Accipiters\", the adult northern goshawk never has a rusty color to its underside barring.  In Eurasia, the smaller male goshawk is sometimes confused with a female sparrowhawk, but is still notably larger, much bulkier and has relatively longer wings, which are more pointed and less boxy.  Sparrowhawks tend to fly in a frequently flapping, fluttering type flight.  Wing beats of northern goshawks are deeper, more deliberate, and on average slower than those of the Eurasian sparrowhawk or the two other North American \"Accipiters\".  The classic \"Accipiter\" flight is a characteristic \"flap flap, glide\", but the goshawk, with its greater wing area, can sometimes be seen steadily soaring in migration (smaller \"Accipiters\" almost always need to flap to stay aloft).  In North America juveniles are sometimes confused with the smaller Cooper's hawk (\"Accipiter cooperii\"), especially between small male goshawks and large female Cooper's hawks.  However, the juvenile goshawk displays a heavier, vertical streaking pattern on chest and abdomen, with the juvenile Cooper's hawk streaking frequently (but not always) in a “teardrop” pattern wherein the streaking appears to taper at the top, as opposed to the more even streaking of the goshawk.  The goshawk sometimes seems to have a shorter tail relative to its much broader body.  Although there appears to be a size overlap between small male goshawks and large female Cooper's hawks, morphometric measurements (wing and tail length) of both species demonstrate no such overlap, although weight overlap can rarely occur due to variation in seasonal condition and food intake at time of weighing.  Rarely, in the southern stretches of its Asian wintering range, the northern goshawk may live alongside the crested goshawk (\"Accipiter trivirgatus\") which is smaller (roughly Cooper's hawk-sized) and has a slight crest as well as a distinct mixture of denser streaks and bars below and no supercilia. Northern goshawks are sometimes mistaken for species even outside of the \"Accipiter\" genus especially as juveniles of each respective species.  In North America, four species of buteonine hawk (all four of which are smaller than goshawks to a certain degree) may be confused with them on occasion despite the differing proportions of these hawks, which all have longer wings and shorter tails relative to their size.  A species so similar it is sometimes nicknamed the “Mexican goshawk”, gray hawk (\"Buteo plagiatus\") juveniles (overlapping with true goshawks in the southwest United States into Mexico) have contrasting face pattern with bold dusky eye-stripes, dark eyes, barred thighs and a bold white “U” on the uppertail coverts.  The roadside hawk (\"Rupornis magnirostris\") (rarely in same range in Mexico) is noticeably smaller with paddle shaped wings, barred lower breast and a buff “U” on undertail coverts in young birds.  Somewhat less likely to confused despite their broader extent of overlap are the red-shouldered hawk (\"Buteo lineatus\") which have a narrow white-barred, dark-looking tail, bold white crescents on their primaries and dark wing edges and the broad-winged hawk (\"Buteo playpterus\") which also has dark wing edges and a differing tapered wing shape.  Even wintering gyrfalcon (\"Falco rusticolus\") juveniles have been mistaken for goshawks and vice versa on occasion, especially when observed distantly perched, but have many tell-tale falcon characteristics like pointed, longer wings, a brown malar stripe as well as its more extensive barring both above and below. The northern goshawk is always found solitary or in pairs.  This species is highly territorial as are most raptorial birds, maintaining regularly spaced home ranges that constitute their territory.  Territories are maintained by adults via display flights.  During nesting, the home ranges of goshawk pairs are from 600 to and these vicinities tend to be vigorously defended both to maintain rights to their nests and mates as well as the ranges’ prey base.  During display flight goshawks may engage in single or mutual high-circling.  Each sex tends to defend the territory from others of their own sex.  Territorial flights may occur almost through the year, but peak from January to April.  Such flights may include slow-flapping with exaggerated high deep beats interspersed with long glides and undulations.  In general, territorial fights are resolved without physical contact, often with one (usually a younger bird seeking a territory) retreats while the other approaches in a harrier-like warning flight, flashing its white underside at the intruder.  If the incoming goshawk does not leave the vicinity, the defending goshawk may increase the exaggerated quality of its flight including a mildly undulating wave-formed rowing flight and the rowing flight with its neck held in a heron-like S to elevate the head and maximally expose the pale breast as a territorial threat display.  Territorial skirmishes may on occasion escalate to physical fights in which mortalities may occur.  In actual fights, goshawks fall grappling to the ground as they attempt to strike each other with talons. Although at times considered rather sedentary for a northern raptor species, the northern goshawk is a partial migrant.  Migratory movements generally occur between September and November (occasionally extending throughout December) in the fall and February to April in the spring.  Spring migration is less extensive and more poorly known than fall migration, but seems to peak late March to early April.  Some birds up to as far north as northern Canada and central Scandinavia may remain on territory throughout the winter.  Northern goshawks from northern Fennoscandia have been recorded traveling up to 1640 km away from first banding but adults seldom are recorded more than 300 km from their summer range.  In Sweden, young birds distributed an average of 377 km in the north to an average of 70 km in the south.  In northern Sweden, young generally disperse somewhat south, whereas in south and central Sweden, they typically distributed to the south (but not usually across the 5-km Kattegat straits).  On the other hand, 4.3% of the southern Swedish goshawks actually moved north.  Migrating goshawks seem to avoid crossing water, but sparrowhawks seems to be able to do so more regularly.  In central Europe, few birds travel more than 30 km throughout the year, a few juveniles have exceptionally been recorded traveling up to 300 km .  In Eurasia, very small numbers of migratory northern goshawks cross the Strait of Gibraltar and Bosporus in autumn but further east more significant winter range expansions may extend from northern Iran & southern Turkmenia to Aral & Balkhash lakes, from Kashmir to Assam, extreme northwestern Thailand, northern Vietnam, southern China, Taiwan, Ryukyu Islands and South Korea.  Migratory goshawks in North America may move down to Baja California, Sinaloa and into most of west Texas, but generally in non-irruptive years, goshawks winter no further south than Nebraska, Iowa, Illinois, Indiana, eastern Tennessee and western North Carolina.  Some periodic eruptions to nearly as far as the Gulf of Mexico have been recorded at no fewer than 10 years apart.  In one case, a female that was banded in Wisconsin was recovered 1860 km in Louisiana, a first ever record of the species in that state. Prey availability may primarily dictate the proportion of goshawk populations that migrate and the selection of wintering areas, followed by the presence of snow which may aid prey capture in the short-term but in the long-term is likely to cause higher goshawk mortality.  Showing the high variability of migratory movements, in one study of winter movements of adult female goshawks that bred in high-elevations forests of Utah, about 36% migrated 100 to to the general south, 22% migrated farther than that distance, 8.3% migrated less far, 2.7% went north instead of south and 31% stayed throughout winter on their breeding territory.  Irruptive movements seem to occur for northern populations, i.e. those of the boreal forests in North America, Scandinavia, and possibly Siberia, with more equal sex ratio of movement and a strong southward tendency of movements in years where prey such as hares and grouse crash.  Male young goshawks tend to disperse farther than females, which is unusual in birds, including raptors.  It has been speculated that larger female juveniles displace male juveniles, forcing them to disperse farther, to the incidental benefit of the species’ genetic diversity.  In Cedar Grove, Wisconsin, there were more than twice as many juvenile males than females recorded migrating.  At the hawk watch at Cape May Point State Park in New Jersey, few adult males and no adult females have been recorded in fall migration apart from irruptive years, indicating that migration is more important to juveniles.  More juveniles were recorded migrating than adults in several years of study from Sweden.  In northern \"Accipiters\" including the goshawk, there seems to be multiple peaks in numbers of migrants, an observation that suggests partial segregation by age and sex. As typical of the genus \"Accipiter\" (as well as unrelated forest-dwelling raptors of various lineages), the northern goshawk has relatively short wings and a long tail which make it ideally adapted to engaging in brief but agile and twisting hunting flights through dense vegetation of wooded environments.  This species is a powerful hunter, taking birds and mammals in a variety of woodland habitats, often utilizing a combination of speed and obstructing cover to ambush their victims.  Goshawks often forage in adjoining habitat types, such as the edge of a forest and meadow.  Hunting habitat can be variable, as in a comparison of habitats used in England found that only 8% of landscapes used were woodlands whereas in Sweden 73-76% of the habitat used was woodland, albeit normally within 200 m of an opening.  In North America, goshawks are generally rather more likely than those from Eurasia to hunt within the confines of mature forest, excluding areas where prey numbers are larger outside of the forest, such as where scrub-dwelling cottontails are profuse.  One study from central Sweden found that locally goshawks typically hunt within the largest patches of mature forests, selecting second growth forest less than half as often as its prevalence in the local environment.  The northern goshawk is typically considered a perch-hunter.  Hunting efforts are punctuated by a series of quick flights low to the ground, interspersed with brief periods of scanning for unsuspecting prey from elevated perches (short duration sit-and-wait predatory movements).  These flights are meant to be inconspicuous, averaging about 83 seconds in males and 94 seconds in females, and prey pursuits may be abandoned if the victims become aware of the goshawk too quickly.  More sporadically, northern goshawks may watch from prey from a high soar or gliding flight above the canopy.  One study in Germany found an exceptional 80% of hunting efforts to be done from a high soar but the author admitted that he was probably biased by the conspicuousness of this method.  In comparison, a study from Great Britain found that 95% of hunting efforts were from perches.  A strong bias for pigeons as prey and a largely urbanized environment in Germany explains the local prevalence of hunting from a soaring flight, as the urban environment provides ample thermals and obstructing tall buildings which are ideal for hunting pigeons on the wing. Northern goshawks rarely vary from their perch-hunting style that typifies the initial part of their hunt but seems to be able to show nearly endless variation to the concluding pursuit.  Hunting goshawks seem to not only utilize thick vegetation to block them from view for their prey (as typical of \"Accipiters\") but, while hunting flying birds, they seem to be able to adjust their flight level so the prey is unable to see its hunter past their own tails.  Once a prey item is selected, a short tail-chase may occur.  The northern goshawk is capable of considerable, sustained, horizontal speed in pursuit of prey with speeds of 38 mph reported.  While pursuing prey, northern goshawks has been described both “reckless” and “fearless”, able to pursue their prey through nearly any conditions.  There are various times goshawks have been observed going on foot to pursue prey, at times running without hesitation (in a crow-like, but more hurried gait) into dense thickets and brambles (especially in pursuit of galliforms trying to escape), as well as into water (i.e. usually waterfowl).  Anecdotal cases have been reported when goshawks have pursue domestic prey into barns and even houses.  Prey pursuits may become rather prolonged depending upon the goshawk’s determination and hunger, ranging up to 15 minutes while harrying a terrified, agile squirrel or hare, and occasional pair hunting may benefit goshawks going after agile prey.  As is recorded in many accipitrids, hunting in pairs (or “tandem hunting”) normally consist of a breeding pair, with one bird flying conspicuously to distract the prey, while the other swoops in from behind to ambush the victim.  When gliding down from a perch to capture prey, a goshawk may not even beat its wings, rendering its flight nearly silent.  Prey is killed by driving the talons into the quarry and squeezing while the head is held back to avoid flailing limbs, frequently followed by a kneading action until the prey stops struggling.  Kills are normally consumed on the ground by juvenile or non-breeding goshawks (more rarely an elevated perch or old nest) or taken to a low perch by breeding goshawks.  Habitual perches are used for dismantling prey especially in the breeding season, often called “plucking perches”, which are may consist fallen logs, bent-over trees, stumps or rocks and can see years of usage.  Northern goshawks often leave larger portions of their prey uneaten than other raptors, with limbs, many feathers and fur and other body parts strewn near kill sites and plucking perches, and are helpful to distinguish their kills from other raptors such as large owls, who usually eat everything.  The daily food requirements of a single goshawks are around 120 to and most kills can feed a goshawk for 1 to 3 days.  Northern goshawks sometimes cache prey on tree branches or wedged in a crotch between branches for up to 32 hours.  This is done primarily during the nestling stage.  Hunting success rates have been very roughly estimated at 15-30%, within average range for a bird of prey, but may be reported as higher elsewhere.  One study claimed hunting success rates for pursuing rabbits was 60% and corvids was 63.8%. Northern goshawks are usually opportunistic predators, as are most birds of prey.  The most important prey species are small to medium-sized mammals and medium to large-sized birds found in forest, edge and scrub habitats.  Primary prey selection varies considerably not just at the regional but also the individual level as the primary food species can be dramatically different in nests just a few kilometers apart.  As is typical in various birds of prey, small prey tends to be underrepresented in prey remains below habitual perches and nests (as only present in skeletal remains within pellets) whereas pellets underrepresent large prey (which is usually dismantled away from the nest) and so a combined study of both remains and pellets is recommended to get a full picture of goshawks’ diets.  Prey selection also varies by season and a majority of dietary studies are conducted within the breeding season, leaving a possibility of bias for male-selected prey, whereas recent advanced in radio-tagging have allowed a broader picture of goshawk’s fairly different winter diet (without needing to kill goshawks to examine their stomach contents).  Northern goshawks have a varied diet that has reportedly included over 500 species from across its range, and at times their prey spectrum can extend to nearly any available kind of bird or mammal except the particularly large varieties as well as atypical prey including reptiles and amphibians, fish and insects. However, a few prey families dominate the diet in most parts of the range, namely corvids, pigeons, grouse, pheasants, thrushes and woodpeckers (in roughly descending order of importance) among birds and squirrels (mainly tree squirrels but also ground squirrels especially in North America) and rabbits and hares among mammals. Birds are usually the primary prey in Europe, constituting 76.5% of the diet in 17 studies.  In North America, by comparison, they constitute 47.8% in 33 studies and mammals account for a nearly equal portion of the diet and in some areas rather dominate the food spectrum.  Studies have shown that from several parts of the Eurasian continent from Spain to the Ural mountains mammals contributed only about 9% of the breeding season diet.  However, mammals may be slightly underrepresented in Eurasian data because of the little-studied presence of mammals as a food source in winter, particularly in the western and southern portions of Europe where the lack of snowfall can allow large numbers of rabbits. Staple prey for northern goshawks usually weighs between 50 and , with average prey weights per individual studies typically between 215 and . There is some difference in size and type between the prey caught by males and larger females.  Prey selection between sexes is more disparate in the more highly dimorphic races from Eurasia than those from North America.  In the Netherlands, male prey averaged 277 g whereas female prey averaged 505 g , thus a rough 45% difference .  In comparison, the average prey caught by each sex in Arizona was 281.5 g and 380.4 g , respectively, or around a 26% difference.  Northern goshawks often select young prey during spring and summer, attacking both nestling and fledgling birds and infant and yearling mammals, as such prey is often easiest to catch and convenient to bring to the nest.  In general, goshawks in Fennoscandia, shift their prey selection to when the birds produce their young: first waterfowl, then quickly to corvids and thrushes and then lastly to grouse, even though adults are also freely caught opportunistically for all these prey types.  This is fairly different from Vendsyssel, Denmark, where mostly adult birds were caught except for thrushes and corvids, as in these two groups, the goshawks caught mostly fledglings. Overall, one prey family that is taken in nearly every known part of the goshawk’s range is the corvids.  Some 24 species have been reported in the diet.  The second most commonly reported prey species in breeding season dietary studies from both Europe and North America are both large jays, the 160 g Eurasian jay (\"Glarius glandarius\") and the 128 g Steller's jay (\"Cyanocitta stelleri\").  which in studies from northeastern Poland and the Apennines of Italy (where the Eurasian jays made up a quarter of the food by number) and in northwestern Oregon and the Kaibab Plateau of Arizona (where the Steller's made up 37% by number) were the main prey species by number.  The conspicuously loud vocalizations, somewhat sluggish flight (when hunting adult or post-fledging individuals) and moderate size of these jays make them ideal for prey-gathering male goshawks.  Another medium-sized corvid, the 218 g Eurasian magpie (\"Pica pica\") is also amongst the most widely reported secondary prey species for goshawks there.  Magpies, like large jays, are rather slow fliers and can be handily outpaced by a pursuing goshawk.  Some authors claim that taking of large corvids is a rare behavior, due of their intelligence and complex sociality which in turn impart formidable group defenses and mobbing capabilities.  One estimation claimed this to be done by about 1-2% of adult goshawks during the breeding season (based largely on studies from Sweden and England), however, on the contrary many goshawks do routinely hunt crows and similar species.  In fact, there’s some recorded cases where goshawks were able to exploit such mobbing behavior in order to trick crows into close range, where the mob victim suddenly turned to grab one predaceously.  In the following areas \"Corvus\" species were the leading prey by number: the 440 g hooded crow (\"Corvus cornix\") in the Ural mountains (9% by number), the 245 g western jackdaw (\"Corvus frugilegus\") in Sierra de Guadarrama, Spain (36.4% by number), the 453 g rook (\"Corvus frugilegus\") in the Zhambyl district, Kazakhstan (36.6% by number) and the 457 g American crow (\"Corvus brachyrhynchos\") in New York and Pennsylvania (44.8% by number).  Despite evidence that northern goshawks avoid nesting near common ravens (\"Corvus corax\"), the largest widespread corvid (about the same size as a goshawk at 1040 g ) and a formidable opponent even one-on-one, they are even known to prey on ravens seldomly.  Corvids taken have ranged in size from the 72 g gray jay (\"Perisoreus canadensis\") to the raven. In Europe, the leading prey species numerically (the main prey species in 41% of 32 European studies largely focused on the nesting season) is the 352 g rock pigeon (\"Columba livia\").  Although the predominance of rock pigeons in urban environments that host goshawks such as the German cities of Hamburg (where they constituted 36% by number and nearly 45% by weight of the local diet) or Cologne is predictable, evidence shows that these development-clinging pigeons are sought out even within ample conserved woodland from Portugal to Georgia.  In areas where goshawk restrict their hunting forays to field and forest, they often catch another numerous pigeon, the 490 g common wood pigeon (\"Columba palumbus\") (the largest pigeon the goshawk naturally encounters and is known to hunt).  The latter species was the main prey in the diet of northern goshawks from in the Dutch-German border (37.7% of 4125 prey items) and Wales (25.1% by number and 30.5% by biomass of total prey).  It has been theorized that male goshawks in peri-urban regions may be better suited with their higher agility to ambushing rock pigeons in and amongst various manmade structures whereas females may be better suited due the higher overall speeds to taking out common wood-pigeons, as these typically forage in wood-cloaked but relatively open fields; however males are efficient predators of common wood-pigeons as well.  Studies have proven that, while hunting rock pigeons, goshawks quite often select the oddly colored pigeons out of flocks as prey, whether the plumage of the flock is predominantly dark or light hued, they disproportionately often select individuals of the other color.  This preference is apparently more pronounced in older, experienced goshawks and there is some evidence that the males who select oddly-colored pigeons have higher average productivity during breeding.  Around eight additional species of pigeon and dove have turned up in the goshawks diet from throughout the range but only in small numbers and in most of North America, goshawks take pigeons less commonly than in Eurasia.  One exception is in Connecticut where the mourning dove (\"Zenaida macroura\"), the smallest known pigeon or dove the goshawk has hunted at 119 g , was the second most numerous prey species. The northern goshawk is in some parts of its range considered a specialized predator of gamebirds, particularly grouse.  All told 33 species of this order have turned up in their diet, including most of the species either native to or introduced in North America and Europe.  Numerically, only in the well-studied taiga habitats of Scandinavia, Canada and Alaska and some areas of the eastern United States do grouse typically take a dominant position.  Elsewhere in the range, gamebirds are often secondary in number but often remain one of the most important contributors of prey biomass to nests.  With their general ground-dwelling habits, gamebirds tend to be fairly easy for goshawks to overtake if they remain unseen and, if made aware of the goshawk, the prey chooses to run rather than fly.  If frightened too soon, gamebirds may take flight and may be chased for some time, although the capture rates are reduced considerably when this occurs.  Pre-fledgling chicks of gamebirds are particularly vulnerable due to the fact that they can only run when being pursued.  In several parts of Scandinavia, forest grouse have historically been important prey for goshawks both in and out of the nesting season, principally the 1080 g black grouse (\"Tetrao tetrix\") and the 430 g hazel grouse (\"Bonasa bonasia\") followed in numbers by larger 2950 g western capercaillies (\"Tetrao urogallus\") and the 570 g willow ptarmigan (\"Lagopus lagopus\") which replace the other species in the lower tundra zone.  The impression of goshawks on the populations of this prey is considerable, possibly the most impactful of any predator in northern Europe considering their proficiency as predators and similarity of habitat selection to forest grouse.  An estimated 25-26% of adult hazel grouses in Finnish and Swedish populations in a few studies fall victim to goshawks, whereas about 14% of adult black grouse are lost to this predator.  Lesser numbers were reportedly culled in one study from northern Finland.  However, adult grouse are less important in the breeding season diet than young birds, an estimated 30% of grouse taken by Scandinavian goshawks in summer were neonatal chicks whereas 53% were about fledgling age, the remaining 17% being adult grouse.  This is fairly different than in southeastern Alaska, where grouse are similarly as important as in Fennoscandia, as 32.1% of avian prey deliveries were adults, 14.4% were fledglings and 53.5% were nestlings. Northern goshawks can show somewhat of a trend for females to be taken more so than males while hunting adult gamebirds, due to the larger size and more developed defenses of males (such as leg spurs present for defense and innerspecies conflicts in male of most pheasant species).  Some authors have claimed this of male ring-necked pheasant (\"Phasianus cochilus\"), but these trends are not reported everywhere, as in southern Sweden equal numbers of adult male and female ring-necked pheasants, both sexes averaging 1135 g , were taken.  While male goshawks can take black and hazel grouse of any age and thence deliver them to nests, they can only take capercaillie of up to adult hen size, averaging some 1800 g , the cock capercaillie at more than twice as heavy as the hen is too large for a male goshawk to overtake.  However, adult female goshawks have been reported attacking and killing cock capercaillie, mainly during winter.  These average about 4000 g in body mass and occasionally may weigh even more when dispatched.  Similarly impressive feats of attacks on other particularly large gamebirds have been reported elsewhere in the range, including the 2770 g Altai snowcock (\"Tetraogallus altaicus\") in Mongolia and, in at least one case, successful predation on an estimated 3900 g adult-sized young wild turkey (\"Meleagris gallopavo\") hen in North America (by an immature female goshawk weighing approximately 1050 g ), although taking adults of much larger-bodied prey like this is considered generally rare, the young chicks and poults of such prey species are likely much more often taken.  At the other end of the size scale, the smallest gamebird known to be hunted by northern goshawk was the 96 g common quail.  Domestic fowl, particularly chickens (\"Gallus gallus domesticus\") are taken occasionally, especially where wild prey populations are depleted.  While other raptors are at times blamed for large numbers of attacks on fowl, goshawks are reportedly rather more likely to attack chickens during the day than other raptors and are probably the most habitual avian predator of domestic fowl, at least in the temperate-zone.  Particularly large numbers of chickens have been reported in Wigry National Park, Poland (4th most regular prey species and contributing 15.3% of prey weight), Belarus and the Ukraine, being the third most regularly reported prey in the latter two. In a study of British goshawks, the red grouse (\"Lagopus lagopus scotica\"), a race of willow ptarmigan, was found to be the leading prey species (26.2% of prey by number).  In La Segarra, Spain, the 528 g red-legged partridge (\"Alectoris rufa\") is the most commonly reported prey species (just over 18% by number and 24.5% by weight).  Despite reports that grouse are less significant as prey to American goshawks, the 560 g ruffed grouse (\"Bonasa umbellus\") is one of the most important prey species in North America (fourth most reported prey species in 22 studies), having been the leading prey species for goshawks in studies from New York, New Jersey and Connecticut (from 12 to 25% of prey selected) and reported as taken in high numbers elsewhere in several parts of their mutual range.  The 1056 g sooty grouse (\"Dendragapus fuliginosus\") was reported as the leading prey species in southern Alaska (28.4% by number).  In the boreal forests of Alberta, grouse are fairly important prey especially in winter. Among mammalian prey, indisputably the most significant by number are the squirrels.  All told, 44 members of the Sciuridae have turned up in their foods.  Tree squirrels are the most obviously co-habitants with goshawks and are indeed taken in high numbers.  Alongside martens, northern goshawks are perhaps the most efficient temperate-zone predators of tree squirrels.  Goshawks are large and powerful enough to overtake even the heaviest tree squirrels unlike smaller \"Accipiters\" and have greater agility and endurance in pursuits than do most buteonine hawks, some of which like red-tailed hawks (\"Buteo jamaicensis\") regularly pursue tree squirrels but have relatively low hunting success rates due to the agility of squirrels.  The 296 g red squirrel (\"Sciurus vulgaris\") of Eurasia is the most numerous mammalian prey in European studies and the sixth most often recorded prey species there overall.  In Oula, Finland during winter (24.6% by number), in Białowieża Forest, Poland (14.3%), in the Chřiby uplands of the Czech Republic (8.5%) and in Forêt de Bercé, France (12%) the red squirrel was the main prey species for goshawks.  In North America, tree squirrels are even more significant as prey, particularly the modestly-sized pine squirrels which are the single most important prey type for American goshawks overall.  Particularly the 240 g American red squirrel (\"Tamiasciurus hudsonicus\") is significant, being the primary prey in studies from Minnesota, South Dakota, Wyoming and Montana (in each comprising more than 30% of the diet and present in more than half of known pellets) but also reported everywhere in their foods from the eastern United States to Alaska and Arizona.  Much like the American marten (\"Martes americana\"), the American distribution of goshawks is largely concurrent with that of American red squirrels, indicating the particular significance of it as a dietary staple.  In the Pacific northwest, the 165 g Douglas squirrel (\"Tamiasciurus douglasii\") replaces the red squirrel in both distribution and as the highest contributor to goshawk diets from northern California to British Columbia.  The largest occurrence of Douglas squirrel known was from Lake Tahoe, where they constituted 23% of prey by number and 32.9% by weight. Larger tree squirrels are also taken opportunistically, in New York, New Jersey and Connecticut, the 530 g eastern gray squirrel (\"Sciurus carolinensis\") was the third most significant prey species.  Much larger tree squirrels such as western gray squirrels (\"Sciurus griseus\") and fox squirrels (\"Sciurus niger\"), both weighing about 800 g , are taken occasionally in North America.  Ground squirrels are also important prey species, mostly in North America, 25 of 44 of squirrel species found in the diet are ground squirrels.  Particularly widely reported as a secondary food staple from Oregon, Wyoming, California and Arizona was the 187 g golden-mantled ground squirrel (\"Callospermophilus lateralis\").  In Nevada and Idaho’s Sawtooth National Forest, the 285 g Belding's ground squirrel (\"Urocitellus beldingi\") fully dominated the food spectrum, comprising up to 74.3% of the prey by number and 84.2% by biomass.  Even much bigger ground squirrels such as prairie dogs and marmots are attacked on occasion.  Several hoary marmots (\"Marmota caligala\") were brought to nests in southeast Alaska but averaged only 1894 g , so were young animals about half of the average adult (spring) weight (albeit still considerably heavier than the goshawks who took them).  In some cases, adult marmots such as alpine marmots (\"Marmota marmota\"), yellow-bellied marmots (\"Marmota flaviventris\") and woodchucks (\"Marmota monax\") have been preyed upon when lighter and weaker in spring, collectively weighing on average about 3500 g or about three times as much as a female goshawk although are basically half of what these marmots can weigh by fall.  About a dozen species of chipmunk are known to be taken by goshawks and the 96 g eastern chipmunks (\"Tamias striatus\") were the second most numerous prey species at nests in central New York and Minnesota.  Squirrels taken have ranged in size from the 43 g least chipmunk (\"Tamias minimus\") to the aforementioned adult marmots. Northern goshawks can be locally heavy predators of lagomorphs, of which they take at least 15 species as prey.  Especially in the Iberian peninsula, the native European rabbit (\"Oryctolagus cuniculus\") is often delivered to nests and can be the most numerous prey.  Even where taken secondarily in numbers in Spain to gamebirds such as in La Segarra, Spain, rabbits tend to be the most significant contributor of biomass to goshawk nests.  On average, the weight of rabbits taken in La Segarra was 662 g (making up 38.4% of the prey biomass there), indicating most of the 333 rabbits taken there were yearlings and about 2-3 times lighter than a prime adult wild rabbit.  In England, where the European rabbit is an introduced species, it was the third most numerous prey species at nests.  In more snowbound areas where wild and feral rabbits are absent, larger hares may be taken and while perhaps more difficult to subdue than most typical goshawk prey, are a highly nutritious food source.  In Finland, females were found to take mountain hare (\"Lepus timidus\") fairly often and they were the second most numerous prey item for goshawks in winter (14.8% by number).  In North America, where mammals are more important in the diet, more lagomorphs are taken.  In Oregon, snowshoe hares (\"Lepus americanus\") are the largest contributor of biomass to goshawks foods (making up to 36.6% of the prey by weight), in eastern Oregon at least 60% of hares taken were adults weighing on average 1500 g , and in one of three studies from Oregon be the most numerous prey species (second most numerous in the other two).  This species was also the second most numerous food species in Alberta throughout the year and the most important prey by weight.  Eastern cottontails (\"Sylvilagus floridanus\"), also averaging some 1500 g in mass per the study (and thus mostly consisting of adult cottontails in their prime), were the most significant prey both by weight (42.3%) and number (13.3%) in Apache-Sitgreaves National Forest of Arizona.  Eastern cottontails are also taken regularly in New York and Pennsylvania.  In some parts of the range, larger leporids may be attacked, extending to the 2410 g black-tailed (\"Lepus californicus\") and the 3200 g white-tailed jackrabbit (\"Lepus townsendii\"), the 3800 g European hares (\"Lepus europaeus\"), as well as the mountain hare.  In Europe, males have been recorded successfully attacking rabbits weighing up to 1600 g , or about 2.2 times their own weight, while adult mountain hares overtaken by female goshawks in Fennoscandia have weighed from 2700 to or up to 2.4 times their own weight.  Despite historic claims that taking prey so considerably larger than themselves is exceptional beyond a small region of Fennoscandia, there is evidence that as grouse numbers have mysteriously declined since 1960, adult mountain hare are increasingly the leading prey for wintering female goshawks, favoring and causing an increase of larger bodied females in order to overpower such a substantial catch.  Asian and American goshawks also take about a half dozen species of pikas, much smaller cousins of rabbits and hares, but they are at best supplementary prey for American goshawks and of unknown importance to little-studied Asian populations. Some 21 species of woodpecker have been reported from northern goshawk food studies around the world.  With their relatively slow, undulating flight adult and fledged woodpeckers can easily be overtaken by hunting goshawks, not to mention their habitat preferences frequently put them within active goshawk ranges.  Most of the widespread species from Europe and North America have been observed as prey, most commonly relatively large woodpeckers such as the 76 g greater spotted woodpecker (\"Dendrocopos major\") and the 176 g European green woodpecker (\"Picus viridis\") in Europe and the 134 g northern flicker (\"Colaptes auratus\") in North America.  Indeed the flicker is the third most regularly reported prey species in America.  In south-central Wyoming, the northern flicker was the second most numerous prey species and it was the main prey species in a study from New Mexico (here making up 26.4% of prey by number).  All sizes of woodpeckers available are taken from the 19.8 g lesser spotted woodpecker (\"Dryobates minor\") to the 321 g black woodpecker (\"Dryocopus martius\") in Europe and from the 25.6 g downy woodpecker (\"Picoides pubescens\") to the 287 g pileated woodpecker (\"Dryocopus pileatus\") in North America.  In many areas, northern goshawks will pursue water birds of several varieties, although they rarely form a large portion of the diet.  Perhaps the most often recorded water birds in the diet are ducks. All told, 32 waterfowl have been recorded in their diet.  In the Ural mountains, the nearly cosmopolitan 1075 g mallard (\"Anas platyrhynchos\") was third most numerous prey species.  The ducks of the genus \"Aythya\" are somewhat frequently recorded as well, especially since their tree-nesting habits may frequently put them in the hunting range of nesting goshawks.  Similarly, the wood duck (\"Aix sponsa\") from America and the mandarin duck (\"Aix galericulata\") from Asia may be more vulnerable than most waterfowl at their tree nests.  Although etymologists feel that the goshawk is an abbreviation of “goose-hawk”, geese are seldomly taken considering their generally much larger size.  Nonetheless, four species have been taken, including adults of species as large as the 2420 g greater white-fronted goose (\"Anser albifrons\").  Adult common eiders (\"Somateria mollissima\"), the largest northern duck at 2066 g , have also been captured by goshawks.  Various other water birds reported as taken include red-throated loon (\"Gavia stellata\") chicks, adult little grebes (\"Tachybaptus ruficollis\"), adult great cormorants (\"Phalacrocorax carbo\") (about the same size as a greater white-fronted goose), adult crested ibis (\"Nipponia nippon\"), black stork (\"Ciconia nigra\") chicks and five species each of heron and rail.  Among shorebirds (or small waders), goshawks have been reported preying on 22 sandpipers, 8 plovers, 10 species of gull and tern, 2 species of alcids and the Eurasian stone-curlew (\"Burhinus oedicnemus\"), the Eurasian oystercatcher (\"Haematopus ostralegus\") and the long-tailed jaeger (\"Stercorarius longicaudus\"). Corvids as aforementioned are quite important prey.  Although they take fewer passerines than other northern \"Accipiters\", smaller types of songbirds can still be regionally important to the diet.  This is especially true of the thrushes which are often delivered to nests in Europe.  17 species of thrush have been identified in goshawk food across their range.  The numerous 103 g Eurasian blackbird (\"Turdus merula\") is often most reported from this family and can even be the main prey at some locations such as in the Netherlands (23.5% of prey by number) and in Norway (just over 14% by number and two studies showed thrushes collectively make up nearly half of the prey items in Norwegian nests).  All common \"Turdus\" species are taken in some numbers in Europe, being quite regular and conspicuous in the woodland edge zones most often patrolled by male goshawks, especially while singing in spring and summer.  Even where larger, more nutritious prey is present such as at pheasant release sites, the abundant thrushes are more often delivered to the nest because of the ease of capture such as in Norway.  Smaller numbers of thrush are taken in general in North America but the 78 g American robin (\"Turdus migratorius\") are fairly regular prey nonetheless and were the most numerous prey in the Sierra Nevadas of California (30.7% by number and 21.4% by weight).  Thrush taken have ranged in size from the 26.4 g western bluebird (\"Sialia mexicana\"), the smallest bluebird and lightest North American thrush on average, to the 118 g mistle thrush (\"Turdus viscivorus\"), Europe’s largest thrush.  Beyond corvids and thrushes, most passerines encountered by northern goshawks are substantially smaller and are often ignored under most circumstances in favor of more sizable prey.  Nonetheless, more than a hundred passerines have been recorded their diet beyond these families.  Most widespread passerine families from North America and Europe have occasional losses to goshawks, including tyrant flycatchers, shrikes, vireos, larks, swallows, nuthatches, treecreepers, wrens, mimids, Old World warblers, Old World flycatchers, pipits and wagtails, starlings, waxwings, New World warblers, emberizine sparrows, cardinalids, icterids, finches and Old World sparrows. Avian prey has even ranged to as small as the 5.5 g goldcrest (\"Regulus regulus\"), the smallest bird in Europe.  In North America, the smallest known bird prey is the 8.2 g American redstart (\"Setophaga ruticilla\").  Among smaller types of passerines, one of the most widely reported are finches and, in some widespread studies, somewhat substantial numbers of finches of many species may actually be taken.  Finches tend to fly more conspicuously as they cover longer distances, often bounding or undulating as they do, over the canopy than most forest songbirds, which may make them more susceptible to goshawk attacks than other small songbirds.  Non-passerine upland birds taken by goshawks in small numbers include nightjars, swifts, bee-eaters, kingfishers, rollers, hoopoes and parrots. Outside of the squirrel family, relatively few other types of rodents are taken in many regions.  In eastern Oregon, the 132 g northern flying squirrel (\"Glaucomys sabrinus\") (flying squirrels are not true squirrels) was the third or fourth most frequently caught prey species.  Microtine rodents which are so essential to most northern non-accipiter hawks and a majority of owls are at best a secondary contributor to goshawk diets, even though 26 species have been reported in their diet.  Exceptionally, in a study of the Carpathian mountains of the Ukraine, the 27.5 g common vole (\"Microtus arvalis\") was the second most numerous prey species.  Relatively high numbers of the 18.4 g bank vole (\"Myodes glareolus\") were reported in diets from Poland in Gmina Sobótka and the Białowieża Forest.  During summer in Alberta, the 44 g meadow vole (\"Microtus pennsylvanicus\") was the third most frequently reported prey species, the only known study where large numbers of microtine rodents were taken in North America.  Microtine rodents taken by goshawks have ranged in size from the 11 g western harvest mouse (\"Reithrodontomys megalotis\") to the 1105 g muskrat (\"Ondatra zibethicus\").  Other miscellaneous rodents reported sporadically in the diet include dormice, porcupines, kangaroo rats, mountain beavers (\"Aplodontia rufa\"), jumping mice, Old World mice and rats, zokors, gophers and jirds. Insectivores are taken in low numbers including moles, shrews and hedgehogs.  The smallest mammalian prey species known to be attacked by goshawks was the 3.65 g masked shrew (\"Sorex cinereus\").  Even more sporadically attacked by goshawks, given this prey’s nocturnal habits, are bats.  In one case a juvenile golden snub-nosed monkey (\"Rhinopithecus roxellana\"), which was successfully taken by a goshawk.  Ungulates such as deer and sheep are sometimes consumed by goshawks but there is no evidence that they prey on live ones (as much larger accipitrids such as eagles can sometimes do), but these are more likely rare cases of scavenging on carrion, which may more regularly occur than once thought in areas with harsh winter weather. In a few cases, northern goshawks have been recorded hunting and killing prey beyond birds and mammals.  In some of the warmer drier extensions of their range, reptiles may be available to them to hunt.  Only one species of snake is recorded from their diet, the small innocuous grass snake (\"Natrix natrix\"), at 66 g ; however about a half dozen lizards are recorded in their diet, primarily from the Iberian peninsula but also from the Ural mountains and the American southwest.  The only known location in the northern goshawk’s range where reptiles were taken in large numbers was Sierra de Guadarrama, Spain, where the 77 g ocellated lizard (\"Timon lepidus\") was the second most numerous prey species.  Amphibians are even rarer in the diet, only recorded more than singly in one study each from Spain and from England.  Fish are similarly rare in the diet, recorded twice each in Bavaria and Belarus.  A few pellets have included remains of insects, much of which may be ingested incidentally or via the stomachs of birds that they have consumed.  However, there is some evidence they at times will hunt large ground-dwelling insects such as dung beetles. Northern goshawks are often near the top of the avian food chain in forested biomes but face competition for food resources from various other predators, including both birds and mammals.  Comparative dietary studies have shown that mean sizes of prey, both in terms of its size relative to the raptor itself and absolute weight, for goshawks is relatively larger than in most buteonine hawks in North America and Europe.  Studies show even buteonine hawks slightly larger than goshawks on average take prey weighing less than 200 g whereas average goshawk prey is usually well over such a mass.  This is due largely to the much higher importance of microtine rodents to most buteonine hawks, which, despite their occasional abundance, are ignored by goshawks in most regions.  Similarly, mean prey mass for sharp-shinned and Cooper's hawks in North America is between about 10 and 30% of their own mass, whereas the mean prey of American goshawks is between about 25 and 50% of their own mass and therefore are the goshawks takes prey that is on average relatively much larger.  In many of the ecosystems that they inhabit, northern goshawks compete with resources with other predators, particularly where they take sizeable numbers of lagomorphs.  About a dozen mammalian and avian predators in each area all primarily consume European rabbits and snowshoe hares alongside goshawks in the Iberian peninsula and the American boreal forest regions where these became primary staple foods.  Like those co-habitant predators, the goshawk suffers declines during the low portion in the lagomorph’s breeding cycles, which rise and fall cyclically every 10 to 12 years.  However, even where these are primary food sources, the northern goshawk is less specialized than many (even \"Bubo\" owls, some of the more generalist avian predators become extremely specialized lagomorph hunters locally, to a greater extent than goshawks) and can alternate their food selection, often taking equal or greater numbers of tree squirrels and woodland birds.  Due to this dietary variation, the northern goshawk is less effected than other raptorial birds by prey population cycles and tends to not be depleted by resource competition. Despite their propensity to pursue relatively large prey and capability to pursue alternate prey, northern goshawks can be locally outcompeted for resources by species that are more adaptable and flexible, especially in terms of habitat and prey.  Most northern buteonine hawks largely take small rodents such as voles (which are usually ignored by goshawks) but can adapt to nearly any other type of prey when the staple local rodent prey populations go down.  Comparisons with goshawks and red-tailed hawk nesting in abutting areas of Arizona (other large common \"Buteos\" like Swainson's hawks (\"Buteo swainsonii\") and ferruginous hawks (\"Buteo regalis\") utilize open habitats and so do not come into conflict with goshawks) shows the red-tailed hawks as being able to take a broader range of prey than goshawks and nest in more varied habitats, the latter species being perhaps the most commonly seen, widespread and adaptable of diurnal American raptors.  On occasion, goshawks are robbed of their prey by a diversity of other birds, including harriers, other hawks, eagles, falcons and even gulls. Northern goshawks from North America are less prone to nesting outside of mature forests and take larger numbers of mammals as opposed to abundant birds than in Europe.  This may be in part due to heavier competition from a greater diversity of raptors in North America.  In Europe, the goshawk only co-exists with the much smaller sparrowhawk within its own genus, while in North America, it lives with the intermediately-sized Cooper's hawk.  The latter species much more readily nests in semi-open and developed areas of North America than goshawks there and hunts an broad assemblage of medium-sized birds, whereas such prey is more readily available to male goshawks from Europe than to goshawks in North America.  Although the Cooper's hawk usually avoids and loses individual contests against the larger goshawk, its adaptability has allowed it to become the most widespread and commonly found North American \"Accipiter\".  The slightly larger goshawks of Europe have been shown, in some but not all areas, to outcompete and possibly lower productivity of the slightly smaller common buzzard (\"Buteo buteo\") when their ranges overlap.  Usually, however, the dietary habits and nesting preferences are sufficiently distinct and thus effect neither buzzard or goshawk populations.  Both can mutually be very common even when the other is present.  On the other hand, American goshawks are slightly smaller on average than their European counterparts and can be up to 10% smaller in mass than red-tailed hawks.  However, studies have indicated that the goshawk has, beyond its superior speed and agility, has stronger feet and a more forceful attack than that of the red-tailed hawk.  All in all, individual competitions between red-tailed hawks and goshawks can go either way and neither is strongly likely to deter the other from nesting given their distinct nesting habitats.  Other raptors, including most medium to large-sized owls as well as red-tailed hawks and falcons, will use nests built by northern goshawks, even when goshawks are still in the area. To many other raptorial birds, the northern goshawk is more significant as a predatory threat than as competition.  The northern goshawk is one of the most dangerous species to other raptors, especially to those considerably smaller than itself.  In many cases, raptors of any age from nestlings to adults are taken around their nests but free-flying raptors too are readily taken or ambushed at a perch.  One example is a study from northern England, the common kestrels (\"Falco tinnunculus\"), which average about 184 g , recorded as prey at goshawk nests (mainly in March and April) numbered 139, a larger number than kestrels recorded alive in the spring in the same area.  In the Veluwe province of the Netherlands, the percentage of nest of European honey buzzards (\"Pernis apivorus\"), weighing on average 760 g , predated by goshawks increased from a little as 7.7% in 1981-1990 to 33% in 2000-2004.  As their habitat preferences may overlap with goshawks, all other \"Accipiters\" encountered may be predated in multiple cases, including the 238 g Eurasian sparrowhawk, the 188 g levant sparrowhawk (\"Accipiter brevipes\"), the 136 g sharp-shinned hawk, the 122 g Japanese sparrowhawk (\"Accipiter gularis\") and the 440 g Cooper's hawk. Other assorted accipitrids of up to their own size to be predated by goshawks include the 747 g black kite (\"Milvus migrans\"), the 1080 g red kite (\"Milvus milvus\"), the 712 g western marsh-harrier (\"Circus aeruginosus\"), the 316 g Montagu's harrier (\"Circus pygargus\"), the 390 g pallid harrier (\"Circus macrourus\") and \"Buteos\" of up to adults including the 776 g common buzzard, the 424 g broad-winged hawk (\"Buteo platypterus\"), the 610 g red-shouldered hawk (\"Buteo lineatus\") and the 1065 g red-tailed hawk.  Even raptors somewhat larger than a northern goshawks have been considered as prey, although it is not clear whether adults are among the victims, including the 1494 g osprey (\"Pandion haliaetus\"), 1147 g crested honey-buzzard (\"Pernis ptilorhynchus\") and the 1370 g lesser spotted eagle (\"Clanga pomarina\"). Outside of the accipitrid group, heavy predation on different varieties of raptorial birds by northern goshawks can continue unabated.  Many types of owl are taken and in Europe, the northern goshawk is the second most prolific predator of owls behind the Eurasian eagle owl (\"Bubo bubo\").  In Bavaria, Germany, the 287 g long-eared owl (\"Asio otus\") was the second most common prey species for nesting goshawks.  In the Białowieża Forest of Poland, fairly high numbers of the 475 g tawny owl (\"Strix alucco\") were taken.  In all, some 18 species of owl have been recorded in the diet, ranging in size from the Eurasian (\"Glaucidium passerinum\") and northern pygmy owls (\"Glaucidium gnoma\") at 58.5 g and 61.8 g , respectively, to all the large northern \"Strix\" owls including adults and even the 1400 g great horned owl.  Whether adults have ever been killed as prey though is unknown but goshawks have been known to kill great horned owls that they’ve found near their nests.  In addition, about eight species of falcon have been identified in the foods of goshawks.  Adult falcons of small species such as kestrels and merlins (\"Falco columbarius\") can be overpowered quite easily if they can manage to surprise the prey.  Larger falcons have turned up in the diet as well, including the 720 g prairie falcon (\"Falco mexicanus\") and the 966 g saker falcon (\"Falco cherrug\"), although possible only nestlings of these species.  Brief aerial skirmishes between goshawks and peregrine falcons (\"Falco peregrinus\") have been described but neither species is known to have killed one another in the wild.  In Schleswig-Holstein, Germany, at least four small passerines species were recorded as nesting close to active goshawk nest, due to the incidental shelter that the fierce goshawks inadvertently provides from smaller raptors which are their main predators.  Such raptors, including Eurasian kestrels, Eurasian sparrowhawks and long-eared owls, not only avoid goshawk activity where possible but also were found to have lower nest productivity any time they nested relatively close to goshawks per the study.  A similar phenomenon, with goshawks inadvertently providing shelter to small passerines, has been recorded in North America as well. Competition for northern goshawks can also come from mammalian carnivores.  Martens, and to a lesser extent other weasels, are presumably one of their more major competitors as their diet often consists of similar prey primarily during spring and summer, tree squirrels and woodland birds, but little has been studied in terms of how the two types of predator effect each other.  Most recorded interactions have been predatory, as the goshawk has been recorded preying on a dozen species, from the 122 g least weasel (\"Mustela nivalis\") to the 1700 g stone marten (\"Martes foina\").  Northern goshawks have also been recorded as feeding on much bigger predators such as the 5775 g red fox (\"Vulpes vulpes\"), the 4040 g raccoon dog (\"Nyctereutes procyonoides\") and the 3500 g striped skunk (\"Mephitis mephitis\"), but it is not clear whether these were actual kills, as many may be encountered as already dead carrion.  Domestic carnivores are sometimes eaten, including dogs and cats, the latter of which has reportedly been taken alive by goshawks.  The red fox is a surprisingly considerable competitor for resources with northern goshawks.  It was found in Norway that goshawk numbers were higher when voles were at peak numbers, not due to voles as a food source but because foxes were more likely to eat the rodents and ignore grouse, whereas during low vole numbers the foxes are more likely to compete with goshawks over grouse as prey.  A decrease of the fox population of Norway due to sarcoptic mange was found result in an increase of grouse numbers and, in turn, northern goshawks.  In some areas, red foxes have been found to steal up to half of the goshawks’ kills. Unlike the predators at the top of the avian food chain such as eagles and the largest owls, which are rarely endangered by predation as adults, the northern goshawk is itself susceptible to a fairly extensive range of predators.  The most deadly are likely to be the Eurasian eagle owl and the great horned owl, which not only predate goshawks of any age and at any season but also opportunistically take over their own prior nests as their own nesting site.  Of the two, the American horned owl nesting habits are more similar to goshawks, which most often consists of tree nests whereas the eagle owl usually nests in rock formations.  Thus, the northern goshawk is more likely to victimized by the great horned owls, which can stage nightly ambushes and destroy an entire goshawk family as they pick off both adults and nestlings.  In radio-tagging studies of adult and immature goshawks in the Great Lakes region and Arizona, up to half of the studied birds were killed by great horned owls at night, while the horned owls accounted for 40% of the nest failures in studies from Arizona and New Mexico.  In comparison, in Schleswig-Holstein, 59% of reintroduced eagle owls used nests built by goshawks and no goshawk pairs could successfully nest within 500 m of an active eagle-owl nest.  18% of nest failures here positively were attributed to eagle owl predation, with another 8% likely due to eagle-owls.  Other larger raptorial birds can threaten them.  The golden eagle (\"Aquila chrysaetos\") and the bald eagle (\"Haliaeetus leucocephalus\") in North America, have killed wintering goshawks, but given the discrepancy in their habitat preferences, such cases are presumably rare.  Other avian predators known to have successfully preyed on goshawks including adults (usually in singular cases) include white-tailed eagles (\"Haliaeetus albicilla\"), eastern imperial eagles (\"Aquila heliaca\"), snowy owls (\"Bubo scandiacus\") and red-tailed hawks. The same mammalian predators that sometimes compete for food with northern goshawks also sometimes kill them, with seemingly the nestlings, fledglings and brooding females with impaired flight due to their wing feather moults seemingly the most vulnerable.  In one case, the American marten, which at 660 g is the smallest marten and is sometimes taken as prey by the goshawks, successfully ambushed and preyed on a brooding female goshawk.  In Chequamegon-Nicolet National Forest of Wisconsin, the main source of mortality for northern goshawks is reintroduced fishers (\"Martes pennanti\"), which are the largest kind of marten at 3900 g and capably kill many chicks, fledglings and brooding females.  In contrast, in Europe, the pine marten (\"Martes martes\") has only been known to prey on young goshawks still in the nest and not adults.  Other mammals capable of climbing trees have been observed or inferred to predate goshawks, either mostly or entirely young in the nests, including wolverines (\"Gulo gulo\"), North American porcupines (\"Erethizon dorsatum\"), raccoons (\"Procyon lotor\"), bobcats (\"Lynx rufus\") and American black bears (\"Ursus americanus\").  Overall, the range of nest predators is more extensive in North America than in Eurasia, in the latter continent most recorded nest depredations are by eagle owls, with martens and corvids usually only preying on goshawk nestlings when low food supplies cause the goshawks to have lower nest attendance (and presumably effect these predators to the extent that they take the risk of coming to the goshawk nest).  Fledgling goshawks are also vulnerable to canids such as coyotes (\"Canis latrans\"), gray wolves (\"Canis lupus\") and red foxes as they may perch lower to the ground and are clumsier, more unsteady and less cautious than older birds.  In one case, a goshawk that was ambushed and killed at a kill by a mangy vixen fox was able to lethally slash the windpipe of the fox, which apparently died moments after partially consuming the goshawk. Apart for aforementioned predation events, northern goshawks have at times been killed by non-predators, including prey that turned the tables on their pursuer, as well as in hunting accidents.  In one case, a huge group (or murder) of hooded crows heavily mobbed a goshawk that they caught in a relatively open spot, resulting in a prolonged attack that ended up killing the goshawk.  In another instance, a goshawk drowned while attempting to capture a tufted duck (Aythya fuligula).  One young goshawk managed to escape a red fox that had caught it with a chewed wing, only to drown in a nearby creek.  Another, and rather gruesome, hunting mishap occurred when a goshawk caught a large mountain hare and, while attempting to hold it in place by grasping vegetation with its other foot, was torn in half. The northern goshawk is one of the most extensively studied raptors in terms of its breeding habits.  Adult goshawks return to their breeding grounds usually between March and April, but locally as early as February.  If prey levels remain high, adults may remain on their breeding ground all year.  Courtship flights, calls and even nest building has been recorded in Finland exceptionally in September and October right after young dispersed, whereas in most of Fennoscandia, breeding does not commence any earlier than March and even then only when it is a warm spring.  Most breeding activity occurs between April and July, exceptionally a month earlier or later.  Even in most areas of Alaska, most pairs have produced young by May.  Courtship flights typical are above the canopy on sunny, relatively windless days in early spring with the goshawks’ long main tail feathers held together and the undertail coverts spread so wide to give them an appearance of having a short, broad-tail with a long dark strip extending from the center.  Display flights not infrequently escalate into an undulating flight, similar to a wood pigeon but with sharper turns and descents, and are sometimes embellished with sky-dives that can cover over 200 m .  One study found undulating display flights more than three times more often done by males than females.  After display flights have concluded, the male typically brings a prepared fresh prey item to the female as part of the courtship.  In general, these displays are presumably to show (or reinforce) to the potential mate their health and prowess as breeding partner.  Copulation is brief and frequent, ranging up to nearly 520 times per clutch (on average about 10 times a day or 100-300 throughout the season), and may be the male’s way of ensuring paternity since he is frequently away gathering food by the time of egg-laying, although extra-pair copulation is extremely rare.  Female solicits copulations by facing away from male with drooped wings and flared tail-coverts.  Male, wings drooped and tail-coverts flared, drops from a branch to gain momentum, then swoops upward and mounts her back.  Both birds usually call while mating.  Fidelity studies from Europe show that about 80-90% of adult females breed with the same male in consecutive years, whereas up to 96% of males mate with the same female in consecutive years.  In California, 72% of males retained relationship with the same mates in consecutive years while 70% of females did the same.  Males intruding in Hamburg, Germany territories were in some cases not evicted and ended up mating with the female, with the male of the pair not stopping it.  In migratory, northernmost populations, mate retention in consecutive years is low.  Males are sometimes killed by females during courtship and encounters can be dangerous especially if he does not bring food to courtship and he often seems nervous withdrawing with a trill at a given chance. Nesting areas are indefinite, a nest may be used for several years, also a nest built years prior may be used or an entirely new nest may be constructed.  When nest constructing, the will pair often roost together.  Males construct most new nests but females may assist somewhat if reinforcing old nests.  While the male is building, the female perches in the vicinity, occasionally screaming, sometimes flying to inspect the nest.  At other times, the female may take a more active role, or even the primary one, in new nest construction and this is subject to considerable individual variation.  For the nesting tree, more than 20 species of conifer have been used including spruce, fir, larch, pine and hemlock.  Broadleaf trees used including ash, alder, aspen, beech, birch, elm, hickory, hornbeam, lime, maple (including sycamore), oak, poplar, tamarack, wild cherry and willow.  In some areas, the nests may be line with hard pieces of bark and also with green sprigs of conifers.  Often the tallest tree in a given stand is selected as the nest tree and this is often the dominant tree species within the given region and forest.  Therefore, hardwood trees are usually used as the nesting tree in the eastern United States while conifers are usually used in the western United States.  Most nest are constructed under the canopy or near the main fork of a tree and in North America, averaging nest height ranged from 5.8 m (in the Yukon territory) to 16.9 m (in New Mexico), elsewhere as in Europe average height is between 9 and .  In the dwarf trees of the tundra, nests have been found at only 1 to off the ground, and, in the tundra and elsewhere, very rarely on felled trees, stumps or on the ground.  In some studies from North America up to 15% of nests are in dead trees but this is far rarer in Eurasia.  More significant than species is the maturity and height of the nesting tree, its structure (which should have ample surface around the main fork) and, perhaps most significantly, little to no understory below it.  Multiple studies note the habit of nest being built in forests close to clear-fellings, swamps and heaths, lakes and meadows, roads (especially light-use logging dirt roads), railways and swathes cut along power cables, usually near such openings there'd be prominent boulders, stones or roots of fallen trees or low branches to use as plucking points.  Canopy cover averaged between 60 and 96% in Europe.  As is typical in widely distributed raptors from temperate-zones, those from cold regions faced south, 65% in Alaska, 54% in Norway and also in high latitudes such as sky-forests of the Arizona Rockies, otherwise usually nests face north and east. Nests, especially after initial construction, may average between 80 and in length and50 to in width, and are around 20 to deep.  After many uses, a nest can range up to 160 cm across and 120 cm in depth and can weigh up to a ton when wet.  Northern goshawks may adopt nests of other species, common buzzards contributed 5% of nests used in Schleswig-Holstein, including unusually exposed ones on edges of woods and another 2% were built by common ravens or carrion crows, but 93% were built by the goshawks themselves.  While colonizing peri-urban areas in Europe, they may displace Eurasian sparrowhawks not only from their territories but may actually try to use overly small sparrowhawk nests, usually resulting in nest collapse.  One nest was used continuously by different pairs for a period of 17 years.  A single pair may maintain up to several nests, usually up to two will occur in an area of no more than a few hundred kilometers.  One nest may be used in sequential years, but often an alternate is selected.  During an 18-year-study from Germany, many alternate nests were used, 27 pairs had two, 10 had 3, 5 had 4, one had five and one pair had as many as 11.  Other regions where pairs had on average two nests were Poland, California and Arizona’s Kaibab Plateau.  The extent of use of alternate nests is unknown as well as their benefit, but they may reduce significant of parasites and diseases within the nest.  In central Europe, the goshawk’s nest area can be as small 1 to of woods and less than 10 hectares are commonplace.  Usually only 1 active nest occurs per 100 ha , are they avoid edges as nest sites and occupied nests are seldom less than 600 m apart.  The most closely spaced active nests by a separate pair on record was 400 m in central Europe, another case of two active nests 200 m apart in Germany was a possible case of polygamy. The eggs are laid at 2 to 3 day intervals on average between April and June (usually May), taking up to 9 days for a clutch of 3-4 and 11 days for a clutch of 5.  The eggs are rough, unmarked pale bluish or dirty white.  In \"A. g, atricapillus\", the average dimensions of the eggs are reported at 57.76 to in height by 44.7 to in width, with ranges of 52 - x 42 - .  In Spanish eggs, the average dimensions were 56.3 x compared to German ones, which averaged 57.3 x .  Goshawks from Lapland, Finland lay the largest known eggs at 62 - x 47 - , while other Finnish goshawk eggs ranged from 59 - x 45 - .  Weight of the eggs average 59 g in America, 63 g in Great Britain and 50 to in Poland and Germany, with extreme weights from the latter nations of 35 to .  Clutch size almost always averages between 2 and 4 eggs, with a median around 3, rarely as few as 1 or as many 5-6 will be laid.  In combination spring weather and prey population levels seem to drive both egg laying dates and clutch size.  If an entire clutch is lost, a replacement can be laid within 15 to 30 days. During incubation, females tend to become quieter and more inconspicuous.  The mother can develop a brooding patch of up to 15 by on her underside.  She may turn the eggs as frequently as every 30 to 60 minutes.  Males may incubate as many as 1 to 3 hours, but usually less than an hour, early in incubation but rarely do so later on.  During daylight females can do as much as 96% of the observed incubation.  The incubation stage last for any time between 28 and 37 days (rarely up to 41 days in exceptionally big clutches), varying in different parts of the range.  After hatching occurs, the male does not come directly to the nest but instead just delivers food (usually already plucked, beheaded or otherwise dismembered) to a branch near the nest which the female tears apart and shares between herself and the nestlings.  Food deliveries by the male can be daily or as infrequent as every 3 to 5 days.  In turn, the female must feed the young about twice a day in order for the chicks to avoid starvation.  Caching of food has been recorded near the nest, but only before the young start feeding themselves.  Food deliveries must average about 250 to per young goshawk per day for them to successfully fledge, or 700 to total daily and 60 to throughout the season for an average sized clutch of around three.  Females will also start capturing prey later on, but usually only after the young have already fledged.  In Europe, female goshawks may press down on their nest if a human approaches, others may unobtrusively leave the nest, although are more reluctant to leave the nest late in incubation.  In North America, the behavior of parent goshawks differs, as they often vigorously defend their territories fiercely from all intruders, including passing humans.  The northern goshawk has a reputation as the most aggressive American raptor when the vicinity of their nest is approached.  Here, when the nest is approached (especially soon after hatching) the goshawk will engage in their defensive kakking vocal display accompanied by exaggerated swooping in flight which quickly phases into a violent attack, potentially causing painful (but usually minor) injuries and blood loss.  Research has indicated that attacks on humans are mostly done by adult females (more than 80% of the time) and are rarely pressed unless a person is by themselves.  However, large groups and loud noise can appear to irritate the female and may cause her to attack the next lone person who comes near the nest.  The higher aggression towards humans in North America than in Europe has been linked to both a more extensive range of potential nest predators for American goshawks causing them to develop a more aggressive display or the lower rates of persecution in America than in Europe, who may account for the relative shyness in the latter continent.  Occasionally, both males and females have been recorded abandoning the nest and their mates.  There are a few rare cases where males successfully reared up to 4 young after the female abandoned the nest or was killed between the 2 and 3rd week.  Otherwise male will continue delivering prey but without the female all the nestlings will starve to death and the food simply rots.  In cases where the male abandons the female and the brood, she may be able to successfully brood but usually only nestling is likely to survive to fledge without the male’s contribution of prey.  At other times the mother may be replaced, sometimes forcefully, by another female, usually an older mature one.  Exceptional cases of polygamy, with a male mating with two females, have been reported in Germany and The Netherlands and typically these breeding attempts fail. Hatching is asynchronous but not completely, usually an average sized clutch takes only 2 to 3 days to hatch, although it may take up to 6 days to hatch a clutch of more than 4 eggs.  Hatchlings start calling from within the shell as much as 38 hours before hatching, as a faint \"chep, chep, chack, peep, peep, peep\" may be heard.  The young are covered with down and altricial (as are all raptors) at first but develop rapidly.  Hatchlings measure about 13 cm long at first and grow about 5 to in length each week until they fledge.  The mothers typically brood the nestlings intensively for about two weeks, around the time grayer feathers start to develop through the nestlings’ down.  The most key time for development may be at three weeks, when the nestlings can stand a bit and start to develop their flight feathers.  Also at the three-week stage, they can reach about half the adults’ weight and females start to noticeably outgrow the males.  However, this growth requires increased food delivery so frequently results in lower nest attendance and, in turn, higher predation rates.  Also rates of starvation at this stage can exceed 50% especially in the youngest of large clutches of 4 to 5.  Nestlings at 4 weeks are starting to develop strong flight feathers, which they frequently flap; also they can start to pull on food but are still mainly fed by female and begin to make a whistling scream when she goes to fetch food from the male.  More active feeding behavior by nestlings may increase their aggression towards each other.  By the 5th week, they've developed many typical goshawk behaviors, sometimes mantling over food, testing balance by extending one leg and one wing at edge of nest (called \"warbling\" by falconers) and can wag their tails vigorously.  Starvation risk also increases at this point due their growing demands and, due to their incessant begging calls, vocal activity may court predators.  In 6th week, they become \"branchers\", although still spend much of the time by the nest, especially by the edge.  The young goshawks \"play\" by seizing and striking violent at a perch or by yanking off leaves and tossing them over their back.  Wing feathers do not develop highly dimorphically, but male branchers are better developed than females who have more growing to do and can leave the nest up to 1-3 days sooner.  The young rarely return to the nest after being 35 to 46 days of age and start their first flight another 10 days later, thus becoming full fledglings.  Goshawk nestlings frequently engage in “runting”, wherein the older siblings push aside and call more loudly and are thus are feed more often at food deliveries, until the younger siblings may either starve to death, be trampled or killed by their siblings (referred to as siblicide or “cainism”).  There is some evidence that mother goshawks may lessen the effects of runting by delaying incubation until their last eggs are laid.  Food supply may be linked to higher rates of siblicides and, in many locations with consistent prey levels, runting and siblicide can occur somewhat seldomly (meaning the northern goshawk is a “facultative” rather than “obligate cainist”).  Nonetheless, either by predation, starvation or siblicide, few nests produce more than 2 to 3 fledglings.  One pair in North America was able to successfully fledge all four of its young.  Somewhat larger numbers of female fledglings are produced in Europe with their larger size, but the opposite is true in North America where sexual dimorphism is less pronounced.  When food supplies are very high, though, European goshawks actually can produce somewhat more males than females. At about 50 days old, the young goshawks may start hunting on their own but more often eat carrion either provided by parents or biologists.  Most fledglings stay within 300 m of the nest at 65 days of age but can wander up to 1000 km before dispersal at between 65-80 days old in sync with the full development of their flight feathers.  Between 65 and 90 days after hatching, more or less all young goshawks become independent.  There is no evidence that parents aggressively displace the young in the fall (as other raptorial birds have sometimes been reported to do), therefore the young birds seek independence on their own.  Goshawk siblings are not cohesive together past 65 days, except for some lingering young females, whereas common buzzard broods are not recorded at their nests after 65 days but remain strongly cohesive with each other.  5% of radio-tagged young in Gotland, Sweden (entirely males) were found to disperse to another breeding area and join a different brood as soon as their flight feathers were developed enough.  These seem to be cases of moving to a better food area.  Parents and adoptive young seem to tolerate this, although parents do not seem to be able to tell the difference between their own and other young.  It is only after dispersal that goshawks typically start to hunt and seem to drink more often than older birds, sometimes spend up to an hour bathing. Nest success averages between 80 and 95% in terms of the number of nests that produce fledglings, with an average number of 2 to 3 fledglings per nest.  About equal numbers of eggs and nestlings may be lost (6% lost in each the incubation and brooding periods per a study from Arizona) but according to a study from Spain large clutches of 4 to 5 had higher losses overall than medium sized clutches of 2 to nearly 4.  Total losses averaged 36% in Spain across clutches of 2-5.  Similar results were found in Germany, with similar numbers of fledglings produced in very large clutches (more than 4) as in medium-sized ones (2-4).  A grading of success from a study in Sweden found categories of competent and less competent pairs, with losses averaging 7% and 17% in these two groups, respectively.  Studies from Finland and the Yukon Territory found that average number of fledglings varied dramatically based on food supply based on the cyclical nature of most prey in these northern areas, varying from average success rates of 0 to 3.9 fledglings in the latter region.  Similar wide variations in breeding success in correlation to prey levels were noted at other areas, including Nevada (where the number of fledglings could be up to seven times higher when lagomorphs were at their population peaks) and Wisconsin. Poor weather, which consists of cold springs that bear late cold spells, snow and freezing rain, causes many nests to fail, and may also hamper courtship and lower brood size and overall breeding attempts.  However, the most important cause of nest failure were found to be nest destruction by humans and other predations, starvation, then bad weather and collapse of nests in declining order.  On average, humans are responsible based on known studies for about 17% of nest failures in Europe.  32% of 97 nestlings in Bavaria, Germany died because of human activities, while 59% of 111 broods in England failed due to this factor.  Low food supplies are linked to predation, as it seems to cause greater risk of predation due to the lower nest attendance.  Lower densities of pairs may actually increase nesting success, as per studies from Finland where the highest median clutch size, at 3.8, was in the area with the lowest densities.  Similarly, in Schleswig-Holstein, nest failure was 14% higher where active nests were closer than 2 km apart compared to nests farther than this.  Age may also play a factor in nest success, pairings where one mate is not fully mature (usually the female, as males rarely breed before attaining adult plumage) is less than half as successful as ones where both were mature, based on studies from Arizona.  Overall, males do not normally breed at any younger than 3 years of age (although they are in adult plumage by two years) and females can breed at as young as 1 to 2 years old, but rarely produce successful, viable clutches.  The age at sexual maturity is the same as other northern \"Accipiters\" as well as most buteonine hawks (eagles on the other hand can take twice as long to attain full sexual maturity).  6-9 years of age seem to be the overall peak reproductive years for most northern goshawks.  However, some females can reproduce at as old as 17 years old and senescence is ambiguous in both sexes (possibly not occurring in males).  Median values of brood success was found to be 77% in Europe and 82% in North America overall.  Conversely, the median brood size is about half a chick smaller in North America than in Europe.  In Europe, clutch size overall averages 3.3, the number of nestlings averages 2.5 and fledglings averages 1.9. The lifespan in the wild is variable.  It is known that in captivity, northern goshawks may live up to 27 years of age.  Wild birds that survive their first two years can expect a lifespan of up to 11 years.  There is one record (apparently sourced to the AOU) of a 16 year, 4 month old goshawk.  In Fennoscandia, starvation was found to account for 3-6% of reported deaths.  In Norway, 9% of deaths were from starvation, but the percentage of demises from this increased to the north and effected juveniles more so than adults.  In Gotland, Sweden, 28% of mortality was from starvation and disease.  Both bacterial and viral diseases have been known to cause mortality in wild northern goshawks.  Variable numbers of goshawks are killed by flying into man-made objects such as powerlines and buildings and by automobiles, although lesser numbers are effected by powerline collisions than larger types of raptor. The breeding range of the northern goshawk extends over one-third of North America and Asia each and perhaps five-sixths of Europe, a total area of over 30,000,000 km2 .  Densities in western and central Europe were recorded at 3-5 pairs per 100 km2 .  In boreal Sweden, numbers vary from 1 to 4.5 pairs per 100 km2 , while in similar habitat in Alaska there were 0.3 to 2.7/100 km2 .  An average of only 1 pair per 100 km2 would give world population of 600,000 breeding birds, likely at least half as many immature and other non-breeders.  Recent study found 145,000-160,000 in Europe or 1 pair/60 km2 to 1 pair/54 km2 .  The total population of northern goshawks in the world probably ranges well over a million.  The total European populations, estimated at as many as 160,000 individuals, makes it the fourth most numerous raptor in the continent, after the common buzzards (>700,000 pairs), Eurasian sparrowhawk (>340,000 pairs) and common kestrel (>330,000 pairs).  The most populated countries by goshawks in Europe were Sweden (an estimated 10,000 pairs), Germany (8,500 pairs), Finland (6,000 pairs) and France (5,600 pairs).  The highest densities of breeding pair per 100 km2 of land were in The Netherlands, Latvia and Switzerland, although this is biased due to the small land area of these countries.  Russia has a roughly estimated 85,000 pairs of northern goshawk.  In North America, there are an broadly estimated 150,000-300,000 individuals.  In North America, most western populations at mid-latitudes have approximately 3.6-10.7 pairs100 km2 .  A total of 107 nesting territories (1991-1996) were located on a 1754 km2 study area on the Kaibab Plateau, AZ, resulting in a density of 8.4 pairs/100 km2 .  The estimated density in Pennsylvania (1.17 pairs/100 km2 ) suggests that eastern populations may occur at lower densities than western populations, but densities of eastern populations may increase as these populations recover.  Typically, populations at far northern latitudes may occur at lower densities than those of southwestern and western populations in North America.  Although median densities was similar, populations are overall much denser in Europe than in North America.  The hotspots of density for goshawks in Europe lie in east-central Europe (around Poland) and in west-central area (the Netherlands/West Germany). Mortality rates for first-year goshawks is often considerably higher than older birds.  In studies from Gotland, Sweden, Schleswig-Holstein, Germany and the Netherlands, 40-42% of first-years died.  By the second year, mortality rates drop to 31-35%, based on ring studies from the Netherlands and Finland.  Based on studies from Gotland, Finland and the southwestern United States, annual mortality for adults is 15-21%, however feather results indicate that annual mortality for adult goshawks is up to 7% higher in Europe than in North America.  In many parts of the range, especially Europe, historic populations decreased regionally due to human persecution (especially shooting), disturbance and epidemic loss of habitat, especially during the 19th century and early 20th.  Some states, like Pennsylvania, paid $5 bounties on Goshawks in the 1930s.  From 1880-1930, an estimated 3,000-5,500 goshawk were being killed annually in Norway when bounties were offered.  Shooting rate lowered later, causing the average number of goshawks shot to drop to 654 to for the period 1965-1970.  In Finland, where the species was not legally protected, 4,000-8,000 goshawks were being killed annually from 1964-1975.  Most goshawks shot are incautious juveniles, with 58% of juvenile mortality in Germany and 59% from the Netherlands being killings by humans.  Increase of pheasant releases in Vendsyssel, Denmark from 6,000 to 35,000 since 1994 has resulted in fewer goshawks as they often hunt the pheasants in winter and are shot, legally, by the region’s gamekeepers.  As recently as about 5 years before that, intentional killing by humans continued as the main cause of mortality for goshawks on Gotland, Sweden, causing 36% of deaths.  In the United Kingdom and Ireland, the northern goshawk was extirpated in the 19th century because of specimen collectors and persecution by gamekeepers, but in recent years it has come back by immigration from Europe, escaped falconry birds, and deliberate releases.  The goshawk is now found in considerable numbers in Kielder Forest, Northumberland, which is the largest forest in Britain.  Overall there are some 200 birds in Great Britain. In the 1950s-1960s declines were increasingly linked with pesticide pollution.  However, in early 1970s pesticide levels in the United States for goshawks were low.  Eggshell thinning has not been a problem for most populations, although California eggshells (weight and thickness index) pre-1947 (pre-DDT) to 1947-1964 (DDT in use) declined some 8-12%.  In Illinois, migratory goshawks during the winter of 1972-1973 invasion year contained less organochlorine and PCB residues than did other raptors, however these birds were probably from nonagricultural, northern forests.  Higher DDT levels seemed to have persisted quite recently in Europe.  This was the case in Germany, especially in former East Germany where DDT was widely available until 1988, having been largely discontinued elsewhere after the 1970s.  Goshawks, which had increased in The Netherlands after World War II due to less persecution, new woodlands and increased pigeon numbers, were found to have suddenly crashed from the late 1950s on.  It was later revealed that this was due to DDT, the number of breeding pairs decreasing 84% from 1958 to 1963.  As opposed to DDT, the main contaminant found to have reduced goshawks in Scandinavia during the 20th century were methyl mercury seed dressings used to reduce fungal attack in livestock. Seemingly the remaining persistent conservation threat to goshawks, given their seeming overall resilience (at the species level) to both persecution and pesticides, is deforestation.  Timber harvests are known to destroy many nests and adversely regional populations.  Harvest methods that create extensive areas of reduced forest canopy cover, dropping to cover less than 35-40%, may be especially detrimental as cases of this usually cause all goshawks to disappear from the area.  However, the mortality rates due to foresting practices are unknown and it is possible that some mature goshawks may simply be able to shift to other regions when a habitat becomes unsuitable but this is presumably unsustainable in the long-term.  In harvest forests of California, where overstory trees are frequently removed, goshawks have been found to successfully remain as breeding species as long as some mature stands are left intact.  Despite the decline of habitat quality and the frequent disturbances, this region’s goshawks breeding success rates somewhat improbably did not reduce.  Similarly, a study from Italy and France shows that goshawks only left woodlots when the canopy was reduced by more than 30%, although the European goshawk populations have long been known to be adaptable to some degree of habitat fragmentation.  Based on habitat usage studied in New Jersey and New York, this adaptability is not seen everywhere, as here nests were further from human habitation than expected on the basis of available habitat, an observation suggesting that disturbance regionally can reduce habitat quality.  Similarly, studies from the American southwest and Canada have indicated that heavily logged areas caused strong long-term regional declines for goshawks.  In Arizona, it was found that even when the nests were left intact, the noisy timber harvest work often caused failure of nesting during the incubation stage, and all nesting attempts that were occurring within 50 to of active logging failed, frequently after parents abandoned the nest.  Other noisy activity, such as camping, have also caused nests to failure.  Wildlife researchers and biologists do not seem to negatively affect goshawk nests, as they aware to keep forays to the nest brief and capture of adult goshawks for radio-tagging was found to not determent their success at raising broods. In North America, several non-governmental conservation organizations petitioned the Department of Interior, United States Fish & Wildlife Service (1991 & 1997) to list the goshawk as \"threatened\" or \"endangered\" under the authority of the Endangered Species Act.  Both petitions argued for listing primarily on the basis of historic and ongoing nesting habitat loss, specifically the loss of old-growth and mature forest stands throughout the goshawk's known range.  In both instances, the U.S. Fish & Wildlife Service concluded that listing was not warranted, but state and federal natural resource agencies responded during the petition process with standardized and long-term goshawk inventory and monitoring efforts, especially throughout U.S. Forest Service lands in the Western U.S.  The United States Forest Service (US Dept of Agriculture) has listed the goshawk as a \"sensitive species\", while it also benefits from various protection at the state level.  In North America, the goshawk is federally protected under the Migratory Bird Treaty Act of 1918 by an amendment incorporating native birds of prey into the Act in 1972.  The northern goshawk is also listed in Appendix II of the Convention on International Trade in Endangered Species (CITES). The northern goshawk appears on the flag of the Azores.  The archipelago of the Azores, Portugal, takes its name from the Portuguese language word for goshawk, (\"açor\"), because the explorers who discovered the archipelago thought the birds of prey they saw there were goshawks; later it was found that these birds were kites or common buzzards (\"Buteo buteo rothschildi\").  The goshawk features in the crest of the Drummond Clan. The name \"goshawk\" is a traditional name from Anglo-Saxon \"gōshafoc\", literally \"goose hawk\".  The name implies prowess against larger quarry such as wild geese, but were also flown against crane species and other large waterbirds.  The name \"goose hawk\" is somewhat of a misnomer, however, as the traditional quarry for goshawks in ancient and contemporary falconry has been rabbits, pheasants, partridge, and medium-sized waterfowl, which are similar to much of the prey the species hunts in the wild.  A notable exception is in records of traditional Japanese falconry, where goshawks were used more regularly on goose and crane species.  In ancient European falconry literature, goshawks were often referred to as a yeoman's bird or the \"cook's bird\" because of their utility as a hunting partner catching edible prey, as opposed to the peregrine falcon, also a prized falconry bird, but more associated with noblemen and less adapted to a variety of hunting techniques and prey types found in wooded areas.  The northern goshawk has remained equal to the peregrine falcon in its stature and popularity in modern falconry. Goshawk hunting flights in falconry typically begin from the falconer's gloved hand, where the fleeing bird or rabbit is pursued in a horizontal chase.  The goshawk's flight in pursuit of prey is characterized by an intense burst of speed often followed by a binding maneuver, where the goshawk, if the prey is a bird, inverts and seizes the prey from below.  The goshawk, like other accipiters, shows a marked willingness to follow prey into thick vegetation, even pursuing prey on foot through brush.  Goshawks trained for falconry not infrequently escape their handlers and, extrapolated from the present day British population which is composed mostly of escaped birds as such, have reasonably high survival rates, although many do die shortly after escape and many do not successfully breed.  The effect of modern day collection of northern goshawks for falconry purposes is unclear, unlike some falcon species which can show regional declines due to heavy falconry collections but can increase in other areas due to established escapees from falconers.\n\nSpeed limits in the United States by jurisdiction Speed limits in the United States vary depending on jurisdiction, with 75 to common in the Western United States and 65 to common in the Eastern United States.  States may also set special speed limits for trucks and night travel along with minimum speed limits.  The highest speed limit in the country is 85 mph , which is posted on a single stretch of tollway in rural Texas. In Alabama, it is illegal to drive at a speed that is not \"reasonable and prudent\" for the current conditions and hazards.  Drivers must also not drive so slow that they impede the flow of traffic.  If the speed limit is not otherwise posted, it is: Trucks carrying hazardous materials are not to exceed 55 mph . In Alaska, many of the major highways carry a 65 mph speed limit, including: The Minnesota Drive Expressway features a 60 mph speed limit, as does the Richardson Highway between Fairbanks and North Pole. Since the mid-1990s, Alaska's major highways have gradually been upgraded from 55 mph to 60 or 65 mph.  However, several continue to carry the default 55 mph speed limit, including: Engineering studies are needed to define which road segments to post a speed limit higher than 55 mph . Default speed limits in Alaska are: The speed limit when towing a mobile home is 45 mph . The default speed limit outside of \"business or residential\" districts in Arizona is 65 mph , within those districts the default speed limit is 25 mph .  The school zone speed limit is 15 mph , while some may be 25 to 35 mph .  Exceeding these limits only in the best of driving conditions is considered \"prima facie\" evidence of speeding.  Altered speed limits are not \"prima facie\". The maximum speed limit on Interstate Highways is 75 mph .  This limit may be applied outside of \"urbanized areas\", where speeds of over 85 mph on any highway (regardless of the posted speed limit) is considered a criminal (rather than civil) offense.  However, Interstate 10 near the California border is reduced to 65 mph .  Some portions of Interstate 15 have the same regulations due to sharp curves.  There is an exception of urban highway in Casa Grande, with a speed limit of 75 mph , while other urban highways are capped at 55 or 65 mph .  Within \"business or residential\" districts, exceeding the speed limit by more than 20 mph is considered criminal.  Within \"urbanized areas\", 55 mph speed limit citations are given for \"waste of a finite resource\".  This exception only applies within a 10 mph threshold.  As long as the speed does not exceed 65 mph , the infraction is not recorded as a traffic violation for the purposes of a point system. Non-passenger vehicles in excess of 13 ST , or \"vehicles drawing a pole trailer\" weighing more than 3 ST may not exceed 65 mph unless signs are posted that allow such a speed.  Yet this does not differ from the default speed limit, and has the practical effect of requiring extra consideration for posting a standard speed limit sign in excess of 65 mph .  A non-numeric minimum speed limit is incorporated with the basic speed rule in Arizona, which also prohibits speeds higher than would be \"reasonable and prudent\". Night speed limit signs are posted on some roads within Tucson city limits that do not have street lights.  Examples: Fort Lowell Road from Oracle Road to Country Club Road, 22nd Street from Interstate 10 to Cherry Avenue. Urban districts by default are posted at .  Outside of the municipal limits, two-lane state and federal highways have a speed limit of unless otherwise posted, and 2 lane county roads have a speed limit of unless otherwise posted.  In June 2015, the Arkansas Highway Commission authorized the Arkansas Highway and Transportation Department (AHTD) to raise the speed limit on undivided 4 and 5 lane roads from while divided 4 lane roads are set to go from , these changes affect 285 miles of Arkansas highways.  Furthermore, AHTD has established freeway default speed limits.  Along rural freeways the limit is while suburban freeways are posted at .  School zone speed limits apply only when children are present, or when a lighted beacon is on if one is provided and such speed limits are set at unless otherwise posted.  It is fairly common however that schools serving only higher grade levels will not have a school speed limit in rural areas or where such school sits more than 500 feet off of the highway or street. On March 16, 2017, the Arkansas House introduced a bill that would allow the state highway commission to increase speed limits up to 75 mph in rural interstate freeways and sets rural non-divided highway speed limits to 65 mph.  This bill became law on April 7, 2017. The new 75 mph (121 km/h) speed limit doesn't apply to trucks.  Trucks will still be limited to 70 mph.  (Fact check, this provision was removed from act 1097.) California's \"Basic Speed Law\", part of the California Vehicle Code, defines the maximum speed at which a car may travel as a \"reasonable and prudent\" speed, given road conditions.  The reasonable speed may be lower than the posted speed limit in conditions such as fog, heavy rain, ice, snow, gravel, sharp corners, blinding glare, darkness, crossing traffic, or when there is an obstructed view of orthogonal traffic—such as by road curvature, parked cars, vegetation, or snow banks—thus limiting the Assured Clear Distance Ahead (ACDA).  Basic speed laws are statutized reinforcements of the centuries-old common law negligence doctrine as specifically applied to vehicular speed.  California Vehicle Code section 22350 is typical; it states that \"No person shall drive a vehicle upon a highway at a speed greater than is reasonable ... and in no event at a speed which endangers the safety of persons or property\". Speed limits in California are mandated by statute to be set: (1) at or below the 85th percentile operating speed; as determined by a traffic and engineering survey—this is the speed that no more than 15% of traffic exceeds; or (2) the prima facie limits mandated when certain criteria are met as described in the vehicle code.  These criteria include school zone, alleyway, and residential area. If the 85th percentile operating speed as measured by a Traffic and Engineering Survey exceeds the design speed, compulsory legal protection is given to that speed—even if it is unsafe with regard to certain technical aspects such as sight distance.  This speed creep may continue until the 85th percentile operating speed is comparable to speed psychologically perceived as uncomfortably hazardous. The theory behind California's 85th percentile statute is that, as a policy, most of the electorate should be seen as lawful, and limits must be practical to enforce.  However, there are some circumstances where motorists do not tend to process all the risks involved, and as a mass choose a poor 85th percentile speed.  This rule in substance is a process for voting the speed limit by driving; and in contrast to delegating the speed limit to an engineering expert. The numerical limit set by Caltrans engineers for speed limit signs, generally found on all non-controlled-access routes, is considered a presumptive maximum \"reasonable and prudent\" speed. Many speed limit signs are identified as \"maximum speed\", usually when the limit is 55 mph (89 km/h) or more.  When the National Maximum Speed Law was enacted, California was forced to create a new legal signage category, \"Maximum Speed\", to indicate to drivers that the did not apply for speeds over the federally mandated speed cap; rather, it would be a violation to exceed the fixed maximum speed indicated on the sign, regardless of whether the driver's speed could be considered \"reasonable and prudent\". A driver can receive a traffic citation for violating the even if their speed is below the \"maximum speed limit\" if road, weather, or traffic conditions make that speed unsafe.  However, because the Basic Speed Law establishes \"prima facie\" limits, not absolute ones, they can also defend against a citation for speeding \"by competent evidence that the speed in excess of said limits did not constitute a violation of the basic speed law at the time, place and under the conditions then existing\", per section 22351(b) of the California Vehicle Code.  As attorney David W. Brown says in his book \"Fight Your Ticket & Win in California\", \"a person traveling over the speed limit–but less than the usual 65 mph maximum speed (55 mph for two-lane undivided highways)–isn't necessarily violating the law\" and that \"you can defend against a charge of violating the Basic Speed Law not only by showing you weren't exceeding the speed limit, but also by establishing that even if you were over the limit, your speed was nevertheless 'safe' under the circumstances.\" Rural freeways, such as parts of I-5, I-8, I-10, I-15, I-40, I-205, I-215, I-505, I-580 (between I-205 and I-5), U.S. 101 between San Miguel and King City, and SR 99 south of Madera and Fresno, have speed limits.  The highest speed limit on I-80 is because it passes exclusively through urban and mountainous areas.  However, the speed limit on the San Francisco–Oakland Bay Bridge and in San Francisco is only .  In downtown Los Angeles, the maximum speed limit is .  This includes the entire length of the Pasadena Freeway between Pasadena and downtown Los Angeles, and portions of the Hollywood, Santa Ana, Santa Monica, and Harbor Freeways.  The default limit on 2-lane roads is .  However, Caltrans or a local agency can post a speed of up to after an engineering study. There is a speed limit for trucks with 3 or more axles and all vehicles while towing. In California, the Maximum Speed in school zones is , but may only be in effect when children are present within that school zone. The maximum speed limit in Colorado is on rural Interstate highways, even though Interstate 70 in the Rocky Mountains has a limit because of steep grades and curves and a limit at the east and west ends of the Eisenhower Tunnel.  The maximum speed limit on other rural highways is . There are also basic prima facie speed limits in Colorado. On certain stretches of rural highways, notably US 160 between Durango and Pagosa Springs and US 550 between Durango and Silverton, nighttime speed limits are in effect during peak migratory periods for area wildlife.  Speeding fines are doubled when nighttime speed limits are in effect. Speed limits in Connecticut are normally on rural freeways; up to on rural divided and undivided highways.  In urban areas speed limits vary from on residential streets and central business districts, on arterial roadways, and from on urban freeways.  Limited-access divided highways have a minimum speed of , but this is not always posted, and rarely enforced.  Connecticut was among the last states to raise its maximum speed limit from originally established by the National Speed Limit in 1974.  The statewide maximum speed limit was increased from to on October 1, 1998, making Connecticut the last state in the continental United States to raise its speed limit above . Speed limits for all roads within Connecticut—including local streets—are established by the State Traffic Commission, an agency composed of members of the Department of Motor Vehicles (CTDMV), the Department of Emergency Services and Public Protection (DESPP), and the Department of Transportation (CONNDOT). The State Traffic Commission typically sets speed limits following engineering studies performed by CONNDOT.  Data used in setting speed limits includes traffic volume vs. roadway capacity, design speed, road geometry, the spacing of intersections and/or interchanges, the number of driveways and curb cuts, and accident rates. Municipalities are normally required to seek approval from the State Traffic Commission for changes to the posted speed limits on locally owned streets after appropriate engineering studies are performed. Speeding fines are doubled in school zones when children are present, and construction areas when workers are present. Prior to enactment of the National Speed Limit Law in 1974, Connecticut permitted a maximum speed limit of on rural freeways. In Delaware, three roads carry a 65 mph speed limit: I-495, the toll road portion of Delaware Route 1, and Interstate 95 from the Maryland line to the southern junction with I-495.  The remainder of I-95 between the southern junction with I-495 and the Pennsylvania line and the freeway portion of Delaware Route 141 are 55 mph while Interstate 295 is 50 mph .  Prior to the National Maximum Speed Law that went into effect nationwide, I-95 used to have a 60 mph speed limit except around Wilmington.  In May 2015, the state of Delaware increased the speed limit on Interstate 95 from 55 mph to 65 mph between the Maryland state line and the I-495 interchange.  In January 2017, the speed limit on Delaware Route 1 between Trap Shooters Road and the Puncheon Run Connector in Dover was increased from 55 mph to 60 mph while the speed limit on the Puncheon Run Connector was increased from 50 mph to 60 mph . All rural two-lane state-owned roads have 50 mph speed limits, while all urban speed limits, regardless of location, are held at 25 mph for two-lane roads and up to 35 mph for four-lane roads.  Four lane highways such as US 13, US 113, portions of US 40 near Bear and Glasgow, and the at-grade portions of DE 1 are normally 55 mph . School zones have 20 mph speed limits. Interstate 495, which forms a bypass around Wilmington, features changeable speed limit signs for environmental purposes.  These signs typically display a 65 mph speed limit, but this limit changes to 55 mph on days when air quality is a concern.  The limit is also lowered during construction, weather conditions, and when accidents occur. All neighborhoods and subdivisions in Delaware have a maximum speed limit of 25 mph as set by state law.  Frequent ad campaigns on in-state radio stations remind residents of this (as of January 2013). Florida has a maximum speed limit of , found on freeways, including rural Interstate Highways, some urban freeways including I-4 in Lakeland, I-75 in Tampa and Miami (where I-75 ends), I-95 near Daytona Beach and from Military Trail to Florida State Route 706 in Palm Beach County, portions of the Orlando area Toll Roads such as SR 417 and SR 429, Florida's Turnpike through Port St Lucie and Orlando, I-10 close to Tallahassee, and most other rural limited access toll roads such as the Suncoast Parkway and the Beachline Expressway and rural portions of Florida's Turnpike.  is typical on rural 4-lane highways (such as US-19 north of St. Petersburg, among other US Highways) as well as most other urban freeways and tollways.  Rural two-lane roads typically have a speed limit of (the default limit for such roads), although FDOT is permitted to post on appropriately-suited highways.  This is typically done on very rural state roads (such as SR 471) and US Highways (such as US 98 along most of the state's panhandle). Florida typically does not post night speed limits, but there are a few exceptions.  For the most part, these night time reduced speeds are located in wildlife preserves for such endangered species as the Florida panther and the key deer.  Most of the Tamiami Trail through the Big Cypress National Preserve has a night speed limit.  On some stretches of road where the speed limit is reduced at night, the daytime speed limit sign is non-reflective, such that at night, only the night limit is visible. Florida's minimum speed limit on Interstate Highways is in 70 mph zones.  In 55 mph, and 65 mph urban interstate zones, the minimum is .  At one time, these minimum speeds required signage, but these limits have since been codified in state law; signs indicating these minimum speeds still exist, but now simply serve as a reminder.  Urban freeways with speed limits of 50 mph typically do not have minimum speed limits, such as on I-375 in St. Petersburg.  Additionally, the new Gandy Freeway in St Petersburg has a speed limit as low as . Florida also does not impose lower truck speed limits.  As such, all traffic is permitted to travel at the same speed. School zones in Florida usually have limits.  Most have flashing yellow lights activated during the times they are in effect as well as accompanying signs that post the times these reduced speed limits are effective.  All are strictly enforced and carry an increased penalty for violations. Rural Interstate Highways are posted at .  Until 2014, sections of Interstates passing through a municipality or metropolitan area with a population over fifty thousand were capped at .  However, a new law has permitted urban interstates to now be posted as high as 70 mph, and some have already reflected this change, such as I-95 through Brunswick, I-85 in Gwinnett County, and I-75 in Macon, Valdosta, and Tifton.  Most urban interstates, however, still remain at or below 65 mph.  I-285 in the Atlanta area was recently increased from to (with Variable speed limits on the north portion.  I-95 and I-16 through suburban Savannah (the 65 mph limit on I-95 is only for a 1.5 mile section in the vicinity of the I-16 interchange), I-16 from the interchange with I-75 in central Macon eastbound past Exit 2 is at 65 mph, and I-185 in Columbus remain at 65 mph, while the Downtown Connector and portions of I-20 in Atlanta are posted as low as .  Most non-interstate freeways such as SR 400 and the Athens perimeter highway, are posted at 55 to 65 mph. Four lane arterials and expressways can be posted as high as .  However, Dillon's Rule enables counties outside municipalities to keep four-lane GRIP corridors at .  However, in recent years, US 1 between Augusta and Wrens raised the speed limit to 65 mph.  Other rural four-lane highways with a 65 mph include portions of US 441 near Irwinton, US 25 between Augusta and Statesboro, SR 88 between Sandersville and Wrens, SR 16 between Griffin and I-75, much of US 341 between Brunswick and I-75, and much of US 82 in South Georgia. Two lane state roads by default are posted at .  County maintained roads will rarely have speed limits above in middle & south Georgia, in north Georgia.  Both in the Atlanta area, Ronald Reagan Parkway is posted at as a county maintained freeway and Sugarloaf Parkway is posted at along its new eastern freeway portion. Inside the municipality, speed limits are generally posted at while it is in the downtown area. All roadways maintained by GDOT that are subject to speed limit reductions are given advanced notice with signage that says \"REDUCED SPEED AHEAD\".  Furthermore, GDOT has a policy of doing increments but never higher than 10 mph. Georgia is one of few states with anti-speed trap laws passed in the late 1990s.  Speed violations less than over the speed limit will have no points assessed.  Fines are not assessed for motorists going less than over the speed limit.  In 2009, Georgia introduced the \"Super Speeder\" law, which adds an additional speeding fine of $200 for motorists convicted of traveling or more on a two-lane or undivided road and or more on a divided highway. Hawaii was the last state to raise its maximum speed limit after the National Maximum Speed Law was repealed in 1995, and still has the lowest maximum speed limit of any state.  In 2002, following public outcry after a controversial experiment with speed enforcement using traffic enforcement cameras, the state Department of Transportation raised the speed limit to on Interstate H-1 between Kapolei and Waipahu, and Interstate H-3 between the Tetsuo Harano Tunnels and the junction with H-1.  All other freeways, including Interstate H-2, have a maximum speed limit of , with the limit dropping to in central Honolulu.  Other highways generally have speed limits of 55 mph and in many cases much less.  On July 6, 2016, Governor David Ige has signed a bill to allow the Speed Limit on Saddle Road to increase from 55 mph to 60 mph (the limit was increased to 60 mph the week of February 5, 2017). Hawaii has a minimum speed along much of Interstate H-1 of only below the speed limit.  The minimum speed is usually when the speed limit is 55, and when the speed limit is 50. The speed limit on a freeway in Idaho is generally in rural areas and in urban areas.  Trucks are limited to .  Generally, both single- and multi-lane rural roads have limits.  Roads with traffic lights are posted at or below.  The school zone speed limit in Idaho is . Idaho senator Bart Davis brought SB 1284a to the House of Representatives for discussion in early 2014.  The bill passed the Senate on February 25 and was signed into law by Governor Butch Otter on March 18, 2014, which was set to raise the speed limit on rural interstates to on July 1, 2014, the same date Wyoming raised its speed limit.  Days before the law was to go into effect, however, it was put on hold in order to allow a more thorough review of the effects of a raised speed limit.  A vote on July 14, 2014 approved the 80 mph increase on limited sections of interstates in the southern portions of Idaho.  Studies will begin for other areas later this summer. The bill also would raise truck and two-lane highway speed limits to .  As of July 24, 2014, the new 80 mph signs are up on rural Idaho Interstates. On March 22, 2017 the speed limit on a four-lane, divided stretch of US 20 between Idaho Falls and Ashton was raised from to 70 mph. Interstate Highways in Illinois are usually posted with both minimum and maximum speed limits, except in some urban areas, particularly Chicago.  The standard speed limit is 70 mph for rural interstates, a 45 mph minimum speed limit, 65 mph for other 4 lane divided highways, and 55 mph for all other highways.  Urban freeway/interstate speed limits can range from as low as 45 mph in downtown Chicago, where all the major interstates merge, to as high as 65 mph in the outer portions of the Chicago metro and East St. Louis metro areas, and in some smaller cities.  Some interstates in small cities (e.g. I-55/74 through Bloomington-Normal, I-39/90 through the Rockford area, I-57 & I-74 through Champaign-Urbana) do not have reduced speed limits, and a stretch of I-355 between I-55 and I-80 in the southwest suburbs of Chicago is signed at 70 mph.  Most freeways and interstates in Cook, DuPage, and Lake Counties, and some interstates and freeways in Will County maintain a 55-60 mph speed limit.  Due to the high population density, the only freeways in Cook County that exceed a speed limit of 60 mph are I-57 at the southern edge of the county, part of I-80 between Central Ave and Harlem Ave, and the southern segment of I-355, which passes through Cook County briefly before crossing into Will County to both the north and south.  As of January 2010, a reduced speed limit posted in a construction zone must be obeyed 24 hours a day, regardless of whether workers are present. On February 10, 2017, a bill was introduced to raise the speed limit on select rural interstate freeways and highways to 75 mph.  The bill also contains a provision to increase maximum speed limits to 60 mph for all rural highways, roads, and streets that do not have 4 or more lanes of traffic. As of May 27, 2017, on I-90 (Jane Addams Tollway), the speed limit remains 55 mph all the way to Elgin west of Randall Road.  Despite I-290 being posted 60 mph for a few miles in Schaumburg from I-90 down to the new Tollway IL-390. In Indiana speed limits on Interstate Highways are usually 70 mph (110 km/h) for cars and 65 mph (100 km/h) for trucks with a gross vehicular weight (GVW) of 13 tons or greater, except in urban areas, where it is generally 55 mph (90 km/h) in city centers (except stretches of I-70 in Indianapolis where it is 50 mph) and 65 mph (100 km/h) cars/60 mph (95 km/h) trucks in suburban areas, with the exception of the Indiana Toll Road towards the South Bend and Gary areas, which carries a 70/truck: 65 mph speed limit.  The 65/truck: 60 signs are posted only for a short distance on Interstate 74 just west of Indianapolis and the western segment of Interstate 465. Most non-Interstate Highways are 55 mph, but some rural four-lane divided highways (such as rural stretches of U.S. 31, U.S. 40 and U.S. 41, among others) are set at 60 mph.  These limits often decrease to 30–50 mph (50–80 km/h) approaching urban areas, and within cities a speed limit of 20–30 mph (30–50 km/h) is not uncommon, though larger arterial roads within cities may reach as high as 45 mph (70 km/h). On February 6, 2012 the Indiana toll road was raised from the Illinois state line to mile marker 20 to 70 mph after a major highway reconstruction project. In Iowa, the majority of highways have a 55 mph speed limit.  Rural Interstate Highways carry a 70 mph limit and a 40 mph minimum.  Urban Interstate limits generally range from 55 to , but may be lower in areas.  The Interstate 74 Bridge from Bettendorf to Moline, Illinois, for instance, has a 50 mph limit because the bridge is narrow and has no shoulders.  Four-lane roads may have a 65 mph limit.  If the road is built to freeway standards, such as US 20 between I-35 and Dubuque, it may have a minimum speed limit, but otherwise four-lane roads carry no minimum limit so slow-moving farm vehicles may use the roadway. On February 21, 2017, a bill was introduced to raise speed limits from 70 mph to 75 mph on interstate freeways. After the National Maximum Speed Limit was repealed, Kansas raised its general interstate speed limit to 70 mph ; a study found \"no statistically significant increases in crash, fatal crash and fatality rates were noted during the after period on either rural or urban interstate highway networks.  On the other hand, statistically significant increases in crash, fatal crash and fatality rates were observed on the 2-lane rural highway network.\" .  In 2011 Governor Sam Brownback signed legislation raising Kansas' top speed limit to 75 mph on Rural Interstates and limited access portions of U.S. Routes, effective July 1, 2011.  The Kansas Department of Transportation announced on June 21, 2011, that 807 miles of roadway, comprising the rural areas of I-70, I-35, I-135, the Kansas Turnpike and the freeway-improved sections of US-69 and US-81, will be raised to 75 mph.  Other four-lane, non-limited access divided highways have a speed limit of 70 mph, with 65 mph on two-lane undivided roads, and 55 mph on township roads.  Prior to the National Maximum Speed Limit, the speed limit on the Kansas Turnpike used to be 80 mph , but was reduced to 75 mph on August 17, 1970.  The minimum speed limit on Kansas Interstates is 40 mph. Kentucky generally has a 70 mph speed limit on rural freeways as of 2007.  The speed limit is reduced to 55 on multi-lane highways in some urban areas (I-71/75 near Cincinnati, I-64, I-65, I-71 and I-264 in Louisville, U.S. 60 bypass in Owensboro), and KY 4 in Lexington.  There are two 50 mph areas in Louisville.  One approaching the Sherman Minton Bridge crossing the Ohio River into Indiana on I-64, and one approaching the Kennedy Bridge on I-65 towards Indiana.  The Transportation Cabinet is now authorized to raise any multilane, divided rural highway up to 65 MPH based on speed and design studies.  Anyone may request an increase by contacting their local Transportation Cabinet office and specifying the roadway to be raised.  Two-lane, undivided highways are limited to 55 MPH.  Points are not assessed for speeds less than 10 mph over the speed limit only on limited access highways, or for tickets received by Kentucky licensed drivers out of state. Louisiana's highest speed limit is 75 mph, found on 154 miles of Interstate 49 in Saint Landry, Avoyelles, Evangeline, Rapides, Natchitoches, DeSoto and Caddo parishes.  The 75 zone was established by the Louisiana Department of Transportation and Development in 2011 after a 2010 bill authorized the DOTD to implement 75 zones where proven to be safe. 70 mph is posted on Interstates 10, 12, 20, 49, 55, 59, 220, and 310. A speed limit of 60 mph is posted on I-10 in Lake Charles, Baton Rouge, and from LaPlace to New Orleans, I-12 in Baton Rouge, I-20 in Shreveport and Monroe, I-49 in Alexandria and Shreveport, I-310 in Destrehan, I-220 in Shreveport, U.S. Routes 71 and 167 in Kingsville, LA 3132, and Interstates 110, 210, 510, 610, and 910 (note: part of I-10 in Baton Rouge was raised to 70 mph, and part of I-12 in Baton Rouge was also raised to 65 mph). Most two-lane highways in Louisiana have a maximum speed limit of 55 mph. In August 2003, Governor Mike Foster announced speed and lane restrictions on trucks on the 18 mile (29 km) stretch of Interstate 10 known as the Atchafalaya Swamp Freeway.  The restrictions lower the truck speed limit to 55 mph and restrict them to the right lane for the entire length of the elevated freeway. There are exceptions to the basic highway and speed laws Divided highways in rural areas have a 65 mph speed limits.  Louisiana law R.S. 32:61(B) & 32:62(A) states;65 mph on other multi-lane divided highways which have partial or no control of access. Louisiana operates under the reasonable and prudent basic law;No person shall drive a vehicle at a speed greater than is reasonable and prudent under the conditions and potential hazards then existing, having due regard for the traffic on, and the surface and width of, the highway, and the condition of the weather.  R.S. 32:64(A) A person, who is operating a motor vehicle on a multilane highway at less than the normal speed of traffic, shall drive in the right-hand lane then available for traffic.  R.S. 32:71(B)(1) Maine carries the highest speed limit on the East Coast, with Interstate 95 carrying a 75 mph limit between Old Town and Houlton.  Sections of I-95 south of Old Town as well as 295 carry 70 mph limits, except for brief 50-65 zones in more populated areas (NOTE: the speed limit on I-295 was recently restored to 65 mph between Falmouth and Topsham as of March 27, 2017).  The Saco stub I-195 is 60 mph, and I-395 is 60 mph in Bangor and 65 mph in Brewer. Default speed limits in Maine are: In Maine, school buses may not exceed 45 mph on roads with higher speed limits while transporting students.  At other times, the limit is 55 mph, unless on an Interstate highway, in which case the posted limit applies. Also the speed limit on county road is 30 MPH Fines for speeding are at least $50 by law.  Exceeding the speed limit by 30 mph or more is considered a criminal offense. The speed limit on Maryland's Interstate Highways are posted by default at 65 mph although 70 mph limits can be posted after a traffic and engineering study.  Effective October 1, 2015 the speed limit on I-68 is 70 mph except for a seven-mile section around Cumberland.  Effective April 4, 2016, the speed limit on I-70 has been increased to 70 mph from the Pennsylvania state line to MD 180 in Frederick County and from MD 144 in Frederick County to US 29 in Howard County.  Maryland's urban freeways normally have speed limits of 55 mph (like I-495) or 60 mph, although some stretches are signed for 65 mph travel such as portions of I-95 and I-97 in and around the Baltimore suburbs, I-70 around Frederick, and I-81 around Hagerstown.  I-70 around Hagerstown is posted at 70 mph.  More restrictive limits are found on I-83 south of North Avenue when approaching downtown Baltimore and on I-68 through Cumberland, both sections being marked at 40 mph. Four lane non-interstates and non-freeways are posted at 55 mph.  This includes the expressway grade roadways like US 50 and US 301 east of the Bay Bridge, US 15 north of Frederick to the Pennsylvania state line, MD 404 around Denton and US 29 between I-495 and I-70. Normally, speed limit drops are in 5 mph to 10 mph increments.  However, one speed zone drops from 55 to 25 mph along US 301 southbound at the Nice Bridge for the toll plaza. Two lane roads are generally posted at 50 mph but there are a handful of routes posted at 55 mph.  It is more common to see 55 mph on the Eastern Shore and in Frederick and Carroll counties than the Baltimore-Washington corridor and Western Maryland. Urban and downtown speed limits are generally posted at 30 mph. As prescribed by Massachusetts law, default speed limits are the following: State highways and other arterials are often posted at in urban areas and in rural areas.  A select number of undivided roads are posted at .  Divided highways are usually posted at in rural areas as well as business districts.  Interstate highways and some non-Interstate controlled-access highways in suburban and rural areas are posted at , but many non-Interstate highways are posted at , such as the freeway portions of US 3, US 6, Route 2, and Route 128, or , such as Massachusetts Route 3 South of Boston.  Urban freeways are often posted at and occasionally lower, but some rural freeways that pass through urban areas maintain their speed limit, such as the Massachusetts Turnpike through the Springfield and Worcester areas. A \"thickly settled district\" is an area where building structures such as residential and commercial are less than 200 ft apart for a distance of 1 mi or more.  This can be subjective since a large part of eastern Massachusetts is built up with many different jurisdictions and different speed limits assigned. The maximum speed limit in Michigan is .  Michigan uses a formula based on the number of driveways and streets, or on the 85th percentile of free-flowing traffic, and if none those methods are used a default applies.  In rural areas, speed limits are as follows: Freeways in Michigan are usually signed with both minimum and maximum speeds.  By default, the freeway speed limit is , with a minimum speed of for all vehicles, despite a truck speed limit of —effectively permitting trucks only a range of legal speeds.  The Mackinac Bridge, due to its substandard design, has a speed limit of for passenger vehicles and for trucks.  The Michigan Department of Transportation and the Michigan State Police may raise the speed limit to after it is deemed safe to do so.  MDOT and the MSP announced on April 26, 2017 that the speed limit was increased to 75 mph on several Michigan freeways, including I-75 from Bay City to Sault Ste. Marie (excluding the Mackinac Bridge), I-69 from Business Loop 69 in Clinton County to I-94 in St. Clair County (excluding the section in and around Flint, which remains at ), US 127 from I-69 in Clinton County to I-75 in Crawford County (excluding the 15-mile stretch between St. Johns and Ithaca, which is not freeway standard), and US 131 from M-57 in Kent County to the end of the freeway north of Manton.  These increases commenced on May 1, 2017 and were completed by May 15, 2017.  Speed limits in freeway work zones are statutorily limited to 24 hours per day.  If workers are present (and not behind a barrier wall), drivers must slow to for the workers' safety. Michigan's speed limits on urban Interstates are typically higher than its neighboring states.  For example, in the Detroit metro area, I-75 southbound enters Detroit at M-102 (8 Mile Road, exit 59) and maintains a limit all the way until the interchange with I-94 (exit 53), where the speed limit drops to .  Other freeways in Detroit such as I-94 and I-96 also have speed limits in and around the city's downtown area, but rise to relatively soon after leaving the downtown area.  In Downtown Grand Rapids, I-196 has a speed limit of , the only other urban Interstate Highway to have a reduced speed limit.  US 131 in Grand Rapids is one of the only non-Interstate urban freeways in Michigan with a 70 mph speed limit, which was raised from 55 mph since 2013.  Furthermore, speed limits in smaller cities, such as Kalamazoo, Ann Arbor, and Lansing (the state capital) remain at . The default speed on all other highways, whether two or four lanes, is .  However, Michigan permits speed limits of up to after a safety study concludes the higher limit is safe to implement.  Until 2016, this provision only applied to four-lane divided non-limited access highways.  A 20 mile stretch of US 127 between St. Johns and Ithaca was posted at , as a compromise to allow a freer flow of traffic due to insufficient funds to improve the section to freeway standards.  The speed limit on US 2 between Rapid River and Gladstone in the Upper Peninsula was also raised to .  In Summer 2017 Speed limits began increasing to 65 mph on several select two-lane roads in both the Lower and Upper Peninsulas of Michigan, including US 2 from St. Ignace to Rapid River and M-28 from I-75 to Munising. A 70 mph speed limit is only allowed on Minnesota's Interstates outside of urban areas.  A speed limit of 55 mph is typically used in urban areas where a higher speed limit might be used, but traffic congestion or other reasons require a lower speed limit.  Examples include I-94, I-35W and I-35E in and around Minneapolis, Moorhead and Saint Paul.  35E goes down to a speed limit of 45 mph in some areas of Saint Paul.  A speed limit of 60 mph is typically used in suburban areas such as I-494 and I-694 loops in the Twin Cities metro area. Non-Interstate divided highways (both freeways and expressways) have speed limits of 65 mph in rural areas and up to 55 mph in urban or suburban areas.  Rural two lane State and US highways in Minnesota have a default speed limits of 55 mph although several 60 mph speed limits have been posted after a traffic and engineering study that will continue until 2019.  County roads have speed limits of up to 55 mph for 2 lanes and 60 for divided sections. A speed limit of 70 mph is only allowed on Mississippi's rural freeways; only the Interstates (except I-110), U.S. Highway 78, Mississippi Highway 304, and a portion of U.S. Highway 82 have speed limits of 70 mph, with these lengths making up approximately 86% of the state's freeway mileage. A speed limit of 65 mph is typically used on the state's four-lane divided highways, which include parts of the following roadways: A speed limit of 60 mph is typically used in urban areas where a higher speed limit might be used, but traffic or geometric conditions constitute a lower speed limit, including the following areas: House Bill 3, passed during the 2008 First Extraordinary Session of the state legislature, permits speed limits up to 80 mph on toll roads in the state; however, as of 2016 , no such road has been constructed. Mississippi has a minimum speed of 30 mph on four-lane U.S. highways when no hazard exists.  Strangely, there is no law for the minimum speed of the state's growing number of four-lane state highways.  The minimum is 40 mph on Interstate Highways and on four-lane U.S. designated highways that have a 70 mph speed limit.  In 2004, Mississippi posted minimum speed limits (40 mph) on all rural Interstates, but this minimum speed limit was already state law before then. Statutory speed limits in Missouri are as follows: Freeways are defined as: \"a limited access divided highway of at least ten miles in length with four or more lanes which is not part of the federal interstate system of highways which does not have any crossovers or accesses from streets, roads or other highways at the same grade level as such divided highway within such ten miles of divided highway.\" Expressways are defined as: \"a divided highway of at least ten miles in length with four or more lanes which is not part of the federal interstate system of highways which has crossovers or accesses from streets, roads or other highways at the same grade level as such divided highway.\" Urban Areas are defined as: \"an area of fifty thousand population at a density at or greater than one thousand persons per square mile\". The highways and transportation commission may raise or lower the speed limit on these highways, but no speed limit may be set above 70 mph on a numbered highway and 60 mph on a lettered highway. Interstate highways have minimum speed limits of 40 mph. Missouri concluded a two-year experiment with variable speed limits along I-270 around St. Louis.  Digital signs had been erected along the freeway as well as additional signs alerting drivers about the use of variable speed limits.  The limits will vary between 40 and 60 miles per hour, depending on traffic conditions, and could change by up to 5 mph every 5 minutes.  These speed limits, as of January 2012, are now posted as \"Advisory Speed Limits\". During the closure and major rebuild of I-64 in St. Louis, an additional lane was added to I-44 and I-70, and the speed limit was thus reduced to 55 mph on those roads within the St. Louis County and City.  The I-64 construction has been completed, and the extra lanes were removed in 2010.  In October 2010, the speed limit was restored to 60 mph on both I-44 and I-70. Most two-lane roads with shoulders have a 60 mph speed limit in Missouri.  Two-lane roads without shoulders are usually, but not always, limited to 55 mph.  However, the following two-lane highways have a 65 mph speed limit when bypassing or outside of incorporated areas. Most rural expressways have a 65 mph speed limit, but the following have a 70 mph speed limit. Most Missouri lettered highways are 55 mph, and in densely populated areas they can be less.  There are several that have a speed limit of 60 mph, though. In the urban areas of: St. Louis, Kansas City, Columbia, St. Joseph, and Springfield, the speed limit typically drops to 60 mph on Interstates and freeways.  In addition, on I-44 in Rolla the speed limit is reduced to 60 mph from just west of Exit 184 to Exit 186 because of a substandard design. Freeway speed limits in urban areas can be as low as 45 or 50 mph in a few very short sections in downtown Kansas City and St. Louis, or as high as 65 mph in the outer portions of the St. Louis, Kansas City and St. Joseph areas.  The Cape Girardeau and Joplin areas have no reduced freeway speed limits, and I-435 around Kansas City has a 70 mph limit from I-35 in Claycomo to the Kansas State Line around the northern and western part of the metro area. I-29 in Kansas City has a limit of 70 mph north of Barry Road in Platte County to south of Highway 169 in Buchanan County where the limit drops to 65 mph.  North of Frederick Road in Buchanan County the limit returns to 70 mph until the Iowa state line. As of October 1, 2015, the maximum speed limit in Montana is 80 mph.  On May 5, 2015, a bill to increase Montana's rural interstate highway speed limit to 80 mph was signed into law by Governor Steve Bullock. In the years before the 1974 national 55 mph limit, and for three years after the 1995 repeal of the increased 65 mph limit, Montana had a non-numeric \"reasonable and prudent\" speed limit during the daytime on most rural roads.  Montana Code Annotated (MCA) Section 61-8-303 said \"A person ... shall drive the vehicle ... at a rate of speed no greater than is reasonable and proper under the conditions existing at the point of operation ... so as not to unduly or unreasonably endanger the life, limb, property, or other rights of a person entitled to the use of the street or highway.\" Montana law also specified a few numeric limits: a night speed limit, usually 55 or 65 mph (89–105 km/h), depending on road type; 25 mph (40 km/h) in urban districts and 35 mph (56 km/h) in construction zones. The phrase \"reasonable and prudent\" is found in the language of most state speed laws.  This allows prosecution under non-ideal conditions such as rain or snow when the speed limit would be imprudently fast. On March 10, 1996, a Montana patrolman issued a speeding ticket to a driver traveling at 85 mph (136 km/h) on a stretch of State Highway 200.  The 50‑year‑old driver (Rudy Stanko) was operating a 1996 Chevrolet Camaro with less than 10,000 miles (16,093 km) on the odometer.  Although the officer gave no opinion as to what would have been a reasonable speed, the driver was convicted.  The driver appealed to the Montana Supreme Court.  The Court reversed the conviction in case No. 97-486 on December 23, 1998; it held that a law requiring drivers to drive at a non-numerical \"reasonable and proper\" speed \"is so vague that it violates the Due Process Clause ... of the Montana Constitution\". Effective May 28, 1999, as a result of that decision, the Montana Legislature established a speed limit of 75 mph. Despite this reversal, Montana's then-governor, Marc Racicot, did not convene an emergency session of the legislature.  Montana technically had no speed limit whatsoever until June 1999, after the Montana legislature met in regular session and enacted a new law.  The law's practical effect was to require numeric speed limits on all roads and disallow any speed limit higher than 75 mph (121 km/h). Montana law still contains a section that says \"a person shall operate a vehicle in a careful and prudent manner and at a reduced rate of speed no greater than is reasonable and prudent under the conditions existing at the point of operation, taking into account the amount and character of traffic, visibility, weather, and roadway conditions.\"  However, this is a standard clause that appears in other state traffic codes and has the practical effect of requiring a speed lower than the speed limit where a lower speed is necessary to maintain a reasonable and prudent road manner. Montana also has limited sections of night speed limits.  Other speed limits in Montana are 25 mph in a school zone, 30 mph in a residential district, 35-45 mph on boulevards, 55 mph on traffic-light highways, 65-70 mph on rural divided 4-lane highways, and 55-70 mph on rural 2-lane undivided highways. The maximum speed limit in Nebraska is 75 mph on rural Interstate highways.  This speed limit only applies to Interstate 80 between Omaha and Lincoln, west of Lincoln to the Wyoming state line, and the small section of Interstate 76 that enters the southwestern corner of the state from Colorado to join I-80.  The speed limit in rural areas of Nebraska is 65 unless otherwise posted, although the majority of two-lane state highways and I-680 are posted at 60 mph. The maximum speed limits in Nevada are on rural freeways, on other rural divided highways, on primary undivided roads, and on urban freeways. Prior to the imposition of the speed limit in late 1973, Nevada also had a \"reasonable and proper\" speed Limit (non-numeric) on most of its rural highways, both freeway and others.  The speed limit on certain two-lane highways is 70 mph including US 95 to the Oregon border (where the speed limit drops to 65 mph for trucks) and on certain sections of US 6 and 50 as they cross the Nevada desert.  Speed limits are 15 to 25 mph in school zones and 25 to 30 mph in residential districts.  In 2015, the Nevada State Legislature voted to increase the statewide maximum speed limit to 80 mph to take effect in October of that year.  However, between October 2015 and May 2017, no 80 mph speed limits were posted in the state. On May 8th, 2017, the Nevada DOT installed 80 mph speed limit signs on the 130 mile stretch of I-80 between Fernley and Winnemucca, excluding the Lovelock (exits 106 and 107) area. The highest speed limit in New Hampshire is 70 mph (112 km/h).  It can be found on Interstate 93 from mile marker 45 to the Vermont border (excluding the Franconia Notch Parkway).  All other freeways and turnpikes have a maximum of 65 mph.  The minimum speed on Interstate Highways in New Hampshire is 45 mph where posted. Provided that no hazard exists that requires lower speed, the speed of any vehicle not in excess of the limit is deemed to be \"prima facie\" lawful.  The limit for \"rural residential districts\" and Class V highways outside the city or town compact is 35 mph.  The limit for any \"business or urban residence district\" is 30 mph.  School zones receive a 10 mph reduction in the limit 45 minutes before and after the beginning and end of a school day.  The speed limit for a road work or construction area is 10 mph lower than the normal speed limit, but not more than 45 mph, when work is in progress.  The speed limit for all other locations is 55 mph.  The minimum limit that a speed can be set in a rural or urban district is 25 mph. The speed limit on Interstate 93 through Franconia Notch State Park falls to 45 mph where the highway narrows to one lane in each direction, but rises back to 70 mph (in 10 mph increments going south) once the highway leaves Franconia Notch.  Interstate 393 in Concord has a 55 mph posted speed limit for its entire length, with the exception of 45 mph and 35 mph zones on the westbound portion closest to the city center and the end of the highway.  Interstate 293's speed limit through downtown Manchester falls to 50 mph as it runs along the Merrimack River, but increases to 55 mph on either side of the city center. Prior to 1974, the New Hampshire Turnpike, along with rural sections of Interstates 89 and 93, were posted at 70 mph. New Jersey's only statutory speed limits are 50 mph rural, 25 mph urban.  Since the state is largely suburbanized, it ranges between 25–50 mph depending the jurisdiction of the road and whether the municipality is township, village, borough or city status. The common limited access freeway speed limit is 65 mph.  However, shorter length freeways such as US 202, Route 15, and Route 33 remain at 55 mph.  In all 65 mph speed zones, the fines for speeding and other moving violations are doubled.  This was the condition set for higher speed limits in New Jersey back in the late 1990s.  Urban freeway speed limits are 50 to 55 mph.  However, some freeways in urban areas retain a 65 mph speed limit such as the New Jersey Turnpike up to Exit 13 (Interstate 278), I-80 from the Delaware Water Gap (Exit 4) to the Passaic River (Exit 53) and I-78 from the Delaware River to the Newark border (Exit 55).  Only the New Jersey Turnpike has variable speed limits on its entire length. Four lane or greater state highways (often with a jersey divider or grass median) are generally posted at 55 mph (Such as Route 73 from CR 544 in Evesham Township to the Atlantic City Expressway).  County four-lane highways and municipal maintained four-lane roads (with a jersey divider or grass median) are not posted above 50 mph. Two-lane rural state highways and county maintained roads generally have 40 to 50 mph limits.  The only two lane surface roads posted at 55 mph in New Jersey are County Route 539, Route 70, and Route 72 in the Pine Barrens of Ocean and Burlington Counties and Route 54 in Atlantic County.  The Route 33 Freehold Bypass section where it is a super two freeway is also 55 mph. Urban two lane roads in boroughs and cities are typically 25 mph or 35 mph.  Residential streets at the municipal or county level are generally posted at 25 mph speed limits in boroughs and cities.  However, they can be as high as 40 to 45 mph at the county level, less likely in municipal maintained roads.  Generally, anything above 40 mph becomes uncommon.  However, there are a handful of 45 mph residential stretches such as Terill Road in Scotch Plains and Woodbridge Avenue (CR 514) in Edison.  All rural non posted roads have a speed limit of 50 mph, as per state law. School zones through urban and suburban areas on two lane roadways normally have a speed limit of 25 mph when children are present.  However, this limit can be as low as 20 mph and as high as 35 mph in some areas. With the exception of wartime, New Mexico had no default numeric speed limit until the early 1950s.  Prior to the national 55 mph limit in 1974, the speed limit on rural Interstates was 75 miles per hour during the day and 70 mph at night.  Primary highways in open areas had daytime speed limits of 70 mph and nighttime ones of 60 mph.  Secondary highways in open areas had daytime speed limits of 60 mph and nighttime ones of 50 mph.  Before the end of federal speed controls, the maximum speed limit was 65 mph on Interstate routes and 55 mph elsewhere.  In May 1996 legislation enacted by Governor Gary Johnson raised the absolute speed limit in New Mexico to 75 mph.  Signs are posted on the vast majority of the mileage of Interstate routes to that effect.  The default speed limit for any road where no speed limit is posted is 55 mph. New Mexico has six major freeway facilities, which include three lengthy Interstate routes.  Part of US-70 (as a divided highway) between Las Cruces and Alamogordo is the only section of non-Interstate route as well as being the only road in New Mexico that's not a freeway to have the 75 mph limit; New Mexico, Nevada (US 95 south of US 93), and Texas are the only three states with 75 mph limits on roads that aren't freeways.  There is no statutory requirement for reduced speeds on urban freeways so that, for example at Santa Fe and Las Vegas the speed limit remains 75 mph on I-25.  New Mexico and Texas are the only two states to have a speed limit greater than 70 mph in urban highways.  Nonetheless, there are 65 mph limits on freeways in more heavily urbanized areas such as Albuquerque and Las Cruces.  Other reduced speed limits do exist, but the lowest speed limit under normal conditions on New Mexico's freeways is 55 mph, which can be found on two sections of Interstate 25: The first section being three miles from the Big I to Gibson Boulevard in Albuquerque, and the second being a short stretch near Raton Pass.  These particular stretches of I-25 were originally built as relocations of US-85, whose design and construction predate the interstate highway era.  As such, these stretches do not meet modern interstate highway standards, and have closely spaced interchanges, sharp curves, and/or limited sight distances. By statute, other state maintained roads may have speed limits of up to 75 mph.  Four-lane divided highways in open areas often have 65 mph limits, with some 70 mph limits, such as almost the entire length of US 550 from Bloomfield to Bernalillo.  There is a trend toward posting a 70 mph limit on these highways, such as the recent 70 mph speed limit posting (increased from 65 mph) on a 23-mile stretch of U.S. 70 west of Roswell. Primary two-lane highways in open areas with parking shoulders often have 65 mph limits. Most primary two-lane highways without parking shoulders in open and mixed rural areas still have a 55 mph limit, but some have 60 mph limits. A 65 mph left lane minimum speed limit is sometimes indicated on 75 mph roads with steep grades, \"slower traffic keep right\" is also in effect.  On one-way roadways state law reserves the left and center lanes of two or more lanes for passing.  There are reduced advisory speed limits for some roads during poor weather.  Speeding fines are doubled in construction zones and designated safety corridors, with signs often stating this.  There are no longer night speed limits, nor are there any differential speed limits for heavy trucks. There are two other statutory speed limits in New Mexico that are often altered, especially on urban arterials or even city or countywide: thirty miles per hour in a \"business or residence district\" and fifteen miles per hour near schools at certain times.  For example, in Albuquerque the default speed limit is thirty miles per hour as per state law, but many streets have a different speed limit.  Some school zones there have twenty mile per hour speed limits.  The city of Santa Fe's default speed limit is twenty five miles per hour.  Although there are no signs to make drivers aware of the altered limit, the limit is signed on most roads where it applies.  Los Alamos County alters the urban default and absolute speed limits to twenty five miles per hour and 50 mph respectively, but posts signs at county lines. Outside of Bernalillo County, no points are assessed to one's license for speeding in rural areas in New Mexico, unless the excessive speed was a contributing factor to a traffic accident. Speed limits are statutory (set by law) or regulatory (enacted by regulation), not necessarily by engineering standards.  New York has a blanket statutory \"Reasonable and Prudent\" speed law. The highest posted speed limit in New York is 65 mph (105 km/h), found only on limited-access freeways (including some state highways, most of the New York State Thruway and select Interstate Highways).  The default speed limit, posted as the \"State Speed Limit\", is 55 mph, which is in effect unless otherwise posted or in the absence of speed limit signs. The New York State Department of Transportation sets speed limits in the vast majority of the state.  Counties and most towns must petition DOT to change a speed limit.  State law allows villages, cities, towns with more than 50,000 residents, and certain towns defined by law to be \"suburban\" to set speed limits on state, county, and local roads within their borders. There is no state law regarding minimum speed limits, but minimum speed limit of 40 mph has been set on the entire length of Interstate 787 and the entire length of Interstate 495 (the Long Island Expressway).  The New York State Thruway does not have a firm minimum speed, but there are signs advising drivers to use their flashers when traveling at speeds below 40 mph. While New York does not have truck speed restrictions \"per se\", the New England Thruway (Interstate 95) features \"State Speed Limit 55\" signs right next to \"Truck Speed Limit 50\" signs. New York law allows area speed limits.  An area speed limit applies to all highways within a specified area, except those specifically excluded.  The area may be an entire municipality, or only a specific neighborhood.  The defined area may also be the grounds of a school, hospital, or other institution.  Area speed limits are signed at their perimeters with signs reading \"Area Speed Limit\" and the speed limit value shown below.  \"Area\" may be replaced with a term that more precisely defines the area boundaries, such as \"Town\", \"City\", \"Park\", \"Village\" or \"Campus\". Normally, the end of a lowered speed limit is marked with a sign reading \"State Speed Limit 55\", indicating that the statewide speed limit applies.  In areas where a curve or other road condition makes the state speed limit inadvisable, a sign reading \"End XX m.p.h. Limit\" may be used, with XX replaced with the speed limit value.  A \"State Speed Limit 55\" sign should be installed after the curve.  This sign is sometimes misused in locations where the speed limit changes to a speed other than 55 mph.  This is mainly applied on both undivided and divided rural non-freeway routes.  Though rarely seen, some divided roadways are set as low as 45 mph but mainly stay at the state speed limit of 55 mph; in one exceptional case, that of the Scajaquada Expressway, the speed limit was lowered to 30 mph in 2016 after a fatality. The top speed limit in most residential/urban and business district areas is at 30 mph, and state law prohibits speed limits below 25 mph on most common residential areas, though a speed limit of 25 is mainly only used in the New York City area and rarely seen outside of said area.  However, School speed limits may be set as high as 30 mph to as low as 20 mph.  New York City has established a number of 20 mph \"Neighborhood Slow Zones\" in residential neighborhoods.  In residential neighborhood areas outside of New York City range between 30–40 mph and 35–45 mph on suburban/urban arterial routes. New York's Criminal Procedure Law prevents law enforcement personnel from issuing a ticket for any offense that they did not witness personally, meaning that, among other ramifications, the state's electronic toll collection system can not be used for speed enforcement. Many expressways and parkways in the New York City suburbs were posted as high as 65 mph.  During the 1973 Oil Embargo, New York lowered its speed limit to 50.  The National Maximum Speed Law brought statewide speed limits up to 55.  The city of New York, being a city, retained the 50 mph speed limit.  New York did not take advantage of the 1987 Congressional provision allowing 65 mph zones on rural freeways, instead waiting until NMSL's repeal in late 1995. Until September 2003, the state legislature needed to approve individual 65 mph zones, a lengthy process taking months or years of politically motivated debate.  Then-Governor George Pataki signed legislation in September 2003 that enables NYSDOT and New York State Thruway Authority to raise speed limits to 65 mph on its roads that meet established design and safety standards.  This legislation became active in March 2004, and was the reason for the 65 zones on NY Route 7 (locally known as \"Alternate Route 7\") and Interstate 84. By July 2018, New York State is likely to raise the maximum speed limit from 65 mph to 70 mph.  For example, such freeways include Interstate 87 and Interstate 90. Along rural primary and secondary roads outside municipal limits in North Carolina, the statutory speed limit is 55 mph unless otherwise posted.  Prior to the National Maximum Speed Law that went into effect nationwide, North Carolina used to have 60 mph speed limits on two-lane primary and secondary roads.  Inside the municipal limits, the statutory speed limit is 35 mph unless otherwise posted.  The downtown statutory speed limit is 20 mph unless otherwise posted.  \"Reduce Speed Ahead\" (RSA) signage is the norm whenever the speed limit drops at any level.  Note that the NC DOT uses the imperative verb \"reduce\" instead of the adjectival form \"reduced\" that is standard in other states, although some municipalities now use \"reduced\" in their signs.  In addition, a speed limit drop of 15 mph or greater normally includes a second warning sign after the RSA.  For example, in a 55 mph zone, a sign prior to the RSA sign would say \"BEGIN 35 1000 FEET AHEAD\" and then the 35 mph posted speed limit.  Three to eight lane boulevards with or without center turn lanes, range from 35 mph to 50 mph within municipal limits statewide. It is rare that NCDOT will assign a speed drop greater than 20 mph.  In Bertie County, the US 17 bypass in Windsor drops from 70 mph to 45 mph.  In Moore County, Shady Lane Road outside of Carthage in the Hillcrest community drops from 55 mph to 30 mph. School zone speed limits generally entail a 10 to 20 mph reduction below the original speed limit during times of day used for school arrivals and departures.  Such a speed limit would be indicated when entering the school zone.  Also, the default or modified speed limit is indicated after leaving the school zone.  A school zone speed limit cannot be less than 20 mph. Military bases are generally posted at a maximum of 50 mph.  As of May 2010, Fort Bragg military two-lane roadways are now posted at 55 mph instead of 50 mph.  Prior to May 2010, the speed limits higher than 50 mph through military bases were only on N.C. Highway 690 along the north side of Fort Bragg, Murchison Road (also known as N.C. Highway 210) and the All American Freeway (which is classified and numbered as a state-maintained secondary road even though it is a freeway). The state park speed limit is 25 mph unless otherwise posted.  These are not limited to places such as Hanging Rock State Park and Mount Mitchell State Park.  The Blue Ridge Parkway is 45 mph.  However, there are occasional 35 mph stretches.  The National Park Service is responsible for highway maintenance and speed enforcement on the Parkway. The county governments of North Carolina do not have any control over speed limits or any other aspect of road operation, as there are no county roads in the state.  Municipalities, on the other hand, can set speed limits on city-controlled roadways, subject to applicable state laws.  Freeways and expressways with no primary route number are part of the state secondary road system and bear route numbers of 1000 or greater.  Their maximum posted speed limit is 55 mph with four exceptions. A speed limit of 70 mph is relatively uncommon in much of metropolitan North Carolina, though it applies to many rural interstate highways in the state, as well as several non-interstate freeways.  The following are the only roads with 70 mph limits: These lengths make up approximately 589 miles, or 27%, of the total freeway mileage in North Carolina (405 miles or 31% of the state's growing Interstate system).  Four-lane freeway-grade highways are generally posted at 65 mph through the state of North Carolina. Interstate freeways with 60 mph speed limits are found along on I-26 between Asheville and Hendersonville and north of Asheville to Tennessee; I-40 between Asheville and Waynesville and through Greensboro; on I-85 in Gaston (east of US 321 to the Mecklenburg County line) and Mecklenburg counties and through Durham; on I-440 along the northern half of Raleigh's Beltline Non-Interstate Freeways (US Highways) with 60 mph speed limits are found along on the US 1 Henderson Bypass (as of December 2011); US-23 Waynesville Bypass; US 64 over the Virginia Dare Memorial Bridge between Manns Harbor and Manteo; US 74 Laurinburg bypass and in Brunswick County from the Leland Industrial Park to NC 133; US 301/Business 95 between Fayetteville and Eastover; the US-311 High Point Bypass and the US-401 Fayetteville bypass. Only two State Secondary Road freeways in the state that has a 60 mph speed limit are the Wade Avenue Extension as of February 2012 and Aviation Parkway (from south of Globe Rd to Airport Blvd) as of July 2014, which are both in Raleigh. 60 mph speed limits along non-freeway segments are growing in popularity and have replaced 55 mph limits on several boulevard and expressway segments throughout the state.  The boulevard speed limit changes go against the NCDOT rationale behind signing 60 mph speed limits along only freeway and expressway segments.  As of June 1, 2008, some examples of the affected boulevards are US 17 north of Elizabeth City, US 74 east of Wadesboro and NC 11 in Pitt County. 60 mph speed limits along Expressways are US 1 in northeastern Moore County, US 17 on bypass routes in Brunswick County, US 74 east of I-95, US 117 in Wayne County & Duplin County, US 220 in Rockingham County only along bypass segments and on NC 16 in Lincoln and Catawba counties (as of October 2012).  Previously, the entire US 220 alignment from NC 68 to the Virginia state line was 60 mph but went back to 55 mph between NC 68 and NC 704. Three State Secondary road expressways in the state that have a 60 mph speed limit are the US 117 Connector in Sampson and Duplin Counties between US 117 and I-40 (The designation in Sampson County leaving I-40 towards Duplin County is SR 1783/Connector Road) and the un-numbered connector northwest of Kinston serving the Global Transpark. There is a default minimum speed limit on Interstate and primary highways only when signs are present.  The minimum is 40 mph if the maximum is 55 mph.  The minimum is 45 mph if the maximum is at least 60 mph.  These minimums do not apply to vehicles that are towing other vehicles. North Carolina as well as other states operates a Safe Driving Incentive Plan (SDIP), a program that leads to insurance surcharges for moving violations based on a point system.  In general, for speeding violations less than 10 mph over the posted speed limit in a speed zone less than 55 mph, one point is assessed; two points are assessed for exceeding 10 mph over the limit or speeding in a zone with a speed limit of greater than 55 mph. A driver's licence will be suspended for travelling faster than 15 mph over the speed limit, provided the speed travelled is greater than 55 mph; suspensions can result for other speeding infractions, such as travelling faster than 75 mph in a 65 mph or less zone or faster than 80 mph in a 70 mph zone. North Dakota's major interstates, I-29 and I-94, hold a 75 mph speed limit in most rural areas, with 55-65 mph zones within portions of the Fargo and Bismarck/Mandan urban areas (I-29 in Grand Forks is signed at 75 mph).  The state's 4 lane Divided Highways as well as 2 short stretches of undivided 4 lane highways (US 2 around Rugby and U.S. 83 as it passes the eastern Lake Sakakawea Reservoir) usually carry a 70 mph limit, with 2 lane restricted to up to a 65 mph limit, and gravel roads have up to 55 mph limits.  Roads within cities hold their own defined limits with 25 MPH speed limits common in residential streets and 30-40 mph limits on Urban 4-6 lane divided streets, with school zones at 15–25 mph. On January 3, 2017, a bill was introduced to increase the rural speed limit on I-29 and I-94 to 80 mph.  The bill also would've called for 75 mph speed limits on 4-lane divided highways and 70 mph speed limits for two lane roads.  However state Senate rejected both bills. The maximum speed limit found on highways in Ohio is 70 mph on the Ohio Turnpike, Rural Freeways, and both the Expressway and Freeway portions of US 30 from Mansfield, Ohio to the Indiana State Line and US 33 from Wapakoneta to St. Mary's Ohio.  The speed limit ranges from 55 mph to 70 mph on other divided highways.  A small portion of the westbound lane (less than 500 feet) of State Route 16 in Licking County is signed at 70 MPH slightly before the route upgrades from a two-lane non-divided to a four-lane divided highway, but otherwise, no non-divided highway in the state currently has a speed limit higher than 55 mph , though ODOT is now permitted to increase undivided roads to 60 mph . Ohio is the only state east of the Mississippi River to allow 70 mph speed limits on non freeway roads.  Both divided and non divided roads. Ohio has an urban speed limit of 65 mph on Interstates by state law, yet many urban areas have lower speed limits due to safety concerns found in speed studies.  These commonly are in the 50–60 mph range.  For instance, in most of metro Dayton and Cincinnati, as well as in downtown Columbus, the speed limit is 55 mph , while in Cleveland, Toledo, and Akron the speed limit is 60 mph ; however, in central Cleveland along the Inner Belt, the speed limit is 50 mph .  On one case, however, the Ohio Turnpike has a 70 mph in the outer suburbs of Toledo, Akron, and Cleveland, even since all 241 miles of the highway has the 70 mph speed limit.  Some urban areas are also posted with minimum speed limits, usually with a minimum of 40 or 45.  At one time, portions of Interstate 76 and Interstate 77 in downtown Akron had a maximum speed limit of 50 mph and a minimum speed limit of 35 mph. School zones in Ohio normally have a 20 mph speed limit, regardless of the road's normal speed limit, in effect during school hours. In Oklahoma, the maximum posted speed limit is 75 miles per hour on turnpikes and 70 mph on all other freeways.  Most other rural highways, divided or undivided, have a 65 mph speed limit (although some rural divided highways have a 70 mph limit).  The minimum speed limit on almost all Interstate Highways, freeways, and turnpikes is 40 mph, except when the speed limit is 75 mph (found only on turnpikes), in which case the minimum speed is 50 mph.  In addition, rural sections of turnpike are supplemented with a sign warning \"no tolerance\".  On May 9, 2016, Governor Fallin signed HB 3167 which removes numerical caps on rural highway speed limits in Oklahoma which takes effect in November 2016.  Even with this bill the speed limits across the state are not expected to change because of budget issues in doing studies The highest posted speed limit in Oregon is 70 mph on I-84 east of The Dalles, I-82 and US 95.  Oregon state statutes allow for a maximum speed limit of 70 mph on rural interstate highways, and the law gives the Oregon Department of Transportation (ODOT) discretion to define which freeway segments to post the 70 mph speed limit.  ODOT did not raise speed limits beyond 65 mph on other freeways, and strongly opposed legislative efforts to raise the maximum allowable speed limit.  However, in July 2015, Governor Kate Brown signed a bill raising speed limits on several highways in eastern Oregon; the bill included provisions to raise the speed limit to 70 mph on I-84 and US 95. Although the speed limit can be raised to 70 mph based on engineering studies under current law, ODOT has not identified any freeway sections that it believes should be raised beyond the 65 mph limit.  In 2004, ODOT had released an engineering study on the average and 85th percentile speeds on interstate highways.  This study found that the 85th percentile speed on rural interstates by passenger cars was 71.1 mph with a statewide average of 66.3 mph.  The engineering study recommended raising the speed limit on rural interstates to 70 mph.  Despite this, ODOT initially refused to post the 70 mph limit due to safety concerns and its associated monetary costs; however, there appears to be no higher fatality levels in other states when the limit was raised.  In all rural areas in Oregon, the speed limit is 55 unless otherwise posted. Until 2002, Oregon state law required that all speed limit signs omit the word LIMIT from their display.  The reasoning behind this was related to the explicit \"basic speed\" law that existed, which allowed citation for exceeding speeds \"too fast for conditions\" regardless of the posted speed.  The typeface of the numerals on the signs varies greatly depending on which jurisdiction made the sign, due to its non-standardized design.  In 2002, the Oregon Department of Transportation revised its supplement to the MUTCD, mandating the omission of the word LIMIT \"except\" on signs posted on Interstate highways and within city limits.  As of 2014, ODOT has replaced nearly all SPEED signs posted on Interstates with SPEED LIMIT signs, but it was left to the various city governments to replace signs in their jurisdictions at their leisure, if at all.  Thus, older SPEED signs are still a common sight across the state. Throughout the late 1990s the Oregon state legislature passed multiple bills that would have raised the speed limit to 75 miles per hour on rural Interstate Highways and up to 70 mph on certain rural two-lane highways in the eastern portions of the state.  Each year Governor John Kitzhaber vetoed the bill due to safety concerns and he was worried that the 20 mph increase in car and truck speeds would raise road hazards.  In 2003, the Oregon state legislature passed a bill that would have raised the maximum permissible speed limit on Interstate Highways to 70 mph for cars with a 5 mph differential for trucks, up from the previous 65 mph limit for cars with a 10 mph differential; this bill was signed into law by then newly elected Governor Ted Kulongoski on September 26, 2003.  Although ODOT's 2004 study revealed that it is safe for cars to be traveling at 70 mph and trucks at 60 mph the Oregon Department of Transportation decided to not initially implement the increase out of concerns that it would not be safe to have trucks traveling at 65 mph.  Prior to the National Maximum Speed Law, the speed limit on Oregon freeways was 75 mph with some 70 limits on two-lane roads in eastern portions of the state.  On July 20, 2015, Governor Brown signed HB 3402 into law.  This bill raises the speed limit on I-84 east of The Dalles, I-82 (HB 4047 signed 2/23/16) and US 95 to 70 mph for cars and 65 mph for trucks.  It also increases speed limits on several other two lane rural highways to 65 mph for cars and 60 mph for trucks in Eastern Oregon.  The law took effect on March 1, 2016. In 2004, a law was passed revising Oregon's school speed limit laws.  In school zones, on roads with speed limits of 30 mph or below, drivers were required to slow to 20 mph 24 hours a day, 365 days a year, regardless of whether or not children were present.  This replaced most 'when children are present' placards.  If the speed limit was 35 mph or higher, the school zone limit would be imposed either by flashing yellow lights or a placard denoting times and days of the week when the limit was in effect.  The at-all-times rule was highly unpopular with motorists and was widely ignored.  In 2006, the law was revised again, taking away the 'at all times' requirement and replacing it with a time-of-day system (usually school days, 7 a.m. to 5 pm).  School crossings with flashing yellow lights remain.  In many communities, school zones are strictly enforced and speed traps in these areas are commonly employed. ODOT has not chosen a variation of speed between two-lane roads in Oregon, regardless of the terrain.  Any rural two-lane road in the state has a default speed limit of 55 mph.  Town speed limits are 20 mph in an alley, 20 mph in a school zone, 35 mph on boulevards, and 45 mph on roads with traffic lights. In Pennsylvania the maximum freeway speed limit is generally 65 mph (105 km/h), with select sections of rural freeway and most of the Pennsylvania Turnpike signed at 70 mph (113 km/h), or 55 mph (89 km/h) such as most of standalone Interstate 70.  The speed limit on urban freeways ranges from a low of 50 mph in downtown Pittsburgh and Philadelphia to a high of 65 mph in some suburban areas. In 1940, when the Pennsylvania Turnpike was opened between Irwin and Carlisle, the entire 160 mile limited-access toll road did not have a speed limit, similar to that of the German Autobahns.  In 1941, a speed limit of 70 mph (113 km/h) was established, only to be reduced to 35 mph (56 km/h) during the war years (1942–45).  After WWII, the limit was raised to 70 mph on the four-lane sections, with the two-lane tunnels having 50 mph (80 km/h) for cars and 40 mph (64 km/h) for trucks.  Prior to the 1974 federal speed limit law, all Interstates and the Turnpike had a 65 mph (105 km/h) speed limit on rural stretches and 60 mph (97 km/h) speed limit in urban areas. In 1995, the state raised the speed limit on rural stretches of Interstate Highways and the Pennsylvania Turnpike system to 65 mph (105 km/h), with urban areas having a 55 mph (89 km/h) limit.  In 1997, PennDOT raised the speed limit to some rural non-Interstate Highway bypasses to 65 mph (105 km/h).  In 2005, with the change in the designation of \"urban zones\" in the state (actually the law eliminating the prohibition against 65 mph limits in urban areas was enacted in 2000.  It took PennDOT and the PTC 5 years to realize the law had been changed), the entire lengths of both the Pennsylvania Turnpike's east–west mainline and Northeast Extension were given 65 mph (105 km/h) limits, except at the tunnels and through the very winding 5.5 mile (9 km) eastern approach to the Allegheny Mountain Tunnel. The 70 mph speed limit was authorized by House Bill 1060, which was signed by Governor Tom Corbett on November 25, 2013.  On July 18, 2014, the Pennsylvania Turnpike Commission announced the return of the 70 mph speed limit on a 97-mile stretch of the mainline from the Blue Mountain interchange (MP 201) to the Morgantown interchange (MP 298).  Signs were erected on July 22, 2014.  On July 23, 2014, PennDOT announced the speed limit will be increased to 70 mph on I-80 between interchange 101 in DuBois, Clearfield County and milepost 189 in Clinton County and on I-380 between interchange 8 (MP 10) near Mount Pocono, Monroe County and the junction with I-84 in Lackawanna County, on or around August 11, 2014 as a pilot project.  On March 15, 2016, the Pennsylvania Turnpike Commission approved raising the speed limit on the remaining 65 mph sections of the turnpike to 70 mph ; sections that are posted at 55 mph will retain that speed limit.  On May 2, 2016, PennDOT announced that the speed limit will be increased to 70 mph on about 800 mi of roadway across the state, with conversion to take place on May 3.  A total of 396 mi of the Pennsylvania Turnpike system will increase from 65 mph to 70 mph , including the extensions in Southwestern Pennsylvania.  The speed limit of the Pennsylvania Turnpike will remain 55 mph within construction zones and tunnels, at mainline toll plazas, on the eastern approach of the Allegheny Mountain Tunnel, and between Bensalem and the Delaware River Bridge.  The speed limit will also be increased to 70 mph on 400 mi of highway maintained by PennDOT, including rural stretches of I-79, I-80, I-99, I-380, and US 15. On non-freeway roads, speed limits are generally held at 55 mph (89 km/h) for rural four-lane roads, 55 mph (89 km/h) for rural two-lane roads, 45–55 mph (72–89 km/h) for urban four lane roads and 40–45 (sometimes, but rarely, 50 mph)  mph (64–72 km/h) for urban two lane roads, 35–45 mph for roads in commercial business areas, 30-35 mph (56 km/h) for major roads in residential areas, 20-25 mph (40 km/h) for most municipal residential streets, including main north–south and east–west roads in county seats and other mid-sized to large towns, and 15-20 mph (24 km/h) for school zones during school arrival and departure times only.  It is also only in effect on days that the school the road goes near is in session.  Many schools have signs that blink when the school speed limit is in effect.  There is no reduced school speed on divided highways, even if the school sits right beside the highway. All state-owned two-lane roads in rural areas within Pennsylvania have a default speed limit of 55 mph unless otherwise posted. The Pennsylvania Turnpike has a minimum speed limit of 15 mph below the posted maximum speed, though the minimum is only sporadically posted.  This is not enforced for slow-moving trucks in areas with steep grades and signs are posted that instruct drivers to use their flashers if traveling below 50 mph (40 if the speed limit is 55).  Pennsylvania has no default minimum speed limit on any other roads.  However, minimum speed limits on certain highways may be enacted and posted as provided by Section 3364(c) of the Pennsylvania Motor Vehicle Code (Title 75 of the Pennsylvania Consolidated Statutes). §3364(a) also requires, \"Except when reduced speed is necessary for safe operation or in compliance with law, whenever any person drives a vehicle upon a roadway having width for not more than one lane of traffic in each direction at less than the maximum posted speed and at such a slow speed as to impede the normal and reasonable movement of traffic, the driver shall, at the first opportunity when and where it is reasonable and safe to do so and after giving appropriate signal, drive completely off the roadway and onto the berm or shoulder of the highway.  The driver may return to the roadway after giving appropriate signal only when the movement can be made in safety and so as not to impede the normal and reasonable movement of traffic.\" Drivers cannot be stopped by police for driving less than 6 mph over the posted speed limit (10 mph if the speed limit is less than 55 mph and non-radar timing devices are used, as use of radar devices is limited to \"members of the Pennsylvania State Police\" by §3368c2). The US territory of Puerto Rico regulates and posts speed limits in miles per hour, although highway signage for distances are in kilometers.  Some people are hoping to have the speed limit in autopistas raised to 70 mph, along with 60 mph for trucks.  Tolled \"Autopistas\" as of 2015, can have speed limits up to 65 mph, while other expressways have speed limits up to 60 mph.  The rural default speed limit is 45 mph but may be increased to 55 mph.  In residential areas, only multilane roads have limits up to 35 mph, other roads are restricted to a maximum speed of 25 mph.  Only rural school zones have the higher 25 mph limit.  Speed limits for \"heavy motor vehicles\", such as school buses, are always 10 mph lower than that allowed for lighter vehicles, except in urban school zones where the limit is 20 mph.  Vehicles carrying hazardous materials are limited to 30 mph in rural areas and 20 mph in urban ones. Along two-lane roadways, the default speed limit is 50 mph during the daytime outside a business or residential district.  \"Daytime\" means a half-hour before sunset and a half-hour after sunrise.  At night time and also uncommon on the East Coast, the default speed limit is 45 mph outside a business or residential district.  Through the CBD and residential district, the default speed limit is 25 mph.  Through school zones within 300 feet, the default speed limit is 20 mph.  Local governments are barred from raising the default speed limits during the day and at night.  Divided highways such as rural Interstates are generally posted at 65 mph but 55 mph closer to Providence.  Divided arterials and expressways are posted no higher than 55 mph.  This includes US 1 south of RI 4 to Westerly.  A speed limit of 65 mph may be allowed on rural interstates based on engineering studies.  Currently, however, the only interstates raised to 65 are Interstate 95 from exits 1 to exit 8 and Interstate 295. Interstate speed limits in South Carolina are posted at 70 mph.  Interstates passing through \"Urban\" areas are dropped to 60 mph.  The urban area assignment of 60 mph usually includes the metropolitan area and the actual inner city area.  The three exceptions to the rule are I-385 in Greenville, the SC 31 freeway around Myrtle Beach and I-95 around Florence.  I-385 has a 55 mph speed limit at its terminus within the Greenville area.  SC 31 is posted at 65 mph even though it is in the greater Myrtle Beach area.  SC 31 was originally posted at 60 mph when it was built in 2004.  I-95 even as a 6 lane semi-urban built freeway, maintains a 70 mph speed limit through the Florence area (as of June 2013, from just south of exit 160 to just south of exit 164, the speed limit has been reduced to 60 mph, a textbook speed trap).  It is 6 lanes from SC 327 to I-20.  It is one of five states (Pennsylvania and Delaware at 55 & 65 mph and New Hampshire at 65 mph (I-95 in NH drops to 55 mph about 2 miles south of the ME line)) from Maine to Florida in which I-95 retains one speed limit throughout the entire state. Four-lane arterials by default are posted at 60 mph.  Four-lane bypasses at 60 mph can be found in Marion and Sumter, but others remain at 55 mph.  It is not uncommon that 55 mph can be expected in more built-up areas prior to municipalities and/or if the engineering on the highway is below standards.  However, U.S. Route 123 has a divided segment where the speed limit is 65 mph. Two-lane roads are 55 mph by default.  However, a handful of counties maintained as either state secondary roads or county roads are posted at 45 mph. Central business districts (CBDs) are posted at 30 mph.  Unlike North Carolina with their default downtown speed limit of 20 mph, they are rare to find in South Carolina in downtown areas.  A recent trend is occurring with CBD speed limits that they are being signed at 25 mph in random municipalities around the state. Speed limit drops generally are done in 10 mph increments but 20 mph are not uncommon.  Improvements in the mid-2000s were done by SCDOT to warn motorists ahead of time for speed drops on various roadways.  However, there are still some roadways that have not received that treatment.  However, there are a couple roadways that get 25 mph to 30 mph drops as well.  The speed limit drops from 55 mph to 25 mph at a traffic circle with US 378 and SC 391 in eastern Saluda County.  On US 52 northbound approaching Kingstree, the speed limit drops from 60 mph to 35 mph Shortly after the December 1995 repeal of the 65/55 mph National Maximum Speed Law, South Dakota raised its general rural speed limits to 75 mph on freeways and 65 mph on other roads along with 70 on a few 4 lane divided highways.  Almost a decade after posting the 75 mph limit, average speeds on South Dakotan rural freeways remain at or below the speed limit.  In the urban areas of Sioux Falls and Rapid city, a 65 mph speed limit is posted on Interstates 90 and 29.  In March 2015, SD State Legislature has passed the bill to raise the speed limit on Interstate 29 and Interstate 90 to 80 mph.  It was signed into law and took effect in April 2015.  While the 80 MPH speed limit was initially signed on all rural freeways in the state, the Interstate 90 stretch from Rapid City to the Wyoming Border returned to 75 mph due to safety concerns. Tennessee generally posts its rural interstates at 70 mph.  These include all of the state's two-digit Interstates (except Interstate 55).  70 mph speed limits are also allowed on other controlled access highways.  Controlled-access portions of U.S. 27 and 51 and S.R. 22 and 111 are posted at 70 mph.  Urban interstates are generally posted at 55 to 65 mph.  Four-lane divided highways are posted as high as 65 mph but are sometimes 55 mph.  These are often hotbeds for speed camera enforcement leading into a municipality.  Two-lane state-maintained roads are generally but not always posted at 55 mph outside the municipal limits.  In east Tennessee, most mountain roads leading to North Carolina are poorly posted at 45 mph.  Furthermore, two-lane roads such as US 321 and TN 91 have the same mountain and road width characteristics when leaving Tennessee into North Carolina or Virginia at 55 mph. Inside a municipality, speed limit assignments are often a mystery on state-maintained roads.  This could range from 15–55 mph depending on the type of roadway.  This is because the state of Tennessee grants strong home rule powers to municipalities and Dillon's Rule for unincorporated areas in the county for speed limit assignments.  Unlike other states, this weakens TennDOT's ability to sign predictable speed limits in a reasonable and prudent matter.  Also, Tennessee has a high percentage of roadways maintained by the counties. In the 2000s, all the counties of the major cities in Tennessee except for Davidson (Nashville) and Montgomery (Clarksville) petitioned TDOT to enact environmental speed limits, limiting all vehicles to 65 mph and trucks to 55 mph.  Knox County (Knoxville) also petitioned the state to enact these speed limits in all of the neighboring counties of the Knoxville Metropolitan Area.  These moves were controversial, as people believed the actual purpose was to increase revenue from speeding tickets, and have raised safety concerns.  In Nashville, speed limits for freeways are posted at 55 mph at the center of the city, and 65 and 70 mph beyond, with no separate restrictions for trucks. Prior to 1974, the maximum speed limit on Tennessee's Interstate highways was 75 mph day or night for cars and 65 mph day or night for trucks.  Other rural highways had a maximum speed limit of 65 mph day and 55 mph night for cars and 50 mph day or night for trucks.  Many of these other class roadways also had separate day and night speed limits as well. Texas is the only state that does not prescribe a different speed limit for each road type in its state or federal highway system.  Texas law generally prescribes a statutory speed limit of 70 mi/h for any rural road that is numbered by the state or federal government (United States Numbered Highways and Interstate Highways)—whether two lane, four lane, freeway, or otherwise—60 mi/h for roads outside an urban district that are not federal or state highways, and 30 mi/h for streets in an urban district. The law allows raising or lowering the statutory limit only if an engineering and traffic investigation indicates that a different limit is appropriate.  Texas allows a speed limit of up to 75 mi/h to be posted on federal or state highways, city maintained roads, and toll roads, and up to 70 mi/h on county roads.  Through a separate provision, speed limits up to 80 or 85 mph can be established on certain highways. Texas once had separate, systemwide truck speed limits, but they were repealed in 1999 and 2011. The truck speed limit used to be 60 mph (97 km/h) day/55 mph (89 km/h) night when the regular limit was higher.  This speed limit did not apply to buses or to trucks transporting United States Postal Service mail. Truck speed limits disappeared when all speed limits were capped at 55 mph (89 km/h) in 1974.  They reappeared with the introduction of 65 mph (105 km/h) limits in 1987. Effective September 1, 1999, Texas repealed truck speed limits on all roads except farm to market and ranch to market roads. In 2001, a bill allowing 75 mph speed limit on roads in certain counties excluded trucks, introducing a 70 mph truck speed limit on roads with a higher limit.  A bill in 2005 allowing 80 mph speed limits still excluded trucks.  However, truck speed limits were fully repealed in 2011. Before September 1, 2011, Texas had a statutory 65 mph (105 km/h) night speed limit on all roads with a higher daytime limit.  In 2011, the Texas Legislature banned night speed limits effective September 1, 2011. just outside of Snyder, Texas. Texas is the first state to lower speed limits for air quality reasons, although the lowered limits may not meaningfully improve air quality. In roughly a 50-mile (80 km) radius of the Houston–Galveston and Dallas–Ft. Worth regions, the Texas Commission on Environmental Quality convinced the Texas Department of Transportation to reduce the speed limit on all roads with 70 mph (113 km/h) or 65 mph (105 km/h) speed limits by 5 mph.  This was instituted as part of a plan to reduce smog-forming emissions in areas out of compliance with the federal Clean Air Act. Initial studies found that lower speed limits could provide roughly 1.5% of the emissions reductions required for Clean Air Act compliance.  However, follow-up studies found that the actual reduction is far less: With both of these facts combined, it is possible that the speed limit reductions only provide a thousandth of the total emissions reductions necessary for Clean Air Act compliance. In mid-2002, all speed limits in the Houston–Galveston area were capped at 55 mph (89 km/h).  Facing immense opposition, poor compliance, and the finding that lowered speed limits produced only a fraction of the originally estimated emissions reductions, the TCEQ relented and reverted to the 5 mph reduction scheme. Due to its enormous unpopularity, the Texas Commission on Environmental Quality examined alternatives to the 55 mph speed cap.  Analysis suggested that the vast majority of emissions reductions were from reduced heavy truck emissions.  A proposed alternative was to restore passenger vehicle limits but retain a 55 mph truck speed limit.  Concerns about safety problems and enforceability of such a large differential (up to 15 mph on many roads) scuttled that proposal, and a compromise plan, described above, was enacted that retained uniform, but still reduced, speed limits. In 2003, the Texas Legislature prospectively banned environmental speed limits, effective September 1, 2003.  The wording of the bill allows environmental speed limits already in place to remain, but no new miles of roadway may be subjected to environmental speed limits. In 2009, the North Texas Tollway Authority raised the speed limit by 10 mph on two tollways.  Several miles of these tollways had 60 mph environmental speed limits.  These new 70 mph limits exceeded what is allowable under the environmental speed limit regime.  NTTA was allowed to raise the speed limits by offsetting the higher limits' theoretical emissions increases with other transportation-related emissions reduction measures, including implementation of all-electronic tolling, which eliminated the need for some vehicles to stop at a toll booth. In 2015, the Texas Department of Transportation cancelled all remaining environmental speed limits in the Dallas-Fort Worth region.  Some speed limits were changed back to those in place before the environmental speed limits were enacted.  On some roads the speed limit was not changed.  On other roads, including some that never had environmental speed limits, speed limits were raised higher than they were before the environmental speed limits were enacted. Because Texas law allows 75 mph speed limits on any numbered state highway or city maintained road, it is the only state with 75 mph limits on two-lane roads.  Speed studies undertaken by TxDOT in response to legislation passed in 2011 took about 2 years, and the result is that the mileage of highway with a speed limit of 75 mph has increased from about 1,400 to about 19,000.  70 mph speed limits have become rare on Texas interstates.  They are retained in stretches of I-10 and I-35 in Bexar County, on I-410, and on I-35 from San Antonio to Austin.  Most Texas interstates had been posted at 70 mph, notably in east Texas and the panhandle for 16 years from December 1995/early 1996 to early 2012.  Also, there are multiple urban Interstates and Tollways in Texas where there can be a 75 mph speed limit such as parts of State Highway 45, I-20 in Odessa and Midland (prior to Feb 2016, it is now 65 mph on I-20 in Odessa-Midland), and I-10 in the outer parts of San Antonio Texas statutorily allows 80 mph (130 km/h) speed limits on I-10 and I-20 in certain counties named in the statute, each of which has a low population density.  Additionally, the Texas Transportation Commission may set a speed limit up to 85 mph on any part of the state highway system if that part is \"designed to accommodate travel at that established speed or a higher speed\" and an \"engineering and traffic investigation\" determines the speed is \"reasonable and safe\". As of now, the roads with an 80 mph limit are: As of now, the only road with an 85 mph speed limit is a 41-mile portion of Texas State Highway 130 from the northern terminus of its concurrency with US-183 ( ), southward to I-10 near Seguin( ). Factoring in rounding (most countries round speed limits to the nearest 5 mph or 10 km/h), the 80 mi/h limit is approximately the same speed as the 130 km/h (81 mph) recommended speed on the Autobahn and the actual 130 km/h (81 mph) speed limit for freeway-class roads in thirteen European countries and the Australian Northern Territory. As of September 2012, the only limits higher than the 85 mi/h limit are the 140 km/h (87 mph) limits of Poland, Bulgaria and Abu Dhabi, though some jurisdictions like Germany and Isle of Man have roads without any posted maximum limit. For \"motorcars, pick-up trucks, or motorcycles\", the fastest speed limit in this territory is 55 mph and is found on one road, the divided highway and freeway known as the Melvin H. Evans Highway on the island of St. Croix.  Outside of towns, these vehicles are limited to 35 mph unless posted lower, except on the above-mentioned divided highway and parts of Centerline Road, which is limited to 40 mph.  Within towns, these vehicles are limited to 20 mph. \"Motor trucks and buses\" are limited to 40 mph on St. Croix's main divided highway, 30 mph on other highways outside of towns, and 15 mph within towns. In Utah, there is a minimum speed limit of 45 mph on Interstate Highways when conditions permit.  The maximum speed limit on Interstates is normally 70 mph in cities and, on most highways, 80 mph elsewhere.  UDOT has now implemented HB83, raising the speed limit to 80 mph on an additional 289 miles of rural interstate, including I-80 from Nevada to mile marker 99, I-84 from Idaho to I-15, and additional sections of I-15. On April 3, 2013, Utah Department of Transportation spokesman John Gleason said \"We’d only do it in a situation that would make sense: flat, straight roadways.  The Utah Department of Transportation is looking at expanding zones where it can increase the speed limit from 75 to 80 miles per hour.  The Utah State Legislature recently approved a bill allowing for a series of zones to become permanent, as well as expanding them in other places around the state.  UDOT began a study on Monday (April 1, 2013) to place more zones on rural parts of I-15, I-80 and I-84.  The areas under consideration, UDOT spokesman said, are on I-80 from Grantsville (exit 99) to Wendover, on the Utah-Nevada border; I-84 from Tremonton to the Utah-Idaho border; I-15 from Brigham City (North interchange) to the Utah-Idaho border; and I-15 from Santaquin to North Leeds.\"  The speed limit on these sections has been increased from 80 mph as of September 17, 2013. On February 13, 2014, UDOT voted to increase the speed limit on I-80 from Salt Lake City across the Bonneville Salt Flats to the Nevada border to 80 mph .  The change went into effect on July 1, 2014.  By July 1, 2014, the state raised the speed limit on all rural interstates in Utah to 80 mph except I-80 from the Wyoming border to Salt Lake City, on I-84 from its junction with I-80 to Ogden and on twisty sections of Interstate 70 from its I-15 junction to the Colorado border (the speed limit on I-70 still varies between 60 mph and 80 mph depending on the topography of the section of freeway.) .  The speed limit on every other highway is 55 mph unless otherwise posted, although several two-lane, undivided roads have 65 mph speed limits, with divided sections of U.S. Route 40 and U.S. Route 189 posted at 65 mph as well. By the end of 2016, UDOT raised speed limits to 80 mph on additional sections of I-15 and I-70. The standard speed limit in Vermont stands at 50 mph.  This is applied to rural two-lane roads.  On urban freeways, divided at-grade expressways, and rural two-lane limited access roads, the speed limit is 55 mph, such as on I-189 and Interstate 89 in Burlington, and US Route 7 and Vermont State Route 279 outside of Bennington.  Rural freeways are posted at 65 mph.  Furthermore, the speed limit drops from 65 mph on rural highways to 40 mph at the approach to the Canada–US border on Interstates 89 and 91, at Highgate and Derby Line, respectively.  In school zones, the speed limit can range from 20 mph to 25 mph, depending on local authority.  The minimum speed is defined at 40 mph only on Interstate highways.  That includes where the limit is posted at 55 and 65 mph.  However, as old signs are being replaced, the \"40 MINIMUM\" is being phased out, keeping only \"SPEED LIMIT 65\". A Virginia statute provides that the default speed limit \"shall be 55 mph on interstate highways or other limited access highways with divided roadways, nonlimited access highways having four or more lanes, and all state primary highways\".  \"The maximum speed limit on all other highways shall be 55 miles per hour if the vehicle is a passenger motor vehicle, bus, pickup or panel truck, or a motorcycle, but 45 miles per hour on such highways if the vehicle is a truck, tractor truck, or combination of vehicles designed to transport property, or is a motor vehicle being used to tow a vehicle designed for self-propulsion, or a house trailer.\" The same statute contains a number of exceptions, however, allowing higher speed limits \"where indicated by lawfully placed signs, erected subsequent to a traffic engineering study and analysis of available and appropriate accident and law-enforcement data\".  This provision allows speed limits of up to 70 mph on Interstate highways; multilane, divided, limited-access highways; and express or high-occupancy vehicle lanes if said lanes are physically separated from the regular travel lanes.  (As of August 2015, Virginia has three such barrier-separated facilities: high-occupancy vehicle lanes on I-64 in the Tidewater area; as well as high-occupancy/toll \"Express Lanes\" on I-495 and I-95, and HOT/HOV lanes on I-395, all in Northern Virginia.)  The statute also allows 60-mph speed limits on a number of specified non-limited access, multilane, divided highways. The 70-mph provision was added to Section 46.2-870 via an amendment effective on July 1, 2010.  The previous version of the statute had authorized a 70-mph speed limit only on I-85; the maximum limit permitted elsewhere was 65 mph.  Notably, the revised statute does not \"require\" a 70-mph speed limit on any road nor make such limit automatic, due to the requirement for traffic and engineering studies.  The Virginia Department of Transportation began studying Interstate highways with 65-mph speed limits during April 2010 to determine which roads should receive the 70-mph limit and announced that the studies would be conducted in three phases over a period of several months, with the initial phase focusing on 323 miles of highway with \"no significant levels of crashes and congestion\".  As of July 1, 2010, VDOT increased the speed limit to 70 mph on a portion of one highway (I-295 south of I-64).  On October 20, 2010, Governor Bob McDonnell announced that by the end of 2010, VDOT would post 70-mph speed limits on 680 miles of Virginia Interstates located outside of urban areas, representing 61 percent of Virginia's total 1,119 miles of Interstate highways.  While the statute allows for speed limits up to 70 mph on urban Interstates, as of March 2015 VDOT has declined to post a limit higher than 65 mph on any urban highway. The statute also allows 70-mph speed limits on routes other than Interstates.  Initially VDOT declined to consider any such routes for the higher limit, but in early 2012 VDOT posted a 70-mph limit on a portion of US-29 near Lynchburg. Other Virginia statutes prescribe exceptions to the general rules set forth above.  The notable aspect of Virginia's current speed limit laws is that the Department of Transportation has no authority to raise speed limits above the statutory limits unless the General Assembly passes a statute permitting the change.  Since the National Maximum Speed Law was repealed in 1995, such statutory exceptions were largely confined to a highway-by-highway basis, as evidenced by the list of 60-mph exceptions in Va.  Code § 46.2-870. Notably, Virginia's reckless driving statute provides that driving 20 mph over the speed limit, or in excess of 80 mph regardless of the posted speed limit, is grounds for a reckless driving ticket.  Thus, in a 70-mph zone traveling 11 mph over the speed limit is prosecutable as a misdemeanor with penalties of up to a $2,500 fine and/or 12 months in jail. Virginia law does not prescribe a fixed minimum speed limit, although a statute does authorize the posting of such limits where traffic and engineering studies indicate that they would be appropriate. Virginia is the only state that prohibits the use of radar detectors (the District of Columbia does as well, though it is not a state). The Revised Code of Washington permits speed limits of 75 mph in sections deemed appropriate by an engineering study.  As of February 2016, the typical speed limit on a Washington freeway is 70 mph rural, 60 mph urban (the speed limits on these types of freeways only vary in the Tri Cities), with a truck speed limit no higher than 60 mph.  The posted truck speed limit does not apply to any auto stage towing a trailer or trucks less than 10,000 pounds gross weight.  Limits were raised to current speeds following the elimination of the federal 55 mph speed limit, to more closely reflect the common speeds of traffic at that time.  However, Washington State does not have a standard legislated statewide speed limit, leaving it to WSDOT to set individual speed limits for specific segments of road.  This has resulted in a wide range of speed limits statewide, with many rural undivided 2 lane highways set at 65 mph, while some congested urban divided highways have limits as low as 40 mph. The default speed limit on a rural 2-lane highway in Washington is 60; however, the limit on undivided highways varies.  In mountainous country like the Cascades and Olympic Mountains, certain twisty roads are limited to 55 mph, whereas some flat, straight highways in eastern Washington have a limit of 65.  The speed limit for motorhomes and autos with trailers is 60 like it is for trucks.  Roads with traffic lights are limited to 55 mph.  The school zone speed limit is 20 mph but is in effect only if children are present.  Divided highways in Washington are rare, however, U.S. Route 395 between Pasco and Ritzville is a high-speed divided highway with a maximum speed limit of 70 mph. Some areas within Washington State use variable speed limits such as on portions of I-90 between Seattle and Issaquah and over Snoqualmie Pass. On January 23rd, 2017, the WSDOT considered raising the speed limit on I-90 between Ellensburg and Spokane to 75 mph. The speed limit on most rural Interstates is 70 mph.  Urban Interstate speed limits generally vary from 55 mph to 65 mph.  Sections of I-64 and I-68 have lower truck speed limits because of steep grades; otherwise, West Virginia does not post separate truck speed limits.  The West Virginia Turnpike between Chelyan and Mahan has a 60 mph speed limit because of sharp curves. Speed limits on 4-lane divided highways are normally 65 mph although some stretches within cities are posted as low as 50 mph.  Open country highways have a statutory limit of 55 mph, which includes most rural two-lane highways and even includes some one lane back country roads or any road without a posted speed limit.  Cities and towns set their own speed limits, which are usually between 25 and 55 mph.  School zones have a statutory speed limit of 20 mph.  Speed limits are commonly reduced by 15 mph in work zones. On April 7, 2017, the House of Delegates passed a resolution that asks the Department of Highways to study increasing the speed limit to 75 mph on rural interstates where deemed safe. The state of Wisconsin's speed limits are set out in statutory law but may often be modified by the maintaining government entity.  In addition to a basic speed rule, Wisconsin law specifies certain occasions where reduced speeds are required includingand not limited to the approaches and traverses of rail crossings, winding roads, roads where people are present, and the crests of grades.  Although there is no numeric minimum speed limit, state law prohibits the impediment of traffic by unreasonably slow speeds.  Vehicles that lack rubber tires filled with compressed air and/or carry a slow moving vehicle orange safety triangle have a hard limit of 20 mph. The state of Wisconsin has four default speed limits.  20 mph limits apply in school zones (on major roads during school arrival and dismissal periods only), near parks with children, and in alleys. 10 mph default speed limits apply, unless modified by the managing authority, on \"service roads\" within corporate limits.  Within municipal boundaries and in areas of dense urban development a 35 mph limit is in effect unless another speed limit is indicated.  In some jurisdictions, the 25 mph limit is the default speed limit for residential areas.  The entry to such an area is to be marked by speed limit signs.  Outside of built-up areas (these include denser business, industrial or residential land uses according to the relevant law) a 55 mph limit is effective in the absence of other indications. While all 2 lane roads maintained by WisDOT as of 2015 have a 55 mph maximum, a small portion of Minnesota State Highway 23 that passes through the state south of Superior but is maintained by MNDOT has a 60 mph limit through the state. Along with the aforementioned default speed limits, there are other statutory speed limits that more often require signs to be effective.  70 mph limits on freeways and 65 mph limits on expressways require signs to be effective.  The default speed limit on these types of roads is 55 mph as they do not directly interact with the built-up environment.  In the densest urban districts a statutory 25 mph limit is effective when adequate signage is used, as are 35 mph limits in areas of light development.  The same applies to 45 mph limits on highways designated as \"rustic\" roads.  However, \"an alleged failure to post [such a speed limit sign] is not a defense to a prosecution\" in the case of such statutory limits. Wyoming's highest speed limit is 80 mph, found on its Interstate highways, and 70 mph on its four lane divided highways.  The speed limit for school zones is 20 mph, 30 mph in urban districts and residential areas, 70 mph for other paved roads, and 55 mph for unpaved roads.On I-80 in Evanston and Cheyenne, I-25 in Cheyenne and Casper, and I-90 in Buffalo, the speed limit is 65 mph. In February 2014, the state Assembly passed a bill that would raise the speed limit from 75 to 80 mph on certain freeway segments that would meet safety standards.  The bill passed the Senate on Feb 25 and raised the speed limit on certain freeway sections to 80 mph on July 1, 2014.  However, an attempt to raise the speed limit to 70 on two lane highways such as Wyoming 120 and US 14 was turned down that same month, but this same provision became law in February 2015.  On February 1, 2016, the speed limit on WY 120 (two lane highway) from the Chief Joseph Highway to the Montana border increased to 70 mph.  Two other two lane highway sections, WYO 130 from Interstate 80 south to Saratoga and US 85 from east of I-25 to Newcastle, increased to 70 mph.\n\nArchitecture of Wales Architecture of Wales is an overview of architecture in Wales from the Medieval period to the present day, excluding castles and fortifications, ecclesiastical architecture and industrial architecture.  It covers the history of domestic, commercial, and administrative architecture. There is little evidence for domestic architecture in Wales which predates the 14th century.  The earliest domestic buildings are the stone tower houses, which may date back to about 1400, and various partially fortified first-floor hall houses such as Candleston Castle and Eastington at Rhoscrowther in Pembrokeshire.  Most of the Welsh examples are in the southern coastal border area of Wales and particularly in Pembrokeshire.  So far no Welsh timber-framed houses can be securely dated to before 1400, but the description by the poet Iolo Goch of Owain Glyndŵr’s house at Sycharth shows that houses with timber cruck framing were being built well before this date.  It has been suggested that the devastation caused following Owain Glyndŵr's revolt may have caused the destruction of many earlier timber-framed houses in the Welsh Marches \"see also\": Welsh Tower houses The distribution of tower houses in Wales has been discussed by both Hilling and Smith.  Welsh tower houses, rectangular structures, consisting of two or more storeys, are closely related to those in Ireland and Scotland.  In 1976 Hilling produced a map (with listing) showing 17 examples.  Further houses have been added by Suggett and it is possible that new examples will be recognised as being incorporated into existing buildings, as at Sandyhaven House in Pembrokeshire. A further example is likely to be the prominent East Gate tower of Powis Castle.  The East Gate appears to have been a tower house, which had an entrance made through the vaulted undercroft, probably in the 17th century.  An extra storey was added to the tower in 1815–1818 when Sir Robert Smirke re-fenestrated the castle and added Gothic Revival battlements.  Also on the Welsh border, close to Welshpool is Wattlesburgh.  Many of the English tower houses, such as Tattershall Castle or Buckden Palace are slightly later and larger than the Welsh examples, and built of brick. Apart from tower houses there are a number of stone-built first floor hall buildings, where the hall is mounted over an undercroft.  These include Owain Glyndŵr's Parliament House in Machynlleth.  Most examples are found in southern Wales with a cluster of buildings in Pembrokeshire.  The distribution of Tower and other houses in Wales with vaulted ceilings have been mapped and listed by Peter Smith. They also occur as early merchant's houses in Haverfordwest, Pembroke and Tenby.  In some cases the hall is accessed by an outer stair, as is the case at Pentre Ifan Barn at Nevern in Pembrokeshire.  Another example is Eastington at Rhoscrowther, Pembrokeshire, which has been called a tower house, but is more correctly a first floor hall house, with an outside stair and crenulations with a side tower.  Eastington belonged to the Perrot family in the 15th century.  There is a further complex of Medieval stone houses at East Orchard, St Athan, in Glamorgan which belonged to the de Berkerolles family in the 14th century.  The group of buildings includes a first floor hall house with outside chimney, which also had a separate kitchen block. From the later part of the 15th century, some of the Welsh castles underwent a transformation into grand houses.  Some of these such as Chirk Castle and Powis Castle have remained as houses, but others such as Raglan Castle in Monmouthshire and Carew Castle in Pembrokeshire are ruins which can provide some idea of their grandeur.  At Carew Sir Rhys ap Thomas from about 1480 onwards undertook a grand re-modelling including an almost entire re-fenestration with straight headed windows.  This was continued after 1558 by Sir John Perrot who replaced the north range with a splendid frontage with a long gallery at the second floor level in the fashion of Robert Smythson. An even more impressive residence on palatial scale was Raglan Castle.  The earliest building is the freestanding hexagonal great tower, which is surrounded by a moat.  It was probably built by Sir William ap Thomas before 1445.  It would have served the function of a strongly defended tower house.  This was followed in 1461–69 by the enlargement of the castle by Sir William Herbert with a gatehouse to the NE and to the SW a range of sumptuously decorated state apartments.  Further apartment ranges were built round the SW court.  The two sets of apartments were approached by an impressive main staircase.  From about 1549 to 1559 these buildings were extended, by William Somerset, 3rd Earl of Worcester, particularly around the \"Pitched Stone\" Court and also with the long-gallery with its elaborately decorated Renaissance fireplaces.  The slighting of the castle in the English Civil War and its subsequent partial demolition make it hard to appreciate Raglan as one of the major domestic buildings of Wales. Another early house connected with the Herbert family was Tretower Court in Breconshire.  It was here that William Herbert settled his step brother Roger Vaughan who built a house, which was to develop round a courtyard and continued being added to until the 17th century.  Recently the arched braced truss roof of the great hall has been dated by Dendrochronology to c. 1455. There are a number of houses in north Wales that have been traditionally associated with Owain Glyndŵr and there is also the Parliament House of Edward I at Ruthin.  They may not be certainly associated with these historic figures, but they are important as evidence for early stone buildings in Wales.  Best known is Glyndŵr's Parliament house in Machynlleth.  This building has been substantially altered in more recent times, but fortunately Edward Pugh published a fine coloured lithograph of the building in 1816.  Recent dendrochronological dating of the felling of a roof timber to 1470, does not necessarily mean that the stone structure of the building in not associated with Glyndŵr.  The original building is a hall house with a four-unit plan: storeyed outer room of two bays, open passage (2 bays between partition trusses), open hall (three bays with dais-end partition), and a storeyed inner-room of two bays.  The carpentry is refined: purlins and ridge are tenoned into the trusses.  The principal rafters of each truss are unusually shaped ('extruded') to receive the tenoned collar.  In the hall the purlins are moulded with two tiers of wind braces (replaced), and the trusses have shaped feet.  The upper-end truss is set forward from the dais partition to form a shallow canopy. At Carrog near Corwen parts of Owain's \"Prison\" stood, possibly into the 20th century.  Thomas Pennant wrote in about 1776 that \"The prison where Owen confined his captives was not far from his house, in the parish of Llansantfraid Glyndwrdwy and the place is to this day called 'Carchardy Owen Glyndwrdwy'.  Some remains are still to be seen near the church, which form part of a habitable house.  It consists of a room 13 feet square and ten and a half high.  The sides consist of three horizontal beams, with upright planks, not four inches asunder, mortised into them.  In these are groves in the bottom, as if there had been cross bars, or grates.  The roof is exceedingly strong, composed of strong planks almost contiguous.  It seems as if there had been two stories; but the upper part at present is evidently modern\".  In 1794 John Ingleby was employed to make a watercolour record of the building, which stood just to the SE of the church and overlooked the River Dee.  The building which was thatched and has some timber close studding and also a Gothic arched window and Gothic arched doors.  There seems to be evidence for an outer stair leading to a first floor hall, which suggests that parts of the building could well have been contemporary with Owain Glyndŵr.  The site of the building was on the modern Glyndŵr Terrace. There was another Parliament House of Glyndŵr in the centre of Dolgellau.  It was moved in 1885 to Newtown and re-erected in a much altered form.  It may have been first referred to as the Parliament House in 1555. This is building is now known as \"Plas Cwrt yn Dre\".  It was an aisled hall house, so is likely to have been a building of considerable importance, but is unlikely to date back to the time of Glyndŵr.  Much restored by A B Phipson for Sir Pryce Pryce-Jones as a two storey three bay house.  Largely timber framed with stone end walls with interlocking herring-bone decorative framing to a jettied first floor, which is supported by vine scroll brackets.  There is an external stone staircase to a plank door at extreme left and square panelled timber framing to ground floor.  The right stone bay incorporates re-used medieval masonry and a two light window with a central stone mullion.  Arcade patterns in tile inset to stack may reproduce the arcade designs on the chimney stack shown in a lithograph of 1810 by Cornelius Varley. Another building that might be of considerable antiquity is the Parliament House of Edward III in Rhuddlan where it was thought that the Statute of Rhuddlan was promulgated.  Thomas Pennant remarks in 1778 \"A piece of antient building called the Parlement is still to be seen in Rhuddlan: probably where the king sat in council.\"  Pennant was to get John Ingleby to provide a watercolour of the building.  Today the building still partially stands in Parliament Street, with a late 13th-century doorway and a 14th-century cusped ogee door head.  There is no definite evidence that this building is connected with Edward III. Timber-framed houses in Wales are concentrated particularly in the historic counties of Montgomeryshire and Denbighshire and mainly in areas which lack good building stone but have an abundance of ancient woodland that provided the timber for construction.  The Welsh Poets often provide good descriptions of these early houses from the 14th century onwards, when praising their patrons.  This is the case of Iolo Goch's description of Owain Glyndwr's house at Sycharth in the late 14th century, when the poet mentioned that the house was constructed with crucks and had a slate roof. The earliest timber framed houses are hall houses, which were single storied houses the main room of which was heated by a fire on an open hearth with the smoke escaping through a vent in the roof.  In peasant houses the hall would have only consisted of a single bay.  These smaller houses are rarely recognised now and where they exist the single bay is likely to form part of the structure of a larger house with more recent additions. Since the 1990s the availability of dates provided by tree-ring dating or Dendrochronology has revolutionised the study of early buildings in Wales and is particularly relevant for timber framed buildings.  The earliest tree ring date associated with a building in Wales is a date commissioned by CADW for a door at Chepstow Castle which was made from wood felled between 1159 and 1189. Aisled-framed hall houses have one or more rows of interior posts.  These interior posts typically carry more structural load than the posts in the exterior walls.  Aisled Hall houses are early in the sequence of timber framed houses and were high status dwellings .  In his study of these houses Peter Smith recorded 20 examples of this construction, mainly in NE Wales and particularly in Denbighshire.  In some cases such as Plas Uchaf at Llangar, which has now been restored by the Landmark Trust the roof was supported by both aisle and cruck trusses.  Plas Cadogan at Esclusham near Wrexham survived as the finest example of a Welsh aisled house with an open hall up to roof.  Despite being a Grade I listed building it was demolished in 1967.  The roof has now been re-erected at the Avoncroft Museum at Bromsgrove. The Vernacular Architecture Group currently has records of 1002 historic cruck framed buildings in Wales Of these 520 are in Powys and by far the greatest concentration are in the historic county of Radnorshire with 318 examples and Montgomeryshire with 161.  The praise poetry of Iolo Goch describing Owain Glyndŵr’s houses at Sycharth indicate that cruck construction was well established in 14th-century Wales.  The earliest cruck framed house to be dated so far is Hafodygarreg at Erwood in Breconshire, which has a date of 1402.  These cruck buildings are part of the Hall-house tradition with central fireplaces and the smoke escaping through vents in the roof.  Some of the cruck framed houses were extended by adding wings, providing an H shaped layout.  With the introduction of box framed and jettied houses in the mid-15th century, the use of crucks gradually went out of fashion.  At this time many cruck houses were converted into barns and evidence for fireplaces and chimney stacks stripped.  A good example of a house that has been converted into a barn, possibly as late as the 18th century is at Ty-coch Llangynhafal, Denbighshire.  This has recently been restored by Denbighshire County Council and it has been dated to 1430.  There are many instances in Montgomeryshire where more elaborate timber framed farmhouses are associated on the same site with earlier houses which were converted into barns.  At Rhyd y Carw in Trefeglwys the original cruck framed hall house dated to about 1525 while nearby stands the impressively decorated box-framed Rhydycarw farmhouse dating from the earlier part of the 17th century. The idea of the Sub-Medieval House in Wales was first developed by Sir Cyril Fox and Lord Raglan in their study of the Vernacular architecture of Monmouthshire, which was published between 1951 and 1954.  Fox and Raglan recognised that around 1550 a great change occurred in Welsh House building.  While the earlier Medieval traditions of constructing with crucks and timber framing continued, many new features start to appear in domestic architecture.  Chimneys start to be inserted into the halls of houses instead of the open fireplaces and chimney stacks may either be built on the gable ends of houses or as \"lateral chimneys\" on the side walls.  At the same time timber framed and stone houses start to be built with one and even more storeys.  Box framing starts to supplant the older timber framing with crucks and in order to gain more floor space at the upper levels these floors were jettied out from the building line.  Fox and Raglan considered that in Monmouthshire, the building of \"Sub-Medieval\" houses continued until around 1620.  In the later part of the 16th century, sub-medieval houses continued to be built in parallel with often grander houses showing Renaissance style or influences. An excellent example of a \"Sub Medieval\" house is Llancaeach-Fawr at Gelligaer in Glamorgan.  John Newman comments that in contrast with other buildings of the period \"it is a delight to find one so nearly perfectly preserved\".  It is of three storeys and largely of a single build period.  It was built by either Richard ap Lewis or his son David ap Richard (Prichard), who was resident here in the 1530s.  The windows emphasise the importance of the first floor rooms. The ideas of Fox and Raglan were developed by Smith in his study of \"Houses of the Welsh Countryside\", first published in 1975 and re-issued as an enlarged addition in 1988.  Smith classifies five main types of \"Sub-Medieval\" house based on the position of the chimney or chimneys and the position of the main entrance door.  These groups are: A good example of a long house is Cilewent Farmhouse from Llansanffraid Cwmteuddwr, nr Rhayader, Radnorshire which has been reconstructed at St Fagans.  This is a \"long-house\", with cattle being accommodated at one end and humans at the other, with a passageway between the two parts.  This type of farmhouse was once common in mid- and south Wales.  This cruck and timber-framed house was originally built about 1470 as an open hall house.  The original timber walls were rebuilt in stone in 1734, with the date being carved on the head of the entrance door frame.  All that remains of the original house are the two cruck trusses in the cow house and the timber-framed partition between the cow house and the dwelling. Another re-construction at St Fagans is Hendre'r-ywydd Uchaf Farmhouse from Llangynhafal in Denbighshire.  This a cruck-framed hall-house dendrochronologically dated to 1508 and typical of the better class of Welsh farmhouse in the late Middle Ages.  The building is divided into five bays, the lower two used for housing for cattle and horses, the centre bay serving as a work-room and the upper two comprising the open hall and a bedroom.  The outside walls are timber-framed, the panels being in-filled with wattle and daubed with clay.  Both the daubed panels and the timber work are limewashed as was common in the Middle Ages.  The open hearth is placed in the centre of the hall, smoke from the fire escaping through the roof and the unglazed windows. Excavations in 2003 by Bill Britnell at Tŷ Draw at Llanarmon Mynydd Mawr in north Powys have done much to elucidate the relationship of the cow house with the hall of a longhouse.  This is a classic three unit hall house of longhouse type.  It has an open hall of two bays set between inner and outer rooms, the outer room acting as an cow byre.  Dendrochrononology on the roof purlins suggest that Tŷ Draw was completed shortly after 1479/80 and it has been possible to suggest from this dating that the house was built by Hywel ap Rees.  In the byre area, which was accessed by door, a series of small post holes were noted, that have been interpreted as wickerwork hurdles.  These would have provided stalls for cattle which were overwintered from November to March each year.  Similar evidence for stalls for cattle have been found at Tŷ Mawr and Tyddyn Llwydion in Montgomeryshire. The idea of the \"Longhouse\" or \"Tŷ-hir\" was first discussed by Iorwerth Peate in his pioneering book \"The Welsh House\"(1940).  This was the description of a house where both people and beasts were housed together under the same roof, as portrayed in the Medieval Welsh poem the Dream of Rhonabwy.  Peate thought that the Welsh Longhouse had had a long history and that it occurred in all parts of Wales.  This view was challenged by Peter Smith, who had gathered a vast amount of information in his \"Houses of the Welsh Countryside\", published in 1975.  This showed that longhouses were rarely \"one-phase\" buildings and often the byre had been added onto the house.  This showed that longhouses were not an upland phenomenon and are noticeably absent from Gwynedd where \"Snowdonia Houses\" are detached from their farm buildings.  The longhouses occur particularly in Ceredigion, Radnorshire and North Powys.  Suggett uses the example of Nannerth-ganol near Rhayader to illustrate the close connection of with the family who lived in this house with cattle rustling, which was particularly prevalent in mid-Wales in the Elizabethan period. Box framing is a simple timber frame made of straight vertical and horizontal pieces with a common rafter roof.  The term box frame is not well defined and has been used for any kind of framing other than cruck framing.  The distinction presented here is the roof load is carried by the exterior walls.  The timber framework, when exposed will be visible as squares or rectangles panels.  and the houses will also show signs of bracing, particularly at the corners.  Many of the Welsh houses have decorative features in the panels, such as quatrefoils and lozenge or herringbone deceptive woodwork.  The panels may also be filled by Close studding.  Box framing was used for the wings of earlier cruck or aisled timber framed houses, but it was not until the mid-16th that it was used as the main construction form for free standing houses in the \"Sub-Medieval tradition\", such as Glas Hirfryn, Llansilin. In the latter part of the 16th century, box framed houses grew larger and more elaborate.  They would now be built to three or four storeys with tall gabled projecting wings and decorative porches. A good example of this is Plas yn Pentre at Trevor near Wrexham.  On the dissolution of the abbey in 1536 it came into the possession of the High Sheriff of Denbighshire, Ieuan Edwards.  His grandson partially re-built the house in 1634.  His initials and the date can be seen carved into the exterior of the west gable.  Many of these large timber box framed houses have disappeared, but watercolours and prints record Aberbechan Hall and Garthmyl Hall, Berriew in Montgomeryshire and Bychton in Flintshire. These form an unusual group of \"Sub-Medieval\" Houses which were studied by E.L Barnwell in 1867-8 and by J Romilly Allen in 1902.  Characteristically these are a form of Hall house with a lateral chimney stack, which may be either round or conical.  Typically these chimneys have a lean to outshoot on either side of the stack with one of these outshoots acting as a porch.  Cottages and houses with these chimneys were mapped by Peter Smith and he showed that they form two groups, one around St Davids and the other to the south of Pembroke.  There is a good example of one of these chimneys on the Merchants House, Tenby.  Houses with very similar plans and lateral outshoot, but with square chimneys, also cluster on the Gower Peninsular. The timber-framed lobby entrance house emerged in the mid-16th century in Mid Wales.  The majority of these houses occur in Montgomeryshire with outliers in Radnorshire and Denbighshire.  The chimney in these houses is generally in the middle of the house.  There is no cross passage, unlike the Longhouses and the \"Snowdonia\" houses and instead the main doorway opens into a small lobby on the side of the fireplace.  The chimney generally stands between the kitchen and the parlour, and the key feature of these houses is the emphasis placed on the parlour which takes the place of a hall.  Many of these houses are earlier cruck framed hall houses, and some are box framed, which have had chimney's inserted and interior floors.  An example of this one the best known of the lobby entrance houses is Penarth (Newtown and Llanllwchaiarn), which stands prominently by the road between Newtown and Welshpool.  This originally appears to have had a cruck framing.  It has had two cross wings added and is strikingly decorated with close studded and herringbone timber work decoration. A development of this is \"Severn Valley Houses\" which particularly congregate along the Severn Valley in Montgomeryshire especially between Newtown and Welshpool.  A typical feature of the Severn Valley houses are the elaborate entry porches to the houses which often have decorative scroll brackets supporting a jettied upper story.  These porches often are added features to an earlier timber framed house and lead directly into a lobby entrance.  A well known example of the Severn Valley type, which has added timber framed wings to the house is Trewern Hall near Welshpool.  A house which has been dated by dendrochronology is Lower Cil on the outskirts of Berriew.  This is a well-preserved farmhouse.  Its left side is 16th-century (the square framing under the render was felled in 1583), probably a hall-house enlarged when the close-studded taller right end was rebuilt in the early 17th century to provide a new parlour and porch, both slightly jettied.  The porch has open sides with turned rails, and the original inner door.  The remodelling included the typical Severn Valley lobby-entry central chimney, with its triple-moulded brick stacks. Snowdonia houses have recently been the subject of considerable study by the Royal Commission on the Ancient and Historical Monuments of Wales and the Dating Old Welsh Houses Group.  These houses are typical of the \"Sub Medieval\" houses appearing in Wales in the earlier part the 16th century, which are a development from the Hall House.  Characteristically Snowdonia Houses are now built on a vertical rather than horizontal plan with two or more storeys and lateral chimney stacks set against the end gables.  The older cruck construction is now replaced with roofs constructed of trusses and purlins supported on stone walls.  The centrally placed doorway may now be set under a massive stone \"cyclopean\" arch as at Faenol Fawr, Bodelwyddan or else under a fan arch of stone slabs as at Y Garreg Fawr.  Y Garreg Fawr from Waenfawr in Caernarfonshire has been re-constructed at St Fagans and has been dated to 1544 The earliest example of a Snowdonia House dated by dendrochronology is Dugoed at Penmachno.  This has been dated to 1516–7.  The nearby Tŷ Mawr Wybrnant, known as the birthplace of William Morgan, translator of the bible into Welsh has been dated to 1565, but there is evidence that this was the re-building of an earlier cruck hall house of around 1500.  The construction of the typical Snowdonia Houses continued into the 17th century, as at Cymbrychan at Llanfair which is dated 1612. It should also be noted that the distribution of Snowdonia type houses extends into Aberconway and Caernarfonshire.  A good example of this type of house is the smaller house which stands immediately next to the mansion at Faenol Fawr near St Asaph.  This is likely earlier 16th century in date.  It appears to have been a two storied, hall house, with cruck framing and stone walls.  The evidence for the cruck roof is from a photograph by the Rev N W Watson, and this roof may still be in place.  \"Cyclopean\" doorways have been studied by Peter Smith, who shows that they are distributed mainly in Denbighshire and Merionethshire. These massive arched stone door lintels were introduced at a time, probably around 1600, when stone walling was replacing timber framing and may encase an earlier timber structure.  A much altered \"post and panel\" screens passage with three entrances, now in the hall area of the main house, is likely to have been removed from the hall in the older house.  This screens passage would have been associated with the finely moulded beams in the older house.  These moulded beams can be compared with similar beams at Maesycastell in Caernarvonshire and Perthywig in Denbighshire which are illustrated by Smith Another example is Gilar in Pentrefoelas, presumably built by Cadwaladr ap Maurice after receiving a substantial grant of land from Henry VIII in 1545-6 His son was the poet Rhys Wyn ap Cadwaladr (fl.  c. 1600), Renaissance architectural styles and influences start appearing in the eastern corners of Wales during the reign of Queen Elizabeth.  In Glamorgan an early example of Renaissance alteration was made to the facade of the outer gatehouse of the now ruined Old Beaupre, near Cowbridge in 1580.  This was followed by the more striking Porch in the inner courtyard of 1600 at Old Beaupre. A later and more developed example of Renaissance architecture is Ruperra Castle, built in 1628 for the Welsh soldier Sir Thomas Morgan and the design ideas may originate from his travels on the Continent.  The castle is rectangular in layout with round towers at the corners.  A feature of Ruperra is the rectangular three light windows with drip moulds, with the centre light higher than the side lights.  These windows appear on other Welsh Renaissance houses.  The Castle is rectangular with round corner towers and a Porch with a classical doorway.  Unfortunately as the result of a fire in 1941 it now stands partly ruined.  In the north-east of Wales a very important surviving Renaissance house was Plas Teg near Mold.  Rectangular in form with rectangular corner towers it is the typical form of Renaissance house seen over much of Europe.  A close parallel would be the first phase (unfinished) of Drumlanrig Castle in Scotland.  Another important Renaissance house, demolished in 1973, was Brymbo Hall near Wrexham.  The house was built for John Griffith in 1625 and a Baroque wing was added later in that century.  The core of the house was in brick with a fine classical doorway.  Fortunately a watercolour by John Ingleby in the National Museum of Wales records this building.  A further example of the Renaissance classicism was the Banqueting Hall at Margam Abbey.  This was recorded by Thomas Dineley in 1684, but only the stone facade now remains, erected in its present position in 1835.  It is three bays wide with fluted Ionic columns carrying richly detailed entablatures. The Renaissance comes to north-east Wales rather earlier with the building of Bach-y-Graig, at Tremeirchion near Mold by Sir Richard Clough, an extremely wealthy merchant, who established the Royal Exchange in the City of London with his business partner Sir Thomas Gresham.  Clough had lived in Antwerp, and upon his return to Denbighshire in 1567 he built \"Bach-y-graig\" and \"Plas Clough\".  Bach-y-Graig appears to have served as a lodge-cum-office, with large associated warehouse ranges set around a courtyard, the while the more traditional Plas Clough was clearly intended from the outset as his main house.  The houses were built in the Antwerp style by Flemish craftsmen and were the first brick houses in Wales. On the evidence of the similarities of these houses with the Renaissance buildings Antwerp and also the Royal Exchange in London, a strong case has been made out that their design should be attributed to the Flemish architect Henrick van Passe.  The Crow stepping on the gable, at Plas Clough near Denbigh, is typical of Flemish architecture and was widely copied on the grander houses that were being built in north Wales at this time, such as Plas Mawr(1576–85) in Conwy and Faenol Fawr (1597), near St Asaph.  Peter Smith maps the distribution of houses with stepped gables which are concentrated around Denbigh and Ruthin, Conwy, the Menai Straits and the southern part of Merionethshire.  He lists a total of 48 examples.  Another feature seen at Bach-y- Graig was the arrangement of multiple dormer windows on the roof.  Another property with three tiers of dormer windows that belonged to Clough was the Myddelton Arms in the market place at Ruthin.  Tiered dormer window were also copied on the roof of the older portion of the Manor House in Llanfyllin in Montgomeryshire. The major houses built in the 16th and earlier 17th centuries are often difficult to classify on stylistic grounds.  The Welsh families who built them often were less interested in the outside display of architectural features and more interested in the interior decoration, particularly elaborate plasterwork, painted walls and elaborately carved woodwork with armorials commemorating their family descent.  Many of these houses such as Bodysgallen, which was started in 1620 and Mostyn Hall are an amalgamation of different styles of architecture over many years.  The front is of 1631–2.  In the case of Nercwys Hall near Mold it is known that the contractor who built the Hall was Raffe Booth of Chester and the plans for the house were drawn up by his carpenter Evan Jones.  The contract for the building is 1637 and the datestone on the building is 1637. The influence of English architectural fashion can also be seen in Hen Blas, at Llanasa in Flintshire.  Built in 1645 at the start of the Civil War it is built of the local stone with ashlar facing.  As Edward Hubbard remarks Hen Blas is reminiscent of houses in the limestone belt of England. Another notable house is Gloddaeth near Llandudno, which retains its hall still up the original hammer-beam roof and also a painted dais above the high table at the end of the hall. The Old Hall at Y Faenol (Y Vaynol), Port Dinorwic is an E shaped building consisting of low 16th-century blocks with a more ornate right wing, which was probably added in the 1660s by Sir Griffith Williams.  This has a crow stepped gable. A feature of many of the larger houses of the 16th century is that they are set round walled courtyards that were entered through an arched gatehouse.  The most notable examples of these which are set in the countryside are Corsygedol in Merionethshire, Cefnamwlch near Tudweiliog and Rhiwaedog near Bala.  The impressive example at Llwydiarth in Montgomeryshire is now only known from earlier drawings and the gatehouse at Madryn on the Llyn survives, but not the house. Plas Mawr in Conway is one of the most impressive surviving courtyard houses of this period, which has recently been restored by CADW.  An Elizabethan townhouse, dating from the 16th century.  The property was built by Robert Wynn, a member of the local gentry, following his marriage to his first wife, Dorothy Griffith.  Plas Mawr occupied a plot of land off Conwy's High Street and was constructed in three phases between 1576 and 1585 at a total cost of around £800. There three phases of house construction – 1576–77, 1580 and 1585 – were probably overseen by several different senior craftsmen, possibly working to an original plan determined by a surveyor or mason working at the English royal court.  Judging by the details of the roof design, a single master carpenter may have been used for all three parts of the build. Other buildings, such as stables formed part of the courtyard.  The courtyard layout of these houses in north Wales may be compared with similar houses of the Elizabethan Period in England, particularly brick built houses in East Anglia, such as Erwarton Hall in Suffolk.  It should noted that there many examples of houses with gatehouses and courtyards in south Wales, such as Great Porthamel near Talgarth in Breconshire Great attention was paid to the interior decoration of the Great Houses.  Extensive use was made of decorative plasterwork and the restoration of the plasterwork which has been painted to show the original colour at Plas Mawr in Conway gives some idea as to how this would have appeared.  Where expensive wallhangings or tapestries could not be afforded, extensive painted imitations would be used.  Wooded panelling and armourial carving still survive in some houses, often over fireplaces.  A carved panelled room has been returned to Gwydir Castle and a carved over-mantle at Faenol Fawr records the building of the house in 1597 and the armourials of the Lloyd family.  At Gregynog, a room has been re-assembled with the insignia of the ancestry of the Blaney family. At Plas Mawr seven rooms still possess elements of their original plasterwork, which Peter Smith has described as \"the most perfect and the most complete memorial to Elizabethan Wales\", and their original wooden carved panels that line the walls.  The plasterwork includes extensive heraldry, badges and symbols: in the upper north range alone, 22 different heraldic emblems are moulded into the ceilings and walls.  The gatehouse shows the royal arms, as do the great chamber and the parlour, probably because they were intended to host senior guests.  The badges of numerous monarchs are included throughout the house, including those of Richard II, Richard III, Henry IV and Henry VII.  The badges of other prominent nobles, such as Robert Dudley, are also featured in the house. The plasterwork in the parlour displays the arms of Robert Wynn himself, and the brewhouse shows the combined arms of the Wynn and Griffith families, which are generally given equal prominence throughout the house.  Robert Wynn's arms are most prominent in the hall and the bedchambers, where the royal arms are smaller and less prominent.  In the 16th century, Wynn's heraldry would probably have been echoed in the furnishings of the house, including the fabrics, cups and silverware.  The plasterwork also incorporates a number of classical themes, but these are not as well executed as the badges and other emblems: Turner describes them as \"rather token additions\", and Smith considers this part of the decoration to be \"naive\". At Maenan Hall near Llanrwst, there is splendid plaster work which is dated 1582 and at the \"Town Hall\" at Portmeirion Clough Williams-Ellis was able to preserve the mid-17th Century plaster ceiling from Emral Hall in Maelor Gymraeg. Brick building in Wales only became fashionable very slowly and in some areas of Western and south western Wales only starts to appear in the 19th century.  Brick makers tended to be itinerant until the mid-19th century, digging clay and firing bricks took place close to the building that was to be constructed.  One of the more permanent brickyards was the Herbert's (Earls of Chirbury) brickyard at Stalloe near Montgomery, which would have been the likely source for the impressive \"New Build\" at Montgomery Castle and for large quantities of bricks used in building of the service wings at Lymore near Montgomery 1664–67 and also for 17th-century brick-faced town houses in Montgomery and possibly Welshpool.  The earliest use of brick in the 16th century was for the construction of massive chimney stacks of \"Stellar\" form with multiple flues within timber-framed houses.  These stacks would have greatly reduced the risk of fire, and the study by Peter Smith of the distribution of these stacks show them to be clustered along the Welsh border from Montgomeryshire northwards. While brick making may have started by Flemish brick makers working for Sir Richard Clough, building in brick was also becoming established in Shropshire and in Cheshire.  The earliest of the typical Elizabethan Houses using brick with stone dressing was Trevalyn Hall built for John Trevor in 1576 Brick with stone dressing was used for the construction of Brynkinalt at Chirk, near to the Welsh border with England.  This is an E plan house of Elizabethan or Jacobean appearance that was built for Sir Edward Trevor in 1612 It has been noted that Brymbo Hall (1625) was largely brick, but the Cheshire influence of brick building is also apparent in Halghton Hall in Maelor Gymraeg of 1662 In Montgomeryshire the earliest brick house was the \"New Build\" at Montgomery Castle, which was built for Edward Herbert by Scampion between 1622 and 1625. Bodwrdda, near Aberdaron on the Llyn peninsular provides an example of an earlier house was that was re-fronted in brick in 1621.  In Monmouthshire the establishment of brick building is shown by the massive brick service block (now Castle Farmhouse, Raglan) that was built for the older branch of the Herbert family for Raglan Castle, probably just before the English Civil War. Timber framed houses of the 15th to 18th century are present in many of the Welsh towns in North, Central and SE Wales.  The distribution of these houses has been mapped by Peter Smith who shows that in some areas in Wales such as Glamorgan and Anglesey, timber framed houses were being built in towns, but not in the countryside, where stone would have been the usual building material.  Modern commercial development has tended to remove most of the timber framed houses from the high streets of Welsh towns, leaving the occasional examples, often public houses such as the ‘‘Buck’’ in Newtown and the thatched \"Horse and Jockey\" in Wrexham.  Many more examples of timber framed houses exist behind brick facades of the 18th and 19th centuries.  This is particularly the case in the small market town of Montgomery, where the Herbert family encouraged the inhabitants to rebuild the houses with brick frontages from the 1670s onward. In most instances timber framed houses in towns are smaller versions of the timber framed houses of the countryside, but adapted to fit onto the more restricted burgage plots.  Earlier examples of timber framing may be jettied forward towards the street and particularly good examples exist in Beaumaris on Anglesey and Conway, formerly in Caernarfonshire. The Royal House in Machynlleth is a rare example of a relatively unaltered mercer's dwelling and store-house and has some claim to be one of the oldest shops in Wales.  Dendrochronological or Tree ring dates indicate it was built between 1559 and 1561.  It was said that Owain Glyndŵr imprisoned David Gam there, and it was also said that King Charles I stayed at the house when travelling to Chester – hence the origin of the name 'Royal House'.  It occupies one of the original Medieval burgage plots laid out around 1291.  The long range has three parts with a house set between an upper shop and a lower store. Another early trading house was Aberconwy House in Castle Street Conwy, now in the care of the National Trust.  It is the one survivor of a group of merchant cum warehouses of the English merchants who traded in Conwy.  It is a three storey building, the first two storeys of which have stone rubble walls and the upper is a jettied out timber-framed construction.  It has been tree ring dated to about 1420 . In Tenby there is the Tudor Merchants House on Quay Hill, also in the care of the National Trust.  This dates from the late 15th century and is possibly the most complete Medieval merchant’s house in Wales.  Stone built with three storeys and the roof consists of five bays of crucks.  At the third floor level a lateral chimney stack and a mullioned window are corbelled out and there is a large cylindrical chimney stack to the north.  There is some painted decoration inside.  The house appears to have been part of a larger merchant’s complex. Market halls and town halls are a prominent feature in most Welsh Towns.  Early market halls were placed in the market area with an open area below for the market traders and an upper area that was used as a court room and council chamber.  In the major County towns these buildings would also have been the meeting place of the Court of Great Sessions which as well as holding the Assizes supervised the county administration, and these halls were often known as the Shire Hall.  Many were timber framed, but the only surviving example of this type is at Llanidloes in Montgomeryshire.  The Court House in Ruthin, now the National Westminster Bank, probably served a similar purpose and Dendrochronological dating has recently shown that this building is earlier and the timbers used for its construction were felled in 1421 The Llanidloes Market Hall is known to have been constructed from trees felled between 1611 and 1622 A slightly earlier town built of stone is the Shire Hall at Denbigh of 1572, with an open colonnaded Market Hall on the ground-floor and a council and court room on the first-floor.  In the 1780s with a new roof, rusticated entrance and new fenestration, including Venetian windows were added.  The colonnades have been enclosed In the Georgian period much more impressive town halls started to be built and the Shire Hall at Monmouth is a particularly good example.  It is in a classical style of Bath stone by Fisher of Bristol with giant Ionic pilasters In Montgomery, the earlier timber framed town hall was replaced by a red brick Town Hall by William Baker of Audlem in 1748–51, for Henry Arthur Herbert, 1st Earl of Powis.  The original design of the Town Hall shows the ground floor market space was open with five bay arcades.  The upper floor housed the Council Chamber and Court of Great Sessions and Quarter Sessions.  In 1828 Thomas Penson, at the expense of Lord Clive, raised the roof level over the first floor and introduced sash-windows, rebuilding the pedimented gable, but without the coat of Arms.  An extension was added at the rear with rounded quadrant corners and was tied in by extending the string courses around the building, adding a matching pediment to that at the front.  The arcade arches were infilled with glazing and bricks in 1887, and the attractive clock tower was added in 1921. An important building, although altered, in Palladian style is Sir Robert Taylor's Guildhall at Carmarthen, built between 1767 and 1777.  This has a trio of giant first floor windows which are over-arched over large Palladian windows with Ionic columns and with blind panels above.  Taylor used similar windows to light the Court Room of the Bank of England In the 19th century the design of Market Halls changed, they were now single storied and larger areas were made available for trading.  A particularly notable example now houses the Nelson Museum and local history centre in Monmouth.  This was built in Bath stone in the Greek Doric style by the architect George Vaughan Maddox of Monmouth in 1837-9 The best known early bridge in Wales is over the river Conwy at Llanrwst which is often attributed to Inigo Jones.  The bridge has three arches and a steep camber.  The bridge was constructed in 1634 four Lancashire masons, Barnard Wood, James Stott, Thomas Crompton and John Mellor.  They may well have working to designs drawn up by Jones based on a design by Palladio.  Sir Richard Wynn of nearby Gwydir, as Treasurer to the Queen, is likely to have known Jones and commissioned work from his master mason Nicholas Stone.  An earlier bridge, with nine arches is at Holt over the River Dee, dated to 1254,' which is still in use.  This bridge is commemorated by a famous painting by Richard Wilson, now in the National Gallery, which also shows the gatehouse chapel which stood at the east end.  Another early bridge which still has a standing gatehouse is Monmouth Bridge over the river Monnow which was constructed as part of the town defences of Monmouth during the period 1297–1315 In south Wales a notable bridge architect and engineer was William Edwards, (1719–1789), who in 1746 was contracted to build a new bridge over the River Taff at Pontypridd.  The first bridge was washed away and the second bridge collapsed, but his third bridge was a single arch bridge of 140 feet, then the largest of the type in the world, which he completed in 1756, which is still standing, now known as the Old Bridge.  In order to reduce the weight of the bridge he pierced large cylindrical holes through the haunches of the bridge, which solved a constructional problem and gives it its elegant appearance.  He used the same technique on other bridges, such as Cenarth Bridge at Cenarth and he was able to construct arches of much larger radius with less gradient over the arch. A number of fine bridges were built in Montgomeryshire in the 18th century, which include Llandrinio bridge of 1769–75, probably by the noted Shrewsbury bridge builder John Gwynn.  An ashlar bridge of three arches of pink sandstone, with rusticated voussoirs.  Another impressive bridge is the single arched bridge at Dolanog over the Vyrnwy, which was portrayed by the artist Edward Pugh in 1813 but probably dates from the mid-17th century. The construction of the Holyhead Road and other work by Thomas Telford resulted in a number major bridges.  At Betws-y-Coed Telford constructed the early iron Waterloo Bridge across the Llugwy.  This bridge with a span of over 30 metres, was cast at William Hazledine's foundry.  This bridge has the inscription \"This arch was contrasted in the same year as the battle of Waterloo was fought\", but it was completed in 1816. Another iron bridge to be completed in 1816 was John Rennie's elegant bridge over the river Wye at Chepstow which was also produced at Hazeldine's foundry. In 1819 Thomas Penson became County Surveyor for Montgomeryshire and he built many new bridges in the county including a notable series of iron bridges over the river Severn, including those at Garthmyl at Berriew, Brynderwen at Abermule and Llandinam.  The inscription of the Brynderwyn, Penson copies Telford with an inscription over the arch \"This is the second iron bridge constructed in the county of Montgomery, was erected in the year 1852.  Thomas Penson , County Surveyor : Brymbo Company Ironfounders\" \"Tŷ unnos\" (plural \"Tai unnos\" ) (one night house), is a traditional Welsh belief that if a person could build a house on common land in one night, that the land then belonged to them as a freehold. In the Restoration period following the Civil War a number of major larger houses were built, particularly in southern Wales.  The first and most impressive of these was rebuilding of Tredegar House at Newport by William Morgan in the mid-1660s.  This was probably the work of two carpenter architects, Roger and William Hurlbutt from Warwick.  A brick house that is richly decorated with stone dressings and the principal doorway with foliage clad twisted columns that support a pediment.  Tredegar House was to be followed by Great Castle House at Monmouth in 1673 for Henry Somerset, who became the 1st Duke of Beaufort in 1682 and was also Lord President of the Marches.  Another house of this period was Penpont in Breconshire built around 1666.  A double-pile house It has been much altered.  It was encased in Bath stone in 1828–35, when a ground floor colonnade was added to the front of the house. In 1683 work was begun on the construction of Erddig, on the outskirts of Wrexham.  Erddig was a similar house to Troy House.  The architect was a Thomas Webb, who is described as a ‘freemason\".  This house is in the style of the leading Restoration architects Hugh May and Roger Pratt.  The house was later extended with wings to either side c. 1721–23. At Trawsgoed (Crosswood) in Ceredigion the earlier house was partially rebuilt after damage during the Civil War and the house survives today in much altered form.  The appearance of the house in 1684 is provided by a drawing by Thomas Dineley.  There was a three bay central house with dormers and a classical doorway with earlier side wings forming an inner courtyard, and outer gated garden courtyard. The brick houses of the Late Stuart period with projecting wings continued to be built until well into the Georgian period as shown by Trevor Hall near Wrexham, which was built in 1742. Architecture of the Georgian period in Wales may be considered to start with houses such as the recently restored Llanelly House.  This was built in 1714 by Sir Thomas Stepney in Llanelli.  At the time Llanelli was only a village and this should be considered a Country House rather than a town house The House has its original lead downspouts which are dated 1714, but there is no evidence as to whom the architect was.  It is of seven bays with sash windows and a parapet with big gadrooned urns.  Similar large block-like houses continued to be built during the reigns of George I and George II.  Nanteos near Aberystwyth has a foundation stone of 1739 and completion date on the rainwater head of 1757. Taliaris in Carmarthenshire is another house of this form with a facade of Bath stone.  It was probably built shortly after the marriage of Richard Gywnne to Ann Rudd in 1722-3.  Taliaris is by an unknown, but on stylistic grounds it has been suggested that it is the work of a Bristol or Somerset mason or architect A further example of this type of house was the early 18th-century Glanbran, Cynhordy, Carmarthenshire which is described as Palladian with Mannerist touches.  There was an ornamented Venetian window and a top window with paired pilasters.  It was finally demolished in 1987. Houses with the typical Palladian arrangement of a central block attached wings or flanking pavilions were built at Dyffryn Aled in Llansannan in Denbighshire and Trawscoed at Guilsfield in Montgomeryshire.  Dyffryn Alyn was built to designs by Joseph Turner in 1777, and the pavilions were added in a matching design by James Wolfe.  Thomas Pennant records that the house replaced an old house of the Wynne family and Dianna Wynne built a new house \"in a most elegant and magnificent manner, on the side of the hill opposite the antient mansion\" and cased it in Bath stone.  \"The very day after the workmen had finished their work, almost the whole casing fell down: which occasioned a vast expense in the repair\".  The house was demolished around 1920, but Pennant provided an excellent picture of this grand house in his extra-illustrated volumes of the tour, now in the National Library of Wales A development of the Palladian style was Pengwern Place (or Hall) near Rhuddlan of 1778.  This was a Mostyn family house and today is much altered from its original appearance, which is shown in its original form in an engraving in \"Neale's Seats\" of 1818.  The main block is of two and a half storeys and five bays with octagonal wings in brick with stone dressings.  The central pediment over three bays on a giant order of Ionic pilasters.  On either side at first floor level are two Venetian Windows.  An impressive composition which is already starting to show the influence of Robert Adam An important architect who established himself at Swansea in this period was William Jernegan, (c. 1751 – 1836).  He probably came to Wales as an assistant to John Johnson, in the 1770s.  He was to design a number of Regency terraces in Swansea which have now largely disappeared, the Assembly Rooms of 1810 and produced plans for the Swansea Copper works.  In the area around Swansea he was responsible for the Marino, which was incorporated into Singleton Abbey, the re-modelled Kilvrough in c. 1785, Stouthall, Reynollston, 1787-9, and Sketty Hall and Sketty Park House.  He was responsible for the Mumbles Lighthouse in 1793.  He is also thought to have been involved in the design and layout of Milford Haven in Pembrokeshire. By far the most important architect to work in Wales in the 18th century was John Nash.  Nash had trained in London under Sir Robert Taylor.  Nash left London in 1784 to live in Carmarthen, where his mother had retired to, her family being from the area.  He set up with Samuel Simon Saxon, another London architect, to work as building contractors and suppliers of building materials.  Nash's London buildings had been standard Georgian terrace houses, but it was in Wales that he matured as an architect.  His first major work was Carmarthen Prison (1789–92).  The prison was planned by the penal reformer John Howard and Nash developed this into the finished building.  He went on to design the prisons at Cardigan (1791–96) and Hereford (1792–96).  In 1789 St David's Cathedral was suffering from structural problems, the west front was leaning forward by one foot, Nash was called in to survey the structure and develop a plan to save the building.  His solution completed in 1791, was to demolish the upper part of the facade and rebuild it with two large but inelegant flying buttresses. In 1790 Nash met Uvedale Price, of Downtown Castle, whose theories of the Picturesque would influence Nash's town planning.  Price would commissioned Nash to design Castle House Aberystwyth (1795).  Its plan took the form of a right-angled triangle, with an octagonal tower at each corner, sited on the very edge of the sea. One of Nash's most important developments were a series of medium-sized country houses that he designed in Wales, these developed the villa designs of his teacher Sir Robert Taylor.  Most of these villas consist of a roughly square plan with a small entrance hall with a staircase offset in the middle to one side, around which are placed the main rooms, there is then a less prominent Servants' quarters in a wing attached to one side of the villa.  The buildings are usually only two floors in height, the elevations of the main block are usually symmetrical.  One of the finest of these villas is Llanerchaeron, and at least a dozen villas were designed throughout south Wales. Others, in Pembrokeshire, include Ffynone, built for the Colby family at Boncath near Manordeifi, and Foley House, built for the lawyer Richard Foley (brother of Admiral Sir Thomas Foley) at Goat Street in Haverfordwest.  Villas of this type were widely imitated in Wales, particularly by Joseph Bromfield of Shrewsbury. As Nash developed his architectural practice it became necessary to employ draughtsmen, the first in the early 1790s was Augustus Charles Pugin, then a bit later in 1795 John Adey Repton son of Humphrey Repton.  It was presumably through Nash that Repton gained commissions in Wales, such as Stanage Park in Radnorshire. In 1796 Nash spent most of his time working in London; this was a prelude to his return to the capital in 1797.  At this time Nash designed the delicate Strawberry Hill Gothic revival gateway to Clytha Park near Abergavenny in Monmouthshire, and also his alterations in Gothic Revival style in 1794 to Hafod Uchtryd for Thomas Johnes at Devil's Bridge, Cardiganshire Also in c. 1794–95 he advised on the paving, lighting and water supply in Abergavenny and designed an elegant market building.  Other work included Whitson Court near Newport.  After his return to London was to continue to design houses in Wales which were to include Harpton Court in Radnorshire, which, apart from the service wing was demolished in 1956.  In 1807 he drew up plans for the re-building of Hawarden Castle with gothic battlements and towers, but the plan appears to have been modified by another architect when it was carried out.  About 1808 he designed Monachty near Aberaeron.  and later drew up plans for work at Nanteos. In the latter part of the 18th century, as the result of Prison reform new prisons came to be built in most of the Welsh County towns.  The reforms were the result of the work of John Howard, who in 1777 published \"The State of the Prisons\" He proposed that each prisoner should be in a separate cell with separate sections for women felons, men felons, young offenders and debtors.  This was followed by the Penitentiary Act which was passed in 1779.  This act was in implemented in each Welsh county by the Court of Great Sessions and which led to the building of many new prisons across Wales.  These included the gaols built in Carmarthen and Cardigan by John Nash and the gaols at Caernarfon (1793), Ruthin (1785) and Flint(1775) by Joseph Turner.  Most of these prisons were closed in the 1870s, but the Ruthin gaol, now used as the Denbighshire County Record Office is remarkably well preserved.  The Anglesey Gaol at Beaumaris came later in 1828-9 by the architects Hansom and Welch.  This incorporates many of the innovations of the Milbank Penitentiary in London of 1812–21 with wings, a massive curtilage wall and a central glass cupola for the oversight of the prison complex. This plan was developed for the Montgomeryshire County gaol at Montgomery by the County Surveyor Thomas Penson, c. 1830–32.  Brick faced with stone.  The tall octagonal governor's house with the chapel above, was at the centre of four radiating three- and two-storey wings.  One of the yards was fitted with a tread-mill.  The gatehouse was built into the wall to face a new approach in 1866 by J.W. Poundley.  Powerful ashlar triumphal arch, with four giant semi-rusticated pilasters.  The Gaol was closed in 1878 and all that now remains, apart from the gatehouse, is the Governor’s House and the high wall of one cell block. Penson was also to use this design for the workhouses at Llanfyllin and Caersws in Montgomeryshire. Neoclassical architecture came to north Wales mainly as a result of the influence of Samuel Wyatt.  Wyatt had worked for Robert Adam, the leading Neoclassical architect when he became the clerk of Works at Kedleston Hall in 1759.  Between 1776 and 1779 he remodelled Baron Hill at Beaumaris on Anglesey for Viscount Bulkely, while his brother became estate manager for the Pennants at Penrhyn.  Colvin remarks that Wyatt specialised in the designing of medium-sized country houses in an elegant and restrained neo-classical manner.  Characteristic features of his houses were astylar elevations with prominent bowed projections which were domed and were either single or in pairs.  His windows were often tripartite and overarched.  He rarely deviated from the neo-classical, though he did a gothic revival building at Penrhyn Castle which was replaced by Hopper’s Neo-Romanesque Castle.  At Kinmel Park near St Asaph, around 1790, he built a stylish house for the Rev Edward Hughes, who derived great wealth from the development of the Parys Copper mines on Anglesey.  This house had a bowed front and panels with classical swags, possibly of Coade stone.  It was burnt down in 1841, but fortunately it was recorded in a watercolour by John Ingleby in 1794.  A further house in this style was the Old Bishop's Palace in St Asaph which was probably by Samuel Wyatt, while at Brynbella in Tremeirchion a London surveyor, Clement Mead built Brynbella, for Dr Johnson's friend Mrs Thrale.  Bryn Bella was built between 1792 and 1795 with an ashlar facade and double bays and wings with pediments on either side.  Another very fine bow fronted house, Gresford Lodge near Wrexham, was built for John Parry by James Wyatt, the brother of Samuel Wyatt around 1790.  This house was domed over the bow front with a semicircular portico with Ionic columns and tripartite overarched windows.  Gresford Lodge was demolished around 1950 due to subsidence caused by coal mining. In south Wales Neo-classicism was introduced by the Gloucestershire architect Anthony Keck and by William Jernegan, an architect who established a practice at Swansea.  Keck who worked from Kings Stanley Gloucestershire may have worked with Sir Robert Taylor who would have introduced him to clients in Wales.  He built a bow fronted house for Thomas Mansel Talbot (1747–1813) adjacent to Penrice Castle in Glamorgan in 1773–1780.  This building, though earlier than Samuel Wyatt's work in north Wales, lacks features such as the overarched windows.  However, the Orangery he also built for Thomas Mansel Talbot at Margam Abbey from 1787 to 1790, exhibits a much more refined appreciation of Neo-classicism and may well be considered the best example of this architectural style in Wales.  It is the largest Orangery in the British Isles of 17 continuous bays with vermiculated rustication to the more formal swags and arched windows. A house of considerable importance was Piercefield between Chepstow and St Arvans.  Originally known for its gardens laid out by Valentine Morris, it was rebuilt in 1793 to plans prepared by Sir John Soane which were to be modified by Joseph Bonomi.  It still retained a Palladian appearance with a massive central block and side pavilions.  The side pavilions and curved colonnade of Tuscan columns were the additions made by Bononi after 1795.  The house to-day is in a ruinous state. A house which bridged the gap between late Palladian forms and Neo-classism was Middleton Hall in Carmarthenshire, built for Sir William Paxton to the designs of S P. Cockerell between 1793–5.  The giant portico supported by five Ionic Columns was a theme which was to prove popular with architects working in Wales in the following century.  The over-arched windows are Palladian derived and were used by Sir Robert Taylor for Carmarthen Town Hall.  Cockerell had served his pupillage under Sir Robert Taylor, as had also John Nash and these windows are also seen on Nash's Villa type houses in Wales, as at Llanerchaeron.  Middleton Hall was burnt down in 1931, and its gardens, are now the site of the National Botanic Garden of Wales.  Cockerell was also responsible for the design of the nearby Paxton's Tower, a Gothic folly built in 1805 in commemoration of Lord Nelson.  1n 1810 Cockerell was responsible to Sir William Paxton for building the sea water Baths and Assembly Rooms at Tenby.  Over the enclosed bow porch is a Greek inscription taken from Euripides \"The sea washes away all the ills of men\". A Chester architect showing considerable competence in classical revival architecture was Joseph Turner who worked extensively in Flintshire and Denbighshire.  Apart Ruthin and Flint gaols, he was responsible for the County Hall at Ruthin, which served as a courthouse.  It has an ashlar facade with a tetrastyle pedimented portico with Greek Doric capitals and the courtroom has Venetian windows on either side The use Greek revival Ionic Columns under a tetrastyle portico occurs again at Llanphey Court in Pembrokeshire which was completed in 1823 by Charles Fowler who was also the architect for the Covent Garden Market in London. An architect who worked very competently in the Classical style was George Vaughan Maddox (1802–1864), a Monmouth architect whose work is restricted to Monmouth and the area immediately around.  Maddox has been noted above as the architect for New Market in Monmouth which opened in 1837.  This was part of a new street which was built on arches overlooking the river Monnow, which now forms a handsome entrance to the town from the North.  He was architect for the houses in the street and other buildings in the town which include Foley House and the Masonic Hall in Hereford Street Monmouth. This is comparatively well represented in Wales.  As a style it is more severe and modelled more closely on Greek Architecture.  Thomas Harrison of Chester was a leading exponent of the style and in Anglesey was responsible for the Holyhead Memorial and the Marquess of Anglesey's Column in Llanfairpwll on Anglesey in 1816-7, to commemorate the feats of Marquess of Anglesey in the Napoleonic Wars. This was in a restrained Greek revival style with an entrance flanked by Doric columns.Harpton Court, near New Radnor is a further example of a house remodelled in a Classical revival style around 1840.  It was built in 1750 for the Lewis family and was later modified by John Nash.  It was then remodelled on the south front by an unknown architect in stone with nine bays, with the three central bays broken forward with pilasters and surmounted with a pediment.  The house was sold in 1953 and partially demolished in 1956. During the earlier part of the 19th century, architecture in many parts of Wales came increasingly under the influence of a small group of Shrewsbury architects and also Thomas Penson, an architect who worked from offices in Oswestry in Shropshire.  Thomas Penson had trained under Thomas Harrison in Chester, and his most notable building in the classical style was the Flannel Exchange in Newtown of 1832, which although still standing, was badly defaced by its conversion into a cinema in the early 20th century.  The Shrewsbury architects, Joseph Bromfield and the Haycock family had been influenced by the talented Farnolls Pritchard, the original designer of the Iron Bridge.  While this group of architects worked in various styles, they particularly developed the classical style in Wales.  Edward Haycock's most important works were Glynllifon near Caernarfon, the facade to Nanteos stables and Clytha Park near Abergavenny, and the laying out of the small town of Aberaeron in Ceredigion. The Greek revival style was chosen for many public buildings in Wales.  Swansea Museum of 1839–1841, originally the Royal Institution of South Wales is a finely detailed and well balanced example with a three bay portico supported on Ionic columns.  It is faced in Bath ashlar stone.  It was built to designs by Frederick Long, a Liverpool architect. A rather later use of Greek revival is the Shire Hall at Caernarfon of almost oversized proportions and facing Caernarfon Castle.  It was built 1867-9 by the County Surveyor, John Thomas.  It is of seven bays with a central doric portico with a pediment surmounted with the blindfold figure of justice Other work in the Greek Revival style in Wales includes Brecon Shire Hall (now Brecon Museum) by Thomas Henry Wyatt and Bridgend Town Hall by David Vaughan.  Bridgend Town, a tragic loss, demolished as recently as 1971, was built in the style of a Greek temple with Doric columns supporting the portico in antis. An early and unusual combination of Grecian and Italianate architecture is Swansea Old Town Hall.  It is described by Newman as ‘‘as the noblest classical building in Swansea ……a grandiose Corinthian Palazzo’’.  It was built to designs by Thomas Taylor of London between 1848 and 1852 which incorporated the earlier Town Hall of 1825– 27 by Thomas Bowen.  The interior of the building, which is now the Dylan Thomas Centre, was extensively rebuilt in 1993–94 The derivation of Romanesque Revival architecture or Norman Revival architecture can be traced back to the late 17th century, but only became a recognisable architectural style around 1820.  In 1817 Thomas Rickman published his \"An Attempt to Discriminate the Styles of English Architecture from the Conquest To the Reformation\".  It was now realised that \"round-arch architecture\" was largely Romanesque in the British Isles and came to be described as Norman rather than Saxon.  The start of an \"archaeologically correct\" Norman Revival can be recognised in the architecture of Thomas Hopper.  His first attempt at this style was at Gosford Castle in Armagh in Ireland, but far more successful was his Penrhyn Castle near Bangor.  This was built for the Pennant family, between 1820 and 1837.  The style did not catch on for domestic buildings, though many country houses and mock castles were built in the Castle Gothic or Castellated style during the Victorian period, which was a mixed Gothic style.  The Welsh architect Thomas Penson, however, built churches in this style in Montgomeryshire A surprisingly early example of Gothic Revival architecture in Wales is the south wing of Hensol Castle in Glamorgan.  Hensol had three storied east and west wings added with tower-like semi-octagonal bays which were fenestrated with pointed gothic windows and surmounted by battlements.  It has been suggested that this very early Gothic architecture was the work of Richard Morris who also designed Clearwell Castle in Gloucestershire about 1728.  One of the towers at Hensol is dated 1735 and much of the evidence for this early phase at Hensol has been disguised by the later gothicisation of the building. Another example of the simple Gothic revival style was Gnoll near Neath.  This was a remodelling, starting in 1776, of an existing house for Sir Herbert Mackworth, owner of a local copper works.  It had a crenulated parapet and round towers at the two corners of the frontage.  The architect was John Johnson an Essex architect who had been associated with Mackworth's banking interests in London. Following this, examples of Strawberry Hill Gothic start appearing in Wales, the most significant of which was Hafod.  The first stage of Hafod was started 1786 by Thomas Baldwin of Bath for Thomas Johnes, in a Gothic revival style with gothic window, battlements and pinnacles and then in 1793-4 John Nash added a top-lit galleried library and a 300 ft long conservatory. Another early pioneer of the Gothic style was James Wyatt who was employed by the Earl of Uxbridge to build Beaudesert in Staffordshire in the Gothic style in 1771-2.  The Earl was to employ him again at Plas Newydd on Anglesey to rebuild the West front and the interior in a Gothic style from 1793–9 Wyatt worked with the Lichfield architect James Potter and the style of plasterwork lacks the lightness of Strawberry Hill gothic. Plas Newydd was followed by the more remarkable Gothic mansion at Garth at Guilsfield in Montgomeryshire, built for Devereux Mytton, probably in the late 1790s. Another house with Strawberry Hill Gothic features was Bodelwyddan Castle which had a Gothic wing and gothic interior added between 1802 and 1808 by an unknown architect.  During this period a number of other houses in Wales were given Gothic facades with arched pointed windows, such as Llewenni in Denbighshire.  At Hawarden Castle the Palladian house, on a different site to the medieval castle, was transformed by Thomas Cundy in 1809–10 for Sir Stephen Richard Glynne.  It was refronted and machicolations, towers and turrets were added.  A west wing was added with gothic windows.  At Rhiwlas, near Bala the ancient house was re-cased as a Gothic castle in 1809 with three storeys and three polygonal towers, but the house was largely replaced by new house in 1954.  The Gate arch by Thomas Rickman of 1813 still exists. In the 1780s there was another style evolving which sometimes is referred to as a 'Folly Gothic', houses which were intended as eye-catchers.  Possibly the best example of this is Clytha Castle the work of architect and garden designer John Davenport.  This style was a less archaeologically correct form of Gothic revival and was widely used in Wales during the period 1780 to about 1810.  Greater emphasis was placed on prominent arrow slits in round towers, blanked quatrefoil windows and stepped and angled battlements on gables.  The earliest example of this may be the re-building of Penrhyn Castle, sometime before 1782 by Samuel Wyatt for Richard Pennant, who was to develop the Bethesda slate quarries.  This building is only known from drawings by Moses Griffiths, though parts of it were incorporated into Thomas Hopper's rebuilding of Penrhyn.  While it shows the features of this evolving style, the doorway shows the influence of Strawberry Hill gothic. Folly Gothic was a style which was widely adopted for park gates and lodges and for small houses sited in picturesque positions in locations frequented by tourists. A good example is Ogwen Bank near Bethesda, built by Lord Penhryn, possibly to a design by Samuel Wyatt, for visitors to the Ogwen falls.  In the Montgomeryshire at Berriew Bodheilin was built was built in a prominent position overlooking the valley of the river Severn.  This was burnt down in 1906, but an engraving shows a fantastic villa with five towers fronted with a Neoclassical portico. In Wales pointed Gothic windows continued to be widely used until about 1810.  In Montgomeryshire iron framed Gothic windows were used to embellish vernacular houses. Grander houses such as Dol-Llys in Llanidloes, built for George Mears around 1800, by an unknown architect, but in the \"villa\" style of John Nash, had wooden Gothic windows. One of the most eye-catching gothic follies in Wales is Paxton's Tower.  Built by Sir William Paxton (1745–1824), Paxton made his first fortune while with the HEIC in Calcutta with Charles Cockerell, brother of the architect.  He purchased the Middleton Hall estate about 1790 and built this tower 1808.  Designed by Samuel Pepys Cockerell who also worked for Paxton in the development of Tenby.  The tower was built to commemorate Nelson's death at Trafalgar.  The tower is 36 feet high.  The lower part of the tower is triangular in shape with a turret at each corner and on the first floor there is a banqueting room.  On the second floor there is a hexagonal prospect room surrounded by roof terraces. Castellated Gothic was a style that emerged in Wales following the Napoleonic Wars and has been little studied, although a considerable number of Country Houses were built in this style up to about 1870.  It is largely derived from the earlier Castellated Gothic Mansions built Robert Adam in Scotland and Adam was also the designer of one house built in Wales, Wenvoe Castle in Glamorgan in 1776/7 of which only one wing of the building now survives.  This \"Welsh\" style of Castellated Gothic lacks the historical precision of detailing seen in \"Strawberry Hill\" Gothic, but it has borrowed the turrets and battlements some Medieval Castles in Wales such as Raglan and the earlier Cardiff Castle.  In some cases Medieval castles which were still inhabited were by re-built in this style.  Powis Castle on the outskirts of Welshpool was extensively re-built with new windows and battlements in the castellatted gothic style by Sir Robert Smirke between 1815 and 1818.  while Ruthin Castle was rebuilt in this style firstly in 1826 and then more extensively between 1848 and 1853 by Henry Clutton.  In many instances these Castellated Gothic houses, such as Llanerchyddol near Welshpool, have not been attributed with certainty to any architect, and are likely to have been the work of a local architect or one working in Shrewsbury.  The main features of these houses are the prominent machicolated towers and the horizontal, rather than vertical, architectural composition.  Rough hewn stone rather than ashlar was often used and rectangular windows under \"Tudoresque\" drip moulds. Initially older houses such as Bodelwyddan in Denbighshire or Hensol Castle had large extensions added to them.  At Stanage Park in Radnorshire the design has been attributed to John Adey Repton, but he employed as the building contractor John Hiram Haycock.  Haycock, from Shrewsbury, was equally competent as an architect and may have contributed to appearance of the building.  This has led Thomas Lloyd to suggest that the similar appearance of Glandyfi (c. 1812) in Ceredigion, may also be the work of Haycock.  At Brynkinalt in Denbighshire the addition of castellated towers and other feature (now removed) on a late 17th-century house was the work of another Shrewsbury architect Joseph Bromfield.  However, the Gothic entrance gate that can be attributed to Bromfield survives. Castellated Gothic was the style employed by Robert Lugar when he built Cyfarthfa Castle in Merthyr Tydfil for the iron master William Crawshay, in 1824–25.  It is two-storied and battlemented with a turreted entrance porch which leads into a Gothic entrance hall with ribbed ceiling.  Robert Luger was also employed at Maesllwch, Glasbury from 1829 to 1850.  The main portion of this house was demolished in 1951 leaving the eastern tower, service wing and later tower.  Between 1818 and 1830 John Preston Neale published his \"Views of the Seats of Noblemen and Gentlemen in England, Wales, Scotland and Ireland\" in which he included several examples of Castellated houses in Wales, showing this had become an established style.  Further prints of Welsh castellated mansions were included in the Rev Francis Orpen Morris's \"The County Seats of the Noblemen and Gentlemen of Great Britain and Ireland\" which was published in six volumes of coloured lithographs in 1870; and also in the engravings in Thomas Nicholas' \"Annals and antiquities of the counties and county families of Wales; containing a record of all ranks of the gentry … with many ancient pedigrees and memorials of old and extinct families\" published in two volumes by Longmans in 1872.  These volumes give the impression that both the established gentry and the Nouveau riche bankers and industrialists in Wales needed to justify a legitimacy for building in this style and the expenditure they were lavishing on them. Other early castellated buildings in Wales were Gwrych Castle in Flintshire.  \"One of the most amazing of 19th Century castellated mansions\".  It was designed by C A Busby and Thomas Rickman.  The foundation stone was laid in 1819 and the work was probably finished in 1822.  Also in Flintshire, Gyrn Castle, at Llanasa, an older house was converted into a castellated mansion for the Holywell cotton manufacturer John Douglas between 1817 and 1824.  Nearby Halkyn Castle was designed by John Buckler c. 1827 for the second Earl Grosvenor. At Stanage Park, Knighton.  The house was started in 1807 to designs by John Adey Repton.  The arch to the stable block is dated 1807.  The bay window was added in 1833 and in 1841 the Edward Haycock took over as architect, remodelling the interior after a fire and adding a Neo-Norman porch to the rectangular tower.  He continued expanding the house until 1867.  The work was for the Rogers family who followed Thomas Johnes of Hafod as the owners of Stanage. In the 1830s the Castellated Gothic was developed further by Thomas Hopper, who had been responsible for the severe Romanesque revival Penrhyn Castle and the Shrewsbury architect Edward Haycock, Sr. at Margam Castle in Glamorgan which was built between 1830 and 1840.  This was a more ornate and flamboyant form of Tudor Gothic with a massive central lantern tower, modelled on the 16th-century prospect tower at Melbury House in Dorset.  Newman sees the Hopper and Haycock deriving their designs from James Wyatt's Ashridge of 1808–13 and William Wilkin's Dalmeny House near Edinburgh of 1814–17.  While the exterior is Tudor Gothic, there is a spectacular staircase inside the tower in a late Gothic or Perpendicular style with impressive fan-vaulting At Ruthin the Medieval castle was partly rebuilt in 1826 and then transformed in 1848–1853 by the architect Henry Clutton for Frederick Richard West.  Clutton demolished much of the main block of the earlier house and replaced it with a three storied castellated building in bright red sandstone and placed at the west corner a big octagonal tower. This mixed style is also seen at Llantarnam Abbey in Monmouthshire by Thomas Henry Wyatt.  In Montgomeryshire between 1850–6 Leighton Hall was built by the little known Liverpool architect W. H. Gee, probably to designs by James Kellaway Colling.  The Hall is overshadowed by a gothic tower and the interior was sumptuously furnished by the Craces to designs by Pugin. An important development in the development in Gothic rival architecture in Wales was the building of Plas Rhianfa (recently renamed Chateau Rhianfa) in 1849–50.  Plas Rhianfa in Llandegfan near Beaumaris overlooks the Menai Straights.  The architect was Charles Verelst of Liverpool, who was also known as Charles Reed, but the building was inspired by Lady Sarah Hay Williams of Bodelwyddan Castle, who commissioned the building for her two daughters.  Lady Hay Williams, an artist, had sketched the Chateau of the Loire, and presumably greatly influenced the designs of her architect.  The house is described as a forest of steep French roofs covered in fish tail slates, and a skyline fretted with curved and straight pitches and spires, which are level with the road at the back.  On the seaward front narrow drum towers with conical roofs.  Inside there are highly decorative fireplaces, similar to those in Bodelwyddan Castle, which can be seen as precursors to similar decorative work at Cardiff Castle by William Burges.  Stylistically this is the architecture that Eugène Viollet-le-Duc was pioneering in France and it predates the publication of Henry Clutton's \"Remarks…. . on the domestic architecture of France\" which was published in 1853. An early example of the Queen Anne revival style was Garthmyl Hall, Berriew in Montgomeryshire by J K Colling.  It was completed in 1859 and was a pioneering example of the use of Terracotta ornamentation. A more developed example of Queen Anne revival style can be seen nearby in the Severn Valley at Cefnbryntalch in Llandyssil.  The house of 1867–69 by G F Bodley was completed by Philip Webb.  It was built for was Richard Jones, who had made a fortune in the flannel trade in Newtown.  The exterior is of cleanly detailed and well-executed in red brick, with prominent string-courses.  The south front with three big gables and a balance of irregular chimneys and near-regular windows, has many C18 features – a hipped roof, two bays, sash-windows, and the central Venetian window.  The symmetrical entrance front is rather C17 vernacular, while the west front is picturesque and irregular in contrast, an asymmetrical gable anchored by a shafted chimney; lower tile-hung wing.  In many respects this house is the precursor of the later Arts and Crafts houses in Wales and the close studded upper storeys is a feature of houses such as Bryniago at Rhayader by Stephen W. Williams.  The Queen Anne style was further developed by William Eden Nesfield, a close associate of Norman Shaw at Kinmel Park in Denbighshire.  It was constructed 1872-4 incorporating parts of the earlier houses by Samuel Wyatt and Thomas Hopper.  The house consists of 15 bays on the E. front with end pavilions For a short period at the start of the 16th century, Italian craftsmen introduced the art of highly fired Terracotta moulded brickwork and ornamental plaques into Tudor England.  The use of terracotta was largely limited to Great Houses in Eastern England.  Then in the 1830s and 1840s a number of architects started sourcing terracotta from the brickyards that were associated with coal mines in the West Midlands. From about this time terracotta production with matching bricks started to be produced on a large scale in the Wrexham area in association with mines of the north Wales Coalfield.  This was centred on Ruabon and many companies sprang up.  The last of these to go out of business was J. C Dennis in 2010.  However the most important and best known of these companies was J C Edwards, who about 1867 opened up the Pen-y-bont brickyard.  This brickyard produced a distinctive rich red brick and terracotta ornamentation that can be readily recognised.  This was recognised and used by some of the leading architects of the period, most notably Alfred Waterhouse who used it for the Prudential Insurance Offices in both London and Birmingham.  Examples of Ruabon terracotta can be seen on buildings, particularly banks and public institutions throughout England, but as might be expected, terracotta was particularly popular in Wales.  Most towns having several examples often including banks, shops and sometimes houses. One of the most iconic Terracotta buildings in Wales is the Pierhead building at Cardiff Docks, adjacent to the Welsh Assembly building.  The Grade One listed building was built in 1897 and designed by the English architect William Frame.  It was a replacement for the headquarters of the Bute Dock Company which burnt down in 1892.  Frame's mentor was William Burges, with whom Frame worked on the rebuilding of Cardiff Castle and Castell Coch until Burges's death in 1881. A coat of arms on the building's façade bears the company's motto \"wrth ddŵr a thân\" (by fire and water) encapsulating the elements creating the steam power which transformed Wales. A further impressive building using and orangey terracotta with red bricks, probably from J. C Edwards at Ruabon are the former offices of John Summers and Co at Shotton on Deeside in Flintshire These were to become the offices of the Strip Division of the British Steel Corporation.  In 1986 Edward Hubbard described them as \"unconventional and pompous\", but taste to-day might be more appreciative and they can be seen as late and almost playful take on castellated Gothic revival architecture with some Art Nouveau detailing. In small country towns such as Rhayader in Radnorshire the local architect, Richard Wellings Thomas built both the Kington and Radnor Bank of 1904 and the town's Post Office of 1903 using Ruabon Terracotta.  The Bank has heavy classical mouldings while the Post Office for the upper storeys uses the local stone with terracotta dressings.  Terracotta was a popular material for building Post Offices, as at Denbigh and particularly the Post Office in Great Darkgate in Aberystwyth.  The later was the work of T E Morgan, completed in 1901, and has an attractive mosaic fascia.  In Welshpool the J & M Morris's iron foundry had the \"Agricultural Implement Depot\" built in Church Street for the display of their products.  This was to the designs of the work of the Borough surveyor Robert Hurst around 1904, in deep red Ruabon brickwork with arched display windows with masqued heads used as keystones.  The inscription \"Agricultural Implement Depot\" runs along the parapet of the building. Even more remarkable is use of white Doulton faience glazed terracotta for the Motor Palace at Llandrindod Wells by Richard Wellings Thomas in 1906–10.  Now the National Cycle Museum, it has a curving facade of nine bays of white-faience ware and blocked pilasters dividing the display bays, surmounted with lion finials.  It is an early example of steel framed construction.  The building reflects that Llandridod was the social capital of Wales at the time and Tom Norton, for whom it was built was both an early bus proprietor and also aviator, hence the fascia letting CYCLES – MOTORS- AIRCRAFT. This style of architecture for garages was continued after the Ist World War with Humphrey's Garage in Newtown, Montgomeryshire, still displaying the names of the makes of car that it was selling in the 1930s and Pritchard's Garage in Llandridod Wells, with a curving facade, using similar lion finials to those on Tom Norton's Automobile Palace. The use of patterned or polychrome brickwork, sometimes associated terracotta was popular in the towns in Montgomeryshire and North Eastern Wales in the 1870s and 1880s.  A striking example is the Plas Castell Gatehouse at Denbigh, a Tudoresque machicolated tower with bars of yellow brick contrasting with the red bricks.  The tower was built in 1882.  In Kerry, Montgomeryshire the estate architects J W Poundley and D Walker produced an unusual composition of a terrace of houses built for the Naylors next to the former Kerry workhouse.  The red bricks are punctuated by a double string of white brick and a pattern of white and black bricks below the eaves and for the upper voussoirs.  The use of curved bricks in the voussiors give the impression of an Egyptian pharonic head-dress. Prompted by Queen Victoria's Osbourne House, the Italianate style of architecture became popular in the second half of the 19th century.  Features of this stye include belvedere towers and roofs with a shallow slope and wide eaves.  In WalesR. K Penson was a leading exponent of the style.  Penson had an extensive practice in the south of Wales, particular in church building and restoration, but examples of his use of the Italianate style include the Town Hall at Llandovery and the gate lodge to Nanteos.  The style was popular for country houses in Carmarthenshire and include the now demolished Pant Glas at Llanfynydd and Gellideg at Llandyfaelog.  Pant Glas was built in 1850 and Gellideg in 1852.  The architect for the latter being William Wesley Jenkins.  A later example of the Italianate style is the Parc Howard Museum on the outskirts of Llanelli, originally known as Bryncaerau Castle.  The house, faced in Bath stone was built to designs by J. B. Wilson between 1882–6. J.P. Seddon was a London architect who developed an extensive practice in south Wales.  Initially he worked with John Prichard from 1853 to 1859 and then with John Coates Carter, who had an office in Cardiff, until 1904.  Seddon was surveyor to Llandaff Cathedral and most of his work was church building and parsonages for the Llandaff Diocese.  However he built some notable country houses such as Llanilar at Abermad in Ceredigion in 1870-2 and most notably the Old College Building of Aberystwyth University.  The Old College Building is on the seafront and replaced Castle House which had been built for Uvedale Price by John Nash in 1791-4.  Castle House had been bought by the railway entrepreneur Thomas Savin in 1864 and he employed Seddon to rebuild it as a hotel.  Following Savin's bankruptcy in 1866 it was purchased by the future University and until 1890 Seddon together with his partner John Coates Carter continued to rebuild and extend the building.  The Builder described it as \"one of the most original and characteristic monuments of the Gothic Revival\", while Thomas Lloyd writes \"Seddon's originality lies in his very fluid use of curves and complex geometrical forms, and in the blurring of angles and joints, syncopation that has something of Art Nouveau\".  Seddon mixes detail of Early English with Venetian and French late Romanesque and Gothic Architecture.  The stone used comes from Cefn at Minera, dressed with Bath stone.  He also used an artificial stone and concrete in parts of the building.  John Coates Carter was to go on to design the Paget Concert Rooms at Penarth in 1906 and the remarkable monastery complex on Caldey Island, Pembrokeshire. William Burges’ contribution to Welsh architecture was notable but limited to three buildings, Cardiff Castle, Castell Coch and Park House, all three in Cardiff.  His castles also had little influence on other architecture in Wales, with the possible exception of the Settlement Tower on Lake Vyrnwy with its conical roof.  The reason for this would appear to be that Burges started working in his distinctive style for his patron John Crichton-Stuart, 3rd Marquess of Bute, in 1865 and by this time the Gothic Revival style of architecture was already starting to fall out of fashion.  The influence of Park House was much more significant; John Newman considers the house \"revolutionized Cardiff's domestic architecture\" and the Cadw Grade I listed building status given to the house records it as \"the pattern for much housing in Cardiff in later C19.  Perhaps the most important (nineteenth century) town house in Wales.\"  While Burges’ style was highly creative it is often difficult to pin down the stylistic sources of his designs.  Mordaunt Crook remarks that Burges’‘drew on his extensive travels and the studies he had made of the campanilii of San Gimaggnano, Florence and Siena.  He included recollections of Nuremberg and Palermo, of the Chateau de Chillonon on Lake Geneva, the Castello at Milan and the Palais des Papes at Avignon.  Nearer home he took elements from Conway, Caernarvon and Durham Castles.  The main influence on his work does appear to be French Gothic architecture, particularly as interpreted by Viollet le Duc.  Curiously an earlier Welsh example, of 1840 of this style, at Plas Rhianfa in Anglesey has been noted above, but there is no reason to think that Burges or his patron were influenced by this. The Marquis of Bute first met William Burges in 1865 and this was the start of a momentous partnership that was to last for sixteen years, and Cardiff Castle was to be transformed into a Neo- Gothic dream palace.  Work on the castle started in 1869 with Bute's workmen pulled down the houses built against the South Curtain Wall.  Burges restored the stonework, and he added a covered parapet walk with embrasures and arrow slits.  The Clock Tower was built on the site of a Roman bastion and completed in 1875.  The scheme included the Medieval buildings of the West wing which had been ‘‘gothicised’’ by Henry Holland in 1774. In 1872 Lord Bute married the Hon. Gwendolen FitzAlan Howard.  The couple had four children, and Burges designed a Nursery especially for them.  Work continued with the rebuilding of the Bute Tower and Herbert Towers, as well as the new Guest and Tank Towers.  The 15th-century Octagon Tower was restored with the addition of a timber fleche or spire above the battlements.  Burges created a Library and the Banqueting Hall within the late medieval residential block.  When Burges died in 1881, his work was continued by his former assistant William Frame.  Frame built the Animal Wall and was responsible for restoring the newly discovered Roman remains. Castell Coch, a ruined Medieval castle, lying to north of Cardiff, was intended as an occasional summer residence for the Marquess of Bute. Burges's reported on the proposed reconstruction of Castell Coch in 1872 but construction was delayed until 1875, partly because of the pressure of work at Cardiff Castle.  The exterior comprises three towers, \"almost equal to each other in diameter, but arrestingly dissimilar in height . \"The Keep tower, the Well Tower and the Kitchen Tower incorporate a series of apartments, of which the main sequence, the Castellan's Rooms, lie within the Keep.  The Hall, the Drawing Room, Lord Bute's Bedroom and Lady Bute's bedroom comprise a suite of rooms that exemplify the High Victorian Gothic style in 19th-century Britain.  A superb fireplace by Thomas Nicholls features the Three Fates, spinning, measuring and cutting the thread of life.  The octagonal chamber with its great rib-vault, modelled on one designed by Viollet-Le-Duc at Councy, is \"spangled with butterflies and birds of sunny plume in gilded trellis work.\"  Off the hall, lies the Windlass Room, in which Burges delighted in assembling the fully functioning apparatus for the drawbridge, The Marquess's bedroom provides some \"spartan\" respite before Lady Bute's Bedroom.  The room is \"pure Burges: an arcaded circle, punched through by window embrasures, and topped by a trefoil-sectioned dome.\"  The decorative theme is 'love', symbolised by \"monkeys, pomegranates, nesting birds\".  The decoration was completed long after Burges's death 1881, but he was the guiding spirit; \"Would Mr Burges have done it?\"  William Frame wrote to Thomas Nicholls in 1887. Following Burges' death in 1881, work on the interior continued for another ten years.  The castle was not used much: the Marquess never came after its completion, and the family appeared to use it as a sort of sanatorium, although the Marchioness and her daughter, Lady Margaret Crichton-Stuart, did occupy it for a period following the death of the Marquess in 1900.  But the castle remained \"one of the greatest Victorian triumphs of architectural composition,\" summing up \"to perfection the learned dream world of a great patron and his favourite architect, recreating from a heap of rubble a fairy-tale castle which seems almost to have materialised from the margins of a medieval manuscript.\" Park House was built between 1871 and 1875 for James McConnochie, the dock engineer to Bute Estate.  McConnochie was Mayor of Cardiff in 1880.  The house has been used as a restaurant since 2012.  The house draws on various French Gothic elements and is reminiscent of the Town Hall of St. Antonin, restored by Viollet le Duc in 1843, with late Romanesque and a Gothic arcade, but with added 15th-century dormer windows.  It is built with grey Caerphilly stone and Bath stone dressings; steeply-pitched slate roofs, stone chimneys.  Features of the house were imitated by other late Victorian houses in Cardiff, but it should be noted that similar houses such as Llanilar at Abermad (1870–72) in Ceredigion were being built by John Pollard Seddon. During the latter part of the 18th century and during the 19th century, the laying out of towns, villages and industrial settlements gathered momentum.  It was work often done by architects and landsuryors.  The layout and design of Aberaeron can now be confidently assigned to Edward Haycock.  and he probably was involved in the development of Aberystwyth.  William Jernagen of Swansea ….  at Milford Haven.  At Newtown the development of the Crescent and Pen ygloddfa to the work of Thomas Penson The grid pattern layout at Pembroke Docks has been attributed to the land surveyor George Gwyther, while the Royal Dockyard and its buildings were probably to the original design of John Rennie and carried out by Edward Holl, architect to the Navy Board.  In the 19th many estate villages were laid out by large landowners, often by the architects to build or rebuild their own houses.  Intriguingly Lord Sudeley at Gregynog was to experiment in 1870 with concrete houses for his estate workers The following is a selection of some of the Industrial and Estate village built in this period. In many areas of Wales extensive areas of workers housing appeared in the 19th century, The rows of terraced housing for coal miner's stretching along the contours of the south Wales valleys are well known.  In the areas of the Steel and Tinplate industries similar housing exist and Ironworkers cottages at Rhyd-y-Car in Merthyr Tydfil have been rebuilt at St Fagan's Folk Museum.  while slate and other quarrying settlements in north Wales were often located in remote and isolated places such as Cwm [Penmachno] or Nant Gwrtheyrn.  At Nant Gwertheyn, now a Welsh Language learning centre, is situated in a steep ravine and the granite was shipped out by sea.  It was originally laid out c. 1878 for the granite quarry workers.  There are two terraces of cottages, a Quarry manager's house and a chapel round a green.  The quarry closed in 1914 and the last inhabitant left in 1959.  For some skilled workers very much better housing was provided.  Railway workers at Railway Terrace in Ruthin were provided with rather superior accommodation by the long closed Vale of Clwyd Railway in 1864 The work of John Douglas the Chester architect, extended into Wales.  Plas Fynnon, Nercwys, built as the vicarage to St. Mary's Parish Church in Tudorbethan style has been attributed to him.  Built of brown brick with red brick and sandstone detailing under a steeply-pitched tiled roof with over sailing eaves and plain ridge.  Asymmetrical facade with advanced, 2-storey gabled porch with moulded purlin-ends, brackets and plain finial.  Tudor-arched entrance of tooled ashlar, stopped and moulded and with date 1877 carved in the spandrels. The style was used by the Shrewsbury architect James Pickhard for building Fronfraith Hall in Llandyssil in Montgomeryshire in 1863. A more important example of this style is the Neo-Tudor extensions to The Hendre in Monmouthshire, the seat of the Rolls family.  The original hunting lodge was constructed in a Neo-Norman style by an unknown architect in the 1820s.  This was extended by T. H. Wyatt between 1837 and 1841.  Then, from 1870 to the mid-1880s, Wyatt and his clerk of works, Henry Pope added a great Hall, an entrance court and a massive dining-room wing in Neo-Tudor style.  Finally in 1895-6 one of the leading architects of the period Aston Webb added the Arts and Crafts Neo-Tudor Library Wing.  This created a house with a corridor from the front door to the library of no less than 75 metres.  The interior was furnished with much genuine Tudor and Jacobean woodwork, which had been collected from local houses. Arts and Crafts architecture can be seen as an extension of the Tudorbethan Style in Wales.  It is seen as starting c. 1887 under the influence of William Morris and was introduced into Wales by architects such as William Eden Nesfield who was responsible for the rebuilding of Kinmel Hall and the designer W.A.S Benson who was the architect for Clochfaen at Llangurig in Montgomeryshire.  These architects very much favoured the use of half-timbered decoration, red brickworks, roof tiles and tile hanging on walls.  A notable architect in this tradition was Frank Shayler who had set up offices in Oswestry and Shrewsbury and developed an extensive practice particularly in Montgomeryshire.  Shayler, together with other architects in his practice were patronised by Lord Davies of Llandinam and were responsible for a series of Institute buildings as well as restoring a number of half timbered buildings such as the Mermaid in Welshpool and Glyndŵr's Parliament House in Machynlleth.  In Radnorshire the architect Stephen W. Williams also worked in this style and built the Offices in Rhayader for the supervision of the Elan Valley Reservoir project.  A good example of this style providing domestic housing is the Lodge at Chepstow, built between 1902 and 1908 by an unknown architect.  Newman describes this as \"a witty, if rather belated essay in Norman Shaw style\" with stone, tile hanging and half-timbered gables. The Arts and Crafts movement progressed in Wales very much under the influence of C F Voysey, and Edwin Lutyens, who were throwing off the influence of both the Gothic Revival and the half-timbered Tudor revival styls which had been so prevalent in Wales.  Voysey had worked in partnership with J.P. Seddon with offices in Cardiff, but, as yet no examples of his work have been recognised.  Then in 1903 – 6 he comes back to Wales to design the little known Ty Bronna on St Fagans Road, Cardiff.  This is a minor masterpiece with its clean white outline, faced in stone, gabled at each end with a hipped roof and the angled battered buttresses from ground level to the eaves.  It has a bowed east window with a recessed veranda and was restored in 2002.  Pevsner sees buildings such as this by Voysey as being a precursor of Modernist architecture. Architecture of this style was produced by Herbert Luck North in north Wales and on occasions by Clough Williams-Ellis in his designs for council smallholdings adapted by Montgomeryshire County Council.  This style was developed by the Garden City movement and was widely used on Welsh Garden villages and housing schemes until after the second World War. At Harlech the architect George Walton, a Glasgow architect, better known for his Art Nouveau architecture, was to design Wern Fawr in 1908 and also the St Davids Hotel Harlech (1907–11), but burnt down in 1922 \"see also\":Architecture of Cardiff An architect who made a notable contribution to the public and commercial architecture of Cardiff was Edwin Seward.  In 1875, he became part of the James, Seward and Thomas Partnership.  In 1880 Seward won a competition for the design of the Cardiff Free Library, which consisted of a Library, Museum and Schools for Science and Art.  The first phase was completed in 1882, but it was not finally completed until 1896.  . In 1881 Seward enlarged the Cardiff Union Workhouse with a new entrance building on the Cowbridge Road frontage with a 3-storey tower and clock face, still in a late Gothic revival style.  This building was to become the St David's Hospital.  This was also the style Seward adopted for the Cardiff Royal Infirmary of 1883 Seward’s next building, the Cardiff Coal Exchange in Butetown was built between 1883 and 1888 and it is moving more towards a Baroque revival style, although Newman calls it a \"debased French Renaissance style\".  In 1894 Seward produced his \"Dream of the Future\" for Cardiff, which appeared in the Western Mail in February 1894 and also plans for Cardiff Museum.  This, however, was overtaken by the development of Cathays Park starting in 1905, for which he did not get a commission.  In 1895 he designed the Morgan Arcade in Cardiff and the following year the Turner Gallery at Penarth.  Finally in 1902-3 he was responsible for the monumental Swansea Docks Trust Office now Morgans Hotel, Swansea. Baroque Revival architecture is variously described as Neo-Baroque and Edwardian Baroque, and is paralleled in France by Beaux-Arts architecture.  The style is also called \"Wrenaissance\", acknowledging a debt to Sir Christopher Wren.  In Wales the style starts appearing in the 1890s and was used for major public architecture, the newly founded universities and commercial buildings.  It reflected the considerable wealth generated in this period, particularly from coal mining and also the growth of Welsh National Identity.  The first buildings in the newly planned Cathays Park in Cardiff, described as \"the finest civic centre in the British Isles\" were the Cardiff Town Hall, later City Hall and the Law Courts, based on plans drawn up in 1897 and built between 1901 and 1905 to designs by Lanchester, Stewart and Rickards.  Newman sums up the buildings as \"swaggering Baroque . .  setting a new standard setting a new standard for the emergence of the Edwardian grand style for public buildings in Great Britain.  No Victorian architect had hitherto demonstrated such mastery of Continental Baroque, in this case the Baroque of South Germany and Austria, combined with the Neo-Baroque of Charles Garniers' Paris Opera.\"  The setting is given opulence by the use of Portland Stone for the facades.  The National Museum of Wales was added to this grouping in the modified American Neo-Barogue or Beaux-Arts style by the London architects Smith and Brewer and later extended by the Welsh architects T. Alwyn Lloyd and Alex Gordon. The Baroque Revival style was also used for a range of other public buildings, banks and schools and universities.  A refined example of this style was used by Alfred Cross for the Edward Davies Building at Aberystwyth University, was the first purpose-built chemical laboratory in a British university.  It was opened in 1907 by Lord Asquith and remained a functioning Chemistry Department until 1988.  It now serves as the School of Art Building. F Inigo Thomas also remodelled Ffynone House at Newchapel in Pembrokeshire in a neo-Baroque in 1902-7 with massive rusticated quoining added to the facade.  The house had originally been built by John Nash in 1792-7. One of the earliest examples in Wales of the Baroque revival or \"Wrenaissance\" style to appear in Wales is the Barry Dock Offices built for David Davies as the offices for the Barry Docks & Railway Company, and was part of the scheme for the development of Barry Docks.  It was constructed between 1897 and 1900 The architect was Arthur E. Bell.  A very similar building, which appears to be copying the Barry Offices on a lesser scale to this is the \"Stiwt\" or Rhosllannerchrugog Miners' Institute, close to Wrexham, which was built much later, between 1924 and 1926 by the local architects John Owen and F A Roberts.  In Barry the Docks Office was followed in 1903-8 by the Town Hall which was built by the architects Charles E Hutchinson and E Harding Payne in red brick and lavish Bath Stone adjoined by a severn bay public library with the centre three bays defined by giant Ionic pilasters.  Equally ambitious but on a smaller scale is the red brick and limestone Town Hall by F A Roberts at Mold in Flintshire. Baroque revival was also a favoured style for bank architecture.  An example is the former North and South Wales Bank, now HSBC in Aberystwyth.  This was by Woodfalland Eccles of Liverpool and was built in 1908-9.  Three bay frontage, with a recessed centre framed columns and topped by a brocken curved pediment. Hilling, writing in 1976, remarks that in Wales \"the interwar period is almost devoid of significantly progressive buildings and the abstract Neo-classicism of those public building that were erected had more in common with the architecture of Albert Speer and the Nazi and Fascist architecture\".  The Blackwood Miner's Institute built in 1925, for the Oakdale colliery in Monmouthshire shows the transition of Baroque revival architecture to the Art Deco style. The leading Welsh architect of the inter-War years was Sir Percy Thomas.  After returning from the War After the War he returned to Cardiff.  He was commissioned by David Davies, 1st Baron Davies of Llandinam, to design the Temple of Peace in Cathays Park.  He established himself as a leading designer of Civic and University buildings.  His work included Swansea Guildhall, which was built between 1930 and 1934, and includes the City Hall and the Brangwyn Hall and work on the campus at Aberystwyth University The Burton menswear store in Abergavenny is a notable example of Art Deco.  Built in 1937, it is a Grade II* listed building. In Ammanford the impressive classical Miner's Welfare Hall, now the Miner's Theatre was built to the designs of J.O. Parry, around 1935.  Classical front in brick with giant Ionic columns is mixed with modernist fenestration and detailing Examples of Art Deco buildings in Wales are limited largely to Cinemas and houses.  Possibly the best example a cinema is the recently closed Pola Cinema in Berriew Street, Welshpool, with it attractive curved frontage and good stained glass, which was completed in 1938.  An important house in the International Modernist style is the Villa Marina, set on the seafront at Llandudno.  This striking building was designed by Harry Weedon in 1936 well known as a cinema architect.  It has been recently been restored. A particularly striking example of Art Deco architecture is Penarth Pier.  The original cast iron pier was designed by H. F. Edwards in 1892-4.  In 1927-8 a Pier Pavilion was built in Ferro concrete to designs by L.G. Mouchel and Partners.  Mouchel was founded in Briton Ferry now in Neath Port Talbot in 1897 by Louis Gustave Mouchel, who arrived in the UK from France with a licence to use the new technique of reinforcing concrete using iron bars that had been developed by François Hennebique.  was a pioneer in the use of re-enforced concrete, although the pavilion was built after Mouchel's death.  The pavilion has topee shaped dome lets and a semicircular Tuscan colonnade. An even more striking example of Mouchel's use of Ferro-Concrete is the White Bridge at Pontypridd.  This was built in 1907, to designs by P R A Willoughby, surveyor to Pontypridd Urban District Council, in association with L G Mouchel & Partners.  The contractor was Watkin Williams & Page.  Its river span, of 35metres, was when built, the longest reinforced concrete arch in Britain. Clough Williams-Ellis is primarily remembered as the creator of Portmeirion.  While at first he established himself as a London-based architect he was to establish himself as major figure in the development of Welsh architecture in the first half of the 20th century, working in a variety of styles and designing buildings ranging from Country houses to workers housing.  One of his earliest designs of 1905 was for a pair of Welsh labourers cottages in a vernacular style with end gable chimneys which imitate the 16th-century \"Snowdonia Houses\" In 1909 he was to design a house in an advanced Arts and Crafts style for Cyril Joynson at Brecfa in Breconshire In 1913–14 he was to be resonsilble for the rebuilding of Llangoed Hall in Breconshire, one of the very last country houses to be built before the First World War.  While it is a mixture of a number of historic styles it was a modern features with elements such as the chimneys derived from the work of Lutyens Other work in Wales by Clough Williams-Ellis includes the Festiniog Memorial Hospital of 1922, Pentrefelin Village Hall, the Conway Fall Cafe.  At Aberdaron he designed the Old Post Office in a vernacular style in 1950.  An important later commission was the redesign and rebuilding of Nantclwyd Hall in Denbighshire Clough Williams- Ellis was equally capable in working in the Modernist idiom of the interwar years.  This is well demonstrated by the recently restored Caffi Moranedd at Cricieth and the now demolished Snowdon Summit Station of 1934, which was demolished in 2007. However, his more memorable creation in Wales is the capriccio town of Portmeirion on the coast of the Llyn near to Portmadoc.  This is notable not only as an architectural composition, but also because Clough Williams- Ellis was able to preserve fragments from other now demolished buildings from Wales and Cheshire.  These include the plaster ceiling from Emral Hall In the years following the 2nd World War resources mainly went on the provision of housing.  During these years of austerity some public buildings were constructed including the village hall or Neuadd Tysul at Llandysul in Ceredigion of 1955.  This was the work of John Davies the county surveyor.  The concrete frontage has been enlivened by the crow stepped gables and the attractive Festival of Britain lettering. During the 1960s local Government started to commission some notable buildings.  Foremost amongst these is the Wrexham Swimming baths of 1965–7 by F.D. Williamson associates of Bridgend.  The baths have a giant parabolic roof covers three swimming pool with the glassed end with the diving boards rising to four stories.  These architects were also responsible for the Sport Wales National Centre of 1971 in Sophia Gardens, Cardiff.  In Brecon the County Library of 1969 by J.A. McRobbie, is a well designed Brutalist building in Ship Street, but its position is hemmed in and led to destruction of other older buildings in the street In the Post War Period many major building projects started to be awarded to Welsh architectural firms.  Leading firms were \"Percy Thomas Partnership\" and \"Alex Gordon and Partners\" in the south, and \"Colwyn Foulkes Partnership\" and \"Bowen Dann Davies\" of Colwyn Bay in the north.  The Percy Thomas Partnership lost its identity when it was forced into liquidation in 2004.  It has since become part of Capita Symonds The first true skyscraper in Wales was the Capital Tower in Cardiff.  It was completed in 1969–70 and providing 190,000 sq ft (18,000 m) of floor space over 25 storeys.  It was originally known as Pearl House and was designed by the London firm Sir John Burnet and Partners which became Burnet Tait & Lorne. The 1974 Re-organisation of Local Government in Wales led to a rash of vastly ambitious building programme.  This mainly centred on the building of new headquarters for the County Councils to assert their identity and the building of Leisure and Arts centres.  The subsequent further reform of Local Government, particularly in 1996, has made some of these developments look unnecessary and superfluous. A notable project at the end of the 20th century was the creation of the National Botanic Garden of Wales.  The most striking feature of this was the Great Glasshouse.  Designed 1995-6 by Foster and Partners and built 1997-9.  This is the largest single span glasshouse in the world 110 metres long and 60 metres wide.  The roof an \"elliptical torus\" is carried on twenty-four elliptical arches and covers 3,500 square metres, and provides Wales with a building of international note. Ushering in the 21st-century architecture in Wales was Jan Kaplicky's of Future Systems \"Malator\" at Nolton in Pembrokeshire.  The site overlooks St Bride's bay and is within the Pembrokeshire Coast National Park.  The house was built in 1998 and is a notable example of Eco architecture.  It is excavated into the sloping ground and is turf roofed.  The house appears as a low hillock with only a metal flue rising from the grassThe seaward elevation is entirely of glass.  Steel framed construction with a ring beam that supports the roof. At Bridge End the Bus Station by Gillespies of 2004 with its cylindrical tower and clock face, reminiscent of the architecture of the 1950s and 60's, would best be described as Retro style. Currently the tallest building in Wales is The Tower, Meridian Quay at Swansea, which is 107 meters high and completed in 2010. The tower has 29 storeys, double the number of the previous tallest building in Swansea, the BT Tower.  Most of The Tower houses residential apartments.  The design was by Latitude Architects and elliptical shape of the building is reminiscent of the work of the Austrian architect Heinz Tesar.  The only other high rise buildings in Wales are in Cardiff. The tallest residential block in Cardiff, completed in 2005 is Altolusso by Holder Mathias architects with Ove Arup & Partners as the constructional engineer.  It is adjacent to the Meridian Gate, Cardiff which is a residential hotel completed in 2008. The most striking building of 21st-century Wales is the Millennium centre on Cardiff Bay.  The Centre was designed by Jonathan Adams, of local practice Percy Thomas Architects Wales Millennium Centre (Welsh: \"Canolfan Mileniwm Cymru\" ) is an arts centre located in the Cardiff Bay area of Cardiff, Wales.  The site covers a total area of 4.7 acre .  Phase 1 of the building was opened during the weekend of the 26–28 November 2004 and phase 2 opened on 22 January 2009 with an inaugural concert.  The centre has hosted performances of opera, ballet, dance, comedy and musicals. The Centre comprises one large theatre and two smaller halls with shops, bars and restaurants.  It houses the national orchestra and opera, dance, theatre and literature companies, a total of eight arts organisations in residence.  It is also home to the Cardiff Bay Visitor Centre. The main theatre, the Donald Gordon Theatre, has 1,897 seats, the BBC Hoddinott Hall 350 and the Weston Studio Theatre 250. The Senedd houses the debating chamber and committee rooms of the Welsh Assembly.  It was completed in 2006. The building faces south west over Cardiff Bay, it has a glass façade around the entire building and is dominated by a steel roof and wood ceiling.  It has three floors, the first and second floors are accessible is to the public and the ground floor is a private area for officials.  The building was designed to be as open and accessible as possible, the architects, the Richard Rogers Partnership (RRP) said \"The building was not to be an insular, closed edifice.  Rather it would be a transparent envelope, looking outwards to Cardiff Bay and beyond, making visible the inner workings of the Assembly and encouraging public participation in the democratic process.\"  The main area in the building is the debating chamber, called the Siambr, including a public viewing gallery.  Other areas of the building are the Neuadd, which is the main reception area on the first floor and the Oriel on the second floor.  The three committee rooms and the Cwrt are on the ground floor.\n\nFlint water crisis The Flint water crisis began in 2014 when the drinking water source for the city of Flint, Michigan was changed to the Flint River.  Due to insufficient water treatment, over 100,000 residents were potentially exposed to high levels of lead in the drinking water.  After a pair of scientific studies proved lead contamination was present in the water supply, a federal state of emergency was declared in January 2016 and Flint residents were instructed to use only bottled or filtered water for drinking, cooking, cleaning, and bathing.  As of early 2017, the water quality had returned to acceptable levels; however, residents were instructed to continue to use bottled or filtered water until all the lead pipes have been replaced, which is expected to be completed no sooner than 2020. The Flint drinking water contamination issue started in April 2014 when Flint changed its water source from treated Detroit Water and Sewerage Department water (sourced from Lake Huron and the Detroit River) to the Flint River.  Officials failed to apply corrosion inhibitors to the water.  As a result, there was a series of problems that culminated with lead contamination, creating a serious public health danger.  The Flint River water that was treated improperly caused lead from aging pipes to leach into the water supply, leading to extremely elevated levels of the heavy metal neurotoxin.  In Flint, between 6,000 and 12,000 children have been exposed to drinking water with high levels of lead and they may experience a range of serious health problems.  Due to the change in water source, the percentage of Flint children with elevated blood-lead levels may have risen from about 2.5% in 2013 to as much as 5% in 2015.  The water change is also a possible cause of an outbreak of Legionnaires' disease in the county that has killed 10 people and affected another 77. Several lawsuits have been filed against government officials on the issue, and several investigations have been opened.  On January 5, 2016, the city was declared to be in a state of emergency by the Governor of Michigan, Rick Snyder, before President Barack Obama declared it to be in a federal state of emergency, authorizing additional help from the Federal Emergency Management Agency and the Department of Homeland Security less than two weeks later. Four government officials—one from the city of Flint, two from the Michigan Department of Environmental Quality (MDEQ), and one from the Environmental Protection Agency—resigned over the mishandling of the crisis, and one additional MDEQ staff member was fired.  There have also been thirteen criminal cases filed against local and state officials in regards to the crisis. Snyder issued an apology to the citizens and promised to fix the problem, and then sent $28 million to Flint for supplies, medical care, and infrastructure upgrades, and later budgeted an additional $30 million to Flint that will give water bill credits of 65% for residents and 20% for businesses.  Another $165 million for lead pipe replacements and water bill reimbursements was approved by Snyder on June 29, 2016.  A $170 million stopgap spending bill for repairing and upgrading the city of Flint's water system and helping with healthcare costs was approved by the U.S. House of Representatives on December 8, 2016.  The Senate approved it the next day.  $100 million of the bill is for infrastructure repairs, $50 million for healthcare costs, and $20 million to pay back loans related to the crisis.  On January 6, 2017, Snyder signed a bill that accelerates the public notice requirement for lead in drinking water to three business days, from the previous time of 30 days. On January 24, 2017 the MDEQ told Flint Mayor Karen Weaver that the lead content of Flint water had fallen below the federal limit.  The 90th percentile of lead concentrations in Flint was 12 parts per billion from July 2016 through December 2016—below the \"action level\" of 15 ppb.  It was 20 ppb in the prior six-month period.  On the next day, Flint Spokeswoman Kristin Moore said that anywhere from 18,000 to 28,000 homes in the city still needed service lines replaced, and that the city was planning to complete 6,000 homes per year through 2019. On March 7, 2017, it was reported Flint water sampled by the state in February registered below the federal threshold for lead with 90 percent of samples at or below eight parts per billion, the Michigan Department of Environmental Quality says.  The MDEQ said February's water tests mark the seventh straight month in which city water was below the 15 ppb level enforced by the U.S. Environmental Protection Agency.  February's testing also showed 95.8 percent of samples taken at homes at risk of high lead levels were at or below 15 ppb. The following is a sequence of events of the Flint water crisis. Some water service lines in Flint were installed between 1901 and 1920.  As with many other municipalities at the time, all of the service lines from the cast iron water mains to end users' homes were constructed of lead, because it was relatively inexpensive and easy to work.  Lead pipes can leach lead into the water, especially if certain contaminants are present.  However, the water from the Detroit Water and Sewerage Department, where Flint had obtained its water since 1967, had been treated well enough that the leaching from the lead pipes was at levels considered acceptable by state and federal environmental protection agencies.  There are an estimated 43,000 service lines in the city; these include 3,500 lead lines, 9,000 known galvanized lines, and 9,000 unknown service lines. Lead exposure across the U.S. has fallen dramatically since the 1980s, but no blood-lead level is considered completely safe.  Children under age five, and especially infants and unborn children, bear the greatest risk of deleterious and irreversible health outcomes.  From 2012 to 2016, the CDC set a \"reference level\" of 5 micrograms per deciliter (µg/dL), in order to target for case management the 2.5% of young American children with the highest blood-lead levels.  At 45 µg/dL, chelation therapy is considered.  Among the many ways lead can enter a modern American's bloodstream is through lead plumbing.  Acidic water makes it easier for the lead found in pipes, leaded solder, and brass faucets to dissolve and to enter a home's drinking water.  Therefore, public water treatment systems are legally required to use control measures to make water less acidic.  Plumbing that contains lead is often found in buildings constructed in the 1980s and earlier. From 2011 to 2015, Governor Snyder appointed four emergency managers to control Flint’s finances.  After 2015, the city continued to receive financial guidance under the lesser oversight of a Receivership Transition Advisory Board. In 2011, Genesee County initiated the switch to the Karegnondi Water Authority (KWA); the KWA would supply water to both Genesee county and Flint.  On March 25, 2013, the purchase of 16 million gallons per day from the KWA was approved by the Flint City Council.  The KWA informed the council that they could dig Lake Huron (the new water supply) in 30 months using a bored tunnel.  Ed Kurtz, Flint’s emergency manager, along with Mayor Dayne Walling, approved the action and awaited the State Treasurer’s approval. Following this decision, the Detroit Water and Sewerage Department (DWSD) negotiated with Flint officials by offering to restructure water payments.  Flint refused, insisting that KWA was the best water supplier.  DWSD argued that Flint could not spend more money on a new water system and that Lake Huron’s system was more efficient. On April 1, 2013, the Detroit Water and Sewerage Department demanded that the state deny Flint's request, as it would start a water war, which would essentially hurt DWSD.  This press release also provided an option for Flint: the sale of raw, untreated water.  Drain Commissioner Wright of Genesee County, accused the DWSD of media negotiation and then replied, \"It would be unprecedented for the state to force one community to enter into an agreement with another, simply to artificially help one community at the other's expense...this is exactly what the [Detroit Water and Sewerage Department] is arguing...\" On April 15, 2013, State Treasurer, Andy Dillon, approved the water purchase contract with the KWA.  Emergency Manager Kurtz signed the KWA water purchase agreement the following day.  On April 17, the Detroit Water and Sewer Department delivered its one-year termination notice after Flint rejected their last offer.  The DWSD expected that Flint would reimburse the investments for the water system that benefited regional customers.  Flint and Genesee County rejected such responsibility, but indicated their willingness to purchase pipelines. In April 2014, to save about $5 million in under two years, Flint started treating water from the Flint River instead of purchasing Lake Huron water from Detroit.  Previously, the Flint River.  was the backup water source.  In June, 2014, Flint’s emergency manager, Darnell Earley, finalized the sale of a nine-mile section of water pipeline to Genesee County for $3.9 million.  This pipeline fed Detroit water into the county, and after the Huron pipeline was active, would service the eastern part of the county, as well.  By December, 2014, the city had already invested $4 million into its water plant.  On July 1, 2014, Flint emergency manager, Darnell Earley, gave operational authority to Mayor Dayne Walling over two city departments, including Public Works. After the permanent switch to the Flint River, city residents began complaining about the color, taste, and odor of their water.  In August and September 2014, city officials detected levels of coliform bacteria, so residents were advised to boil their water.  The Michigan Department of Environmental Quality determined that cold weather, aging pipes, and a population decline were the cause of this bacteria.  According to Stephen Busch, a DEQ district supervisor, the city took appropriate measures to limit a reccurrence.  General Motors (GM) made the first complaint about the corrosivity of the water.  GM stopped using Flint water in October 2014, after reporting that the water was corroding car parts.  General Motors requested to switch back to the Detroit Water source—a decision that was later approved by city officials. Prior to August 2014, additional chlorine had been added to eliminate bacteria from the Flint River.  However, this is likely the cause of a spike in THMs, an unsafe chlorine byproduct, in one of eight water locations.  Long term exposure of this chemical has been linked to cancer and other diseases.  Following this test, the DEQ placed Flint on violation notice, but did not reveal this to residents until January 2015. The employees of the Flint Public Library declared the water undrinkable after noticing that it was discolored, despite the city’s claim that the water was safe.  Since 2014, the library has provided safe water for the public alongside the state's most prominent bottled water provider. January and February 2015 tests showed that the city water met all health and safety standards.  Nevertheless, the Detroit water system offered to reconnect Flint, waiving a $4 million connection fee, but was declined by emergency manager Jerry Ambrose.  DEQ officials indicated that there is no \"imminent threat to public health,” as the nature of the issue was \"communicated poorly.\" In March 2015, Flint voted to switch back to the Detroit Water and Sewerage Department.  This vote was motivated by residential complaints and recommendations from Veolia North America to prevent the city from further violating the Safe Drinking Water Act.  Jerry Ambrose, Flint’s emergency manager and financial supervisor, disagreed with the reintroduction of the Detroit water source.  Ambrose argued, \"Flint water today is safe by all Environmental Protection Agency and Michigan Department of Environmental Quality standards, and the city is working daily to improve its quality.\" On March 2, 2016, Michigan declared that returning to the Detroit water system must be approved by the State.  When approved, the city was granted an emergency loan of $7 million. In August 2015, local organizations observed that high concentrations of chloride caused the water to be orange and that the water contained high levels of lead.  The lead levels were caused by the omission of orthophosphate; this caused excessive corrosion of the iron pipes.  Consequently, the three organizations,  \"...delivered more than 26,000 online petition signatures to Mayor Dayne Walling, demanding the city end its use of the Flint River and reconnect to the Detroit water system.\"  Fortunately, Flint’s water supply was switched back to Detroit water systems in October 2015.  Subsequently, Flint started adding additional orthophosphate to the water to rebuild the pipe lining. On October 8, 2015, Snyder requested that Michigan Legislators contribute $6 million of the $12 million for Flint to return to Lake Huron water.  The City of Flint would pay $2 million, and the Flint-based Charles Stewart Mott Foundation would pay $4 million.  Jim Ananich, the State Senator representing Flint, demanded that the state refund the $2 million to the city.  Ananich also requested further emergency funding from the state and long-term funding to address the effects of the lead contamination. On September 27, 2016, Flint officials announced that the city will continue to use Detroit water until a new stretch of pipeline is constructed and the Flint River is tested and treated by the KWA. On December 9, 2016 the MDEQ reported that more than 96 percent of water samples in Flint residencies were now below the EPA lead threshold of 15 parts per billion. On March 15, 2017, the Genesee County Water and Waste Services Advisory Board voted to construct a new pipeline; it would be a 7-mile, 42-inch connector to the KWA pipeline.  The pipeline would allow the treatment of raw Lake Huron water, so the city of Flint can continue to buy pre-treated water from the Great Lakes Water Authority.  The $12 million project will allow Flint to remain a customer of the GLWA until at least 2019. In January 2015, a public meeting was held, where citizens complained about the \"bad water.\"  Residents complained about the taste, smell, and appearance of the water for 18 months before a Flint physician found highly elevated blood lead levels in the children of Flint.  During that time period, the Michigan Department of Environmental Quality had insisted the water was safe to drink.  A study by Virginia Tech researchers (see section below) determined that the river water, which, due to higher chloride concentration, is more corrosive than the lake water, was leaching lead from aging pipes.  Dr. Mozhgan Savabieasfahani, an environmental toxicologist based in Ann Arbor, Michigan said this level of lead exposure is comparable with what the Iraqi people have experienced since the U.S. occupation in 2003. While the local outcry about Flint water quality was growing in early 2015, Flint water officials filed papers with state regulators purporting to show that \"tests at Flint's water treatment plant had detected no lead and testing in homes had registered lead at acceptable levels.\"  The documents falsely claimed that the city had tested tap water from homes with lead service lines, and therefore the highest lead-poisoning risks; in reality, the city does not know the locations of lead service lines, which city officials acknowledged in November 2015 after the \"Flint Journal\"/MLive published an article revealing the practice, using documents obtained under the Michigan Freedom of Information Act.  The \"Journal\"/MLive reported that the city had \"disregarded federal rules requiring it to seek out homes with lead plumbing for testing, potentially leading the city and state to underestimate for months the extent of toxic lead leaching into Flint's tap water.\" In a new report released March 1, 2016, 37 of the 423 recently tested sentinel sites had results above the 15 ppb limit.  Eight of the samples exceeded 100 ppb.  A recent study however showed that significantly more samples exceeded the 15 ppb limit in the voluntary or homeowner-driven sampling program whereby concerned citizens decided to acquire a testing kit and conduct sampling on their own (non-sentinel sites). On September 24, 2015, Hurley Medical Center in Flint released a study, led by Dr. Mona Hanna-Attisha, the program director for pediatric residency at Hurley Children's Hospital, confirming that the proportion of infants and children with elevated levels of lead in their blood had nearly doubled since the city switched from the Detroit water system to using the Flint River as its water source.  Using hospital records, Hanna-Attisha found that a steep rise in blood-lead levels corresponded to the city's switch in water sources.  The study was initially dismissed by Michigan Department of Environmental Quality (DEQ) spokesman Brad Wurfel, who repeated a familiar refrain: \"Repeated testing indicated the water tested within acceptable levels.\"  Later, Wurfel apologized to Hanna-Attisha.  The team's study appears in the February 2016 issue of \"American Journal of Public Health\". Hanna-Attisha's research found that the average proportion of Flint children with elevated blood-lead levels (above five micrograms per deciliter, or 5 × 10 grams per 100 milliliters of blood) rose from 2.4% (2013, before the change in water source) to 4.9% (2015, after the change in water source), and in some hotspot areas rose from 4% to 10.6%.  Michigan Childhood Lead Poisoning Prevention Program data agree an increase occurred, suggesting an increase from 2.2% of children (May 2013 – April 2014) to 3.0% (May 2014 – April 2015).  Hanna-Attisha's data were taken from hospital laboratory records for children less than five years old.  Hanna-Attisha's sample numbers were large, both for the pre-switch and post-switch time periods and for Flint children (1,473) and for children not exposed to Flint water (2,202).  Elevated lead levels in children's blood was shown to be correlated with elevated lead levels in Flint water.  Because lead screening is not completed for all children, such data may be skewed toward higher-risk children and thus overestimate lead exposure, especially in non–high-risk areas. Hanna-Attisha and Flint resident LeeAnne Walters were awarded PEN America's Freedom of Expression Courage Award on May 16, 2016. In September, 2015 a team from Virginia Tech arrived in Flint.  Led by professor Dr. Marc Edwards, an expert on municipal water quality, the team came to perform lead level testing on the Flint water supply, working under a National Science Foundation grant.  Edwards had been contacted by Flint resident, Lee-Anne Walters, whose family suffered from extreme health problems, almost immediately following the switch to the Flint River water.  Walters had attempted to act locally, but she was repeatedly ignored by city, state, and EPA officials.  The study found that Flint water was \"very corrosive\" and \"causing lead contamination in homes\".  It concluded in its report that \"Flint River water leaches more lead from plumbing than does Detroit water.  This is creating a public health threat in some Flint homes that have lead pipe or lead solder.\" Edwards was shocked by the extent of the contamination, but even more so by the inaction of the proper authorities after being made well aware of the contamination.  Edwards and his team found that at least a quarter of Flint households had levels of lead above the federal level of 15 parts per billion (ppb) and that in some homes, lead levels were at 13,200 ppb.  Edwards said, \"It was the injustice of it all and that the very agencies that are paid to protect these residents from lead in water, knew or should've known after June at the very very latest of this year, that federal law was not being followed in Flint, and that these children and residents were not being protected.  And the extent to which they went to cover this up exposes a new level of arrogance and uncaring that I have never encountered.\" Edwards' team created a website, called \"\"Flint Water Study\"\", with the main purpose of informing, and creating support for Flint residents during the crisis.  The site also summarized study results and became a comprehensive public database for all information related to the study. On January 11, 2016, the Virginia Tech research team led by Edwards announced that it had completed its work.  Edwards said, \"We now feel that Flint's kids are finally on their way to being protected and decisive actions are under way to ameliorate the harm that was done.\"  Edwards credited the Michigan ACLU and the group Water You Fighting For with doing the \"critical work of collecting and coordinating\" many water samples analyzed by the Virginia Tech team.  Although the labor of the team (composed of scientists, investigators, graduate students, and undergraduates) was free, the investigation still spent more than $180,000 for such expenses as water testing and payment of Michigan Freedom of Information Act costs.  A GoFundMe campaign has raised over $116,000 of the $150,000 needed for the team to recover its costs. On January 27, the city of Flint retained Dr. Edwards to monitor the city's water testing efforts. On March 1, 2016, the Virginia Tech team was given $80,000 from an EPA grant to re-test the lead levels in 271 Flint homes. On August 11, 2016, Kelsey Pieper, a member of Dr. Edwards' research team, said 45 percent of residents that collected samples in July for the lead testing program had no detectable level of particulate lead in their water supply.  She added the study yielded a lead reading of 13.9 ppb, just below the federal action level of 15 ppb.  However, Pieper acknowledged the sampling, which was conducted by volunteer residents, does not fulfill the testing requirements of the federal Lead and Copper Rule.  State testing of the most-recent six month monitoring period, which began January 1 and complied with Lead and Copper Rule regulations, showed a 90th percentile lead reading of 20 ppb, which exceeds the federal action level.  Roughly 93 percent of samples from the third round of expanded state sentinel site testing showed results below the lead action level.  Dr. Edwards called the results the \"beginning of the end\" of the public health disaster associated with the water crisis. On December 2, 2016, Dr. Edwards said lead wasn't detected in 57 percent of 154 Flint homes tested in November 2016 – up from 44 percent in July 2016.  He also advised people to continue using filters. On January 13, 2016, Snyder said that 87 cases of Legionnaires' disease, a waterborne disease, were reported in Genesee County from June 2014 – November 2015, resulting in 12 deaths (two more people later died from the disease).  Although the Michigan Department of Health and Human Services (MDHHS) said that there is no evidence of a clear link between the spike in cases and the water system change, Edwards stated the contaminated Flint water could be linked to the spike, telling reporters: It's very possible that the conditions in the Flint River water contributed.  We've actually predicted earlier this year that the conditions present in Flint would increase the likelihood of Legionnaires' disease.  We wrote a proposal on that to the National Science Foundation that was funded, and we visited Flint and did two sampling events.  The first one was focused on single family homes or smaller businesses.  We did not find detectable levels of Legionella bacteria that causes disease in those buildings.  But during our second trip, we looked at large buildings, and we found very high levels of Legionella that tends to cause the disease. In a second report released January 21, state researchers had still not pin-pointed the source of the outbreak.  The next day, an official at McLaren Regional Medical Center in Flint put out a press release that said: After the City of Flint switched to the Flint River as its water source in April of 2014, we noticed an increase in the number of Legionella cases that were coming to McLaren for treatment, as well as those being reported across the county and at other hospitals.  Because of that concern, and concern over the quality of water that we were receiving from the city, we began aggressively testing our water supply.  An early test result indicated the presence of a low level Legionella.  All Legionella and lead testing continues to show that the McLaren Flint water supply is well within safety and quality standards.  It is important to note that no test have ever determined that McLaren is the source of exposure for any patients testing positive for the Legionella antigen, and that there is no definitive data to support that McLaren Flint is the source of exposure for any patient testing positive for the Legionella antigen. The family of one of the people who died of Legionnaires has filed a $100 million lawsuit against McLaren. \"The Flint Journal\" obtained documents via the Michigan Freedom of Information Act (FOIA) on the Legionnaires' outbreak and published an article on them on January 16, 2016.  The documents indicated that on October 17, 2014, employees of the Genesee County Health Department and the Flint water treatment plant met to discuss the county's \"concerns regarding the increase in Legionella cases and possible association with the municipal water system.\"  By early October 2014, the Michigan DEQ were aware of a possible link between the water in Flint and the Legionnaires' outbreak, but the public was never informed, and the agency gave assurances about water safety in public statements and at public forums.  An internal January 27, 2015 email from a supervisor at the health department said that the Flint water treatment plant had not responded in months to \"multiple written and verbal requests\" for information.  In January 2015, following the complete breakdown in communication between the city and the county on the Legionnaires' investigation, the county filed a FOIA request with the city, seeking \"specific water testing locations and laboratory results ... for coliform, E-coli, heterotrophic bacteria and trihalomethanes\" and other information.  In April 2015, the county health department contacted the Centers for Disease Control and Prevention (CDC), and in April 2015 a CDC employee wrote in an email that the Legionnaire's outbreak was \"very large, one of the largest we know of in the past decade and community-wide, and in our opinion and experience it needs a comprehensive investigation.\"  However, MDHHS told the county health department at the time that federal assistance was not necessary. Emails obtained by Progress Michigan in February 2016 indicate Snyder's office knew about the outbreak since March 2015, despite Snyder's claim he was only informed in January 2016. On March 11, 2016, Governor Snyder ordered an investigation of the MDHHS regarding the outbreak. On February 16, 2017, it was reported the Centers for Disease Control and Prevention discovered the first genetic links between city water and patients diagnosed with Legionnaires' disease in Genesee County.  \"The presence of Legionella in Flint was widespread,\" said Dr. Janet Stout, a research associate professor at the University of Pittsburgh and a national expert on the disease.  \"The (laboratory) results show that strains (of the bacteria) were throughout the water system.\"  Virginia Tech researcher Amy Pruden published a study that found Legionella levels up to 1,000 times higher than normal tap water in Flint, and said finding a patient who's clinical isolates—or bacteria—matched the McLaren water sample without having been hospitalized there \"suggests that same strain may have been elsewhere.\" On March 10, 2017, it was discovered that Dr. Janet Stout, a nationally renowned expert on Legionairres Disease said in an affidavit: (It) is my opinion to a reasonable degree of probability that the source water change and the subsequent management of the municipal water system caused conditions to develop within the municipal water distribution system that promoted Legionella growth and dispersion, amplification, and the significant increases in cases of Legionnaires' disease in Genesee County in 2014 and 2015.\"  J. David Krause, director of Forensic Analytical Consulting Services, and Dr. Hung K. Cheung, a doctor specializing in environmental and occupational medicine agreed with her claims. One focus of inquiry is when Snyder became aware of the issue, and how much he knew about it.  In a July 2015 email, Dennis Muchmore (then Snyder's chief of staff) wrote to a Michigan Department of Health and Human Services (MDHHS) official, \"I'm frustrated by the water issue in Flint.  I really don't think people are getting the benefit of the doubt.  These folks are scared and worried about the health impacts and they are basically getting blown off by us (as a state we're just not sympathizing with their plight).\"  In a separate email sent on July 22, 2015, MDHHS local health services director Mark Miller wrote to colleagues that it \"Sounds like the issue is old lead service lines.\"  These emails were obtained under the Michigan Freedom of Information Act by Virginia Tech researchers studying the crisis, and were released to the public in the first week of January 2016. In October 2015, it was reported that the city government's data on lead water lines in the city was stored on 45,000 index cards (some dating back a century) located in filing cabinets in Flint's public utility building.  The Department of Public Works said that it was trying to transition the data into an electronic spreadsheet program, but as of October 1, 2015, only about 25% of the index card information had been digitized. On October 21, 2015, Snyder announced the creation of a five-member Flint Water Advisory Task Force, consisting of Ken Sikkema of Public Sector Consultants and Chris Kolb of the Michigan Environmental Council (co-chairs) and Dr. Matthew Davis of the University of Michigan Health System, Eric Rothstein of the Galardi Rothstein Group and Dr. Lawrence Reynolds of Mott Children's Health Center in Flint.  On December 29, 2015, the Task Force released its preliminary report, saying that the Michigan Department of Environmental Quality (MDEQ) bore ultimate blame for the Flint water crisis.  The task force wrote that the MDEQ's Office of Drinking Water and Municipal Assistance (ODWMA) adopted a \"minimalist technical compliance approach\" to water safety, which was \"unacceptable and simply insufficient to the task of public protection.\"  The task force also found that \"Throughout 2015, as the public raised concerns and as independent studies and testing were conducted and brought to the attention of MDEQ, the agency's response was often one of aggressive dismissal, belittlement, and attempts to discredit these efforts and the individuals involved.  We find both the tone and substance of many MDEQ public statements to be completely unacceptable.\"  The task force also found that the Michigan DEQ has failed to follow the federal Lead and Copper Rule (LCR).  That rule requires \"optimized corrosion control treatment,\" but MDEQ staff instructed City of Flint water treatment staff that corrosion control treatment (CCT) would not be necessary for a year.  The task force found that \"the decision not to require CCT, made at the direction of the MDEQ, led directly to the contamination of the Flint water system.\" The task force's findings prompted the resignation of MDEQ director Dan Wyant and communications director Brad Wurfel.  Flint Department of Public Works director Howard Croft also resigned. The Flint Water Advisory Task Force's final report, released March 21, 2016, found the MDEQ, MDHHS, Governor's office, and the state-appointed emergency managers \"fundamentally accountable\" for the crisis, saying the people of Flint were \"needlessly and tragically\" exposed to toxic levels of lead and other hazards. On January 8, 2016, the U.S. Attorney's Office for the Eastern District of Michigan said that it was investigating.  A month later, they said they were working with the Federal Bureau of Investigation, the EPA's Office of Inspector General, the EPA's Criminal Investigation Division, and the Postal Inspection Service on the investigation. The Environmental Protection Agency (EPA) \"battled Michigan's Department of Environmental Quality behind the scenes for at least six months over whether Flint needed to use chemical treatments to keep lead lines and plumbing connections from leaching into drinking water\" and \"did not publicize its concern that Flint residents' health was jeopardized by the state's insistence that such controls were not required by law\".  In 2015, EPA water expert Miguel A. Del Toral \"identified potential problems with Flint's drinking water in February, confirmed the suspicions in April and summarized the looming problem\" in an internal memo circulated on June 24, 2015. Despite these \"dire warnings\" from Del Toral, the memo was not publicly released until November 2015, after a revision and vetting process.  In the interim, the EPA and the Michigan DEQ engaged in a dispute on how to interpret the Lead and Copper Rule.  According to EPA Region 5 Administrator Susan Hedman, the EPA pushed to immediately implement corrosion controls in the interests of public health, while the Michigan DEQ sought to delay a decision on corrosion control until two six-month periods of sampling had been completed.  Meanwhile, MDEQ spokesman Brad Wurfel called Del Toral a \"rogue employee\" for his whistle-blowing efforts.  Dr. Marc Edwards, who investigated the lead contamination, wrote that Del Toral had made a \"heroic effort\" that was stymied by the EPA and MDEQ spending months \"wrangling over jurisdiction, technicalities and legalities.\" In an interview with the \"Detroit News\" published on January 12, 2016, Hedman said that \"the recommendation to DEQ (regarding the need for corrosion controls) occurred at higher and higher levels during this time period.  And the answer kept coming back from DEQ that 'no, we are not going to make a decision until after we see more testing results.'\"  Hedman said the EPA did not go public with its concerns earlier because (1) state and local governments have primary responsibility for drinking water quality and safety; (2) there was insufficient evidence at that point of the extent of the danger; and (3) the EPA's legal authority to compel the state to take action was unclear, and the EPA discussed the issue with its legal counsel, who only rendered an opinion in November.  Hedman said the EPA discussed the issue with its legal counsel and urged the state to have MDHHS warn residents about the danger.  On January 21, Hedman's resignation (effective February 1) was accepted. Assessments of the EPA's action varied.  Edwards said that the assessment in Del Toral's original June memo was \"100 percent accurate\" and criticized the EPA for failing to take more immediate action.  State Senate Minority Leader Jim Ananich, Democrat of Flint, said, \"There's been a failure at all levels to accurately assess the scale of the public health crisis in Flint, and that problem is ongoing.  However, the EPA's Miguel Del Toral did excellent work in trying to expose this disaster.  Anyone who read his memo and failed to act should be held accountable to the fullest extent of the law.\"  Del Toral later told \"The Flint Journal\", \"I was stunned when I found out they did not have corrosion control in place.  In my head, I didn't believe that.  I thought: That can't be true...that's so basic.\"  He also confirmed that unfiltered Flint water is still unsafe to drink, and doesn't know when that will change. On January 15, 2016, Michigan Attorney General Bill Schuette announced that his office would open an investigation into the crisis, saying the situation in Flint \"is a human tragedy in which families are struggling with even the most basic parts of daily life.\"  To oversee the AG Office's probe, Schuette appointed Todd Flood as special prosecutor and Andrew Arena as chief investigator, who lead a team of nine full-time investigators.  At a media roundtable in February 2016, Flood said that the investigation could result in involuntary manslaughter charges, if there was gross negligence leading to a death.  Critics have questioned the objectivity of the investigation. In his annual State of the State address on January 19, 2016, Snyder announced that he would release all of his emails from 2014 and 2015 regarding the crisis.  The following day, the governor's office released 274 pages of emails.  The \"New York Times\" summarized, \"the documents provide a glimpse of state leaders who were at times dismissive of the concerns of residents, seemed eager to place responsibility with local government and, even as the scientific testing was hinting at a larger problem, were reluctant to acknowledge it.\"  Later that month in a class action lawsuit related to the crisis, Snyder and the MDEQ were served subpoenas for the release of additional emails dating back to the beginning of 2011.  Emails highlighted by Progress Michigan in January 2016 indicate that Michigan state officials were trucking in bottled water to some of their own employees stationed in Flint as early as January 2015 in regards to the unsafe levels of trihalomethanes, or THMs, a by-product of chlorine that had been added to the water to kill Coliform bacteria. On February 12, 2016, Governor Snyder released additional emails between his office and the MDEQ which about the Legionnaires' outbreak.  On February 26, Snyder's office released several thousand more emails regarding the crisis that date back to 2011.  An additional batch of emails was released on March 10. On January 22, 2016, two MDEQ employees (Liane Shekter Smith, former chief of the department's Office of Drinking Water and Municipal Assistance; and Steve Busch, former district supervisor in the division) were suspended, pending an investigation, as a result of questions regarding actions related to water testing in Flint.  In response, Snyder said, \"Michiganders need to be able to depend on state government to do what's best for them and in the case of the DEQ that means ensuring their drinking water is safe.  Some DEQ actions lacked common sense and that resulted in this terrible tragedy in Flint.  I look forward to the results of the investigation to ensure these mistakes don't happen again.\"  Smith was fired on February 5, 2016. On January 25, 2016, the Genesee County Commission approved a request from Genesee County Prosecuting Attorney David Leyton for $25,000 to conduct an investigation into the crisis.  The money will be used to hire two special prosecutors. On March 4, 2016, a report released by the Michigan Auditor General's office called the MDEQ's Office of Drinking Water and Municipal Assistance \"not sufficient\" in its oversight of the state's Community Water Supply Program. On January 14, 2016 U.S. Representative Brenda Lawrence, Democrat, of Southfield, formally requested congressional hearings on the crisis, saying: \"We trust our government to protect the health and safety of our communities, and this includes the promise of clean water to drink.\"  The House Committee on Oversight and Government Reform began their hearings on the crisis on February 3.  Democratic U.S. Representative Dan Kildee from Flint gave an opening statement.  The first witnesses were EPA acting deputy assistant administrator Joel Beauvais, Dr. Marc Edwards, new MDEQ Director Keith Creagh, and Flint resident LeeAnne Walters (who alerted EPA water expert Miguel A. Del Toral to the problem).  On March 15, the House Oversight and Government Reform Committee examining the Flint water crisis reveals the EPA, state, and municipal officials attempted to fix the situation behind the scenes according to hearing witness and former EPA regional administrator, Susan Hedman, who cited legal and enforcement challenges as the causes for her actions.  Ex-Emergency Financial Manager Darnell Earley, Former Fint Mayor Dayne Walling, and Professor Marc Edwards also testified on that date's hearing.  Governor Snyder and EPA Administrator Gina McCarthy testified before that committee on March 17. On February 10, 2016, a separate committee, the U.S. House Democratic Steering and Policy Committee, held a hearing on the crisis in which Hurley Medical Center pediatrician Dr. Mona Hanna-Attisha; Yanna Lambrinidou, president of Parents for Nontoxic Alternatives, an environmental health group; Flint schools Superintendent Bilal Kareem Tawwab; Eric Scorsone, an expert in local government finances from Michigan State University, and Flint Mayor Karen Weaver testified. On April 13, 2016, the U.S. House of Representatives Subcommittee on Environment and the Economy and Energy Subcommittee on Health held a joint hearing on the crisis in which Keith Creagh of the Michigan Department of Environmental Quality, Nick Lyon from the Michigan Department of Health and Human Services, and Dr. Mona Hanna-Attisha of Hurley Medical Center testified. On February 23, 2016, the Michigan State Legislature started a committee to investigate the crisis.  On March 1, one of its members, Senator Jim Ananich of Flint, introduced a resolution that would grant state lawmakers probing the Flint water crisis subpoena power over the Governor's office, which is immune to the state Freedom of Information Act.  The committee's first hearing was on March 15, 2016. On March 29, 2016, the state's Joint Committee on the Flint Water Public Health Emergency held a hearing on the crisis in Flint during which residents and local experts testified. On December 15, 2015, Mayor Weaver declared the water issue as a citywide public health state of emergency to prompt help from state and federal officials.  Weaver's declaration said that additional funding will be needed for special education, mental health, juvenile justice, and social services because of the behavioral and cognitive impacts of high blood lead levels.  It was subsequently declared a countywide emergency by the Genesee County Board of Commissioners. Starting on January 7, 2016, Genesee County Sheriff Robert Pickell had work crews of offenders sentenced to community service begin delivering bottled water, water filters and replacement cartridges, primarily to residents living in homes built between 1901 and 1920, whose plumbing systems are most likely leaching lead into the water.  The next week, he ordered his department to begin using reverse 911 to advise homebound residents on how to get help. On January 10, Mayor Weaver stressed to residents that it was important to also pick up the testing kits, as the city would like to receive at least 500 water test samples per week. On January 12, officers from the Michigan State Police and Genesee County Sheriff's Department started delivering cases of water, water filters, lead testing kits and replacement cartridges to residents who needed them.  The American Red Cross has also been deployed to Flint to deliver bottled water and filters to residents. On January 14, it was announced Dr. Mona Hanna-Attisha will lead a Flint Pediatric Public Health Initiative that includes experts from the Michigan State University College of Human Medicine, Hurley Children's Hospital, the Genesee County Health Department, and the Michigan Department of Health and Human Services to help Flint children diagnosed with lead poisoning. On January 5, 2016, Michigan Governor Rick Snyder declared Genesee County to be in a state of emergency. On January 6, Snyder ordered the Michigan Emergency Operations Center, operated by the Michigan State Police Emergency Management and Homeland Security Division, to open a Joint Information Center to coordinate public outreach and field questions from the residents about the problems caused by the crisis.  The State Emergency Operations Center recommended that all Flint children under six years old get tested for lead levels as soon as possible, either by a primary care physician or the Genesee County Health Department.  The state has set up water resource sites at several public buildings around Flint where residents can pick up bottled water, water filters, replacement cartridges, and home water testing kits.  They also advised residents to call the United Way to receive additional help if needed. On January 11, Snyder signed an executive order creating a new committee to \"work on long-term solutions to the Flint water situation and ongoing public health concerns affecting residents.\" On January 13, Snyder activated the Michigan Army National Guard to assist the American Red Cross, starting the next day, with thirty soldiers planned to be in Flint by January 15.  The National Guard doubled their number of soldiers deployed to Flint by January 18.  On January 19, Snyder ordered more soldiers to Flint by the next day, for a total of 200. On January 27, Snyder announced the establishment of the new 17-member Flint Water Interagency Coordinating Committee to \"make recommendations regarding the health and welfare of people exposed to lead, study Flint's water infrastructure and determine potential upgrades, review Flint Water Task Force recommendations, and establish ways to improve communication between local and state government.\" On March 2, Snyder announced the state will partner with the employment agency Michigan Works!  Association to hire 81 Flint residents to work at water distribution sites throughout the city. On March 21, Governor Snyder released a 75-point relief plan for addressing the crisis, which includes programs in the fields of health and human services, education, water supply and infrastructure replacements, and jobs and economic development. On April 6, 2016, the state began offering up to $100,000 in grant money from the Disaster and Emergency Contingency Fund to local governments affected by the water crisis. On March 16, 2017, Governor Snyder created the Child Lead Exposure Elimination Commission and appointed Dr. Mona Hanna-Attisha of Flint's Hurley Medical Center, Rebecca Meuninck of Ann Arbor, deputy director of the Ecology Center; Paul Haan of Grand Rapids, executive director of the Healthy Homes Coalition of West Michigan, Inc.; and Lyke Thompson of Ann Arbor, director of the Center for Urban Studies at Wayne State University as its members.  \"Eliminating the risk of child lead exposure will require the coordination and expertise of people across all sectors,\" Snyder said in the announcement.  \"Creating this permanent commission will help advance the strategies recommended to better protect Michigan children from lead exposure.\"  On the same day, Governor Snyder said will lower Michigan's \"action level\" from 15 parts per billion—the federal limit—to 10 ppb. On June 9, 2017, the MDEQ reported their May 2017 testing showed 90 percent of Tier I samples at or below 6 parts per billion of lead with 93.1 percent of the samples at or below 15 ppb. On January 9, 2016, the Federal Emergency Management Agency (FEMA) sent two liaison officers to the Michigan Emergency Operations Center to work with the state to monitor the situation. On January 15, Snyder asked President Obama to grant a federal emergency/major disaster designation for Genesee County, seeking federal financial aid for emergency assistance and infrastructure repair in order to \"protect the health, safety and welfare of Flint residents.\"  The following day, Obama signed an emergency declaration giving Flint up to $5 million in federal aid to handle the crisis.  FEMA released a statement that said: The President's action authorizes the Department of Homeland Security, Federal Emergency Management Agency (FEMA), to coordinate all disaster relief efforts which have the purpose of alleviating the hardship and suffering caused by the emergency on the local population, and to provide appropriate assistance for required emergency measures, authorized under Title V of the Stafford Act, to save lives and to protect property and public health and safety, and to lessen or avert the threat of a catastrophe in Genesee County.  FEMA is authorized to provide equipment and resources to alleviate the impacts of the emergency.  Emergency protective measures, limited to direct federal assistance, will be provided at 75 percent federal funding.  This emergency assistance is to provide water, water filters, water filter cartridges, water test kits, and other necessary related items for a period of no more than 90 days. After Snyder's request for a \"Major Disaster Declaration\" status was turned down, FEMA Administrator W. Craig Fugate wrote a letter to Snyder saying that the water contamination \"does not meet the legal definition of a 'major disaster'\" under federal law because \"[t]he incident was not the result of a natural catastrophe, nor was it created by a fire, flood or explosion.\"  In response, Snyder asked Obama for emergency funding under FEMA's Individuals and Households Program, which provides housing assistance and replacement of personal property.  He will also ask for money and emergency protective measures, according to the release. On March 3, 2016, Governor Snyder filed a second appeal for federal help to replace lead pipes and provide medical support and supplies for affected residents which said the estimated economic impact of the Flint water crisis is beginning to exceed $140 million.  FEMA rejected his request again on March 16. The federal response is being led by the Department of Health and Human Services, with assistance from FEMA, the Small Business Administration, the EPA, the Department of Housing and Urban Development, the Department of Agriculture, the Office of Preparedness and Response, and the Public Health Service Commissioned Corps.  Dr. Nicole Lurie, Assistant Secretary of Health and Human Services for Preparedness and Response, was appointed to coordinate the federal response. The EPA issued a Safe Drinking Water Act Emergency Order and took over collecting and testing of water samples, while ordering state agencies to send them previously collected data, on January 21.  A week later they advised residents to continue using water filters and drink only bottled water. On February 12, the USDA extended their nutrition programs for Flint children diagnosed with high blood lead levels.  On the next day, Governor Snyder asked for additional help from Medicaid and the State Children's Health Insurance Program for affected Flint children.  The Department of Health and Human Services granted his request on February 18, providing an additional $500,000 in Medicaid expansion for affected Flint children and pregnant women.  On March 3, a waiver request to include pregnant women and people up to 21 years of age was approved. On March 1, the U.S. Department of Health and Human Services announced plans to expand its Head Start Program to more Flint children affected by the crisis. On March 23, the U.S. Department of Labor announced up to $15 million in National Dislocated Worker Grants will help provide temporary jobs to assist with Flint's water crisis recovery.  About 400 temporary jobs at water distribution centers throughout the city will be created through the grant.  The workers will take the place of the Michigan National Guard soldiers who have been in place since January. On March 25, 2016, the EPA and FEMA extended the federal emergency until August 14, 2016.  The state took over the emergency response after that date. On April 20, 2016, criminal charges were filed against three people in regards to the crisis by Michigan Attorney General Bill Schuette.  Former MDEQ employees Michael Prysby and Stephen Busch are charged with misconduct in office, conspiracy to tamper with evidence, tampering with evidence, a treatment violation of the Michigan Safe Drinking Water Act, and a monitoring violation of the Federal Safe Drinking Water Act; former city water plant operator Michael Glasgow was charged with willful neglect of office, a misdemeanor, and felony tampering with evidence.  On May 4, 2016 Glasgow accepted a plea deal with prosecutors, admitting to filing false information about lead in Flint water and agreeing to cooperate in other prosecutions.  Exactly a year later, the case against Glasgow was dismissed, with prosecutors acknowledging his cooperation and the fact that he was the person who reported the crimes of his colleagues to the MDEQ. On July 29, 2016, Schuette charged six additional people with crimes in the crisis, three from the Michigan Department of Environmental Quality and three from the Michigan Department of Health and Human Services.  From the MDEQ, Liane Shekter-Smith was charged with misconduct in office and willful neglect of duty; Adam Rosenthal was charged with misconduct in office, conspiracy to tamper with evidence, tampering with evidence, and neglect; Adam Cook was charged with misconduct in office, conspiracy to engage in misconduct in office, and neglect of duty.  From the MDHHS, Nancy Peeler, Corinne Miller, and Robert Scott were charged with misconduct in office, conspiracy to commit misconduct in office, and willful neglect of duty.  MDEQ and MDHHS released a joint statement later that day indicating Peeler, Scott, Cook, and Rosenthal have been suspended without pay.  Miller retired in April and Shekter-Smith was fired in February.  The cases were consolidated for preliminary hearing purposes on August 9, since the same witnesses will testify against all defendants.  The Attorney General's office says it has 10–15 witnesses in each case and roughly 50 exhibits in total.  On September 14, 2016, Miller pleaded no contest to the neglect of duty charge and agreed to testify against the other defendants.  She was later sentenced to a year probation, 300 hours of community service, and fined $1,200. On December 20, 2016, Schuette filed false pretenses, conspiracy to commit false pretenses, willful neglect of duty and misconduct in office charges against former Emergency Managers Darnell Earley and Jerry Ambrose; and false pretenses and conspiracy to commit false pretenses charges against former Flint Utilities Administrator Daugherty Johnson and former Flint Department of Public Works director Howard Croft. On June 14, 2017, Schuette announced new involuntary manslaughter charges—15-year felonies—against Michigan Department of Health and Human Services Director Nick Lyon, former Flint emergency manager Darnell Earley, former Flint Department of Public Works director Howard Croft, former Michigan Department of Environmental Quality Office of Drinking Water chief Liane Shekter-Smith and DEQ District Supervisor Stephen Busch.  Also charged was Dr. Eden Wells, chief medical executive of DHHS, who faces allegations of obstruction of justice and lying to a police officer.  Lyon was also charged with a single count of misconduct in office after being accused of having received notice of the Legionnaires' outbreak at least a year before informing the public and the governor, while Wells is also accused of threatening to withhold funding to the Flint Area Community Health and Environment Partnership unless the partnership ceased its investigation into the source of the Legionnaires' outbreak. On November 13, 2015, four families filed a federal class-action lawsuit in the U.S. District Court for the Eastern District of Michigan in Detroit against Governor Rick Snyder and thirteen other city and state officials, including former Flint Mayor Dayne Walling and ex-emergency financial manager Darnell Earley, who was in charge of the city when the switch to the Flint River was made.  The complaint alleges that the officials acted recklessly and negligently, leading to serious injuries from lead poisoning, including autoimmune disorders, skin lesions, and \"brain fog.\"  The complaint says that the officials' conduct was \"reckless and outrageous\" and \"shocks the conscience and was deliberately indifferent to ... constitutional rights.\"  The case was dismissed on February 3, 2017, with the judge stating his court has lack of subject-matter jurisdiction in the matter.  Their attorneys filed an appeal on February 6. The legal doctrines of sovereign immunity (which protects the state from suit) and official immunity (which in Michigan shields top government officials from personal liability, even in cases of gross negligence) resulted in comparatively few lawsuits being filed in the Flint case, and caused large national plaintiffs' law firms to be reluctant to become involved with the case. On January 14, 2016, a separate class-action lawsuit against Snyder, the State of Michigan, the City of Flint, Earley, Walling, and Croft was filed by three Flint residents in Michigan Circuit Court in Genesee County.  This suit targets lower-level officials who (under Michigan law) do not have immunity from claims arising from gross negligence. A separate suit was filed in January 2016 in the Michigan Court of Claims against the governor and state agencies; that suit alleges violations of the state constitution.  In Michigan, the Court of Claims is the only court with subject-matter jurisdiction over claims against the state and its subdivisions. A new federal lawsuit filed on January 27, 2016, seeks the replacement of all lead service lines in Flint at no cost to residents following claims city and state leaders violated federal laws designed to protect drinking water.  It is also asking the court to force city and state officials to provide safe drinking water to Flint residents and require them to follow federal regulations for testing and treating water to control for lead. On February 2, 2016, a class action lawsuit in U.S. District Court was filed on behalf of Beatrice Boler, a Flint mother of two, Flint pastor Edwin Anderson with his wife, Alline Anderson, and a company, Epco Sales LLC.  against Snyder, the MDEQ, two former state appointed emergency managers and former Flint Mayor Dayne Walling that seeks more than $150 million in refunds and compensation for damages for \"water that was extraordinarily dangerous, undrinkable and unusable.\"  It was dismissed on April 19, 2016, after the judge ruled the allegations fall under the federal Safe Drinking Water Act, which prevents challenges to the law being ruled on in U.S. District Court and states they must be addressed by the Environmental Protection Agency, and the case should be re-filed in the Michigan Court of Claims. Also on February 2, a lawsuit was filed in Michigan Circuit Court on behalf of four Genesee County residents who contracted Legionnaires' disease during the Flint water crisis, including one woman who died seven days after entering the emergency room with a headache.  The suit names McLaren Regional Medical Center and several Michigan DEQ officials as defendants.  Lawyer Geoffrey Fieger represents the plaintiffs. On February 8, 2016 the parents of a two-year-old girl diagnosed with high blood lead levels filed a lawsuit in federal court, naming as defendants the City of Flint, the State of Michigan, Snyder, Earley, and Walling.  The case was dismissed on February 7, 2017, with the judge citing his court has a lack of subject matter jurisdiction. On March 3, 2016, a new lawsuit was filed in state court by LeeAnne Walters, the Flint mother who informed the EPA water expert Miguel Del Toral of the health problems her family experienced after the water switch, against multiple corporate entities and three current and former government employees for their role in the city's water crisis. On March 7, 2016, another class action lawsuit was filed on behalf of seven residents alleging that tens of thousands of residents have suffered physical and economic injuries and damages.  It argues officials failed to take action over \"dangerous levels of lead\" in drinking water and \"downplayed the severity of the contamination.\" On March 8, 2016, a federal class action lawsuit was filed on behalf of 500 county inmates against the Genesee County Sheriff's Department in regards to the water quality at the Genesee County Jail.  The suit seeks only an injunction that will order the sheriff's department to continue to serve inmates only bottled water and dry food that doesn't require water to prepare. On March 24, the City of Flint filed a notice of intent sue in the Court of Claims against the State of Michigan, the MDEQ and four MDEQ employees for their mishandling of the crisis.  A week later, Mayor Weaver said she has no intentions to proceed with a lawsuit, and the move is to \"protect the future interest of the city.\" On March 25, a federal lawsuit filed by the ACLU asked for an order requiring water to be delivered to homes of people without access to transportation or who are physically disabled.  The case was settled a year later for $87 million (with an additional $10 million in reserve), which will be used to replaced 18,000 lead pipes by 2020. On April 6, 2016, a class action lawsuit brought by 15 Flint residents accused Governor Snyder and several state agencies and government officials of being in violation of the Racketeer Influenced and Corrupt Organizations Act in regards to the crisis. On May 18, 2016, the NAACP sued the state of Michigan and Governor Snyder, seeking compensation for property damages, pain and suffering damages, emotional distress damages and medical monitoring for Flint residents and businesses. On June 22, 2016, the Michigan Attorney General's Office filed a civil suit against engineering firms Veolia North America and Lockwood, Andrews & Newnam (LAN) who were hired to consult Flint water plant officials after the switch to the Flint River in April 2015.  The lawsuit accuses Veolia and LAN of professional negligence and public nuisance.  Veolia is also accused of fraud.  Veolia called the accusations \"baseless, entirely unfounded and [appearing] to be intended to distract from the troubling and disturbing realities that have emerged as a result of this tragedy,\" and then added, \"In fact, when Veolia raised potential lead and copper issues, city officials and representatives told us to exclude it from our scope of work because the city and the EPA were just beginning to conduct lead and copper testing.\" On November 15, 2016, Chief Judge Richard B. Yuille, Circuit Court of Genesee County, entered a Case Management Order, wherein he appointed attorney Corey M. Stern, of Levy Konigsberg, L.L.P., “Lead Counsel” for all plaintiffs maintaining claims in the Circuit Court of Genesee County for personal injuries and property damage sustained as a result of the Flint Water Crisis.  Attorney Wayne B. Mason, of Drinker, Biddle & Reath, L.L.P., was appointed \"Lead Counsel\" for the Defendants.  Judge Yuille called for a small number of lawsuits related to the Flint Water Crisis to serve as bellwethers, cases that will be fully developed and tried to verdict with the idea that they will help attorneys in other cases evaluate whether to settle or take their cases to trial. On January 30, 2017, a class action lawsuit with over 1,700 plaintiffs against the EPA seeking $722.4 million was filed, charging them with a violation of section 1431 of the Safe Drinking Water Act, which states, \"upon receipt of information that a contaminant that is present in or likely to enter a public water system or an underground source of drinking water, or there is a threatened or potential terrorist attack or other intentional act, that may present an imminent and substantial endangerment to the health of persons, the EPA Administrator may take any action she deems necessary to protect human health\". On January 7, 2016, Flint Mayor Karen Weaver said that estimates of the cost of fixing water infrastructure in Flint, such as aging pipes, range from millions up to $1.5 billion.  These figures encompass infrastructure alone, excluding any public health costs of the disaster.  DEQ interim director Keith Creagh said that estimation of total costs would be premature.  However, in a September 2015 email released by Snyder in January 2016, the state estimated the replacement cost to be $60 million, and said it could take up to 15 years to do. On January 18, 2016 the United Way of Genesee County estimated 6,000–12,000 children have been exposed to lead poisoning and kicked off a fundraising campaign to raise $100 million over a 10–15 year span for their medical treatment.  On January 27, 2016 Dr. Mona Hanna-Attisha started a fundraiser for the $80,000 needed for the medical treatment of Flint children affected by lead poisoning.  Meridian Health Plan of Detroit has agreed to donate up to $40,000 in matching funds to the Community Foundation of Greater Flint for long-term needs Dr. Hanna-Attisha expects to arise from the lead issue. At his annual State of the State address on January 19, Snyder apologized again, and asked the Michigan Legislature to give Flint an additional $28 million in funding for filters, replacement cartridges, bottled water, more school nurses and additional intervention specialists.  It also will fund lab testing, corrosion control procedures, a study of water-system infrastructure, potentially help Flint deal with unpaid water bills, case management of people with elevated lead-blood levels, assessment of potential linkages to other diseases, crisis counseling and mental health services, and the replacement of plumbing fixtures in schools, child care centers, nursing homes and medical facilities.  The Michigan House Appropriations Committee passed the bill the next day, while the Senate approved it on January 28.  Snyder signed it the next day. On January 21, 2016 President Obama gave an $80 million loan to Michigan for infrastructure repairs, but the amount going to Flint is uncertain. On January 28, 2016 Democratic U.S. Senators Debbie Stabenow and Gary Peters and Representative Dan Kildee proposed an amendment to pending federal energy legislation to add the special appropriation of up to $400 million to replace and repair the lead service lines in Flint and $200 million more to create a center for lead research in Flint.  They also said the state could choose to match up to $400 million for its share of infrastructure repairs in Flint.  The newly amended bill was rejected by the Senate on February 4.  A new $220 million bill to address the crisis was proposed in the U.S. Senate on February 24. At a news conference on February 9, 2016, Flint mayor Karen Weaver said that the city would remove and replace all of the city's 15,000 water service lines containing lead piping.  Work was expected to begin in March 2016.  The project will receive technical advice from the Lansing Board of Water and Light, which removed over 13,000 lead pipes in Lansing, Michigan.  Lansing mayor Virg Bernero volunteered to provide the assistance.  Weaver appointed Michael C.H. McDaniel, a retired National Guard brigadier general, to oversee the group leading the project, the Flint Action and Sustainability Team (FAST).  The city government hopes to complete the project within a year, using 32 work crews, with priority given to the most at-risk households.  The project is expected to cost $55 million, and the funding sources are not yet secured, but the city plans to seek it from local, state, and federal sources.  The crews began working on March 4. On February 16, 2016 the state hired Flint-based engineering firm Rowe Professional Services to begin the process of locating, removing, and eventually replacing lead pipes in the highest risk areas of Flint. On February 18, 2016 the state gave Flint a $2 million grant that will go towards replacing lead service lines. On March 6, 2016 Union Labor Life Insurance Company donated $25 million for lead pipe replacements in the city. On July 18, 2016 city council approved a $500,000 contract with three companies for the second phase of lead pipe replacements: WT Stevens, and Johnson & Wood were awarded $320,000 contracts to do no more than 50 homes each.  Goyette was awarded $619,500 to tackle replacing lead lines at 150 Flint homes.  The city is using $25 million in funding approved by the Michigan legislature in June that was allocated for replacing Flint lead tainted pipes for Fast Start's third phase which will replace infrastructure at an estimated 5,000 homes in Flint. On October 10, 2016 city council approved contracts to replace pipes at 788 more homes before winter.  The third phase will be funded using a portion of $25 million approved by the Michigan Legislature in June that was allocated for replacing Flint lead tainted pipes for Fast Start's third phase, which will replace infrastructure at an estimated 5,000 homes in Flint.  Goyette will be paid $1,663,300.60 for replacements at 260 addresses in city wards two, six and eight.  WT Stevens will be paid $2,306,384 for replacements at 488 addresses in city wards three, four, eight and nine. On October 17, 2016 the second phase of the program was completed on 218 homes.  The project was completed by WT Stevens Construction Inc., Johnson & Wood Mechanical, and Goyette Mechanical.  By November 22, 2016, the total number of homes with new pipes was 460. A University of Michigan study released on December 1, 2016 stated a total of 29,100 lead pipes need to be replaced. On January 19, 2017, an engineer at the Flint Water Plant said the facility is in need of $60 million worth of upgrades, which wouldn't be finished until well into 2019.  On February 7, 2017, another report said the cost would be $108 million. On February 6, 2017, the Genesee Intermediate School District received $6.5 million for the Early On Genesee program to provide free evaluations to as many as 5,000 children up to 5 years old facing possible lead-related developmental delays from the state of Michigan. On March 17, 2017, Flint received a $100 million grant from the EPA for water infrastructure repairs. On June 30, 2017, the Genesee County Health Department's Healthy Start Program received $15 million to provide health and social services for people who have had or are at risk for lead exposure stemming from Flint water crisis. Childhood lead exposure causes a reduction in intellectual functioning and IQ, academic performance, and problem-solving skills, and an increased risk of attention deficit disorder, aggression, and hyperactivity.  According to studies, children with elevated levels of lead in the blood are more likely as adults to commit crimes, be imprisoned, be unemployed or underemployed, or be dependent on government services.  In addition, early-life exposure to lead may increase risk of later-life neurological disorders such as Alzheimer's disease, and this risk is likely to persist into late life long after lead has been removed from the body.  A 2014 study by researchers at Risk Science Center at the University of Michigan, completed before the Flint water crisis came to light, estimated the annual cost of childhood lead exposure in Michigan at $330 million ($205 million in decreases in lifetime earnings, $105 million in additional criminal justice system expenditures, $18 million in health expenditures to diagnose lead positioning and lead-linked attention deficit disorder), and $2.5 million in additional special education expenditures. Because the developmental effects of lead exposure appear over a series of years, the total long-term cost of the Flint water crisis \"will not be apparent in the short term.\"  However, the cost is expected to be high.  Philippe Grandjean of the Harvard T.H. Chan School of Public Health, an expert in the effects of environmental pollution on brain development, said that \"when calculated from the loss of lifetime income, the societal costs from lead exposure (across the United States) reach billion dollar amounts.\" After approving Governor Snyder's application for an emergency declaration, President Barack Obama said of the crisis, \"What is inexplicable and inexcusable is once people figured out that there was a problem there, and that there was lead in the water, the notion that immediately families weren't notified, things weren't shut down.  That shouldn't happen anywhere.\"  President Obama visited Flint on May 4, 2016 to reiterate his thoughts and drank a glass of filtered Flint water to show it was safe. President Donald Trump's plan to fix the crisis in Michigan has been folded into his federal infrastructure plan.  Trump's infrastructure plan proposes $1 trillion in spending on new infrastructure by offering corporations who invest in infrastructure projects tax credits, with the corporations investing approximately $167 billion.  This plan would require a return of 9–10% to investors to remain feasible.  This plan has no direct reference to or specific proposal for the crisis in Flint and as of his election he has not proposed a direct federal intervention. On February 28, 2017, President Trump gave a speech to a joint session of Congress asking them to fund his plan. On March 15, 2017, President Trump briefly met with Flint Mayor Karen Weaver during his trip to a former aircraft factory in Ypsilanti Township to discuss infrastructure funding for Flint.  Weaver said she hopes to speak with Trump about the water crisis in greater detail in the near future. On January 20, 2016, Senator Debbie Stabenow, a Democrat, faulted the state for having \"no sense of urgency whatsoever\" despite warnings from the EPA about the contaminated water.  Senator Gary Peters, also a Democrat, said, \"The water crisis in Flint is an immense failure on the part of the State of Michigan to protect the health and safety of the City's residents, and the State must accept full responsibility for its actions that led to this catastrophe.\"  Peters, along with Stabenow and Representative Dan Kildee, called upon the state to make a \"sustained financial commitment\" to assist Flint \"by establishing a 'Future Fund' to meet the cognitive, behavioral and health challenges\" of children affected by lead poisoning.  Peters also called upon the state to reimburse Flint residents for the money that was paid for contaminated water, to pay the city's legal fees in connection with the water crisis, and to pay for the costs of reconnecting to the Detroit water system. On January 12, 2016, Dan Kildee, Democrat of Flint, said of Snyder, \"It's beyond my comprehension that he continues to treat this as a public relations problem rather than as a public health emergency.  Meanwhile, kids in Flint are still being exposed to high levels of lead in the water.\"  Kildee called upon Snyder to request federal assistance, which Snyder subsequently did.  Kildee, along with fellow Michigan Representative Fred Upton, also sponsored H.R. 4470, the Safe Drinking Water Act Improved Compliance Awareness Act, which will ensure that the public promptly learns of excessive lead levels in their drinking water by setting forth how and when states, EPA, and public utilities communicate their findings.  It has passed the House but has yet to be passed by the Senate. Among the Michigan congressional delegation, only Representative Justin Amash, Republican of Cascade Township, opposed federal aid for Flint.  Amash opined that \"the U.S. Constitution does not authorize the federal government to intervene in an intrastate matter like this one.\" On January 4, 2016, citing the Flint water crisis, Michigan Representative Phil Phelps, Democrat of Flushing, announced plans to introduce a bill to the Michigan House of Representatives that would make it a felony for state officials to intentionally manipulate or falsify information in official reports, punishable by up to five years' imprisonment and a $5,000 fine. On March 2, House Democratic leader Tim Greimel called on Governor Snyder to resign, due to his \"negligence and indifference\" in his handling of the Flint water crisis.  Also on that date, State Democratic Party Chairman Brandon Dillon called for Michigan Treasurer Nick Khouri to resign due to his role in a loan agreement from April 2015 that blocked Flint from switching back to the Detroit system. Democratic Presidential nominee Hillary Clinton repeatedly mentioned the crisis during her campaign, saying: \"The people of Flint deserve to know the truth about how this happened and what Governor Snyder and other leaders knew about it.  And they deserve a solution, fast.  Thousands of children may have been exposed to lead, which could irreversibly harm their health and brain functioning.  Plus, this catastrophe—which was caused by a zeal to save money at all costs—could actually cost $1.5 billion in infrastructure repairs.\"  In a subsequent interview, Clinton referred to her work on lead abatement in housing in upstate New York while a U.S. Senator and called for further funding for healthcare and education for children who will suffer the negative effects of lead exposure on behavior and educational attainment. The crisis was also the catalyst for a town hall style debate in Flint between Clinton and Democratic rival Bernie Sanders on March 6, 2016, two days before the Michigan Presidential primary election.  It was hosted by CNN anchors Anderson Cooper and Don Lemon.  Both candidates called for Governor Snyder to resign during the event. On January 19, 2016, Republican candidate (at the time) Donald Trump said, \"It's a shame what's happening in Flint, Michigan.  A thing like that shouldn't happen.\"  After clinching the Republican nomination, Trump visited Flint on September 14, 2016 and toured the water plant and a Flint church, where he promised to fix the water crisis, and in a brief speech there, he blamed NAFTA for General Motors' abandonment of Flint and the area's subsequent ongoing recession caused by it, saying, \"It used to be that cars were made in Flint and you couldn't drink the water in Mexico.  Now cars are made in Mexico, and you can't drink the water in Flint.  That's terrible.\" An investigative report by Reuters released Dec. 19, 2016 found nearly 3,000 areas in the United States with lead poisoning rates at least double those in Flint. The water disaster called attention to the problem of aging and seriously neglected water infrastructure nationwide.  The Flint crisis recalled recent lead contamination crises in the tap water in various cities, such as the lead contamination in Washington, D.C. drinking water (2001), Columbia, South Carolina (2005); Durham and Greenville, North Carolina (2006); Jackson, Mississippi (2015); and Sebring, Ohio (2015).  \"The New York Times\" notes, \"Although Congress banned lead water pipes 30 years ago, between 3.3 million and 10 million older ones remain, primed to leach lead into tap water by forces as simple as jostling during repairs or a change in water chemistry.\"  Inadequate regulation was cited as one reason for unsafe lead levels in tap water and \"efforts to address shortcomings often encounter push-back from industries like agriculture and mining that fear cost increases, and from politicians ideologically opposed to regulation.\"  The crisis called attention to a \"resource gap\" for water regulators.  The annual budget of the EPA's drinking water office declined 15% from 2006 to 2015, with the office losing over 10% of employees, and the Association of State Drinking Water Administrators reported in 2013 that \"federal officials had slashed drinking-water grants, 17 states had cut drinking-water budgets by more than a fifth, and 27 had cut spending on full-time employees,\" with \"serious implications for states’ ability to protect public health.\" In the aftermath of the water crisis, it was noted that elevated blood-lead levels in children are found in many cities across Michigan, including Detroit, Grand Rapids, Muskegon, and Adrian.  Although statewide childhood lead-poisoning rates have dramatically declined since the removal of lead from gasoline, certain areas of the state (particularly low-income areas with older housing stock) continue to experience lead poisoning, mostly from lead paint in homes built before 1978 and lead residue in dust and soil.  Lead abatement efforts are slow. Civil rights advocates characterized the crisis as a result of environmental racism (Flint's population is 56.6% African American per the 2010 census), a term primarily referring to the disproportionate exposure of ethnic minorities to pollution as a result of \"poverty and segregation that has relegated many blacks and other racial minorities to some of the most industrialized or dilapidated environments.\"  Columnist Shaun King, for example, wrote that the crisis was \"a horrific clash of race, class, politics and public health.\" The Michigan Civil Rights Commission later reiterated this belief in a 138-page report titled \"The Flint Water Crisis: Systemic Racism Through the Lens of Flint\".  Its writers said of it, \"Policy makers, government leaders, and decision makers at many levels failed the residents of Flint,\" said Agustin Arbulu, Director of the Michigan Department of Civil Rights.  \"By not challenging their assumptions, by not asking themselves the tough questions about how policy and decisions play out in different communities, especially communities primarily made up of people of color, those decisions and actions – or in some cases, lack of action – led to the tragedy taking place in Flint.\"  \"We strongly believe that the actions that led to the poisoning of Flint's water and the slow response resulted in the abridgement of civil rights for the people of Flint,\" said Arthur Horwitz, co-chair of the Commission during the time of the investigation.  \"We are not suggesting that those making decisions related to this crisis were racists, or meant to treat Flint any differently because it is a community of color.  Rather, the response is the result of implicit bias and the history of systemic racism that was built into the foundation of Flint.  The lessons of Flint are profound.  While the exact situation and response that happened in Flint may never happen anywhere else, the factors that led to this crisis remain in place and will most certainly lead to other tragedies if we don't take steps to remedy them.  We hope this report is a step in that direction.\"  The Governor's office responded: \"Some findings of the report and the recommendations are similar to those of the (Flint Water Advisory Task Force and) the legislative panel and the Flint Water Interagency Coordinating Committee,\" said Gov. Rick Snyder spokeswoman Anna Heaton.  \"The Governor takes the reporting of each of these panels very seriously, and appreciates the public input that was shared.\"  The findings were no surprise for State Senate Minority Leader Jim Ananich.  \"The presence of racial bias in the Flint water crisis isn't much of a surprise to those of us who live here, but the Michigan Civil Rights Commission's affirmation that the emergency manager law disproportionately hurts communities of color is an important reminder of just how bad the policy is.  Now is the time to address this flawed law,\" Ananich said.  He went on to say, \"The people of Flint deserve the same level of safety, opportunity and justice that any other city in Michigan enjoys\". On October 8, 2015, the editorial board of the \"Detroit Free Press\" wrote that the crisis was \"an obscene failure of government\" and criticized Snyder. On December 31, 2015, the editorial board of the MLive group of Michigan newspapers called upon Snyder to \"drop executive privilege and release all of his communications on Flint water,\" establish a procedure for compensating families with children suffering from elevated lead blood levels, and return Flint to local control. Some of the most important reporting on the crisis was conducted by investigative reporter Curt Guyette, who works not for a news organization but for the American Civil Liberties Union's Michigan Democracy Watch Project.  The work of Guyette and the ACLU was credited with bringing the water contamination to public light. MSNBC host Rachel Maddow has extensively reported on the water crisis on her show since December 2015, keeping it in the national spotlight.  She has condemned Snyder's use of emergency managers (which she termed a \"very, very radical\" change \"to the way we govern ourselves as Americans, something that nobody else has done\") and said, \"The kids of Flint, Michigan have been poisoned by a policy decision.\"  Maddow visited Flint and hosted a town hall with government officials and other involved experts on her show on January 27. In January 2016, the watchdog group Common Cause called upon Snyder to release all documents related to the Flint water crisis.  The governor's office is not subject to the Michigan Freedom of Information Act. The hacktivist group Anonymous released a YouTube video calling for the arrest of Snyder. The filmmaker Michael Moore, a Genesee County native, called for Snyder's arrest for mishandling the water crisis in an open letter to the governor, writing, \"The facts are all there, Mr. Snyder.  Every agency involved in this scheme reported directly to you.  The children of Flint didn't have a choice as to whether or not they were going to get to drink clean water.\"  A spokesman for the governor called Moore's call \"inflammatory.\"  Later, after hearing of the Legionnaires' outbreak, Moore termed the state's actions \"murder.\"  Speaking to reporters in Flint, he emphasized that \"this was not a mistake ... Ten people have been killed here because of a political decision.  They did this.  They knew.\" In a post on her Facebook page, environmental activist Erin Brockovich called the water crisis a \"growing national concern\" and said that the crisis was \"likely\" connected to the Legionnaires' disease outbreak.  Brockovich called for the U.S. Environment Protection Agency to become involved in the investigation, saying that the EPA's \"continued silence has proven deadly.\" On January 16, 2016, the Reverend Jesse Jackson met with Mayor Weaver in Flint and said of the crisis, \"The issue of water and air and housing and education and violence are all combined.  The problem here obviously is more than just lack of drinkable water.  We know the problems here and they will be addressed.\"  Jackson called Flint \"a disaster zone\" and a \"crime scene\" during a rally at a Flint church the next day.  Jackson, in conjunction with the group Concerned Pastors for Social Action, held a major national march in Flint on February 19 to address the water issue, as well as inner city violence and urban reconstruction. On January 18, Nontombi Naomi Tutu, daughter of Desmond Tutu, said in a speech at the University of Michigan–Flint, \"We actually needed the people of Flint to remind the people of this country what happens when political expediency, when financial concerns, overshadow justice and humanity.\" On January 24, actor and clean drinking water advocate Matt Damon called for Snyder's resignation. On March 7, actor Mark Ruffalo, head of the group Water Defense, visited Flint and called for more federal aid in the emergency and Snyder's resignation while saying, \"It's an absolute outrage, it's a moral indecency.\"  Water Defense conducted studies on Flint water in the spring of 2016, claiming it is still unsafe for bathing or showering.  Their findings were disputed by Virginia Tech water expert Dr. Marc Edwards on May 31, 2016. In the third episode of the Adult Swim comedy series \"\", Charles Carroll (member of the group of YouTube comedians \"Million Dollar Extreme\") delivers a monologue where he describes how viewers can recreate the contaminated water in Flint.  In his monologue, the right wing leaning Carroll discusses the concept of tyrannicide with costars Sam Hyde and Andrew Ruse and claims that the situation in Flint is a situation where the violent murder of Republican leadership in the state of Michigan would be justified. During its winter 2016 semester, the University of Michigan–Flint offered a one-credit, eight-session series of public forums dedicated to educating Flint residents and students on the crisis. The University of Michigan (Ann Arbor) committed to spending $100,000 to research the crisis and possible ways to address it. Wayne State University in Detroit will lead a separate study focusing on the Legionnaires' outbreak called the Flint Area Community Health and Environment Partnership.  It will also include researchers from Flint's Kettering University and Detroit's Henry Ford Hospital. On April 20, 2017 Stephen Estes-Smargiassi, director of planning and sustainability at the Massachusetts Water Resources Authority, told a forum on lead water contamination at the Harvard School of Public Health that a chain-reaction of failures, including those by the financial managers, allowed the water crisis to develop as long as it did.  He stated \"What happened in Flint?  Well, a firestorm of things that went wrong.  (Flint) changed (its) source water, didn't do a good job on corrosion control in their treatment\", and added \"They had, about half of the homes had lead service lines.  Money was more important to the emergency manager than people were.  That's pretty clear from the evidence,\" and later went on to say, \"State regulators could have picked up on this, but fell down on the job, maybe worse than that.  We'll see what happens to those who were indicted.  And the federal regulators could have picked up the problem, but didn't until quite late.  All of those things, that firestorm of events, resulted in really awful water quality.\" The crisis highlighted a lack of transparency in Michigan government; the state is one of just two states that exempts the governor's office from state freedom-of-information legislation.  A number of commentators framed the crisis in terms of human rights, writing that authorities' handling of the issue denied residents their right to clean water.  Some have framed it as the end result of austerity measures and given priority over human life.  Jacob Lederman, for example, contends that Flint's poisoned water supply, in addition to high crime rates, devastated schools and crumbling infrastructure, can be attributed to neoliberal economic reforms. Robby Soave, writing in \"Reason\" magazine, said that administrative bloat in public-sector trade unions was to blame for the crisis: \"Let's not forget the reason why local authorities felt the need to find a cheaper water source: Flint is broke and its desperately poor citizens can’t afford higher taxes to pay the pensions of city government retirees.  As recently as 2011, it would have cost every person in Flint $10,000 each to cover the unfunded legacy costs of the city's public employees.\"  \"Flint was a government-made disaster from top to bottom.  Private companies didn't run the system or profit from it,\" Shikha Dalmia wrote in \"Reason Magazine\". The crisis brought the National Water Infrastructure Conference to Flint in early March 2017.  Michigan Governor Rick Snyder and Flint Mayor Karen Weaver spoke on the first day.  Dr. Marc Edwards spoke there two days later. Failed infrastructure and economic decline resulted in the toxic levels of lead in the city's water supply.  According to an article published in the American Journal of Public Health, to prevent another contamination crisis, officials such as Governor Snyder should consult \"professionals\" and make \"qualified\" decisions.  \"Snyder and his administration introduced a corrosive water source into an aging water filtration system without adequate corrosion control (APHA).\"  \"I wonder how many of the individuals who made those bad decisions were professional engineers, licensed plumbers, or water-treatment specialists?\"  asked Larry Clark, Sustainable Performance Solutions LLC .  In addition to professional consultation, EPA reform of water-testing techniques that concentrate on neighborhoods with lead pipes could \"ensure that all cities get an early warning when lead levels rise to the danger point.\" , said Jeff Ruch, executive director of Public Employees for Environmental Responsibility (PEER).  Addressing the government's neglect in Flint's crisis from infrastructure failure due to the city's economic decline could prevent another municipal disaster. Upholding the Clean Water Act passed in 1972 would have prevented an outbreak of lead poisoning in Flint.  This act \"established the basic structure for regulating pollutant discharges into the waters of the United States\".  The EPA has also updated its standards and created six goals bettering the drinking water of the nation.  This plan was created in November 2016 and if upheld should decrease the amount of pollution we see in waters today. As of September 8, 2017, the Ruth Mott Foundation and the Community Foundation of Greater Flint have directed a combined $33,480,494 to various programs to aid both children and adults affected by Flint’s lead-in-water crisis. The United Auto Workers union donated drinking water to Flint via a caravan of trucks to local food banks, and an AmeriCorps team announced that it would deploy to Flint to assist in response efforts. Singer Cher donated 181,000 bottles of water to the Food Bank of Eastern Michigan, while the Legacy Group Water Project coordinated with the Red Cross and the City of Flint as well as Bottles for the Babies to initiate the largest volunteer action to distribute water and filters into the city in a single day since the citywide emergency was declared a month earlier.  Operation Flint, another volunteer group, also began accepting water donations the same day. Rapper Meek Mill donated $50,000 and 60,000 bottles of water to Flint to aid in the crisis. Oskar Blues Brewery and Ball Corporation donated 50,000 cans of water to Flint. Rapper Big Sean, a Detroit native, donated $10,000 to the Community Foundation of Greater Flint. The Flint Firebirds' rivals in the Ontario Hockey League made donations: the Windsor Spitfires donated 40,000 bottles of water, and the Sarnia Sting donated 15,000 bottles of water. Detroit–based Faygo teamed with United Way of America to begin a fund for Flint, where $2 will give someone a full case of free water. Singer Aretha Franklin said she will provide hotel rooms and food for 25–50 Flint residents. Jimmy Fallon donated $10,000 to the Community Foundation of Greater Flint while a group made up of actor Mark Wahlberg and rappers Sean Combs, Eminem, and Wiz Khalifa donated 1 million bottles of water to Flint. Through her company, Parkwood Entertainment, singer Beyoncé raised over $82,000 during her Formation World Tour to assist the people of Flint during the water crisis. The Little River Band of Ottawa Indians donated $10,000 to the Genesee County Sheriff's Department. Detroit Lions defensive end Ziggy Ansah donated 94,000 bottles to Flint, and Terrance Knighton and his Washington Redskins teammates donated 3,600 bottles of water to Flint's Catholic Charities USA.  On the same day, rock band Pearl Jam and a large group of musicians donated $300,000 to the United Way of Genesee County, and started a CrowdRise fundraiser for donations from its fans.  Additionally, fundraising website GoFundMe promised to donate an additional $10,000 to the fund of the winner of a week-long contest that ended on January 29 between a large number of groups trying to raise money for Flint, while Anheuser-Busch donated 51,744 cans of water to the Food Bank of Eastern Michigan. The Detroit Pistons donated $500,000 to the United Way of Genesee County from their FlintNOW fundraising campaign from the previous night's game. A group of retired NBA players led by Derrick Coleman donated 30,000 cases of water to Flint. Walmart, The Coca-Cola Company, Nestlé and PepsiCo announced that they would collectively donate a total of 176 truckloads of water (up to 6.5 million bottles) through the end of 2016.  On the same day, singer Madonna (a native of nearby Bay City) donated $10,000 to the Community Foundation of Greater Flint, and singer Kem donated $10,000 to the Salvation Army of Genesee County.  Also, rapper The Game donated $1,000,000 in water bottles to Flint, while FedEx, along with the city of Memphis, Tennessee donated 12,000 bottles of water to the Food Bank of Eastern Michigan. Detroit Pistons forward Marcus Morris, in conjunction with Philadelphia organizations F.O.E. and the Nehemiah Davis Foundation donated 60,000 cases of water to Flint. The company ShowerPill, which includes several NFL players, donated $100,000 in anti-microbial body wipes, baby wipes and water to the United Way of Genesee County for distribution focused on high schools and senior centers.  On the same day, actor Jussie Smollett visited Flint and donated $10,000 to the Community Foundation of Greater Flint. Meijer announced it is donating $500,000 to three non-profit organizations.  $250,000 of this donation will go to the Flint Child Health and Development Fund, and the United Way of Genesee County's Flint Water Fund and the American Red Cross will receive $125,000 each. A group of nine banks collectively donated $600,000 to the Community Foundation of Greater Flint. AT&T Mobility set up a fund which allows customers to donate $10 to aid in the crisis by texting a certain number. Craigslist founder Craig Newmark donated $50,000 and 25,000 cases of water to the United Way of Southeastern Michigan. Platinum Equity chairman and CEO and Detroit Pistons owner Tom Gores, a Flint native, launched a campaign to raise $10 million for Flint.  On the same day, rapper Pusha T donated 2,000 cases of bottled water to Flint, and the city of Evanston, Illinois donated $5,000 to the United Way of Genesee County. The Michigan State Medical Society donated $10,000 to the Community Foundation of Greater Flint. The LaPorte County, Indiana Sheriff's Office donated 2,300 cases of water to a church in Flint, the Northwest Indiana Truck Club donated 3,500 cases of water to Flint, and NFL player and Flint native Brandon Carr donated $100,000 to the Community Foundation of Greater Flint and $10,000 to the Safe Water Safe Homes Fund. The police fraternity Brothers Before Others donated 330 cases of water bottles, 361 one-gallon (1.3 m³) water jugs and $1,000 to the Flint Police Department.  The charity Resources Unite of Dubuque, Iowa collected 300,000 bottles of water for Flint. A group of students from Ohio State University donated 10,000 pounds of water to Flint's Catholic Charities USA. Amtrak donated 30,000 bottles of water to Flint. Consumers Energy, the area's gas and electricity provider, has donated $50,000 during the crisis ($25,000 to the Community Foundation of Greater Flint and $25,000 to the United Way of Genesee County), and its employees are delivering water to Flint homes.  It is also matching donations from employees and retirees, up to $25,000. The Michigan Masonic Charitable Foundation donated $100,000 to the Community Foundation of Greater Flint. The Waukegan, Illinois Community School District donated 650 cases of bottled water to Flint. The United Food and Commercial Workers, in partnership with Cargill, ConAgra Foods, Hormel Foods, JBS USA, Pinnacle Foods, Downs Food Group and Ryder Logistics donated 125,000 pounds of food to the Food Bank of Eastern Michigan in response to the crisis. The Dow Chemical Company of nearby Midland donated $100,000 to the Community Foundation of Greater Flint, and its Water and Process Solutions division donated and will install 150 reverse osmosis water filtration systems in Flint homes. The Dr Pepper Snapple Group donated 41,000 bottles of water to the Food Bank of Eastern Michigan. Niagara Bottling, in partnership with Dray Technologies, donated over 50,000 gallons (190 m³) of bottled water to Feed the Children. Platinum Equity's FlintNOW Foundation, in conjunction with Huntington Bank, started a $25 million economic development program that will loan aid money to Flint businesses affected by the water crisis. Two prisons in Northern Michigan donated 29,000 bottles of water to the Genesee Intermediate School District. The Kresge Foundation donated $2 million to the Community Foundation of Greater Flint. Friends of Dave (a Dave Matthews Band fan club) donated two truckloads of water to the Catholic Charities of Flint. Dave Chappelle donated $50,000 to the Community Foundation of Greater Flint after performing standup comedy at Flint's Whiting Auditorium. Former Detroit Pistons player Rasheed Wallace delivered three truckloads of water to a party store in Flint. Bruno Mars donated $1 million to the Community Foundation of Greater Flint. Tabernacle Baptist Church in Knoxville, Tennessee donated 70,000 pounds of water to Flint. The United Church of Christ and the Disciples of Christ, two Flint-area Protestant denominations worked together to launch a water distribution effort. Flint Jewish Federation worked in partnership with the American Red Cross to help get clean water to homes. In January 2016, Muslim organizations, including Who is Hussain, Life for Relief and Development, Islamic Relief USA, and the Michigan Muslim Community Council donated and distributed thousands of bottles of water to Flint-area residents.  By May, Michigan's Muslim community had donated more than one million bottles of water to Flint-area residents. Comedians George Lopez, Eddie Griffin, Cedric the Entertainer, Charlie Murphy, and D. L. Hughley performed stand up comedy in Flint's Dort Federal Credit Union Event Center as part of The Comedy Get Down Tour, with the proceeds to go to the Community Foundation of Greater Flint. $50,000 raised at the Meridian Winter Festival in Detroit was donated to the Community Foundation of Greater Flint. On February 28, 2016, coinciding with the 88th Academy Awards ceremony, \"Creed\" director Ryan Coogler and \"Selma\" director Ava DuVernay held a charity event at the Whiting Auditorium in Flint.  The event, titled #JusticeForFlint, was live-streamed by Sean Combs' Revolt.tv network.  Hosted by comedian Hannibal Buress, it featured singers Janelle Monáe and Ledisi, as well as actor-activists Jesse Williams and Jussie Smollett, amongst others.  The event raised $156,000. A telethon led by Detroit TV station WDIV and simulcast on Michigan's other NBC affiliates raised $566,982 for the Community Foundation of Greater Flint.  Detroit Pistons owner and Flint native Tom Gores matched the amount, doubling the amount raised to $1,133,964. A benefit concert to support children affected by the crisis presented by Flint country music station Nash FM 95.1 featuring Granger Smith and Tegan Marie was held at the Dort Federal Center in Flint on April 7, with the proceeds going to Hurley's Children Hospital. A charity celebrity basketball game called Hoop 4 Water featuring former Michigan State Spartans players Morris Peterson (from Flint), Zach Randolph and Jason Richardson, Coach Tom Izzo, and rapper Snoop Dogg was played in Flint on May 22.  Izzo and Snoop Dogg agreed to return to Flint for the same event in 2017, along with other celebrities, held on May 20. Less famous artists also provided some help.  EyeVLeague and Awakened Cincinnatians activist groups gathered 2,000 bottles of water at a free concert with local Cincinnati artists, such as Kenny Bryant, Joey Mack, Zeebro Blanka, James Frost, and Skep Bam DaVinci. Fight for Flint was a boxing fundraiser at Flint's Dort Federal Event Center featuring Tommy \"The Hitman\" Hearns, along with brothers Andre Dirrell and Anthony Dirrell; Mike Hernandez, Troy Albrine Jr., Rakim Johnson; and female boxers Jackie Kallen, Fatuma Zarika and Alicia Ashley.  It was sponsored by Don Elbaum Promotions and the Catholic Charities of Shiawassee and Genesee Counties. A fundraiser called Fashion For Flint held in late January 2017 helped raised money to purchase 10,000 bottles of water. In January 2017, Cher announced plans to produce and star in a Lifetime TV movie about the water crisis titled \"Flint\", about a fictional Flint woman dealing with the water crisis and how it affects her family.  On March 24, Cher dropped out of the project citing a “serious family issue” as the cause.  The project moved forward with filming scheduled to begin in April 2017.  Queen Latifah and Jill Scott were later cast in the film.  Latifah's production company Flavor Unit Entertainment and her partner Shakim Compere will also produce the film.  Lyndie Greenwood later joined the cast.  It is based on the \"Time Magazine\" cover story \"The Poisoning of an American City\".  It will premiere on October 28, 2017. \"Lead and Copper\", a documentary on the Flint water contamination crisis, is scheduled to be released in 2017.  Producers Michael Nozik and William Hart are working with Oscar-winning director Paul Haggis and writer Steven Leckart on the film.  Flint native and publicist Howard Bragman is also involved. On January 4, 2016, DTV News released \"Undrinkable: The Flint Water Emergency\" about the crisis. On January 30, 2016, IAMHH Temple released \"Flint's Water crisis & A Warning for Humanity\" about the crisis. On February 7, 2016, Independent Underground Radio Network released \"Flint Water Crisis – Real Stories, Real People, Real Life – A Mini Documentary \" about the crisis. On March 8, 2016, the American Civil Liberties Union of Michigan released \"Here's to Flint 2016\" about the water crisis. On June 8, 2016, Russian channel RT Documentary released \"Murky Waters of Flint.  How a whole city was poisoned\" about the crisis. The \"Family Guy\" episode \"The Finer Strings\", originally aired on February 19, 2017, mentions the crisis when Brian and Stewie Griffin are trying to \"temporarily\" blind Carter Pewterschmidt with a spray of \"Flint-water\". On March 8, 2017, WDIV-TV in Detroit aired a documentary called \"Failure In Flint: The Crisis Continues\". On May 31, 2017, the PBS show \"Nova\" aired an episode about the water crisis called \"Poisoned Water\".  The Flint Institute of Arts screened the episode early, on May 23. On June 22, 2016, \"Bridge Magazine\", The Center for Michigan, and Mission Point Press published a book about the crisis called \"Poison on Tap\".  It has been described as a \"riveting, authoritative account of the government blunders, mendacity and arrogance\" that caused the crisis. On April 6, 2016, it was announced Anna Clark, in association with Metropolitan Books, will write a book called \"Water’s Perfect Memory\", which will describe “Flint as a canary-in-the-coal-mine tale of how the underfunding of cities across America imperils the lives if its residents.”  Its publication date hasn't been announced. On January 28, 2016, rapper Jon Connor from Flint released a song titled \"Fresh Water for Flint\" about the crisis and how it has affected his family. Eminem raps about the water crisis in the new Big Sean song \"No Favors\" from the album \"I Decided\". In the spring of 2016, Associate professor of conducting at the University of Colorado Boulder, Andrea Ramsey, in reaction to the Flint water crisis, composed a choral song titled, \"But a Flint Holds Fire\".  Children choirs throughout the country have performed the song.  Many of the lyrics for the piece come from Christina Rossetti's 19th-century poem, titled “Flint.”\n\nWell-being contributing factors Well-being is a much-studied topic in psychology, especially positive psychology.  Related concepts are eudaimonia, happiness, flourishing, quality of life, contentment, and meaningful life. Central theories are Diener's tripartite model of subjective well-being, Ryff's Six-factor Model of Psychological Well-being, Corey Keyes work on flourishing, and Seligmann's contributions to positive psychology and his theories on \"authentic happiness\" and P.E.R.M.A. Positive psychology is concerned with eudaimonia, \"the good life\" or flourishing, living according to what holds the greatest value in life – the factors that contribute the most to a well-lived and fulfilling life.  While not attempting a strict definition of the good life, positive psychologists agree that one must live a happy, engaged, and meaningful life in order to experience \"the good life\".  Martin Seligman referred to \"the good life\" as \"using your signature strengths every day to produce authentic happiness and abundant gratification\".  According to Christopher Peterson, \"eudaimonia trumps hedonism\". Research on positive psychology, well-being, eudaimonia and happiness, and the theories of Diener, Ryff, Keyes and Seligmann cover a broad range of levels and topics, including \"the biological, personal, relational, institutional, cultural, and global dimensions of life.\" There has been a significant focus in past research on adulthood, in regards to well-being and development and although eudaimonia is not a new field of study, there has been little research done in the areas of adolescence and youth.  Research that has been done on this age group had previously explored more negative aspects, such as problem and risk behaviours (i.e. drug and alcohol use). Researchers who conducted a study in 2013 recognized the absence of adolescents in eudaimonic research and the importance of this developmental stage.  Adolescents rapidly face cognitive, social and physical changes, making them prime subjects to study for development and well-being.  The \"eudaimonic identity theory\" was used in their research to examine the development of identity through self-discovery and self-realization.  They emphasize the personal value found in discovering and appeasing one's “\"daimon\"” (daemon) through subjective experiences that develop eudaimonic happiness from aligning with one's true self. Researchers focused their studies on PYD (positive youth development) and the \"eudaimonic identity theory\" in the context of 3 developmental elements: \"self-defining activities\", \"personal expressiveness\" and \"goal-directed behaviours\". They determined that adolescents sample multiple \"self-defining activities\"; these activities aid in identity formation, as individuals choose activities that they believe represents who they are.  These \"self-defining activities\" also help determine the adolescent's social environments.  For example, an adolescent involved in sports, would likely surround themselves with like-minded active and competitive people. \"Personal expressiveness\", as coined by psychologist A. S. Waterman, are the activities that we choose to express and connect with our “\"daimon\"” through subjective experiences. Finally, \"goal-directed behaviours\", are developed through goal setting, where individuals work towards identity establishment.  Adolescents recognize their passions, abilities and talents and aim to fulfill their goals and behave in a way that appeases their true self. The study was conducted in Italy, Chile and the United States, which produced slightly varied outcomes.  Outcomes were contingent on availability, access and choice of opportunities (activities).  Socioeconomic context also affected the results, as not all individuals could access the activities that may be more in-line with their true selves. The Personally Expressive Activities Questionnaire (PEAQ) was used to conduct their study.  Adolescence was the youngest age group that the PEAQ was used on.  The PEAQ asked adolescents to self-report on activities they participate in and describe themselves with \"self-defining activities\".  It was reported that 80% of adolescents defined themselves with two to four \"self-defining activities\" signifying an understanding in adolescence of self-concept through the domains of leisure, work and academia. Leisure activities were found to have the largest impact on individuals because these activities were the most self-directed of the three domains, as adolescents had the choice of activity, and were more likely to be able to align it with their true selves.  The study found that subjective experiences were more important than the activities themselves and that adolescents reported higher levels of well-being.  They reported that when adolescents express themselves through \"self-defining activities\" across multiple domains, that they have a clearer image of themselves, what they want to achieve and higher wellness.  \"Goal-setting\" was found to be a unique predictor; when adolescents work towards goals set by themselves and accomplish them, they are likely to have a clearer emerging identity and higher well-being.  Researchers found that more adolescents were happy when they were involved in self-chosen activities because the activities were chosen in line with their true self. The midlife crisis may mark the first reliable drop in happiness during an average human's life.  Evidence suggests most people generally become happier with age, with the exception of the years 40 – 50, which is the typical age at which a \"crisis\" might occur.  Researchers specify that people in both their 20s and 70s are happier than during midlife, although the extent of happiness changes at different rates.  For example, feelings of stress and anger tend to decline after age 20, worrying drops after age 50, and enjoyment very slowly declines in adulthood but finally starts to rise after age 50. Well-being in late life is more likely to be related to other contextual factors including proximity to death.  However most of this terminal decline in well-being could be attributed to other changes in age-normative functional declines including physical health and function.  Also, there is growing debate that assumptions that a single population estimate of age-related changes in well-being truly reflects the lived experiences of older adults has been questioned.  The use of growth mixture modelling frameworks has allowed researchers to identify homogenous groups of individuals who are more similar to each other than the population based on their level and change in well-being and has shown that most report stable well-being in their late life and in the decade prior to death.  These findings are based on decades of data, and control for cohort groups; the data avoids the risk that the drops in happiness during midlife are due to populations' unique midlife experiences, like a war.  The studies have also controlled for income, job status and parenting (as opposed to childlessness) to try to isolate the effects of age. Researchers found support for the notion of age changes inside the individual that affect happiness. This could be for any number of reasons.  Psychological factors could include greater awareness of one's self and preferences; an ability to control desires and have more realistic expectations – unrealistic expectations tend to foster unhappiness; moving closer to death may motivate people to pursue personal goals; improved social skills, like forgiveness, may take years to develop – the practice of forgiveness seems linked to higher levels of happiness; or happier people may live longer and are slightly overrepresented in the elderly population.  Age-related chemical changes might also play a role. Other studies have found older individuals reported more health problems, but fewer problems overall.  Young adults reported more anger, anxiety, depression, financial problems, troubled relationships and career stress.  Researchers also suggest depression in the elderly is often due largely to passivity and inaction – they recommend people continue to undertake activities that bring happiness, even in old age. The activity restriction model of depressed affect suggests that stressors that disrupt traditional activities of daily life can lead to a decrease in mental health.  The elderly population is vulnerable to activity restriction because of the disabling factors related to age.  Increases in scheduled activity as well as social support can decrease the chances of activity restriction. Over the last 33 years, a significant decrease in women's happiness leads researchers to believe that men are happier than women.  Part of these findings could be due to the way men and women differ in calculating their happiness.  Women calculate the positive self-esteem, closeness in their relationships and religion.  Men calculate positive self-esteem, active leisure and mental control.  Therefore, neither men nor women are at greater risk for being less happy than the other.  Earlier in life, women are more likely than men to fulfill their goals (material goals and family life aspirations), thereby increasing their life satisfaction and overall happiness.  However, it is later in life that men fulfill their goals, are more satisfied with their family life and financial situation and, as a result, their overall happiness surpasses that of women.  Possible explanations include the unequal division of labor within the household, or that women experience more variance (more extremes) in emotion but are \"generally\" happier.  Effects of gender on well-being are paradoxical: men report feeling less happy than women, , however, women are more susceptible to depression. A study was conducted by Siamak Khodarahimi to determine the roles of gender and age on positive psychology constructs – psychological hardiness, emotional intelligence, self-efficacy and happiness – among 200 Iranian adolescents and 200 young adults who were questioned through various tests.  The study found that the males of the sample showed significantly higher rates in psychological hardiness, emotional intelligence, self-efficacy and happiness than females, regardless of age. Happiness is partly genetically based.  Based on twin studies, 50 percent of a given human's happiness level is genetically determined, 10 percent is affected by life circumstances and situation, and a remaining 40 percent of happiness is subject to self-control. Determining whether emotions have a genetic trait or not were studied by David Lykken and Auke Tellegen.  They found that up to 80% of a long-term sense of well-being is due to heredity.  Basically, our families are important to our eventual emotional lives as adults because they provide us with genetic material that largely determines our base emotional responsiveness to the world.  Therefore, genetic makeup is far more important to the long-term quality of our emotional lives than is learned behavior or the quality of our early childhood environment, at least as found in our current socio-economic paradigm.  The remaining theoretical 20%, however, still leaves room for significant change in thoughts and behavior from environmental/learned sources that should not be understated, and the interpretation of variance in twin studies is controversial, even among clinical psychologists. Individual differences in both overall Eudaimonia, identified loosely with self-control, and in the facets of eudaimonia are inheritable.  Evidence from one study supports 5 independent genetic mechanisms underlying the Ryff facets of this trait, leading to a genetic construct of eudaimonia in terms of general self-control, and four subsidiary biological mechanisms enabling the psychological capabilities of purpose, agency, growth, and positive social relations It is generally accepted that happiness is at least in part mediated through dopaminergic, adrenergic and serotonergic metabolism.  A correlation has been found between hormone levels and happiness.  SSRIs, such as Prozac, are used to adjust the levels of serotonin in the clinically unhappy.  Researchers, such as Alexander, have indicated that many peoples usage of narcotics may be the unwitting result of attempts to readjust hormone levels to cope with situations that make them unhappy. A positive relationship has been found between the volume of gray matter in the right precuneus area of the brain and the subject's subjective happiness score.  Meditation based interventions, including mindfulness, have been found to correlate with a significant gray matter increase within the precuneus. Neuroscience and brain imaging have shown increasing potential for helping science understand happiness and sadness.  Though it may be impossible to achieve any comprehensive objective measure of happiness, some physiological correlates to happiness can be measured.  Stefan Klein, in his book \"The Science of Happiness\", links the dynamics of neurobiological systems (i.e., dopaminergic, opiate) to the concepts and findings of positive psychology and social psychology. Nobel prize winner Eric Kandel and researcher Cynthia Fu described very accurate diagnoses of depression just by looking at fMRI brain scans. By identifying neural correlates for emotions, scientists may be able to use methods like brain scans to tell us more about the different ways of being \"happy\".  Richard Davidson has conducted research to determine which parts of the brain are involved in positive emotions.  He found that the left prefrontal cortex is more activated when we are happy and is also associated with greater ability to recover from negative emotions as well as enhanced ability to suppress negative emotions.  Interestingly, Davidson found that people can train themselves to increase activation in this area of their brains.  It is thought that our brain can change throughout our lives as a result of our experiences; this is known as neuroplasticity. The evolutionary perspective offers an alternative approach to understanding happiness and quality of life.  Key guiding questions: What features are included in the brain that allow humans to distinguish between positive and negative states of mind?  How do these features improve humans' ability to survive and reproduce?  The evolutionary perspective claims that the answers to these questions point towards an understanding of what happiness is about and how to best exploit the capacities of the brain with which humans are endowed.  This perspective is presented formally and in detail by the evolutionary biologist Bjørn Grinde in his book \"Darwinian Happiness\". One reason to study and cultivate flourishing is to learn about the antonym of languishing and depression.  Cultivating flourishing does not eliminate depression; instead, depression is on the opposite side of the scale from flourishing.  However, it is important to note how much more beneficial flourishing is in comparison to either depression or languishing, which can be considered as the midpoint between the two concepts. A study by Keyes found that there are major costs of depression, which 14% of adults experience annually: it impairs social roles; it costs billions each year due to work absenteeism, diminished productivity, and healthcare costs; finally, depression accounts for at least one-third of suicides.  Therefore, it is important to study flourishing to learn about what is possible if issues such as depression are tackled and how the ramifications of focusing on the positive make life better not just for one person, but also for others around them. Flourishing has significant positive aspects magnified when compared to languishing adults and when languishing adults are compared to depressed adults, as explained by Keyes.  For example, languishing adults have the same amount of chronic disease as those that are depressed whereas flourishing adults are in exceptionally better physical health.  Languishing adults miss as many days at work as depressed adults and, in fact, visit doctors and therapists more than depressed adults. A strengths-based approach to personal positive change aims to have clinical psychology place an equal weight on both positive and negative functioning when attempting to understand and treat distress.  This rationale is based on empirical findings.  Because positive characteristics interact with negative life events to predict disorder the exclusive study of negative life events could produce misleading results. Interventions focusing on strengths and positive emotions can be as effective in treating disorder as other more commonly used approaches such as cognitive behavioral therapy. Psychologists are looking to use positive psychology to treat patients.  Amy Krentzman discussed positive intervention as a way to treat patients.  She defined positive intervention as a therapy or activity primarily aimed at increasing positive feelings, positive behaviors, or positive cognitions, as opposed to focusing on negative thoughts or dysfunctional behaviors.  A way of using positive intervention as a clinical treatment is to use positive activity interventions.  Positive activity interventions, or PAIs, are brief self-administered exercises that promote positive feelings, thoughts, and behaviors.  Two widely used PAIs are “Three Good Things” and “Best Future Self.”  “Three Good Things” requires a patient to daily document, for a week, three events that went well during the day, and the respective cause, or causes.  “Best Future Self” has a patient “think about their life in the future, and imagine that everything has gone as well as it possibly could.  They have worked hard and succeeded at accomplishing all of their life goals.  Think of this as the realization of all of their life dreams.”  The patient is then asked to write down what they imagined.  These positive interventions have been shown to decrease depression.  Positive psychology seeks to inform clinical psychology of the potential to expand its approach, and of the merit of the possibilities.  Given a fair opportunity, positive psychology might well change priorities to better address the breadth and depth of the human experience in clinical settings. Posttraumatic growth (PTG) is a possible outcome after a traumatic event, besides posttraumatic stress disorder (PTSD).  Following a traumatic event, for instance rape, incest, cancer, attack, or combat, \"it is normal to experience debilitating symptoms of depression and anxiety.\"  A person who shows PTG however, will experience these negative outcomes for a time and then show an increase in well-being, higher than it was before the trauma occurred.  Martin Seligman, a founder of positive psychology, emphasizes that \"arriving at a higher level of psychological functioning than before\" is a key point in PTG.  If instead an individual experiences a depressive period but recovers from an incident and returns to their normal level of psychological functioning, they are demonstrating resilience.  This suggests that in PTG, the trauma acts as a turning point for the person to achieve greater well-being.  Seligman recognizes \"the fact that trauma often sets the stage for growth\" and given the right tools, individuals can make the most of that opportunity.\" When reflecting on a traumatic growth, Seligman suggests using the following five elements to facilitate PTG: understand the response to trauma, reduce anxiety, utilize constructive disclosure, create a trauma narrative, and articulate life principles and stances that are more robust to challenge.  Someone experiencing PTG will achieve elements of Seligman’s \"good life\" theory, including a more meaningful and purposeful valuing of life, improved positive relationships, accomplishment, and a more optimistic and open mindset according to the broaden-and-build theory. The phenomenon of PTG is applicable to many disciplines.  The construct is important not only for just soldiers, emergency responders, and survivors of traumatic events, but average, everyday citizens facing typical adversity.  One way to expose citizens to stories of PTG is through constructive journalism.  Constructive journalism, as defined by PhD student Karen McIntyre at University of North Carolina Chapel Hill, is \"an emerging style of journalism in which positive psychology techniques are applied to news work with the aim of engaging readers by creating more productive news stories, all while maintaining core journalistic functions\".  Cathrine Gyldensted, an experienced reporter with a Masters in applied positive psychology and coauthor of two books, demonstrated that typical news reporting, which is associated with negative valence, harms mood.  Using PTG to focus on victims' strengths and instances of overcoming adversity encourages readers to implement similar ideals in their own lives.  \"So the goal of positive psychology in well-being theory is to measure and to build human flourishing.\"  Combining positive psychology constructs like PTG, PERMA, and \"broaden and build\" with journalism could potentially improve affect and inspire individuals about the benefits of positive psychology. PERMA not only plays a role in our own personal lives but also can be used for public major news stories.  With this model, journalists can instead focus on the positives of a story and ask questions about how conflicts or even tragedies have brought people together, how someone has experienced post-traumatic growth, and more.  News stories then shift the perspective from a victimizing one to an uplifting one.  Positive psychology is slowly but steadily making its way through news reporting via constructive journalism.  PERMA helps journalists ask the right questions to continue that progress by bringing the focus of a potentially negative story to the positives and solutions. New researches are increasingly focusing on the perceptual aspect of well-being.  The research program ‘Understanding Positive Emotions’ under the HSL Human Well-being project at Human Science Lab investigates how material well-being and perceptual well-being works as relative determinants in conditioning our mind for positive emotions. Fredrickson and Losada postulated in 2005 that the ratio of positive to negative affect, known as the critical positivity ratio, can distinguish individuals that flourish from those that do not.  Languishing was characterized by a ratio of positive to negative affect of 2.5.  Optimal functioning or flourishing was argued to occur at a ratio of 4.3.  The point at which flourishing changes to languishing is called the Losada line and is placed at the positivity ratio of 2.9.  Those with higher ratios were claimed to have broader behavioral repertoires, greater flexibility and resilience to adversity, more social resources, and more optimal functioning in many areas of their life.  The model also predicted the existence of an upper limit to happiness, reached at a positivity ratio of 11.5.  Fredrickson and Losada claimed that at this limit, flourishing begins to disintegrate and productivity and creativity decrease.  They suggested as positivity increased, so to \"appropriate negativity\" needs to increase.  This was described as time-limited, practicable feedback connected to specific circumstances, i.e. constructive criticism. This positivity ratio theory was widely accepted until 2013, when Nick Brown, a graduate student in applied positive psychology, co-authored a paper with Alan Sokal and Harris Friedman, showing that the mathematical basis of the paper was invalid.  Fredrickson partially retracted the paper, agreeing that the math may be flawed, but maintaining that the empirical evidence is still valid.  Brown and colleagues insist there is no evidence for the critical positivity ratio whatsoever. Most psychologists focus on a person's most basic emotions.  There are thought to be between seven and fifteen basic emotions.  The emotions can be combined in many ways to create more subtle variations of emotional experience.  This suggests that any attempt to wholly eliminate negative emotions from our life would have the unintended consequence of losing the variety and subtlety of our most profound emotional experiences.  Efforts to increase positive emotions will not automatically result in decreased negative emotions, nor will decreased negative emotions necessarily result in increased positive emotions.  Russell and Feldman Barrett (1992) described emotional reactions as core affects, which are primitive emotional reactions that are consistently experienced but often not acknowledged; they blend pleasant and unpleasant as well as activated and deactivated dimensions that we carry with us at an almost unconscious level. Evidence suggests negative emotions can be damaging.  In an article titled \"The undoing effect of positive emotions\", Barbara Fredrickson et al. hypothesized positive emotions undo the cardiovascular effects of negative emotions.  When people experience stress, they show increased heart rate, higher blood sugar, immune suppression, and other adaptations optimized for immediate action.  If unregulated, the prolonged physiological activation can lead to illness, coronary heart disease, and heightened mortality.  Both lab and survey research substantiate that positive emotions help people under stress to return to a preferable, healthier physiological baseline.  Other research shows that improved mood is one of the various benefits of physical exercise. The broaden-and-build theory of positive emotions suggests positive emotions (e.g. happiness, interest, anticipation) broaden one's awareness and encourage novel, varied, and exploratory thoughts and actions.  Over time, this broadened behavioral repertoire builds skills and resources.  For example, curiosity about a landscape becomes valuable navigational knowledge; pleasant interactions with a stranger become a supportive friendship; aimless physical play becomes exercise and physical excellence.  Positive emotions are contrasted with negative emotions, which prompt narrow survival-oriented behaviors.  For example, the negative emotion of anxiety leads to the specific fight-or-flight response for immediate survival. After several years of researching disgust, Jonathan Haidt, and others, studied its opposite; the term \"elevation\" was coined.  Elevation is a pleasant moral emotion, involving a desire to act morally and do \"good\".  As an emotion it has a biological basis, and is sometimes characterized by a feeling of expansion in the chest or a tingling feeling on the skin. Flourishing, in positive psychology, refers to optimal human functioning.  It comprises four parts: goodness, generativity, growth, and resilience (Fredrickson, 2005).  According to Fredrickson (2005), goodness is made up of: happiness, contentment, and effective performance; generativity is about making life better for future generations, and is defined by “broadened thought-action repertoires and behavioral flexibility”; growth involves the use of personal and social assets; and resilience reflects survival and growth after enduring a hardship (p. 685).  A flourishing life stems from mastering all four of these parts.  Two contrasting ideologies are languishing and psychopathology.  On the mental health continuum, these are considered intermediate mental health disorders, reflecting someone living an unfulfilled and perhaps meaningless life.  Those who languish experience more emotional pain, psychosocial deficiency, restrictions in regular activities, and missed workdays (Fredrickson, 2005). Fredrickson & Losada (2005) conducted a study on university students, operationalizing positive and negative affect.  Based on a mathematical model which has been strongly criticized, and now been formally withdrawn by Fredrickson as invalid, Fredrickson & Losada claimed to have discovered a critical positivity ratio, above which people would flourish and below which they would not.  Although Fredrickson claims that her experimental results are still valid, these experimental results have also been questioned due to poor statistical methodology, and Alan Sokal has pointed out that \"given [Fredrickson and Losada's] experimental design and method of data analysis, no data whatsoever could possibly give any evidence of any nonlinearity in the relationship between \"flourishing\" and the positivity ratio — much less evidence for a sharp discontinuity.\" Another study surveyed a U.S. sample of 3,032 adults, aged 25–74.  Results showed 17.2 percent of adults were flourishing, while 56.6 percent were moderately mentally healthy.  Some common characteristics of a flourishing adult included: educated, older, married and wealthy.  The study findings suggest there is room for adults to improve as less than 20 percent of Americans are living a flourishing life.  (Keyes, 2002). Benefits from living a flourishing life emerge from research on the effects of experiencing a high ratio of positive to negative affect.  The studied benefits of positive affect are increased responsiveness, \"broadened behavioral repertoires\", increased instinct, and increased perception and imagination (Fredrickson, 2005, p. 678).  In addition, the good feelings associated with flourishing result in improvements to immune system functioning, cardiovascular recovery, lessened effects of negative affect, and frontal brain asymmetry (Fredrickson, 2005).  Other benefits to those of moderate mental health or moderate levels of flourishing were: stronger psychological and social performance, high resiliency, greater cardiovascular health, and an overall healthier lifestyle (Keyes, 2007).  The encountered benefits of flourishing suggest a definition: \"[flourishing] people experience high levels of emotional, psychological and social well being due to vigor and vitality, self-determination, continuous self- growth, close relationships and a meaningful and purposeful life\" (Siang-Yang, 2006, p. 70). Psychologists Peter Hills and Michael Argyle developed the Oxford Happiness Questionnaire as a broad measure of psychological well-being.  The approach was criticized for lacking a theoretical model of happiness and for overlapping too much with related concepts such as self-esteem, sense of purpose, social interest, kindness, sense of humor and aesthetic appreciation. \"Happiness\" encompasses different emotional and mental phenomena.  One method of assessment is Ed Diener's Satisfaction with Life Scale.  According to Diener, this five-question survey corresponds well with impressions from friends and family, and low incidence of depression. Rather than long-term, big picture appraisals, some methods attempt to identify the amount of positive affect from one activity to the next.  Scientists use beepers to remind volunteers to write down the details of their current situation.  Alternatively, volunteers complete detailed diary entries each morning about the day before.  An interesting discrepancy arises when researchers compare the results of these short-term \"experience sampling\" methods, with long-term appraisals.  Namely, the latter may not be very accurate; people may not know what makes their life pleasant from one moment to the next.  For instance, parents' appraisals mention their children as sources of pleasure, while \"experience sampling\" indicates parents were not enjoying caring for their children, compared to other activities. Psychologist Daniel Kahneman explains this discrepancy by differentiating between happiness according to the \"experiencing self\" compared to the \"remembering self\": when asked to reflect on experiences, memory biases like the Peak-End effect (e.g. we mostly remember the dramatic parts of a vacation, and how it was at the end) play a large role.  A striking finding was in a study of colonoscopy patients.  Adding 60 seconds to this invasive procedure, Kahneman found participants reported the colonoscopy as \"more\" pleasant.  This was attributed to making sure the colonoscopy instrument was not moved during the extra 60 seconds – movement is the source of the most discomfort.  Thus, Kahneman was appealing to the remembering self's tendency to focus on the end of the experience.  Such findings help explain human error in affective forecasting – people's ability to predict their future emotional states. Humans exhibit a variety of abilities.  This includes an ability of emotional \"Hedonic Adaptation\", an idea suggesting that beauty, fame and money do not generally have lasting effects on happiness (this effect has also been called the Hedonic treadmill).  In this vein, some research has suggested that only recent events, meaning those that occurred within the last 3 months, affect happiness levels. The tendency to adapt, and therefore return to an earlier level of happiness, is illustrated by studies showing lottery winners are no happier in the years after they've won.  Other studies have shown paraplegics are nearly as happy as control groups that are not paralyzed (p. 48), after equally few years.  Daniel Kahneman explains: \"they are not paraplegic full time... It has to do with allocation of attention\".  Thus, contrary to our impact biases, lotteries and paraplegia do not change experiences to as great a degree as we would believe. In a newer study (2007), winning a medium-sized lottery prize had a lasting mental wellbeing effect of 1.4 GHQ points on Britons even two years after the event. Adaptation can be very slow and incomplete process.  Distracting life changes such as the death of a spouse or losing one's job can show measurable changes in happiness levels for several years.  Even the \"adapted\" paraplegics mentioned above did ultimately report lower levels of pleasure (again, they were happier than one would expect, but not fully adapted).  Thus, adaptation is a complex process, and while it \"does\" mitigate the emotional effects of many life events it cannot mitigate them entirely. The happiness set point idea is that most people return to an average level of happiness – or a set point – after temporary highs and lows in emotionality.  People whose set points lean toward positive emotionality tend to be cheerful most of the time and those whose set points tend to be more negative emotionality tend to gravitate toward pessimism and anxiety.  Lykken found that we can influence our level of well-being by creating environments more conductive to feelings of happiness and by working with our genetic makeup.  A reason why subjective well being is for the most part stable is because of the great influence genetics have.  Although the events of life have some effect on subjective well being, the general population returns to their set point In recent large panel studies divorce, death of a spouse, unemployment, disability and similar events have been shown to change the long-term subjective well-being, even though some adaptation does occur and inborn factors affect this. Fujita and Diener found that 24% of people changed significantly between the first five years of the study and the last five years.  Almost one in four people showed changes in their well-being over the years; indeed sometimes those changes were quite dramatic.  Bruce Headey found that 5–6% of people dramatically increased their life satisfaction over a 15- to 20-year period and that the goals people pursued significantly affected their life satisfaction. Two different goals on the continuum are known as nonzero-sum goals and zero-sum goals.  Nonzero-sum goals are associated with greater life satisfaction consisting of commitments to family and friends, social or political involvement, and altruism.  This term implies that the person involved and others can both benefit.  Zero-sum goals are associated with a person who gains advantage at the expense of others, did not promote life satisfaction. In her book \"The How of Happiness\", Sonja Lyubomirsky similarly argued people's happiness varies around a genetic set point.  Diener warns, however, that it is nonsensical to claim that \"happiness is influenced 30–50% by genetics\".  Diener explains that the recipe for happiness for an individual always requires genetics, environment, and behaviour too, so it is nonsensical to claim that an individual's happiness is due to only one ingredient. Only differences in happiness can be attributed to differences in factors.  In other words, Lyubomirsky's research does not discuss happiness in one individual; it discusses differences in happiness between two or more people.  Specifically, Lyubomirsky suggests that 30–40% of the difference in happiness levels is due to genetics (i.e. heritable).  In other words, still, Diener says it makes no sense to say one person's happiness is \"due 50% to genetics\", but it does make sense to say one person's difference in happiness is 50% due to differences in their genetics (and the rest is due to behaviour and environment). Findings from twin studies support the findings just mentioned.  Twins reared apart had nearly the same levels of happiness thereby suggesting the environment is not entirely responsible for differences in people's happiness.  Importantly, an individual's baseline happiness is not \"entirely\" determined by genetics, and not even by early life influences on one's genetics.  Whether or not a person manages to elevate their baseline to the heights of their genetic possibilities depends partly on several factors, including actions and habits.  Some happiness-boosting habits seem to include gratitude, appreciation, and even altruistic behavior.  Other research-based habits and techniques for increasing happiness are discussed on this page. Besides the development of new habits, the use of antidepressants, effective exercise, and a healthier diet have proven to affect mood significantly.  In fact, exercise is sometimes called the \"miracle\" or \"wonder\" drug – alluding to the wide variety of proven benefits it provides. It is worth mentioning that a recent book, \"Anatomy of an Epidemic\", challenges the use of non-conservative usage of medications for mental patients, specially with respect to their long-term positive feedback effects. Yongey Mingyur Rinpoche has said that neuro scientists have found that with meditation, an individual's happiness baseline can change.  and meditation has been found to increase happiness in several studies.A study on Brahma Kumaris Raja yoga meditators showed them having higher happiness (Oxford happiness questionnaire) than the control group. Philip Zimbardo suggests we might also analyze happiness from a \"Time Perspective\".  Zimbardo suggested the sorting of people's focus in life by valence (positive or negative) and also by their time perspective (past, present, or future orientation).  Doing so may reveal some individual conflicts, not over whether an activity is enjoyed, but whether one prefers to risk delaying gratification further.  Zimbardo also believes research reveals an optimal balance of perspectives for a happy life; commenting, our focus on reliving positive aspects of our past should be high, followed by time spent believing in a positive future, and finally spending a moderate (but not excessive) amount of time in enjoyment of the present. Arguably, some people pursue ineffective shortcuts to feeling good.  These shortcuts create positive feelings, but are problematic, in part because of the lack of effort involved.  Some examples of these shortcuts include shopping, drugs, chocolate, loveless sex, and TV.  These are problematic pursuits because all of these examples have the ability to become addictive.  When happiness comes to us so easily, it comes with a price we may not realize.  This price comes when taking these shortcuts is the only way to become happy, otherwise viewed as an addiction.  A review by Amy Krentzman on the Application of Positive Psychology to Substance Use, Addiction, and Recovery Research, identified, in the field of positive psychology, three domains that allow an individual to thrive and contribute to society. One of these, A Pleasant Life, involves good feelings about the past, present, and future.  To tie this with addiction, they chose an example of alcoholism.  Research on positive affect and alcohol showed a majority of the population associates drinking with pleasure.  The pleasure one feels from alcohol is known as somatic pleasure, which is immediate but a short lived sensory delight.  The researchers wanted to make clear pleasure alone does not amount to a life well lived; there is more to life than pleasure.  Secondly, the Engaged Life is associated with positive traits such as strength of character.  A few examples of character strength according to \"Character Strength and Virtues: A Handbook and Classification\" by Seligman and Peterson (2004) are bravery, integrity, citizenship, humility, prudence, gratitude, and hope, all of which are shown in the rise to recovery.  To descend into an addiction shows a lack of character strength; however, rising to recovery shows the reinstatement of character strengths, including the examples mentioned above.  Thirdly, the Meaningful Life is service and membership to positive organizations.  Examples of positive organizations include family, workplace, social groups, and society in general.  Organizations, like Alcoholics Anonymous, can be viewed as a positive organization.  Membership fosters positive affect, while also promoting character strengths, which as seen in the Engaged Life, can aid in beating addiction. Researcher Dianne Hales described an \"emotionally healthy\" person as someone who exhibits: flexibility and adaptability to different circumstances, a sense of meaning and affirmation in life, an \"understanding that the self is not the center of the universe\", compassion and the ability to be unselfish, an increased depth and satisfaction in intimate relationships, and a sense of control over the mind and body. Layard and others show that the most important influence on happiness is mental health. L.M. Keyes and Shane Lopez illustrate the four typologies of mental health functioning: flourishing, struggling, floundering and languishing.  However, complete mental health is a combination of high emotional well-being, high psychological well-being, and high social well-being, along with low mental illness. Although health is part of well-being, some people are able to maintain satisfactory wellbeing despite the presence of psychological symptoms. In 2005 a study conducted by Andrew Steptow and Michael Marmot at University College London, found that happiness is related to biological markers that play an important role in health.  The researchers aimed to analyze whether there was any association between well-being and three biological markers: heart rate, cortisol levels, and plasma fibrinogen levels.  The participants who rated themselves the least happy had cortisol levels that were 48% higher than those who rated themselves as the most happy.  The least happy subjects also had a large plasma fibrinogen response to two stress-inducing tasks: the Stroop test, and tracing a star seen in a mirror image.  Repeating their studies three years later Steptow and Marmot found that participants who scored high in positive emotion continued to have lower levels of cortisol and fibrinogen, as well as a lower heart rate. In Happy People Live Longer (2011), Bruno Frey reported that happy people live 14% longer, increasing longevity 7.5 to 10 years and Richard Davidson's bestseller (2012) \"The Emotional Life of Your Brain\" argues that positive emotion and happiness benefit long-term health. However, in 2015 a study building on earlier research found that happiness has no effect on mortality.  \"This \"basic belief that if you're happier you're going to live longer.  That's just not true.\"  Consistent results are that \"apart from good health, happy people were more likely to be older, not smoke, have fewer educational qualifications, do strenuous exercise, live with a partner, do religious or group activities and sleep for eight hours a night.\" Happiness does however seem to have a protective impact on immunity.  The tendency to experience positive emotions was associated with greater resistance to colds and flu in interventional studies irrespective of other factors such as smoking, drinking, exercise, and sleep. Positive emotional states have a favorable effect on mortality and survival in both healthy and diseased populations.  Even at the same level of smoking, drinking, exercise, and sleep, happier people seem to live longer.  Interventional trials conducted to establish a cause-effect relationship indicate positive emotions to be associated with greater resistance to objectively verifiable colds and flu. There is growing evidence that a diet rich in fruits and vegetables is related to greater happiness, life satisfaction, and positive mood as well.  This evidence cannot be entirely explained by demographic or health variables including socio-economic status, exercise, smoking, and body mass index, suggesting a causal link.  Further studies have found that fruit and vegetable consumption predicted improvements in positive mood the next day, not vice versa.  On days when people ate more fruits and vegetables, they reported feeling calmer, happier, and more energetic than normal, and they also felt more positive the next day. Cross-sectional studies worldwide support a relationship between happiness and fruit and vegetable intake.  Those eating fruits and vegetables each day have a higher likelihood of being classified as “very happy,” suggesting a strong and positive correlation between fruit and vegetable consumption and happiness.  Whether it be in South Korea, Iran, Chile, USA, or UK, greater fruit and vegetable consumption had a positive association with greater happiness, independent of factors such as smoking, exercise, body mass index, and socio-economic factors.  This could be due to the protective benefits from chronic diseases and a greater intake of nutrients important for psychological health. Eudaimonic well-being has been found to be empirically distinguishable from hedonic well-being. Individual roles play a part in cognitive well-being.  Not only does having social ties improve cognitive well-being, it also improves psychological health. Having multiple identities and roles helps individuals to relate to their society and provide the opportunity for each to contribute more as they increase their roles, therefore creating enhanced levels of cognitive well-being.  Each individual role is ranked internally within a hierarchy of salience.  Salience is “...the subjective importance that a person attaches to each identity”. Different roles an individual has have a different impact on their well-being.  Within this hierarchy, higher roles offer more of a source to their well-being and define more meaningfulness to their overall role as a human being. Ethnic identity may play a role in an individual's cognitive well-being.  Studies have shown that “...both social psychological and developmental perspectives suggest that a strong, secure ethnic identity makes a positive contribution to cognitive well-being”.  Those in an acculturated society may feel more equal as a human being within their culture, therefore experiencing increased well-being. Learned optimism refers to development of one's potential for a sanguine outlook.  Optimism is learned as personal efforts and abilities are linked to personally desired outcomes.  In short, it is the belief one can influence the future in tangible and meaningful ways.  Learned optimism contrasts with learned helplessness, which consists of a belief, or beliefs, one has no control over what occurs, and that something external dictates outcomes, e.g., success.  Optimism is learned by consciously challenging negative self talk.  This includes self talk on any event viewed as a personal failure that permanently affects all areas of the person's life. Intrapersonal, or internal, dialogues influence one's feelings.  To illustrate, reports of happiness are correlated with the general ability to \"rationalize or explain\" social and economic inequalities. Hope is a powerful positive feeling, linked to a learned style of goal-directed thinking.  Hope is fostered when a person utilizes both \"pathways thinking\" (the perceived capacity to find routes to desired goals) and \"agency thinking\" (the requisite motivations to use those routes). Author and journalist J.B. MacKinnon suggested the cognitive tool of \"Vertical Agitation\" can assist in avoiding helplessness (e.g., paralysis in the face of earth's many problems).  The concept stemmed from research on denial by sociologist Stanley Cohen.  Cohen explained: in the face of massive problems people tend towards learned helplessness rather than confronting the dissonant facts of the matter.  Vertical Agitation involves focusing on one part of a problem at a time, while holding oneself accountable for solving the problem – all the way to the highest level of government, business and society (such as advocating strongly for something: eco-friendly lightbulbs).  This allows each individual in society to make vital \"trivial\" (read: small) changes, without being intimidated by the work needed to be done as a whole.  Mackinnon added: a piecemeal approach also keeps individuals from becoming too 'holier than thou' (harassing friends and family about \"every\" possible improvement), where widespread practice of Vertical Agitation would lead to much improvement. In his book \"Stumbling on Happiness\", psychologist Daniel Gilbert described research suggesting money makes a significant difference to the poor (where basic needs are not yet met), but has a greatly diminished effect once one reaches middle class (i.e. the Easterlin paradox).  One study found money ceased to aid level of happiness after a person makes over US $75,000 a year, and people overestimate the influence of wealth by 100%.  Professor of Economics Richard Easterlin noted job satisfaction does not depend on salary.  In other words, having extra money for luxuries does not increase happiness as much as enjoying one's job or social network.  Gilbert is thus adamant, people should go to great lengths to figure out which jobs they would enjoy, and to find a way to do one of those jobs for a living (that is, provided one is also attentive to social ties). A more recent study has challenged the Easterlin paradox.  Using recent data from a broader collection of countries, a positive link was found between GDP and well-being; and there was no point at which wealthier countries' subjective well-being ceased to increase.  It was concluded economic growth does indeed increase happiness. Wealth is strongly correlated with life satisfaction but the correlation between money and emotional well-being was weak.  The pursuit of money may lead people to ignore leisure time and relationships, both of which may cause and contribute to happiness.  The pursuit of money at the risk of jeopardizing one's personal relationships and sacrificing enjoyment from one's leisure activities seems an unwise approach to finding happiness. Money, or its hectic pursuit, has been shown to hinder people's savoring ability, or the act of enjoying everyday positive experiences and emotions.  In a study looking at working adults, wealthy individuals reported lower levels of savoring ability (the ability to prolong positive emotion) relative to their poorer peers. Studies have routinely shown that nations are happier when people's needs are met. Some studies suggest, however, people are happier after spending money on experiences, rather than physical things. Lottery winners report higher levels of happiness immediately following the event.  But research shows winner's happiness levels drop and return to normal baseline rates within months to years.  This finding suggests money does not cause long-term happiness (1978).  However, in a more recent British study on lottery prizes between £1,000 and £120,000, a positive effect even two years after the event was found, the return to normal being only partial and varying. In many studies it has been shown that Positive Psychology Interventions (PPI) have produced improvements in well-being and depression even long after the intervention.  However, in one gratitude intervention study the effect was negative for depressed people although in several studies it was positive on non-depressed people.  Moreover, the apparent effect of PPIs cannot be caused by publication bias, according to a meta-analysis on 49 studies (2009).  PPIs studied included producing gratitude letters, performing optimistic thinking, replaying positive life experiences, and socializing with people.  PPIs are narrowly defined as building strengths, not fixing something pathological, and generally studied in nonclinical samples. Also in a newer meta-analysis (39 studies, 6,139 participants, 2012), the outcomes were positive: the standardized mean difference was 0.34 higher for subjective well-being, 0.20 for psychological well-being and 0.23 for depression.  Three to six months after the intervention, the effects for subjective well-being and psychological well-being were still significant, so effects seem fairly sustainable.  However, in high-quality studies the positive effect was weaker, though positive, so authors considered further high-quality studies necessary to strengthen the evidence.  They claimed that the above-mentioned meta-analysis (2009) did not put enough weight on the quality of studies. PPIs found positive included blessings, kindness practices, taking personal goals, showing gratitude focusing on personal strengths to enhance well-being. Positive psychology coaching is the application of positive psychology in the practice of coaching, which is backed by scientific research, with availability of intervention tools and assessments that positive psychology trained coaches can utilize to support the coaching process.  Positive psychology coaching uses scientific evidence and insights gained in these areas to work with clients in their goals. Mindfulness is an intentionally focused awareness of one's immediate experience.  \"Focused awareness\" is a conscious moment-by-moment attention to situational elements of an experience: i.e., thoughts, emotions, physical sensations, and surroundings.  An aim of mindfulness is to become grounded in the present moment; one learns to observe the arising and passing of experience.  One does not judge the experiences and thoughts, nor do they try to \"figure things out\" and draw conclusions, or change anything – the challenge during mindfulness is to simply observe.  Benefits of mindfulness practice include reduction of stress, anxiety, depression, and chronic pain.  See also Reverence (emotion). Ellen J. Langer argued people slip into a state of \"mindlessness\" by engaging in rote behavior, performing familiar, scripted actions without much cognition, as if on autopilot. Advocates of focusing on present experiences also mention research by Psychologist Daniel Gilbert, who suggested daydreaming, instead of a focus on the present, may impede happiness.  Fellow researcher, Matt Killingsworth, found evidence to support the harm of daydreaming.  Fifteen thousand participants from around the world provided over 650 000 reports (using an online application on their phones that requested data at random times).  Killingsworth found people who reported daydreaming soon reported less happiness; daydreaming is extremely common.  Zimbardo (see \"Time Perspectives\" above) bestowed the merits of a present-focus, and recommended occasional recall of past positive experiences.  Reflecting on past positive experiences can influence current mood, and assist in building positive expectations for the future. There is research that suggests a person's focus influences level of happiness, where thinking too much about happiness can be counter-productive.  Rather than asking: \"Am I happy?\"  – which when posed just 4 times a day, starts to decrease happiness, it might well be better to reflect on one's values (e.g., \"Can I muster any hope?\") .  Asking different questions can assist in redirecting personal thoughts, and perhaps, lead to taking steps to better apply one's energies.  The personal answer to any particular question can lead to positive actions, and hopefulness, which is a very powerful, and positive feeling.  Hopefulness is more likely to foster happiness, while feelings of hopelessness tend to undermine happiness. Todd Kashdan, researcher and author of \"Designing Positive Psychology\", explained early science's findings should not be overgeneralized or adopted too uncritically.  Mindfulness to Kashdan is very resource-intensive processing; he warned it is not simply better at all times.  To illustrate, when a task is best performed with very little conscious thought (e.g., a paramedic performing practiced, emergency maneuvers).  Nevertheless, development of the skill lends to its application at certain times, which can be useful for the reasons just described; Professor of Psychology and Psychiatry Richard J. Davidson highly recommends \"mindfulness meditation\" for use in the accurate identification and management of emotions. The easiest and best possible way to increase one's happiness is by doing something that increases the ratio of positive to negative emotions.  Contrary to some beliefs, in many scenarios, people are actually very good at determining what will increase their positive emotions.  There have been many techniques developed to help increase one's happiness. A second technique is known as the \"Sustainable Happiness Model (SHM).\"  This model proposes that long-term happiness is determined upon: (1) one's genetically determined set-point, (2) circumstantial factors, and (3) intentional activities.  Lyubomirsky, Sheldon and Schkade suggest to make these changes in the correct way in order to have long-term happiness.  One final suggestion of how to increase one's happiness is through a procedure called \"Hope Training.\"  Hope Training is primarily focused on hope due to the belief that hope drives the positive emotions of well-being.  This training is based on the hope theory, which states that well-being can increase once people have developed goals and believe themselves to achieve those goals.  One of the main purposes of hope training is to eliminate individuals from false hope syndrome.  False hope syndrome particularly occurs when one believes that changing their behavior is easy and the outcomes of the change will be evidenced in a short period of time. In literature the positive psychological approach to personality is correlated often with the concepts of personal/psychosocial development and human development, balanced, strong, mature and proactive personality, character strengths and virtues, evidenced by traits like optimism and energy, pragmatism, active consciousness, assertiveness, free and powerful will, self-determination and self-realization, personal and social autonomy, social adaptability, personal and social efficiency, interpersonal development and professional development, proactive and positive thinking, humanity, empathy and love, emotional intelligence, subjective/psychological well-being, extraversion, happiness, positive emotions. Ed Diener et al. (1999) suggested this equation: positive emotion – negative emotion = subjective well-being.  Since tendency to positive emotion has a correlation of 0.8 with extroversion and tendency towards negative emotion is indistinguishable from neuroticism, the above equation could also be written as extroversion – neuroticism = happiness.  These two traits could account for between 50% to 75% of happiness.  These are all referring to the Big Five personality traits model of personality. An emotionally stable (the opposite of Neurotic) personality correlates well with happiness.  Not only does emotional stability make one less prone to negative emotions, it also predicts higher social intelligence – which helps to manage relationships with others (an important part of being happy, discussed below). Cultivating an extroverted temperament may correlate with happiness for the same reason: it builds relationships and support groups.  Some people may be fortunate, from the standpoint of personality theories that suggest individuals have control over their long-term behaviors and cognitions.  Genetic studies indicate genes for personality (specifically extroversion, neuroticism and conscientiousness), and a general factor linking all 5 traits, account for the heritability of subjective well-being.  Recent research suggests there is a happiness gene, the 5-HTT gene. Well-being has traditionally focused on improving physical, emotional and mental quality of life with little understanding of how dependent they all are on financial health.  However, financial stress often manifests itself in physical and emotional difficulties that lead to increased healthcare costs and reduced productivity.  A more inclusive paradigm for well-being would acknowledge money as a source of empowerment that maximizes physical and emotional health by reducing financial stress.  Such a model would provide individuals with the financial knowledge they need, as well enable them to gain valuable insight and understanding regarding their financial habits, as well as their thoughts, feelings, fears and attitudes about money.  Through this work, individuals would be better equipped to manage their money and achieve the financial wellness that is essential for their overall well-being. It has been argued that money cannot effectively \"buy\" much happiness unless it is used in certain ways.  \"Beyond the point at which people have enough to comfortably feed, clothe, and house themselves, having more money – even a lot more money – makes them only a little bit happier.\"  \"Spending money on others actually makes us happier than spending it on ourselves\". Purpose in life refers broadly to the pursuit of life satisfaction.  It has also been found that those with high purpose in life scores have strong goals and sense of direction.  They feel there is meaning to their past and present life, and hold beliefs that continue to give their life purpose.  Research in the past has focused on purpose in the face of adversity (what is awful, difficult, or absurd in life).  Recently, research has shifted to include a focus on the role of purpose in personal fulfillment and self-actualization. The self-control approach, as expounded by C. R. Snyder, focusses on exercising self-control to achieve self-esteem by fulfilling goals and feeling in control of our own success.  This is further reinforced by a sense of intentionality in both efforts and outcomes. The intrinsic motivation approach of Viktor Frankl emphasized finding value in three main areas: creative, experiential, and attitudinal.  Creative values are expressed in acts of creating or producing something.  Experiential values are actualized through the senses, and may overlap the hedonistic view of happiness.  Attitudinal values are prominent for individuals who are unable to pursue the preceding two classes of values.  Attitudinal values are believed to be primarily responsible for allowing individuals to endure suffering with dignity. A personal sense of responsibility is required for the pursuit of the values that give life meaning, but it is the realization that one holds sole responsibility for rendering life meaningful that allows the values to be actualized and life to be given true purpose.  Determining what is meaningful for one's self provides a sense of autonomy and control which promotes self-esteem. Purpose in life is positively correlated with education level and volunteerism.  However, it has also been found to decrease with age. Purpose in life is both highly individual, and what specifically provides purpose will change over the course of one's lifetime. All three of the above theories have self-esteem at their core.  Self-esteem is often viewed as the most significant measure of psychological well-being, and highly correlated with many life-regulating skills.  Purpose in life promotes and is a source of self-esteem; it is not a by-product of self-esteem. Self-efficacy refers to a belief that one's ability to accomplish a task is a function of personal effort.  Low self-efficacy, or a disconnect between ability and personal effort, is associated with depression; by comparison, high self-efficacy is associated with positive change, including overcoming abuse, overcoming eating disorders, and maintaining a healthy lifestyle.  High self-efficacy also has positive benefits for one's immune system, aids in stress management, and decreases pain.  A related concept, Personal effectiveness, is primarily concerned with planning and the implementation of methods of accomplishment. According to Bloodworth and McNamee sports and physical activities are a key contributor to the development of people's well-being.  The influence of sports on well-being is conceptualized within a framework which includes impermanence, its hedonistic shallowness and its epistemological inadequacy.  Researching the effect of sport on well-being is difficult as some societies are unable to access sports, a deficiency in studying this phenomenon. Suffering can indicate behavior worthy of change, as well as ideas that require a person's careful attention and consideration.  Generally, psychology acknowledges suffering can not be completely eliminated, but it is possible to successfully manage and reduce suffering.  The University of Pennsylvania's Positive Psychology Center explains: \"Psychology’s concern with remedying human problems is understandable and should certainly not be abandoned.  Human suffering demands scientifically informed solutions.  Suffering and well being, however, are both part of the human condition, and psychologists should be concerned with both.\"  Positive psychology, inspired by empirical evidence, focuses on productive approaches to pain and suffering, as well the importance of cultivating strengths and virtues to keep suffering to a minimum (see also Character strengths and virtues (book)). In reference to the Buddhist saying \"Life is suffering\", researcher and clinical psychologist Jordan Peterson suggested this view as realistic, not pessimistic, where acceptance of the reality life is harsh, provides a freedom from the expectation one should always be happy.  This realization can assist in the management of inevitable suffering.  To Peterson, virtues are important because they provide people with essential tools to escape suffering (e.g., the strength to admit dissonant truths to themselves).  Peterson maintained suffering is made worse by false philosophy (i.e., denial that natural suffering is inevitable). Similarly, Seligman believes positive psychology is \"not a luxury\", saying \"most of Positive Psychology is for all of us, troubled or untroubled, privileged or in privation, suffering or carefree.  The pleasures of a good conversation, the strength of gratitude, the benefits of kindness or wisdom or spirituality or humility, the search for meaning and the antidote to \"fidgeting until we die\" are the birthrights of us all.\" Positive coping is defined as \"a response aimed at diminishing the physical, emotional, and psychological burden that is linked to stressful life events and daily hassles\" It is found that proper coping strategies will reduce the burden of short-term stress and will help relieve long-term stress.  Stress can be reduced by building resources that inhibit or buffer future challenges.  For some people, these effective resources could be physiological, psychological or social. Terror management theory maintains that people suffer cognitive dissonance (anxiety) when they are reminded of their inevitable death.  Through terror management, individuals are motivated to seek consonant elements – symbols which make sense of mortality and death in satisfactory ways (i.e. boosting self-esteem). Research has found that strong belief in religious \"or\" secular meaning systems affords psychological security and hope.  It is moderates (e.g. agnostics, slightly religious individuals) who likely suffer the most anxiety from their meaning systems.  Religious meaning systems are especially adapted to manage anxiety about death or dying because they are unlikely to be disconfirmed (for various reasons), they are all encompassing, and they promise literal immortality. Whether emotional effects are beneficial or adverse seems to vary with the nature of the belief.  Belief in a benevolent God is associated with lower incidence of general anxiety, social anxiety, paranoia, obsession, and compulsion whereas belief in a punitive God is associated with greater symptoms.  (An alternative explanation is that people seek out beliefs that fit their psychological and emotional states.) Citizens of the world's poorest countries are the most likely to be religious, and researchers suggest this is because of religion's powerful coping abilities.  Luke Galen also supports terror management theory as a partial explanation of the above findings.  Galen describes evidence (including his own research) that the benefits of religion are due to strong convictions and membership in a social group. The capacity for loving attachments and relationships, especially with parents, is the strongest predictor of well-being later in life. Seligman writes: \"Unlike money, which has at most a small effect, marriage is robustly related to happiness... In my opinion, the jury is still out on what causes the proven fact married people are happier than unmarried people.\"  (pp. 55–56).  Married persons report higher levels of happiness and well being than single people.  Other data has shown a spouse's happiness depends on the happiness of their partner.  When asked, spouses reported similar happiness levels to each other.  The data also shows the spouses' happiness level fluctuates similarly to one another.  If the husband is having a bad week, the wife will similarly report she had a bad week. There is little data on alternatives like polyamory, although one study stated wife order in polygyny did not have a substantial effect on life or marital satisfaction over all.  This study also found younger wives were happier than older wives. On the other hand, at least one large study in Germany found no difference in happiness between married and unmarried people. Studies have shown that married couples are consistently happier and more satisfied with their life than those who are single.  Some research findings have indicated that marriage is the only real significant bottom-up predictor of life satisfaction for men and women, and that those people who have a higher life satisfaction prior to marriage, tend to have a happier marriage. Surprisingly, there has been a steady decline in the positive relationship between marriage and well-being in the United States since the 1970s.  This decline is due to women reporting being less happy than previously and single men reporting being happier than previously.  Research does exist, however, suggesting that compared to single people, married people have better physical and psychological health and tend to live longer. With this, a two-factor theory of love was developed by Barnes and Sternberg.  This theory is composed of two components: passionate love and companionate love.  Passionate love is considered to be an intense longing for a loved one.  This love is often experienced through joy and sexual fulfillment, or even through rejection.  On the other hand, companionate love is associated with affection, friendship and commitment.  Stutzer and Frey (2006) found that the absence of loneliness and the emotional support that promotes self-esteem are both important aspects that contribute to individual well-being within marriage.  Both passionate and companionate love are the foundations for every variety of love that one may experience.  When passionate and companionate love are compromised in a marital relationship, satisfaction is decreased and the likelihood of divorce increases.  In other words, the lack of positive support and validation increases the risk for divorce. Because of the expansive research done on the significance of social support within a marriage, it is important to understand that this research was inspired by a theory called the attachment theory perspective.  Attachment theory stresses the importance of support and care giving in a relationship for the development of trust and security.  Attachment theory, as conceptualized by Collins and Feeney (2000) is an interpersonal, transactional process that involves one partners caregiving responses. While the mantle of parenting is sometimes held as the necessary path of adulthood, study findings are actually mixed as to whether parents report higher levels of happiness relative to non-parents.  Folk wisdom suggests a child brings partners closer; research has found couples actually become less satisfied after the birth of the first child.  The joys of having a child are overshadowed by the responsibilities of parenthood.  Based on quantitative self-reports, researchers found parents prefer doing almost anything else to looking after their children.  By contrast, parents' self-report levels of happiness are higher than those of non-parents.  This may be due to already happy people having more children than unhappy people.  In addition, it might also be that, in the long-term, having children gives more meaning to life.  One study found having up to three children increased happiness among married couples, but not among other groups with children.  Proponents of Childfreedom maintain this is because one can enjoy a happy, productive life without the trouble of ever being a parent. By contrast, many studies found having children makes parents less happy.  Compared with non-parents, parents with children have lower levels of well-being and life satisfaction.  In addition, parents report more feelings of depression and anxiety than non-parents.  However, when adults without children are compared to empty nest parents, parenthood is positively associated with emotional well being.  People found parenthood to be more stressful in the 1970s than they did in the 1950s.  This is thought to be because of social changes in regards to employment and marital status. Males apparently become less happy after the birth of a child due to added economic pressure and taking on the role of being a parent.  A conflict between partners can arise when the couple does not desire traditional roles, or has an increasing number of roles.  Unequal responsibilities of child-rearing between men and women account for this difference in satisfaction.  Fathers who worked and shared an equal part in child-raising responsibilities were found to be the least satisfied.  Research shows that single parents have higher levels of distress and report more mental health problems than married persons. Researchers implemented the Huta & Ryan Scale: Four Eudaimonic Measurement Questionnaire to analyze the participants eudaimonic motives, through motivation towards activities.  The investigation was conducted on Canadian university undergraduates.  The four eudaimonic pursuits as described by Huta & Ryan are: The study determined that participants derived well-being from eudaimonic pursuits only if their parents had role modeled eudaimonia, but not if their parents had merely verbally endorsed eudaimonia. Studies were also conducted on responsiveness and demandingness.  The studies participants were American university undergraduates.  The terms are described as follows; responsiveness satisfies the basic psychological need for autonomy.  This is relevant to eudaimonia because it supports and implements the values of initiative, effort, and persistence, and integration of one's behaviour's values, and true-self.  Autonomy is an important psychological factor because it provides the individual with independence.  Demandingness cultivates many of the qualities needed for eudaimonia, including structure, self-discipline, responsibility, and vision.  Responsiveness and demandingness are reported to be good aspects of parenting.  The studies report both of these qualities as important factors to well-being. The study addressed parenting style by assessing and using adaptions of Baumrind's Parent Behaviour Rating Interview.  Adaptions of this interview were made into a seventy-five question based survey; participants answered questions organized into fifteen subscales.  The study determined that eudaimonically oriented participants reported their parents had been both demanding and responsive towards them.  A multiple regression showed that demandingness and responsiveness together explained as much as twenty-eight percent of the variance in eudaimonia, this suggests parenting played a major role in the development of this pursuit.  This supported the expectation that eudaimonia is cultivated when parents encourage internal structure, self-discipline, responsibility, and vision, and simultaneously fulfill a child's needs for autonomy.  The research concludes that parents who want their children to experience eudaimonia must firstly themselves \"mentor\" their children in the approaches to attain eudaimonia.  To encourage eudaimonia verbally is not sufficient enough to suffice eudaimonia into adulthood.  Parents must clearly role model eudaimonia for it to truly be present in the child's life. In the article \"Finding Happiness after Harvard\", George Vaillant concluded a study on what aspects of life are important for \"successful living\".  In the 1940s, Arlie Bock, while in charge of the Harvard Health Services, started a study, selecting 268 Harvard students from graduating classes of 1942, '43, and '44.  He sought to identify the aspects of life contributing to \"successful living\".  In 1967, the psychiatrist George Vaillant continued the study, undertaking follow-up interviews to gauge the lives of many of the students.  In 2000, Vaillant again interviewed these students as to their progress in life.  Vaillant observed: health, close relationships, and how participants dealt with their troubles.  Vaillant found a key aspect to successful living is healthy and strong relationships. A widely publicized study from 2008 in the \"British Medical Journal\" reported happiness in social networks may spread from person to person.  Researchers followed nearly 5000 individuals for 20 years in the long-standing Framingham Heart Study and found clusters of happiness and unhappiness that spread up to 3 degrees of separation on average.  Happiness tended to spread through close relationships like friends, siblings, spouses, and next-door neighbors; researchers reported happiness spread more consistently than unhappiness through the network.  Moreover, the structure of the social network appeared to affect happiness, as people who were very central (with many friends, and friends of friends) were significantly happier than those on the network periphery.  People closer with others are more likely to be happy themselves.  Overall, the results suggest happiness can spread through a population like a virus.  Having a best friend buffers one's negative life experiences.  When one's best friend is present Cortisol levels are decreased and feelings of self-worth increase. Neuroeconomist Paul Zak studies morality, oxytocin, and trust, among other variables.  Based on research findings, Zak recommends: people hug others more often to get into the habit of feeling trust.  He explains \"eight hugs a day, you'll be happier, and the world will be a better place\". Recently, Anderson et al. found that sociometric status (the amount of respect one has from face-to-face peer group) is significantly and causally related to happiness as measured by subjective well-being. English poet Thomas Gray said \"Where ignorance is bliss, ’Tis folly to be wise.\"  Research suggests neither a good education nor a high IQ reliably increases happiness. Anders Ericsson argued an IQ above 120 has a decreasing influence on success.  Presumably, IQs above 120 do not additionally cause other happiness indicators like success (with the exception of careers like Theoretical physics, where high IQs are more predictive of success).  Above that IQ level, other factors, like social skills and a good mentor, matter more.  As these relate to happiness, intelligence and education may simply allow one to reach a middle-class level of need satisfaction (as mentioned above, being richer than this seems to hardly affect happiness).  According to the findings of the study, Using Theatrical Concepts for Role-plays with Educational Agents by Klesen, she expresses how role- playing embeds information and educational goals and causes people to learn unintentionally.  Studies has shown that enjoyment in things as simple as role playing increases a person's IQ and their happiness. Martin Seligman has said: \"As a professor, I don't like this, but the cerebral virtues—curiosity, love of learning—are less strongly tied to happiness than interpersonal virtues like kindness, gratitude and capacity for love.\" John White (2013) investigated the educational goals at public schools in Britain.  School-education involves both cognitive and conceptual learning, but also the development social skills and personal development.  Ideally, children develop self-confidence, and create purpose for themselves.  According to White, in the past schools only focused on knowledge and education but now Britain has moved to a broader direction.  White's Every Child Matters initiative seeks to enhance children's well-being across the range of children's services. As a basic building block to a better existence, positive psychology aims to improve the quality of experiences.  Within its framework, students could learn to become excited about physical activity.  Playing comes natural to children; positive psychology seeks to preserve this zest (a sense of excitement and motivation for life) for movement in growing and developing children.  If offered in an interesting, challenging and pleasurable way physical activity would thus internalize an authentic feeling of happiness in students.  Positive psychology's approach to physical activity could give students the means of acquiring an engaged, pleasant and meaningful life. Positive psychology is beneficial to schools and students because it encourages individuals to strive to do their best, whereas scolding has the opposite effect.  Clifton and Rath discussed research conducted by Dr. Elizabeth Hurlock in 1925, where fourth, fifth and sixth graders were either praised, criticized or ignored, based on their work on math problems.  Praised students improved by 71%, those criticized improved by 19%, and students provided with no feedback improved a mere 5%.  Praise seems an effective method of fostering improvement. According to Clifton and Rath ninety nine out of one hundred people prefer the influence of positive people.  The benefits include: increased productivity, and contagious positive emotions, which assists one in working to the best of her, or his, abilities.  Even a single negative person can ruin the entire positive vibe in an environment.  Clifton and Rath cited ‘positive emotions as an essential daily requirement for survival’. In 2008, in conjunction with the Positive Psychology Center at the University of Pennsylvania, a whole-of-school implementation of Positive Psychology was undertaken by Geelong Grammar School (Victoria, Australia).  This involved training of teaching staff in the principles and skills of positive psychology.  Ongoing support was provided by The Positive Psychology Center staff, who remained in-residence for the entire year. Staats, Hupp and Hagley (2008) used positive psychology to explore academic honesty.  They identified positive traits displayed by heroes, then determined if the presence of these traits in students predicted future intent to cheat.  The results of their research: ‘an effective working model of heroism in the context of the academic environment’ (Staats, Hupp & Hagley, 2008). According to a study reported in the \"NY Post\" newspaper, 48% of parents reward their children's good grades with cash or something else of meaning.  Among many families in the United States, this is controversial.  Although psychology experts support the offer of reward for good behavior as a better alternative than the use of punishment for bad behavior, in some circumstances, families cannot afford to give their children an average of 16 dollars for every good grade earned.  Alternatives for money include allowing a child extra time on a computer or staying up later than usual.  Some psychology experts believe the best reward is praise and encouragement because material rewards can cause long-term negative effects for children. A study, regarding rewards for children, conducted in 1971 by psychologist, Edward L. Deci, at the University of Rochester, is still referenced today.  Featured in the \"New York Times\", it focused on the short- and long-term effects of rewards for positive behavior.  Deci suggested rewards for positive behavior is an effective incentive for only a short period.  At the outset, rewards can support motivation to work hard and strive towards personal goals.  However, once rewards cease, children showed less interest in the task relative to participants who never received rewards.  Deci pointed out, at a young age, children's natural instinct is to resist people who try to control their behavior, which he cited as support for his conclusion rewards for good behavior have limited effectiveness. In contrast, the \"New York Times\" featured research findings that supported the merits of offering rewards to children for good behavior.  Expert economists argued children experiencing trouble with their behavior or schoolwork should have numerous helpful options, including rewards.  Although children might well experience an initial attraction to financial or material, a love for learning could develop subsequently.  Despite the controversy regarding the use of rewards, some experts believe the best way to motivate a child is to offer rewards at the beginning of the school year, but if unsuccessful they recommend teachers and parents stop using the reward system.  Because of individual differences among children, no one method will work for everyone.  Some children respond well to the use of rewards for positive behavior, while others evidence negative effects.  The results seem to depend on the person. Positive Youth Development focuses on the promotion of healthy development rather than viewing youth as prone to problems needing to be addressed.  This is accomplished through programs and efforts by communities, schools, and government agencies. It has been argued that happiness at work is one of the driving forces behind positive outcomes at work, rather than just being a resultant product. Despite a large body of positive psychological research into the relationship between happiness and productivity, happiness at work has traditionally been seen as a potential by-product of positive outcomes at work, rather than a pathway to success in business.  However a growing number of scholars, including Boehm and Lyubomirsky, argue that it should be viewed as one of the major sources of positive outcomes in the workplace. A practical application of positive psychology is to assist individuals and organizations in identifying strengths so as to increase and sustain well-being.  Therapists, counselors, coaches, various psychological professionals, HR departments, business strategists, and others, are using new methods and techniques to broaden and build upon the strengths of a wide population of individuals.  This includes those not suffering from mental illness or disorder. Positive psychology has been implemented in business management practice, but has faced challenges.  Wong & Davey (2007) noted managers can introduce positive psychology to a workplace, but they might struggle with positive ways to apply it to employees.  Furthermore, for employees to welcome and commit to positive psychology, its application within an organization must be transparent.  Managers must also understand the implementation of positive psychology will not necessarily combat any commitment challenges that exist.  However, with its implementation employees might become more optimistic and open to new concepts or management practices. In their article \"The Benefits of Frequent Positive Affect: Does Happiness Lead to Success?\" , S. Lyubomirsky et al. report: \"Study after study shows that happiness precedes important outcomes and indicators of thriving, including fulfilling and productive work\". Positive psychology, when applied correctly, can provide employees with a greater opportunity to use skills and vary work duties.  However, changing work conditions and roles can lead to stress among employees if they are improperly supported by management.  This is particularly true for employees who must meet the expectations of organizations with unrealistic goals and targets.  Thomas and Tasker (2010) showed less worker autonomy, fewer opportunities for development, less-enriched work roles, and lower levels of supervisor support reflected the effect of industry growth on job satisfaction. Can an organization implement positive change?  Lewis et al. (2007) developed appreciative inquiry (AI), which is an integrated, organizational-level methodology for approaching organizational development.  Appreciative inquiry is based on the generation of organizational resourcefulness, which is accomplished by accessing a variety of human psychological processes, such as: positive emotional states, imagination, social cohesion, and the social construction of reality. A relatively new practice in the workplace is recruiting and developing people based on their strengths (what they love to do, are naturally good at and energises them).  Standard Chartered Bank pioneered this approach in the early 2000s.  More and more organisations are realising the benefit of recruiting people who are in their element in the job as opposed to simply having the right competencies for the job.  Aviva, Morrisons (a large UK supermarket) and Starbucks have all adopted this approach. Psychologist Howard Gardner has extensively researched the merit of undertaking good work at one's job.  He suggested young generations (particularly in the United States) are taught to focus on the selfish pursuit of money for its own sake, although having money does not reliably engender happiness.  Gardner's proposed alternatives loosely follow the pleasant/good/meaningful life classifications outlined above; he believes young people should be trained to pursue excellence in their field, as well as engagement (see flow, above) in accordance with their moral belief systems. Traditional work with offenders has focused on their deficits (e.g., with respect to socialization, and schooling) and other \"criminogenic\" risk-factors.  Rehabilitation more often than not has taken the form of forced treatment or training, ostensibly for the good of the offender, and the community.  Arguably, this approach has shortcomings, suggesting a need to make available additional positive options to treatment staff so they can best assist offenders, and so that offenders can better find their way forward.  Positive psychology has made recent inroads with the advent of the \"Good Lives Model\", developed by Tony Ward, Shadd Maruna, and others.  With respect to rehabilitation: \"Individuals take part ... because they think that such activities might either improve the quality of their life (an intrinsic goal) or at least look good to judges, parole boards and family members (an extrinsic goal).\" Positive criminology and positive victimology are conceptual approaches, developed by the Israeli criminologist Natti Ronel and his research team, that follow principles of positive psychology and apply them into the fields of criminology and victimology, respectively.  Positive criminology and victimology both place an emphasis on social inclusion and on unifying and integrating forces at individual, group, social and spiritual levels that are associated with the limiting of crime and recovery from victimization.  In traditional approaches the study of crime, violence and related behaviors emphasizes the negative aspects in people's lives that are associated with deviance, criminality and victimization.  A common understanding is that human relationships are affected more by destructive encounters than by constructive or positive ones.  Positive criminology and victimology argue that a different approach is viable, based on three dimensions – social integration, emotional healing and spirituality – that constitute positive direction indicators. In economics, the term well-being is used for one or more quantitative measures intended to assess the quality of life of a group, for example, in the capabilities approach and the economics of happiness.  As with the related cognate terms 'wealth' and 'welfare', economics sources often contrast the state with its opposite.  The study of well-being is divided into subjective well-being and objective well-being. Living in an environment with more green spaces is associated with higher well-being, partly due to the beneficial effects on psychological relaxation, stress alleviation, increased physical activity, and reduced exposure to air pollutants and noise, among others. Psychologists in the happiness community feel politics should promote population happiness.  Politics should also consider level of human happiness among future generations, concern itself with life expectancy, and focus on the reduction of suffering.  Based on political affiliation, some studies argue conservatives, on average, are happier than liberals.  A potential explanation is greater acceptance of income inequalities in society leads to a less worried nature.  Luke Galen, Associate Professor of Psychology at Grand Valley State University, mentioned political commitments as important because they are a sort of secular world view that, like religion, can be generally beneficial to coping with death anxiety (see also Terror management theory and religion and happiness). People base their own well-being in relation to their environment and the lives of others around them.  Well-being is also subject to how one feels other people in their environment perceive them, whether that positively or negatively.  Whether or not other cultures are subject to internal culture appraisal is based on that culture's type.  According to Diener and Suh, Collectivistic cultures are more likely to use norms and the social appraisals of others in evaluating their subjective well-being, whereas those [individualistic] societies are more likely to heavily weight the internal [frame of reference] arising from one’s own happiness. Various cultures have various perspectives on the nature of positive human functioning.  For example, studies on aversion to happiness, or fear of happiness, indicates that some individuals and cultures are averse to the experience of happiness, because they believe happiness may cause bad things to happen.  Empirical evidence indicates that there are fundamental differences in the ways well-being is construed in Western and non-Western cultures, including the Islamic and East Asian cultures.  Exploring various cultural perspectives on well-being, Joshanloo (2014) identifies and discusses six broad differences between Western and non-Western conceptions of well-being.  For example, whereas Western cultures tend to emphasize the absence of negative emotions and autonomy in defining well-being, Eastern cultures tend to emphasize virtuous or religious activity, self-transcendence, and harmony. Eunkook M. Suh (University of California) and Shigehiro Oishi (University of Minnesota; now at University of Virginia) examined the differences of happiness on an international level and different cultures' views on what creates well-being and happiness.  In a study, of over 6,000 students from 43 nations, to identify mean life satisfaction, on a scale of 1–7, the Chinese ranked lowest at 3.3; and Dutch scored the highest at 5.4.  When asked how much subjective well-being was ideal, Chinese ranked lowest at 4.5, and Brazilians highest at 6.2, on a scale of 1–7.  The study had three main findings: (1) People living in individualistic, rather than collectivist, societies are happier; (2) Psychological attributes referencing the individual are more relevant to Westerners; (3) Self-evaluating happiness levels depend on different cues, and experiences, from one's culture. The results of a study by Chang E. C. showed that Asian Americans and Caucasian Americans have similar levels of optimism but Asian Americans are far more pessimistic than Caucasian Americans.  However, there were no major differences in depression across cultures.  On the other hand, pessimism was positively linked to problem solving behaviors for Asian Americans, but was negatively linked for Caucasian Americans. Religiousness and spirituality are closely related but distinct topics.  Religion is any organized, and often institutionalized, system of cultural practices and beliefs pertaining to the meaning of human existence.  It occurs within a traditional context such as a formal religious institution.  Spirituality, on the other hand, is a general term applied to the process of finding meaning and a better understanding of one's place in the universe.  It is the individual or collective search for that which is sacred or meaningful in life.  One may therefore be religious but not spiritual, and vice versa. There have been some studies of how religion relates to happiness.  Causal relationships remain unclear, but more religion is seen in happier people.  Consistent with PERMA, religion may provide a sense of meaning and connection to something bigger, beyond the self.  Religion may also provide community membership and hence relationships.  Another component may have to do with ritual. Religion and happiness have been studied by a number of researchers, and religion features many elements addressing the components of happiness, as identified by positive psychology.  Its association with happiness is facilitated in part by the social connections of organized religion, and by the neuropsychological benefits of prayer and belief. There are a number of mechanisms through which religion may make a person happier, including social contact and support that result from religious pursuits, the mental activity that comes with optimism and volunteering, learned coping strategies that enhance one's ability to deal with stress, and psychological factors such as \"reason for being.\"  It may also be that religious people engage in behaviors related to good health, such as less substance abuse, since the use of psychotropic substances is sometimes considered abuse. The \"Handbook of Religion and Health\" describes a survey by Feigelman (1992) that examined happiness in Americans who have given up religion, in which it was found that there was little relationship between religious disaffiliation and unhappiness.  A survey by Kosmin & Lachman (1993), also cited in this handbook, indicates that people with no religious affiliation appear to be at greater risk for depressive symptoms than those affiliated with a religion.  A review of studies by 147 independent investigators found, \"the correlation between religiousness and depressive symptoms was -.096, indicating that greater religiousness is mildly associated with fewer symptoms.\" The Legatum Prosperity Index reflects the repeated finding of research on the science of happiness that there is a positive link between religious engagement and well-being: people who report that God is very important in their lives are on average more satisfied with their lives, after accounting for their income, age and other individual characteristics. Surveys by Gallup, the National Opinion Research Centre and the Pew Organisation conclude that spiritually committed people are twice as likely to report being \"very happy\" than the least religiously committed people.  An analysis of over 200 social studies contends that \"high religiousness predicts a lower risk of depression and drug abuse and fewer suicide attempts, and more reports of satisfaction with sex life and a sense of well-being. However, the links between religion and happiness are always very broad in nature, highly reliant on scripture and small sample number.  To that extent there is a much larger connection between religion and suffering (Lincoln 1034).\"  And a review of 498 studies published in peer-reviewed journals concluded that a large majority of them showed a positive correlation between religious commitment and higher levels of perceived well-being and self-esteem and lower levels of hypertension, depression, and clinical delinquency.  A meta-analysis of 34 recent studies published between 1990 and 2001 found that religiosity has a salutary relationship with psychological adjustment, being related to less psychological distress, more life satisfaction, and better self-actualization.  Finally, a recent systematic review of 850 research papers on the topic concluded that \"the majority of well-conducted studies found that higher levels of religious involvement are positively associated with indicators of psychological well-being (life satisfaction, happiness, positive affect, and higher morale) and with less depression, suicidal thoughts and behaviour, drug/alcohol use/abuse.\" However, there remains strong disagreement among scholars about whether the effects of religious observance, particularly attending church or otherwise belonging to religious groups, is due to the spiritual or the social aspects—i.e. those who attend church or belong to similar religious organizations may well be receiving only the effects of the social connections involved.  While these benefits are real enough, they may thus be the same one would gain by joining other, secular groups, clubs, or similar organizations. Religiousness has often been found to correlate with positive health attributes.  People who are more religious show better emotional well-being and lower rates of delinquency, alcoholism, drug abuse, and other social problems. Six separate factors are cited as evidence for religion's effect on well-being: religion (1) provides social support, (2) supports healthy lifestyles, (3) promotes personality integration, (4) promotes generativity and altruism, (5) provides unique coping strategies, and (6) provides a sense of meaning and purpose.  Many religious individuals experience emotions that create positive connections among people and allow them to express their values and potential.  These four emotions are known as \"sacred emotions,\" which are said to be (1) gratitude and appreciation, (2) forgiveness, (3) compassion and empathy, and (4) humility. Social interaction is necessarily a part of the religious experience.  Religiosity has been identified to correlate positively with prosocial behavior in trauma patients, and prosocial behavior is furthermore associated with well-being.  It also has stronger associations with well-being in individuals genetically predisposed towards social sensitivity in environments where religion prioritizes social affiliation.  It has also been linked to greater resilience against stress as well as higher measures of self-actualization and success in romantic relationships and parental responsibilities. These benefits, while being correlational, may come about as a result of becoming more religiously involved.  The benefit of having a secure social group likely plays a key part in religion's positive effects.  One form of Christian counseling uses religion through talk therapy and assessments to promote mental health.  In another instance, people who were not Buddhist, but were exposed to Buddhist concepts, scored higher on measures of outgroup acceptance and prosociality.  This effect was found not only in Western countries, but also in places where Buddhism is prevalent, indicating a general association of Buddhism with acceptance.  This finding seems to indicate that merely encountering a religious belief system such as Buddhism may allow some of its effects to be transferred to nonbelievers. However, many disagree that the benefits the religious experience are due to their beliefs, and some find there to be no conclusive psychological benefits of belief at all.  For example, the health benefit that the elderly gain from going to church may in fact be the\" \"reason they are able to go to church; the less healthy cannot leave their homes.  Meta analysis has found that find studies purporting the beneficial results of religiosity often fail to fully represent data correctly due to a number of issues such as self-report bias, the use of inappropriate comparison groups, and the presence of criterion contamination.  Other studies have disputed the efficacy of intercessory prayer positively affecting the health of those being prayed for.  They have shown that, when scientifically rigorous studies are performed (by randomizing the patients and preventing them from knowing that they are being prayed for), there is no discernible effect. Religion has power as a cohesive social force, and whether or not it is always beneficial is debated.  Irrespective of a group's beliefs, many find that simply belonging to a tight social group reduces anxiety and mental health problems.  In addition, there may be a degree of self-selectivity amongst the religious; the behavioral benefits they display may simply be common aspects of those who choose to or are able to practice religion.  As a result, whether or not religion can be prescribed scientifically as a means of self-betterment is unclear. While religion is often formalised and community-oriented, spirituality tends to be individually based and not as formalised.  In a 2014 study, 320 children, ages 8–12, in both public and private schools, were given a Spiritual Well-Being Questionnaire assessing the correlation between spirituality and happiness.  Spirituality – and not religious practices (praying, attending church services) – correlated positively with the child's happiness; the more spiritual the child was, the happier the child was.  Spirituality accounted for about 3–26% of the variance in happiness. Meditation has been found to lead to high activity in the brain's left prefrontal cortex, which in turn has been found to correlate with happiness. A study using the Oxford happiness questionnaire on Brahma Kumaris Raja yoga meditators showed them having higher happiness than the control group.  Yongey Mingyur Rinpoche has said that neuro scientists have found that with meditation, an individual's happiness baseline can change. Many people describe themselves as both religious and spiritual, but spirituality represents just one particular function of religion.  Spirituality as related to positive psychology can be defined as \"a search for the sacred\".  What is defined as sacred can be related to God, life itself, or almost any other facet of existence.  It simply must be viewed as having spiritual implications which are transcendent of the individual.  Spiritual well-being addresses this human need for transcendence and involves social as well as existential well-being.  Spiritual well-being is associated with various positive outcomes such as better physical and psychological well-being, lower anxiety, less depression, self-actualization, positive relationships with parents, higher rates of positive personality traits and acceptance.  Researchers have cautioned to differentiate between correlative and causal associations between spirituality and psychology. Reaching the sacred as a personal goal, also called spiritual striving, has been found to correlate highest with well-being compared to other forms of striving.  This type of striving can improve a sense of self and relationships and creates a connection to the transcendent Additionally, multiple studies have shown that self-reported spirituality is related to lower rates of mortality and depression and higher rates of happiness. Currently, most research on spirituality examines ways in which spirituality can help in times of crisis.  Spirituality has been found to remain constant when experiencing traumatic events and/or life stressors such as accidents, war, sickness, and death of a loved one.  When confronted with an obstacle, people might turn to prayer or meditation.  Coping mechanisms involving spirituality include meditative meditation, creating boundaries to preserve the sacred, spiritual purification to return to the righteous path, and spiritual reframing which focuses on maintaining belief.  One clinical application of spirituality and positive psychology research is the \"psychospiritual intervention,\" which represents the potential that spirituality has to increase well-being.  These coping mechanisms that aim to preserve the sacred have been found by researchers to increase well being and return the individual back to the sacred. Overall, spirituality is a process that occurs over a lifetime and includes searching, conserving, and redefining what is sacred in an extremely individualized manner.  It does not always have a positive effect and in fact has been associated with very negative events and life changes.  Research is lacking in spirituality but it is necessary because spirituality can assist in enhancing the experiences of the uncontrollable parts of life. Some policy analysts, citing positive psychology, propose replacing the gross domestic product with gross national happiness as the predominant measure of a nation's success. Much research has pointed at the rising rates of depression, leading people to speculate that modernization may be a factor in the growing percentage of depressed people.  One study found that women in urban America were much more likely to experience depression than those in rural Nigeria.  Other studies have found a positive correlation between a country's GDP per capita, as quantitative measure of modernization, and lifetime risk of a mood disorder trended toward significance (p=0.06). Many people believe it is the increased number of pressures and expectations, increased isolation, increased individualism, and increased inactivity that contribute to higher rates of depression in modern societies. Some evidence suggests sunnier climates do not predict happiness.  In one study both Californians and Midwesterners expected the former's happiness ratings to be higher due to a sunnier environment.  In fact, the Californian and Midwestern happiness ratings did not show a significant difference.  Other researchers say the necessary minimum daily dose of sunlight is as little as 30 minutes. That is not to say weather is never a factor for happiness.  Perhaps the changing norms of sunlight cause seasonal affective disorder, which undermines level of happiness. Positive psychology research and practice is currently conducted and developed in various countries throughout the world.  To illustrate, in Canada, Charles Hackney of Briercrest College applies positive psychology to the topic of personal growth through martial arts training; Paul Wong, president of the International Network on Personal Meaning, is developing an existential approach to positive psychology.  This existential positive psychology approach has been developed into second wave positive psychology (PP 2.0). The research program ‘Understanding Positive Emotions’ at Human Science Lab, London, investigates how material well-being and perceptual well-being work as relative determinants in conditioning our mind for positive emotions. Cognitive and behavioral change, although sometimes slight and complex, can produce an 'intense affect'.  The benefits argue for this focus becoming a legitimate area of study, specifically regarding links in cognition and motivational responses. Isen (2009) remarked, further progress requires suitable research methods, and appropriate theories on which to base contemporary research. Chang (2008) suggested researchers have a number of paths to pursue regarding the enhancement of emotional intelligence, even though emotional intelligence does not guarantee the development of positive affect; in short, more study is required to track the gradient of positive affect in psychology."
  }
]
