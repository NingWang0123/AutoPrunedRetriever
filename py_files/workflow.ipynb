{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/causal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": True,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.4,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"YES_NO\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_26116/3148360877.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        return parser.question_to_graph(question)\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        return parser.question_to_causal_graph(question)\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "        related_triples = doc0.page_content.strip()\n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "            f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    # 2) current question + triples (optional)\n",
    "    triples_text = \"\"\n",
    "    if relations and CONFIG.get(\"include_current_triples\", True):\n",
    "        triples_text = \"\\n\".join(\n",
    "            f\"{u} -> {d.get('rel','related_to')} -> {v}\"\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        )\n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    if triples_text.strip():\n",
    "        q_block += f\"\\n[CURRENT QUESTION'S GRAPH TRIPLES]:\\n{triples_text}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    # 3) task instructions (placed at the end)\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    rules = (\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    sections.append(rules)\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db = None,\n",
    "    prompt = None\n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if faiss_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)  # Number of docs to retrieve\n",
    "        _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "        \n",
    "    if prompt == None:\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    # If return_full_text=False → only new content; else trim prefix\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    # Normalize YES/NO case\n",
    "    if CONFIG.get(\"answer_mode\", \"YES_NO\"):\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        a = answer.strip().lower()\n",
    "        if \"yes\" in a and \"no\" not in a:\n",
    "            answer = yes\n",
    "            print(answer)\n",
    "            return answer\n",
    "        elif \"no\" in a and \"yes\" not in a:\n",
    "            answer = no\n",
    "            print(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    \n",
    "    \n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        G, rels = parse_question_to_graph_generic(parser, q)\n",
    "        text = build_relationship_text(q, G, rels)  # Output [QUESTION][GRAPH][TRIPLES]\n",
    "\n",
    "        # Get LLM answer\n",
    "        answer = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": G.number_of_nodes(),\n",
    "            \"num_edges\": G.number_of_edges(),\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            metadata[\"prompt_snapshot\"] = make_graph_qa_prompt(q, G, rels)\n",
    "\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    vectordb = FAISS.from_documents(docs, emb)\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691a20c",
   "metadata": {},
   "source": [
    "### Answer questions in bulk and load them into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce04b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "FAISS index ready. docs=10\n"
     ]
    }
   ],
   "source": [
    "# 1) Parser\n",
    "parser = RelationshipGraphParser()   # or CausalQuestionGraphParser()\n",
    "\n",
    "# 2) Load Phi-4-mini-reasoning\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                # Automatically select appropriate precision\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,           # Control randomness\n",
    ")\n",
    "\n",
    "# 3) Question set\n",
    "questions = [\n",
    "    \"Is the Earth round?\",\n",
    "    \"Does the Sun rise in the east?\",\n",
    "    \"Is Paris the capital of France?\",\n",
    "    \"Do humans need oxygen to survive?\",\n",
    "    \"Is the Moon a natural satellite of Earth?\",\n",
    "    \"Is the Sahara Desert in South America?\"  ,   # 它在非洲\n",
    "    \"Is the Amazon River longer than the Nile River?\", # 最长的是尼罗河\n",
    "    \"Is Tokyo the capital of South Korea?\"    ,     # 韩国首都是首尔\n",
    "    \"Do penguins live in the Arctic?\"   ,            # 在南极而非北极\n",
    "    \"Is gold heavier than lead?\" ,   \n",
    "]\n",
    "\n",
    "\n",
    "# 4) Build documents (including LLM answers in metadata)\n",
    "docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe, add_prompt_snapshot=False\n",
    ")\n",
    "\n",
    "# 5) Vectorization & Save\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_db = FAISS.from_documents(docs, emb)\n",
    "faiss_db.save_local(\"graph_rag_faiss_index\")\n",
    "print(f\"FAISS index ready. docs={len(docs)}\")\n",
    "\n",
    "\n",
    "# To load later:\n",
    "# faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cf0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Is the Earth round?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698682}, page_content='Earth -> isa -> round'), Document(metadata={'graph_id': 'Q2', 'question': 'Does the Sun rise in the east?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698683}, page_content='rise -> prep_in -> east\\nSun -> subj -> rise'), Document(metadata={'graph_id': 'Q3', 'question': 'Is Paris the capital of France?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': None, 'created_at': 1755698694}, page_content='Paris -> isa -> capital'), Document(metadata={'graph_id': 'Q4', 'question': 'Do humans need oxygen to survive?', 'num_nodes': 4, 'num_edges': 3, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698694}, page_content='humans -> subj -> need\\nneed -> obj -> oxygen\\noxygen -> subj -> survive'), Document(metadata={'graph_id': 'Q5', 'question': 'Is the Moon a natural satellite of Earth?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698695}, page_content='Moon -> isa -> natural satellite'), Document(metadata={'graph_id': 'Q6', 'question': 'Is the Sahara Desert in South America?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': None, 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q7', 'question': 'Is the Amazon River longer than the Nile River?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q8', 'question': 'Is Tokyo the capital of South Korea?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q9', 'question': 'Do penguins live in the Arctic?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='live -> prep_in -> Arctic\\npenguins -> subj -> live'), Document(metadata={'graph_id': 'Q10', 'question': 'Is gold heavier than lead?', 'num_nodes': 4, 'num_edges': 3, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698699}, page_content='gold -> property -> heavy than lead\\ngold -> isa -> heavy\\ngold -> subj -> lead')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94036c",
   "metadata": {},
   "source": [
    "### Test for answering individual questions (adjust prompt with no database context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a04bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n",
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "parser = RelationshipGraphParser()   #\n",
    "\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                #\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "questions = \"Is the Great Wall visible from low Earth orbit?\"\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "#answer = answer_with_llm(questions, gen_pipe, parser)\n",
    "answer = answer_with_llm(questions, gen_pipe, parser, faiss_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400a67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def _normalize_yesno(text: str) -> str:\n",
    "    \"\"\"\n",
    "    归一化 LLM 输出到 'YES'/'NO' 两值（严格外部判断已做，这里再兜底）\n",
    "    任何非明确 'yes'/'no' 的返回都标成 'NO'（与你 prompt 里 '不确定选 NO' 对齐）\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\":\n",
    "        return \"YES\"\n",
    "    if t == \"no\":\n",
    "        return \"NO\"\n",
    "    # 兜底：含有明显 yes/no 词根时也归一化\n",
    "    if \"yes\" in t and \"no\" not in t:\n",
    "        return \"YES\"\n",
    "    if \"no\" in t and \"yes\" not in t:\n",
    "        return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def _ensure_uppercase_yesno(text: str) -> str:\n",
    "    \"\"\"根据 CONFIG['answer_uppercase'] 决定返回大小写；内部评估一律用大写比较。\"\"\"\n",
    "    yn = _normalize_yesno(text)\n",
    "    if CONFIG.get(\"answer_uppercase\", True):\n",
    "        return yn\n",
    "    return yn.lower()\n",
    "\n",
    "# ===== Accuracy & Confusion reporting =====\n",
    "import pandas as pd\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"把 gold label 合并到 batch_measure 结果 df。要求 df.question 字段和 gold_map 对齐。\"\"\"\n",
    "    gold_df = pd.DataFrame(list(gold_map.items()), columns=[\"question\", \"gold\"])\n",
    "    # 统一 gold 大小写\n",
    "    gold_df[\"gold\"] = gold_df[\"gold\"].map(_ensure_uppercase_yesno)\n",
    "    out = df.merge(gold_df, on=\"question\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    输入：包含列 ['label','question','answer','gold'] 的 df\n",
    "    返回：每配置的 acc 表（以及在打印时展示混淆矩阵）\n",
    "    \"\"\"\n",
    "    df = df_with_gold.copy()\n",
    "    # 归一化答案\n",
    "    df[\"pred\"] = df[\"answer\"].map(_ensure_uppercase_yesno)\n",
    "\n",
    "    # 标出是否有金标\n",
    "    has_gold = df[\"gold\"].notna()\n",
    "    if not has_gold.any():\n",
    "        print(\"⚠️ No gold labels found. Please provide a gold_map that covers your questions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df[has_gold].copy()\n",
    "    df[\"correct\"] = (df[\"pred\"] == df[\"gold\"]).astype(int)\n",
    "\n",
    "    # 总体准确率\n",
    "    overall_acc = df[\"correct\"].mean() if len(df) else float(\"nan\")\n",
    "    print(f\"\\n== Overall accuracy: {overall_acc:.3f} (n={len(df)}) ==\")\n",
    "\n",
    "    # 每配置准确率\n",
    "    by_cfg = df.groupby(\"label\")[\"correct\"].mean().reset_index().rename(columns={\"correct\":\"accuracy\"})\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for _, row in by_cfg.iterrows():\n",
    "        n = df[df[\"label\"] == row[\"label\"]].shape[0]\n",
    "        print(f\"{row['label']:<15s}  acc={row['accuracy']:.3f}  (n={n})\")\n",
    "\n",
    "    # 每配置的混淆矩阵\n",
    "    print(\"\\n== Confusion matrices by config ==\")\n",
    "    for cfg, sub in df.groupby(\"label\"):\n",
    "        cm = pd.crosstab(sub[\"gold\"], sub[\"pred\"], rownames=[\"gold\"], colnames=[\"pred\"], dropna=False)\n",
    "        # 确保列/行都有 YES/NO\n",
    "        for val in [\"YES\",\"NO\"]:\n",
    "            if val not in cm.index:\n",
    "                cm.loc[val] = 0\n",
    "            if val not in cm.columns:\n",
    "                cm[val] = 0\n",
    "        cm = cm.loc[[\"YES\",\"NO\"], [\"YES\",\"NO\"]]\n",
    "        print(f\"\\n[Config: {cfg}]\")\n",
    "        print(cm)\n",
    "\n",
    "    return by_cfg\n",
    "\n",
    "def per_question_delta(df_with_gold: pd.DataFrame, base_label: str, target_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    对每道题比较 base vs target 的预测差异。\n",
    "    返回：question, gold, pred_base, pred_target, delta_correct\n",
    "    \"\"\"\n",
    "    df = df_with_gold.copy()\n",
    "    df[\"pred\"] = df[\"answer\"].map(_ensure_uppercase_yesno)\n",
    "    df = df[df[\"gold\"].notna()].copy()\n",
    "\n",
    "    base = df[df[\"label\"] == base_label][[\"question\",\"gold\",\"pred\"]].rename(columns={\"pred\":\"pred_base\"})\n",
    "    tgt  = df[df[\"label\"] == target_label][[\"question\",\"pred\"]].rename(columns={\"pred\":\"pred_target\"})\n",
    "    j = base.merge(tgt, on=\"question\", how=\"inner\")\n",
    "    j[\"delta_correct\"] = (j[\"pred_target\"] == j[\"gold\"]).astype(int) - (j[\"pred_base\"] == j[\"gold\"]).astype(int)\n",
    "    # delta_correct ∈ {-1,0,1}: 1=提升、-1=下降、0=持平\n",
    "    return j.sort_values(by=[\"delta_correct\",\"question\"], ascending=[False, True])\n",
    "\n",
    "def _get_retrieved_docs_for_prompt(\n",
    "    question: str,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    k: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Decide whether to retrieve based on CONFIG['include_retrieved_context'], return hits ([(Document, score), ...]).\"\"\"\n",
    "    if not faiss_db or not CONFIG.get(\"include_retrieved_context\", True):\n",
    "        return None\n",
    "    k = k or CONFIG.get(\"faiss_search_k\", 3)\n",
    "    _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "    return hits if hits else None\n",
    "\n",
    "def _count_tokens(tokenizer, text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def measure_once(\n",
    "    question: str,\n",
    "    gen_pipe,              # pipeline from load_llm_pipeline\n",
    "    tokenizer,             # tokenizer from load_llm_pipeline (used for counting tokens)\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    According to current CONFIG, construct the prompt (controlled by include_retrieved_context / include_current_triples),\n",
    "    then call LLM, measuring once:\n",
    "      - input_tokens / output_tokens / total_tokens\n",
    "      - latency_sec\n",
    "      - (optional) peak_vram_MiB\n",
    "      - record whether retrieval and triples are used\n",
    "    \"\"\"\n",
    "    # 1) Retrieval (if enabled)\n",
    "    retrieved_docs = _get_retrieved_docs_for_prompt(\n",
    "        question, parser, faiss_db=faiss_db, k=CONFIG.get(\"faiss_search_k\", 3)\n",
    "    )\n",
    "\n",
    "    # 2) Parse the current question into graph/triples\n",
    "    G, rels = parse_question_to_graph_generic(parser, question)\n",
    "\n",
    "    # 3) Construct prompt (internally decides whether to include triples based on CONFIG['include_current_triples'])\n",
    "    prompt = make_graph_qa_prompt(\n",
    "        question=question,\n",
    "        G=G,\n",
    "        relations=rels,\n",
    "        retrieved_docs=retrieved_docs\n",
    "    )\n",
    "\n",
    "    # 4) Count input tokens\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # 5) Timing & generation (optional: peak GPU memory)\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # 7) Count output tokens\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    # 8) Peak GPU memory usage\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # 9) Mark whether retrieval/triples were used\n",
    "    used_retrieval = bool(retrieved_docs)\n",
    "    used_triples = bool(rels) and CONFIG.get(\"include_current_triples\", True)\n",
    "\n",
    "    return {\n",
    "        \"label\": label or (\"with_graph_ctx\" if used_triples or used_retrieval else \"no_graph_ctx\"),\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": dt,\n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"used_retrieval\": used_retrieval,\n",
    "        \"used_current_triples\": used_triples,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "# ===== Batch evaluation & summary (optional) =====\n",
    "def batch_measure(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    flip_configs: List[Dict] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run multiple CONFIG combinations (e.g. with/without retrieval, with/without triples) on a question set,\n",
    "    return a summary DataFrame.\n",
    "    flip_configs: each element is a local override of CONFIG, for example:\n",
    "        [{\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "         {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"}]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if not flip_configs:\n",
    "        flip_configs = [ {\"label\": \"current_CONFIG\"} ]\n",
    "\n",
    "    for cfg in flip_configs:\n",
    "        # Save old values, temporarily override\n",
    "        old_retrieve = CONFIG.get(\"include_retrieved_context\", True)\n",
    "        old_triples  = CONFIG.get(\"include_current_triples\", True)\n",
    "        if \"include_retrieved_context\" in cfg:\n",
    "            CONFIG[\"include_retrieved_context\"] = cfg[\"include_retrieved_context\"]\n",
    "        if \"include_current_triples\" in cfg:\n",
    "            CONFIG[\"include_current_triples\"] = cfg[\"include_current_triples\"]\n",
    "\n",
    "        for q in questions:\n",
    "            try:\n",
    "                rec = measure_once(\n",
    "                    question=q,\n",
    "                    gen_pipe=gen_pipe,\n",
    "                    tokenizer=tokenizer,\n",
    "                    parser=parser,\n",
    "                    faiss_db=faiss_db,\n",
    "                    label=cfg.get(\"label\")\n",
    "                )\n",
    "                rows.append(rec)\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"label\": cfg.get(\"label\"),\n",
    "                    \"question\": q,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        # Restore CONFIG\n",
    "        CONFIG[\"include_retrieved_context\"] = old_retrieve\n",
    "        CONFIG[\"include_current_triples\"]   = old_triples\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    \"\"\"Compare average cost of two configurations and print relative changes (%).\"\"\"\n",
    "    A = df[df[\"label\"]==base_label]\n",
    "    B = df[df[\"label\"]==target_label]\n",
    "    if A.empty or B.empty:\n",
    "        print(\"Not enough data for comparison.\")\n",
    "        return\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b-a)/max(1e-9, a)\n",
    "\n",
    "    for col in [\"input_tokens\",\"output_tokens\",\"total_tokens\",\"latency_sec\",\"peak_vram_MiB\",\"prompt_chars\"]:\n",
    "        if col in df.columns:\n",
    "            a,b,d = avg(col)\n",
    "            print(f\"{col:>15s} | {base_label}: {a:8.2f} | {target_label}: {b:8.2f} | Δ%: {d*100:7.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    \"Is the Earth round?\": \"YES\",\n",
    "    \"Is Earth flat?\": \"NO\",                       \n",
    "    \"Does the Earth orbit the Sun?\": \"YES\",\n",
    "\n",
    "    \"Does the Sun rise in the east?\": \"YES\",\n",
    "    \"Does the Sun rise in the west?\": \"NO\",      \n",
    "    \"Is the Sun a star?\": \"YES\",\n",
    "\n",
    "    \"Is Paris the capital of France?\": \"YES\",\n",
    "    \"Is Paris the capital of Germany?\": \"NO\",     \n",
    "    \"Is the Eiffel Tower in Paris?\": \"YES\",\n",
    "\n",
    "    \"Do humans need oxygen to survive?\": \"YES\",\n",
    "    \"Can humans survive without water forever?\": \"NO\", \n",
    "    \"Do humans have two lungs?\": \"YES\",\n",
    "\n",
    "    \"Is the Moon a natural satellite of Earth?\": \"YES\",\n",
    "    \"Does Earth have two moons?\": \"NO\",           \n",
    "    \"Does the Moon orbit the Earth?\": \"YES\",\n",
    "\n",
    "    \"Is the Sahara Desert in South America?\": \"NO\",\n",
    "    \"Is the Sahara Desert in Africa?\": \"YES\",     \n",
    "    \"Is the Sahara Desert the largest desert on Earth?\": \"YES\",\n",
    "\n",
    "    \"Is the Amazon River longer than the Nile River?\": \"NO\",\n",
    "    \"Is the Amazon River in Africa?\": \"NO\",      \n",
    "    \"Is the Nile River in Africa?\": \"YES\",\n",
    "\n",
    "    \"Is Tokyo the capital of South Korea?\": \"NO\",\n",
    "    \"Is Seoul the capital of South Korea?\": \"YES\", \n",
    "    \"Is Tokyo in Japan?\": \"YES\",\n",
    "\n",
    "    \"Do penguins live in the Arctic?\": \"NO\",\n",
    "    \"Do penguins live in Antarctica?\": \"YES\",\n",
    "    \"Do polar bears live in Antarctica?\": \"NO\",  \n",
    "\n",
    "    \"Is gold heavier than lead?\": \"NO\",\n",
    "    \"Is gold a metal?\": \"YES\",\n",
    "    \"Is lead a gas?\": \"NO\"                        \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d6d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "    label                        question  input_tokens  output_tokens  \\\n",
      "0  no_ctx             Is the Earth round?          61.0            1.0   \n",
      "1  no_ctx                  Is Earth flat?          60.0            1.0   \n",
      "2  no_ctx   Does the Earth orbit the Sun?          63.0            1.0   \n",
      "3  no_ctx  Does the Sun rise in the east?          64.0            1.0   \n",
      "4  no_ctx  Does the Sun rise in the west?          64.0            1.0   \n",
      "\n",
      "   total_tokens  latency_sec  peak_vram_MiB used_retrieval  \\\n",
      "0          62.0     0.380892            NaN          False   \n",
      "1          61.0     0.224952            NaN          False   \n",
      "2          64.0     0.244526            NaN          False   \n",
      "3          65.0     0.245734            NaN          False   \n",
      "4          65.0     0.198287            NaN          False   \n",
      "\n",
      "  used_current_triples  prompt_chars answer error  \n",
      "0                False         245.0    YES   NaN  \n",
      "1                False         240.0     NO   NaN  \n",
      "2                False         255.0     NO   NaN  \n",
      "3                False         256.0    YES   NaN  \n",
      "4                False         256.0     NO   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "gen_pipe, tokenizer = load_llm_pipeline()   # Use your loader above\n",
    "parser = RelationshipGraphParser()\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "\"\"\"# Single-question measurement (under current CONFIG)\n",
    "rec = measure_once(\n",
    "    \"Is the Great Wall visible from low Earth orbit?\",\n",
    "    gen_pipe, tokenizer, parser, faiss_db, label=\"current_CONFIG\"\n",
    ")\n",
    "print(rec)\"\"\"\n",
    "\n",
    "# Batch A/B comparison (no context vs. both retrieval & triples)\n",
    "questions = list(GOLD_LABELS.keys())\n",
    "\n",
    "df = batch_measure(\n",
    "    questions, gen_pipe, tokenizer, parser, faiss_db,\n",
    "    flip_configs=[\n",
    "        {\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "        {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"},\n",
    "    ]\n",
    ")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e175c9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "   input_tokens | no_ctx:    63.10 | with_both:   163.45 | Δ%:  159.03%\n",
      "  output_tokens | no_ctx:     1.00 | with_both:     1.00 | Δ%:    0.00%\n",
      "   total_tokens | no_ctx:    64.10 | with_both:   164.45 | Δ%:  156.55%\n",
      "    latency_sec | no_ctx:     0.61 | with_both:     2.03 | Δ%:  235.27%\n",
      "  peak_vram_MiB | no_ctx:      nan | with_both:      nan | Δ%:     nan%\n",
      "   prompt_chars | no_ctx:   255.50 | with_both:   681.75 | Δ%:  166.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "summarize_cost(df, base_label=\"no_ctx\", target_label=\"with_both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36e8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Overall accuracy: 0.633 (n=60) ==\n",
      "\n",
      "== Accuracy by config ==\n",
      "no_ctx           acc=0.633  (n=30)\n",
      "with_both        acc=0.633  (n=30)\n",
      "\n",
      "== Confusion matrices by config ==\n",
      "\n",
      "[Config: no_ctx]\n",
      "pred  YES  NO\n",
      "gold         \n",
      "YES     6  11\n",
      "NO      0  13\n",
      "\n",
      "[Config: with_both]\n",
      "pred  YES  NO\n",
      "gold         \n",
      "YES     6  11\n",
      "NO      0  13\n"
     ]
    }
   ],
   "source": [
    "df_gold = attach_gold(df, GOLD_LABELS)\n",
    "acc_table = evaluate_accuracy(df_gold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
