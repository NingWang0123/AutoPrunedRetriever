{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/causal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": True,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.4,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"YES_NO\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_30272/3148360877.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        return parser.question_to_graph(question)\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        return parser.question_to_causal_graph(question)\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "        related_triples = doc0.page_content.strip()\n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "            f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    # 2) current question + triples (optional)\n",
    "    triples_text = \"\"\n",
    "    if relations and CONFIG.get(\"include_current_triples\", True):\n",
    "        triples_text = \"\\n\".join(\n",
    "            f\"{u} -> {d.get('rel','related_to')} -> {v}\"\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        )\n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    if triples_text.strip():\n",
    "        q_block += f\"\\n[CURRENT QUESTION'S GRAPH TRIPLES]:\\n{triples_text}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    # 3) task instructions (placed at the end)\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    rules = (\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    sections.append(rules)\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db = None,\n",
    "    prompt = None\n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if faiss_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)  # Number of docs to retrieve\n",
    "        _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "        \n",
    "    if prompt == None:\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    # If return_full_text=False → only new content; else trim prefix\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    # Normalize YES/NO case\n",
    "    if CONFIG.get(\"answer_mode\", \"YES_NO\"):\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        a = answer.strip().lower()\n",
    "        if \"yes\" in a and \"no\" not in a:\n",
    "            answer = yes\n",
    "            print(answer)\n",
    "            return answer\n",
    "        elif \"no\" in a and \"yes\" not in a:\n",
    "            answer = no\n",
    "            print(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    \n",
    "    \n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        G, rels = parse_question_to_graph_generic(parser, q)\n",
    "        text = build_relationship_text(q, G, rels)  # Output [QUESTION][GRAPH][TRIPLES]\n",
    "\n",
    "        # Get LLM answer\n",
    "        answer = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": G.number_of_nodes(),\n",
    "            \"num_edges\": G.number_of_edges(),\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            metadata[\"prompt_snapshot\"] = make_graph_qa_prompt(q, G, rels)\n",
    "\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    vectordb = FAISS.from_documents(docs, emb)\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691a20c",
   "metadata": {},
   "source": [
    "### Answer questions in bulk and load them into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2512571",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Is the Earth round?\",\n",
    "    \"Does the Sun rise in the east?\",\n",
    "    \"Is Paris the capital of France?\",\n",
    "    \"Do humans need oxygen to survive?\",\n",
    "    \"Is the Moon a natural satellite of Earth?\",\n",
    "    \"Is the Sahara Desert in South America?\"  ,   \n",
    "    \"Is the Amazon River longer than the Nile River?\", \n",
    "    \"Is Tokyo the capital of South Korea?\"    ,     \n",
    "    \"Do penguins live in the Arctic?\"   ,           \n",
    "    \"Is gold heavier than lead?\" ,   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "FAISS index ready. docs=10\n"
     ]
    }
   ],
   "source": [
    "# 1) Parser\n",
    "parser = RelationshipGraphParser()   # or CausalQuestionGraphParser()\n",
    "\n",
    "# 2) Load Phi-4-mini-reasoning\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                # Automatically select appropriate precision\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,           # Control randomness\n",
    ")\n",
    "\n",
    "# 3) Build documents (including LLM answers in metadata)\n",
    "docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe, add_prompt_snapshot=False\n",
    ")\n",
    "\n",
    "# 4) Vectorization & Save\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_db = FAISS.from_documents(docs, emb)\n",
    "faiss_db.save_local(\"graph_rag_faiss_index\")\n",
    "print(f\"FAISS index ready. docs={len(docs)}\")\n",
    "\n",
    "\n",
    "# To load later:\n",
    "# faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cf0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Is the Earth round?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698682}, page_content='Earth -> isa -> round'), Document(metadata={'graph_id': 'Q2', 'question': 'Does the Sun rise in the east?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698683}, page_content='rise -> prep_in -> east\\nSun -> subj -> rise'), Document(metadata={'graph_id': 'Q3', 'question': 'Is Paris the capital of France?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': None, 'created_at': 1755698694}, page_content='Paris -> isa -> capital'), Document(metadata={'graph_id': 'Q4', 'question': 'Do humans need oxygen to survive?', 'num_nodes': 4, 'num_edges': 3, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698694}, page_content='humans -> subj -> need\\nneed -> obj -> oxygen\\noxygen -> subj -> survive'), Document(metadata={'graph_id': 'Q5', 'question': 'Is the Moon a natural satellite of Earth?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698695}, page_content='Moon -> isa -> natural satellite'), Document(metadata={'graph_id': 'Q6', 'question': 'Is the Sahara Desert in South America?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': None, 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q7', 'question': 'Is the Amazon River longer than the Nile River?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q8', 'question': 'Is Tokyo the capital of South Korea?', 'num_nodes': 0, 'num_edges': 0, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='__EMPTY_TRIPLES__'), Document(metadata={'graph_id': 'Q9', 'question': 'Do penguins live in the Arctic?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755698698}, page_content='live -> prep_in -> Arctic\\npenguins -> subj -> live'), Document(metadata={'graph_id': 'Q10', 'question': 'Is gold heavier than lead?', 'num_nodes': 4, 'num_edges': 3, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'YES', 'created_at': 1755698699}, page_content='gold -> property -> heavy than lead\\ngold -> isa -> heavy\\ngold -> subj -> lead')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94036c",
   "metadata": {},
   "source": [
    "### Test for answering individual questions (adjust prompt with no database context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n",
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "parser = RelationshipGraphParser()   \n",
    "\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                \n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "questions = \"Is the Great Wall visible from low Earth orbit?\"\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "#answer = answer_with_llm(questions, gen_pipe, parser)\n",
    "answer = answer_with_llm(questions, gen_pipe, parser, faiss_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400a67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_yesno(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the LLM output to strict 'YES'/'NO' values.\n",
    "    Any non-explicit 'yes'/'no' output is treated as 'NO' to align with the prompt rule:\n",
    "    \"If uncertain, choose NO.\"\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\":\n",
    "        return \"YES\"\n",
    "    if t == \"no\":\n",
    "        return \"NO\"\n",
    "    # Fallback: if text contains a clear yes/no keyword\n",
    "    if \"yes\" in t and \"no\" not in t:\n",
    "        return \"YES\"\n",
    "    if \"no\" in t and \"yes\" not in t:\n",
    "        return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def _ensure_uppercase_yesno(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure the returned value matches the CONFIG['answer_uppercase'] setting.\n",
    "    Internally, evaluation always uses uppercase comparison.\n",
    "    \"\"\"\n",
    "    yn = _normalize_yesno(text)\n",
    "    if CONFIG.get(\"answer_uppercase\", True):\n",
    "        return yn\n",
    "    return yn.lower()\n",
    "\n",
    "# ===== Accuracy & Confusion Reporting =====\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach gold labels to the batch_measure results DataFrame.\n",
    "    Requires df.question to match keys in gold_map.\n",
    "    \"\"\"\n",
    "    gold_df = pd.DataFrame(list(gold_map.items()), columns=[\"question\", \"gold\"])\n",
    "    # Normalize gold labels to uppercase or lowercase as per config\n",
    "    gold_df[\"gold\"] = gold_df[\"gold\"].map(_ensure_uppercase_yesno)\n",
    "    out = df.merge(gold_df, on=\"question\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: DataFrame with columns ['label','question','answer','gold']\n",
    "    Output: Per-configuration accuracy table and confusion matrices.\n",
    "    \"\"\"\n",
    "    df = df_with_gold.copy()\n",
    "    # Normalize predicted answers\n",
    "    df[\"pred\"] = df[\"answer\"].map(_ensure_uppercase_yesno)\n",
    "\n",
    "    # Check if any gold labels exist\n",
    "    has_gold = df[\"gold\"].notna()\n",
    "    if not has_gold.any():\n",
    "        print(\"⚠️ No gold labels found. Please provide a gold_map that covers your questions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df[has_gold].copy()\n",
    "    df[\"correct\"] = (df[\"pred\"] == df[\"gold\"]).astype(int)\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_acc = df[\"correct\"].mean() if len(df) else float(\"nan\")\n",
    "    print(f\"\\n== Overall accuracy: {overall_acc:.3f} (n={len(df)}) ==\")\n",
    "\n",
    "    # Accuracy per configuration\n",
    "    by_cfg = df.groupby(\"label\")[\"correct\"].mean().reset_index().rename(columns={\"correct\":\"accuracy\"})\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for _, row in by_cfg.iterrows():\n",
    "        n = df[df[\"label\"] == row[\"label\"]].shape[0]\n",
    "        print(f\"{row['label']:<15s}  acc={row['accuracy']:.3f}  (n={n})\")\n",
    "\n",
    "    # Confusion matrix for each configuration\n",
    "    print(\"\\n== Confusion matrices by config ==\")\n",
    "    for cfg, sub in df.groupby(\"label\"):\n",
    "        cm = pd.crosstab(sub[\"gold\"], sub[\"pred\"], rownames=[\"gold\"], colnames=[\"pred\"], dropna=False)\n",
    "        # Ensure YES/NO rows and columns exist\n",
    "        for val in [\"YES\",\"NO\"]:\n",
    "            if val not in cm.index:\n",
    "                cm.loc[val] = 0\n",
    "            if val not in cm.columns:\n",
    "                cm[val] = 0\n",
    "        cm = cm.loc[[\"YES\",\"NO\"], [\"YES\",\"NO\"]]\n",
    "        print(f\"\\n[Config: {cfg}]\")\n",
    "        print(cm)\n",
    "\n",
    "    return by_cfg\n",
    "\n",
    "def per_question_delta(df_with_gold: pd.DataFrame, base_label: str, target_label: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare the predictions between base and target configurations for each question.\n",
    "    Output columns:\n",
    "    - question\n",
    "    - gold (gold label)\n",
    "    - pred_base (prediction from base configuration)\n",
    "    - pred_target (prediction from target configuration)\n",
    "    - delta_correct (-1, 0, 1): 1 = improvement, -1 = regression, 0 = no change\n",
    "    \"\"\"\n",
    "    df = df_with_gold.copy()\n",
    "    df[\"pred\"] = df[\"answer\"].map(_ensure_uppercase_yesno)\n",
    "    df = df[df[\"gold\"].notna()].copy()\n",
    "\n",
    "    base = df[df[\"label\"] == base_label][[\"question\",\"gold\",\"pred\"]].rename(columns={\"pred\":\"pred_base\"})\n",
    "    tgt  = df[df[\"label\"] == target_label][[\"question\",\"pred\"]].rename(columns={\"pred\":\"pred_target\"})\n",
    "    j = base.merge(tgt, on=\"question\", how=\"inner\")\n",
    "    j[\"delta_correct\"] = (j[\"pred_target\"] == j[\"gold\"]).astype(int) - (j[\"pred_base\"] == j[\"gold\"]).astype(int)\n",
    "    return j.sort_values(by=[\"delta_correct\",\"question\"], ascending=[False, True])\n",
    "\n",
    "def _get_retrieved_docs_for_prompt(\n",
    "    question: str,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    k: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve documents based on CONFIG['include_retrieved_context'] setting.\n",
    "    Returns hits as [(Document, score), ...].\n",
    "    \"\"\"\n",
    "    if not faiss_db or not CONFIG.get(\"include_retrieved_context\", True):\n",
    "        return None\n",
    "    k = k or CONFIG.get(\"faiss_search_k\", 3)\n",
    "    _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "    return hits if hits else None\n",
    "\n",
    "def _count_tokens(tokenizer, text: str) -> int:\n",
    "    \"\"\"Count tokens in the given text using the provided tokenizer.\"\"\"\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def measure_once(\n",
    "    question: str,\n",
    "    gen_pipe,              # Pipeline object from load_llm_pipeline\n",
    "    tokenizer,             # Tokenizer from load_llm_pipeline for token counting\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Build a prompt according to current CONFIG (controlled by include_retrieved_context / include_current_triples),\n",
    "    then query the LLM and return:\n",
    "      - input_tokens / output_tokens / total_tokens\n",
    "      - latency_sec (time taken for generation)\n",
    "      - peak_vram_MiB (optional, for GPU usage)\n",
    "      - flags indicating whether retrieval and triples were used\n",
    "    \"\"\"\n",
    "    # 1) Retrieval (if enabled)\n",
    "    retrieved_docs = _get_retrieved_docs_for_prompt(\n",
    "        question, parser, faiss_db=faiss_db, k=CONFIG.get(\"faiss_search_k\", 3)\n",
    "    )\n",
    "\n",
    "    # 2) Parse the question into graph/triples\n",
    "    G, rels = parse_question_to_graph_generic(parser, question)\n",
    "\n",
    "    # 3) Build the prompt (with or without triples based on config)\n",
    "    prompt = make_graph_qa_prompt(\n",
    "        question=question,\n",
    "        G=G,\n",
    "        relations=rels,\n",
    "        retrieved_docs=retrieved_docs\n",
    "    )\n",
    "\n",
    "    # 4) Count input tokens\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # 5) Time the generation (and optionally measure peak GPU memory)\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # 6) Count output tokens\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    # 7) Record peak GPU memory usage if available\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # 8) Flags for retrieval and triple usage\n",
    "    used_retrieval = bool(retrieved_docs)\n",
    "    used_triples = bool(rels) and CONFIG.get(\"include_current_triples\", True)\n",
    "\n",
    "    return {\n",
    "        \"label\": label or (\"with_graph_ctx\" if used_triples or used_retrieval else \"no_graph_ctx\"),\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": dt,\n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"used_retrieval\": used_retrieval,\n",
    "        \"used_current_triples\": used_triples,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "# ===== Batch Evaluation and Summary =====\n",
    "def batch_measure(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    flip_configs: List[Dict] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate multiple CONFIG setups (e.g., with/without retrieval, with/without triples) on a list of questions,\n",
    "    and return a summary DataFrame with all results.\n",
    "    flip_configs: List of dictionaries for temporary CONFIG overrides. Example:\n",
    "        [{\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "         {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"}]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if not flip_configs:\n",
    "        flip_configs = [{\"label\": \"current_CONFIG\"}]\n",
    "\n",
    "    for cfg in flip_configs:\n",
    "        # Temporarily override CONFIG\n",
    "        old_retrieve = CONFIG.get(\"include_retrieved_context\", True)\n",
    "        old_triples  = CONFIG.get(\"include_current_triples\", True)\n",
    "        if \"include_retrieved_context\" in cfg:\n",
    "            CONFIG[\"include_retrieved_context\"] = cfg[\"include_retrieved_context\"]\n",
    "        if \"include_current_triples\" in cfg:\n",
    "            CONFIG[\"include_current_triples\"] = cfg[\"include_current_triples\"]\n",
    "\n",
    "        for q in questions:\n",
    "            try:\n",
    "                rec = measure_once(\n",
    "                    question=q,\n",
    "                    gen_pipe=gen_pipe,\n",
    "                    tokenizer=tokenizer,\n",
    "                    parser=parser,\n",
    "                    faiss_db=faiss_db,\n",
    "                    label=cfg.get(\"label\")\n",
    "                )\n",
    "                rows.append(rec)\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"label\": cfg.get(\"label\"),\n",
    "                    \"question\": q,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        # Restore original CONFIG\n",
    "        CONFIG[\"include_retrieved_context\"] = old_retrieve\n",
    "        CONFIG[\"include_current_triples\"]   = old_triples\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    \"\"\"\n",
    "    Compare average token usage, latency, and GPU memory between two configurations.\n",
    "    Prints relative percentage changes.\n",
    "    \"\"\"\n",
    "    A = df[df[\"label\"] == base_label]\n",
    "    B = df[df[\"label\"] == target_label]\n",
    "    if A.empty or B.empty:\n",
    "        print(\"Not enough data for comparison.\")\n",
    "        return\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b - a) / max(1e-9, a)\n",
    "\n",
    "    for col in [\"input_tokens\", \"output_tokens\", \"total_tokens\", \"latency_sec\", \"peak_vram_MiB\", \"prompt_chars\"]:\n",
    "        if col in df.columns:\n",
    "            a, b, d = avg(col)\n",
    "            print(f\"{col:>15s} | {base_label}: {a:8.2f} | {target_label}: {b:8.2f} | Δ%: {d*100:7.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    \"Is the Earth round?\": \"YES\",\n",
    "    \"Is Earth flat?\": \"NO\",                       \n",
    "    \"Does the Earth orbit the Sun?\": \"YES\",\n",
    "\n",
    "    \"Does the Sun rise in the east?\": \"YES\",\n",
    "    \"Does the Sun rise in the west?\": \"NO\",      \n",
    "    \"Is the Sun a star?\": \"YES\",\n",
    "\n",
    "    \"Is Paris the capital of France?\": \"YES\",\n",
    "    \"Is Paris the capital of Germany?\": \"NO\",     \n",
    "    \"Is the Eiffel Tower in Paris?\": \"YES\",\n",
    "\n",
    "    \"Do humans need oxygen to survive?\": \"YES\",\n",
    "    \"Can humans survive without water forever?\": \"NO\", \n",
    "    \"Do humans have two lungs?\": \"YES\",\n",
    "\n",
    "    \"Is the Moon a natural satellite of Earth?\": \"YES\",\n",
    "    \"Does Earth have two moons?\": \"NO\",           \n",
    "    \"Does the Moon orbit the Earth?\": \"YES\",\n",
    "\n",
    "    \"Is the Sahara Desert in South America?\": \"NO\",\n",
    "    \"Is the Sahara Desert in Africa?\": \"YES\",     \n",
    "    \"Is the Sahara Desert the largest desert on Earth?\": \"YES\",\n",
    "\n",
    "    \"Is the Amazon River longer than the Nile River?\": \"NO\",\n",
    "    \"Is the Amazon River in Africa?\": \"NO\",      \n",
    "    \"Is the Nile River in Africa?\": \"YES\",\n",
    "\n",
    "    \"Is Tokyo the capital of South Korea?\": \"NO\",\n",
    "    \"Is Seoul the capital of South Korea?\": \"YES\", \n",
    "    \"Is Tokyo in Japan?\": \"YES\",\n",
    "\n",
    "    \"Do penguins live in the Arctic?\": \"NO\",\n",
    "    \"Do penguins live in Antarctica?\": \"YES\",\n",
    "    \"Do polar bears live in Antarctica?\": \"NO\",  \n",
    "\n",
    "    \"Is gold heavier than lead?\": \"NO\",\n",
    "    \"Is gold a metal?\": \"YES\",\n",
    "    \"Is lead a gas?\": \"NO\"                        \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d6d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "    label                        question  input_tokens  output_tokens  \\\n",
      "0  no_ctx             Is the Earth round?          61.0            1.0   \n",
      "1  no_ctx                  Is Earth flat?          60.0            1.0   \n",
      "2  no_ctx   Does the Earth orbit the Sun?          63.0            1.0   \n",
      "3  no_ctx  Does the Sun rise in the east?          64.0            1.0   \n",
      "4  no_ctx  Does the Sun rise in the west?          64.0            1.0   \n",
      "\n",
      "   total_tokens  latency_sec  peak_vram_MiB used_retrieval  \\\n",
      "0          62.0     0.380892            NaN          False   \n",
      "1          61.0     0.224952            NaN          False   \n",
      "2          64.0     0.244526            NaN          False   \n",
      "3          65.0     0.245734            NaN          False   \n",
      "4          65.0     0.198287            NaN          False   \n",
      "\n",
      "  used_current_triples  prompt_chars answer error  \n",
      "0                False         245.0    YES   NaN  \n",
      "1                False         240.0     NO   NaN  \n",
      "2                False         255.0     NO   NaN  \n",
      "3                False         256.0    YES   NaN  \n",
      "4                False         256.0     NO   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "gen_pipe, tokenizer = load_llm_pipeline()   # Use your loader above\n",
    "parser = RelationshipGraphParser()\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "\"\"\"# Single-question measurement (under current CONFIG)\n",
    "rec = measure_once(\n",
    "    \"Is the Great Wall visible from low Earth orbit?\",\n",
    "    gen_pipe, tokenizer, parser, faiss_db, label=\"current_CONFIG\"\n",
    ")\n",
    "print(rec)\"\"\"\n",
    "\n",
    "# Batch A/B comparison (no context vs. both retrieval & triples)\n",
    "questions = list(GOLD_LABELS.keys())\n",
    "\n",
    "df = batch_measure(\n",
    "    questions, gen_pipe, tokenizer, parser, faiss_db,\n",
    "    flip_configs=[\n",
    "        {\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "        {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"},\n",
    "    ]\n",
    ")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e175c9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "   input_tokens | no_ctx:    63.10 | with_both:   163.45 | Δ%:  159.03%\n",
      "  output_tokens | no_ctx:     1.00 | with_both:     1.00 | Δ%:    0.00%\n",
      "   total_tokens | no_ctx:    64.10 | with_both:   164.45 | Δ%:  156.55%\n",
      "    latency_sec | no_ctx:     0.61 | with_both:     2.03 | Δ%:  235.27%\n",
      "  peak_vram_MiB | no_ctx:      nan | with_both:      nan | Δ%:     nan%\n",
      "   prompt_chars | no_ctx:   255.50 | with_both:   681.75 | Δ%:  166.83%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "summarize_cost(df, base_label=\"no_ctx\", target_label=\"with_both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36e8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Overall accuracy: 0.633 (n=60) ==\n",
      "\n",
      "== Accuracy by config ==\n",
      "no_ctx           acc=0.633  (n=30)\n",
      "with_both        acc=0.633  (n=30)\n",
      "\n",
      "== Confusion matrices by config ==\n",
      "\n",
      "[Config: no_ctx]\n",
      "pred  YES  NO\n",
      "gold         \n",
      "YES     6  11\n",
      "NO      0  13\n",
      "\n",
      "[Config: with_both]\n",
      "pred  YES  NO\n",
      "gold         \n",
      "YES     6  11\n",
      "NO      0  13\n"
     ]
    }
   ],
   "source": [
    "df_gold = attach_gold(df, GOLD_LABELS)\n",
    "acc_table = evaluate_accuracy(df_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20306aa8",
   "metadata": {},
   "source": [
    "## Text RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295d34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "       # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", \n",
    "\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba0620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\": \"YES\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\": \"YES\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\": \"YES\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\": \"YES\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\": \"YES\",\n",
    "\n",
    "    # --- 原始 NO 类 ---\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\": \"NO\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\": \"NO\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\": \"NO\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\": \"NO\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\": \"NO\",\n",
    "\n",
    "    # --- Follow-up Questions (YES/NO balanced) ---\n",
    "    # Earth shape\n",
    "    \"Despite the Earth's slightly flattened poles, is its shape closer to a sphere than to a flat surface?\": \"YES\",\n",
    "    \"Is the Earth perfectly flat with no curvature anywhere on its surface?\": \"NO\",\n",
    "\n",
    "    # Sun rotation\n",
    "    \"Given the Earth's rotation, is the apparent motion of the Sun consistent with the Sun rising in the east?\": \"YES\",\n",
    "    \"If the Earth did not rotate on its axis, would the Sun still rise and set in the same pattern as it does now?\": \"NO\",\n",
    "\n",
    "    # Paris as capital\n",
    "    \"Considering France's administrative structure, is Paris recognized as the political and economic capital of the nation?\": \"YES\",\n",
    "    \"Is Berlin, rather than Paris, designated as the official capital of France in any historical or legal record?\": \"NO\",\n",
    "\n",
    "    # Oxygen necessity\n",
    "    \"Since oxygen is vital for human life, is it correct to say that humans cannot survive without breathing air containing oxygen?\": \"YES\",\n",
    "    \"Can humans live indefinitely without any access to oxygen in their environment?\": \"NO\",\n",
    "\n",
    "    # Moon as satellite\n",
    "    \"Is the Moon the only large natural body that consistently orbits Earth in the solar system?\": \"YES\",\n",
    "    \"Do humans have multiple moons orbiting the Earth, similar to Jupiter or Saturn?\": \"NO\",\n",
    "\n",
    "    # Sahara Desert\n",
    "    \"Is the Sahara Desert geographically located across multiple countries in northern Africa?\": \"YES\",\n",
    "    \"Is the Sahara Desert primarily located in the continent of South America?\": \"NO\",\n",
    "\n",
    "    # Amazon River\n",
    "    \"Does the Nile River surpass the Amazon River in length when measured by the most widely accepted geographical data?\": \"YES\",\n",
    "    \"Is the Amazon River considered to originate in Europe according to global mapping authorities?\": \"NO\",\n",
    "\n",
    "    # Tokyo\n",
    "    \"Is Tokyo the capital city of Japan and a major economic center in Asia?\": \"YES\",\n",
    "    \"Is Tokyo officially listed as the capital city of South Korea in government documents?\": \"NO\",\n",
    "\n",
    "    # Penguins\n",
    "    \"Do penguins naturally inhabit regions in the Southern Hemisphere, particularly Antarctica?\": \"YES\",\n",
    "    \"Do penguins live alongside polar bears in the Arctic region as part of their natural habitat?\": \"NO\",\n",
    "\n",
    "    # Gold vs Lead\n",
    "    \"Is gold denser than most metals but still slightly less dense than lead?\": \"NO\",\n",
    "    \"Is gold classified as a metal due to its physical and chemical properties?\": \"YES\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399a983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text RAG: 仅用问题文本入库 ===\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_text_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    生成仅文本 RAG 的文档，并使 metadata 字段与图 RAG 对齐：\n",
    "    - graph_id / question / num_nodes / num_edges / llm_model / llm_answer / created_at / prompt_snapshot(可选)\n",
    "    - 其中 num_nodes/num_edges 统一置 0，保持同名键方便评测与对比\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        # 文本版页面内容：只存问题文本\n",
    "        page_content = f\"{q}\"\n",
    "\n",
    "        # 生成 LLM 答案（文本 RAG 检索）\n",
    "        answer = answer_with_llm_text(q, gen_pipe, text_db=text_db)\n",
    "\n",
    "        # metadata 字段与 Graph RAG 对齐\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": 0,                    # 对齐字段\n",
    "            \"num_edges\": 0,                    # 对齐字段\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            # 为了对齐，也提供 prompt_snapshot；注意这里是“文本 RAG”的 prompt\n",
    "            # 为了避免再次触发生成，这里重建与上面一致的 prompt 片段\n",
    "            prompt_snapshot = make_text_qa_prompt(q, None if not text_db else similarity_search_text_docs(q, text_db, k=CONFIG.get(\"faiss_search_k\",3))[1])\n",
    "            metadata[\"prompt_snapshot\"] = prompt_snapshot\n",
    "\n",
    "        docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_docs_text_only(questions: List[str]) -> List[Document]:\n",
    "    return [build_text_doc(q) for q in questions]\n",
    "\n",
    "def build_text_faiss_index(questions: List[str]) -> FAISS:\n",
    "    docs = build_docs_text_only(questions)\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# === Text RAG: 相似度检索（与入库同分布：纯问题文本） ===\n",
    "def similarity_search_text_docs(\n",
    "    user_question: str,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "):\n",
    "    query_text = f\"{user_question}\"\n",
    "    results = vectordb.similarity_search_with_score(query_text, k=k)\n",
    "    return query_text, results\n",
    "\n",
    "\n",
    "# === Text RAG: Prompt（不含图三元组；可拼检索上下文的原问题文本与历史答案） ===\n",
    "def make_text_qa_prompt(\n",
    "    question: str,\n",
    "    retrieved_docs=None\n",
    ") -> str:\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, _ = retrieved_docs[0]\n",
    "        related_q_txt = doc0.page_content.strip()\n",
    "        related_answer = (doc0.metadata or {}).get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION TEXT]:\\n{related_q_txt}\\n\"\n",
    "            f\"[RELATED ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    sections.append(f\"[CURRENT QUESTION]: {question}\")\n",
    "\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    sections.append(\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    return \"\\n\\n\".join(sections)\n",
    "\n",
    "def answer_with_llm_text(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> str:\n",
    "    # 检索（可选）\n",
    "    retrieved_docs = None\n",
    "    if text_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)\n",
    "        _, hits = similarity_search_text_docs(question, text_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "\n",
    "    # Prompt\n",
    "    prompt = make_text_qa_prompt(question, retrieved_docs)\n",
    "\n",
    "    # 生成\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    # 取输出\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    # 归一化 YES/NO（与现有 answer_with_llm 一致）\n",
    "    if CONFIG.get(\"answer_mode\", \"YES_NO\"):\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        a = answer.strip().lower()\n",
    "        if \"yes\" in a and \"no\" not in a:\n",
    "            answer = yes\n",
    "        elif \"no\" in a and \"yes\" not in a:\n",
    "            answer = no\n",
    "        else:\n",
    "            # 回退：保持原样（或你也可和图版一样递归一次，这里避免递归以最少改动为主）\n",
    "            answer = no  # 与“不确定选 NO”的规则一致\n",
    "    return answer\n",
    "\n",
    "def build_text_faiss_index_with_answers(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    bootstrap_db: Optional[FAISS] = None\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    用文本 RAG 路线生成答案并入库，然后返回 FAISS 向量库。\n",
    "    bootstrap_db: 若传入，文本检索会优先引用该库的历史问答作为 retrieved context（冷启动可传 None）。\n",
    "    \"\"\"\n",
    "    docs = build_text_docs_with_answer(\n",
    "        questions=questions,\n",
    "        gen_pipe=gen_pipe,\n",
    "        add_prompt_snapshot=add_prompt_snapshot,\n",
    "        text_db=bootstrap_db,\n",
    "    )\n",
    "    print(docs)\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3966e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_once_mode(\n",
    "    question: str,\n",
    "    mode: str,                 # \"text\" or \"graph\"\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser=None,\n",
    "    text_db: Optional[FAISS] = None,\n",
    "    graph_db: Optional[FAISS] = None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    assert mode in (\"text\", \"graph\")\n",
    "\n",
    "    # 选择检索与prompt\n",
    "    if mode == \"text\":\n",
    "        retrieved_docs = None\n",
    "        if text_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            _, hits = similarity_search_text_docs(question, text_db, k=CONFIG.get(\"faiss_search_k\", 3))\n",
    "            retrieved_docs = hits if hits else None\n",
    "        prompt = make_text_qa_prompt(question, retrieved_docs=retrieved_docs)\n",
    "    else:\n",
    "        retrieved_docs = None\n",
    "        if graph_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            _, hits = similarity_search_graph_docs(question, parser, graph_db, k=CONFIG.get(\"faiss_search_k\", 3))\n",
    "            retrieved_docs = hits if hits else None\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    # token 计数\n",
    "    def _count_tokens(tokenizer, text: str) -> int:\n",
    "        return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # 计时 & 生成\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    out = gen_pipe(prompt)\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    text = out[0][\"generated_text\"]\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    return {\n",
    "        \"label\": label or f\"{mode}_rag\",\n",
    "        \"mode\": mode,\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": dt,\n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "        \"used_retrieval\": bool(retrieved_docs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d847f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compare_text_vs_graph(\n",
    "    questions: List[str],\n",
    "    gen_pipe, tokenizer, parser,\n",
    "    text_db: Optional[FAISS],\n",
    "    graph_db: Optional[FAISS],\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q in questions:\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"text\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"text_rag\")\n",
    "        )\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"graph\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"graph_rag\")\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---- 最小准确率工具（如你已有可忽略）----\n",
    "def _normalize_yesno(text: str) -> str:\n",
    "    if text is None: return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\" or (\"yes\" in t and \"no\" not in t): return \"YES\"\n",
    "    if t == \"no\"  or (\"no\"  in t and \"yes\" not in t): return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    g = pd.DataFrame(list(gold_map.items()), columns=[\"question\",\"gold\"])\n",
    "    g[\"gold\"] = g[\"gold\"].map(lambda x: \"YES\" if str(x).upper()==\"YES\" else \"NO\")\n",
    "    out = df.merge(g, on=\"question\", how=\"left\")\n",
    "    out[\"pred\"] = out[\"answer\"].map(_normalize_yesno)\n",
    "    out[\"correct\"] = (out[\"pred\"] == out[\"gold\"]).astype(int)\n",
    "    return out\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame):\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for k, sub in df_with_gold.groupby(\"label\"):\n",
    "        n = len(sub[sub[\"gold\"].notna()])\n",
    "        acc = sub[\"correct\"].mean() if n else float(\"nan\")\n",
    "        print(f\"{k:<10s} acc={acc:.3f} (n={n})\")\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    A = df[df[\"label\"]==base_label]; B = df[df[\"label\"]==target_label]\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b-a)/max(1e-9,a)\n",
    "    print(\"\\n== Cost (avg) ==\")\n",
    "    for col in [\"input_tokens\",\"output_tokens\",\"total_tokens\",\"latency_sec\",\"peak_vram_MiB\",\"prompt_chars\"]:\n",
    "        if col in df.columns:\n",
    "            a,b,d = avg(col); print(f\"{col:>15s} | {base_label}: {a:8.2f} | {target_label}: {b:8.2f} | Δ%: {d*100:7.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9916527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_size_bytes(path: str) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            fp = os.path.join(root, f)\n",
    "            try: total += os.path.getsize(fp)\n",
    "            except OSError: pass\n",
    "    return total\n",
    "\n",
    "def save_and_report_sizes(text_db: FAISS, graph_db: FAISS, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\"):\n",
    "    text_db.save_local(text_dir)\n",
    "    graph_db.save_local(graph_dir)\n",
    "    def human(n):\n",
    "        u=[\"B\",\"KB\",\"MB\",\"GB\"]; i=0; x=float(n)\n",
    "        while x>=1024 and i<len(u)-1: x/=1024.0; i+=1\n",
    "        return f\"{x:.2f} {u[i]}\"\n",
    "    s_text  = dir_size_bytes(text_dir)\n",
    "    s_graph = dir_size_bytes(graph_dir)\n",
    "    print(f\"[Index size] text_rag  = {human(s_text)}  ({text_dir})\")\n",
    "    print(f\"[Index size] graph_rag = {human(s_graph)}  ({graph_dir})\")\n",
    "    return s_text, s_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# 0) 模型与解析器\n",
    "gen_pipe, tokenizer = load_llm_pipeline()\n",
    "parser = RelationshipGraphParser()\n",
    "\n",
    "# 1) Graph RAG：你已有的流程\n",
    "graph_docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    faiss_db=None  # 冷启动\n",
    ")\n",
    "print(graph_docs)\n",
    "graph_db = build_faiss_index(graph_docs)  # == 你已有 build_faiss_index\n",
    "\n",
    "# 2) Text RAG：并行构建（字段对齐）\n",
    "text_db = build_text_faiss_index_with_answers(\n",
    "    questions,\n",
    "    gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    bootstrap_db=None  # 冷启动；也可传已有 text_db 做增量\n",
    ")\n",
    "\n",
    "# 3) 现在 text_db 与 graph_db 的 Document.metadata 键名一致，你可以直接复用你现有的\n",
    "#    度量、准确率与存储体积对比函数（比如 batch_compare_text_vs_graph / summarize_cost 等）\n",
    "\n",
    "\n",
    "# 2) 存储体积对比\n",
    "save_and_report_sizes(text_db, graph_db, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\")\n",
    "\n",
    "# 3) 成本 & 准确率 A/B（用你现有 GOLD_LABELS）\n",
    "df_ab = batch_compare_text_vs_graph(questions, gen_pipe, tokenizer, parser, text_db, graph_db)\n",
    "df_ab_gold = attach_gold(df_ab, GOLD_LABELS)\n",
    "evaluate_accuracy(df_ab_gold)\n",
    "summarize_cost(df_ab_gold, base_label=\"text_rag\", target_label=\"graph_rag\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
