{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/causal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": False,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.1,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"YES_NO\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_25053/124026649.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        return parser.question_to_graph(question)\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        return parser.question_to_causal_graph(question)\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "        related_triples = doc0.page_content.strip()\n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are its graph triples and its prior answer.\\n\"\n",
    "            f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "            f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    # 2) current question + triples (optional)\n",
    "    triples_text = \"\"\n",
    "    if relations and CONFIG.get(\"include_current_triples\", True):\n",
    "        triples_text = \"\\n\".join(\n",
    "            f\"{u} -> {d.get('rel','related_to')} -> {v}\"\n",
    "            for u, v, d in G.edges(data=True)\n",
    "        )\n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    if triples_text.strip():\n",
    "        q_block += f\"\\n[CURRENT QUESTION'S GRAPH TRIPLES]:\\n{triples_text}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    # 3) task instructions (placed at the end)\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    rules = (\n",
    "        \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "        f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "        \"- Do NOT copy or summarize any context.\\n\"\n",
    "        \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "        \"- If retrieved context conflicts with CURRENT QUESTION'S GRAPH TRIPLES, \"\n",
    "        \"prefer the CURRENT QUESTION'S GRAPH TRIPLES.\\n\"\n",
    "        \"- If uncertain, choose NO.\\n\"\n",
    "        f\"[ANSWER]: \"\n",
    "    )\n",
    "    sections.append(rules)\n",
    "\n",
    "    # Final prompt\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db = None,\n",
    "    prompt = None\n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if faiss_db:\n",
    "        k = CONFIG.get(\"faiss_search_k\", 3)  # Number of docs to retrieve\n",
    "        _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "        retrieved_docs = hits\n",
    "\n",
    "    G, rels = parse_question_to_graph_generic(parser, question)\n",
    "\n",
    "    if prompt == None:\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    out = gen_pipe(prompt)\n",
    "    text = out[0][\"generated_text\"]\n",
    "\n",
    "    # If return_full_text=False → only new content; else trim prefix\n",
    "    if CONFIG.get(\"return_full_text\", True):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    # Normalize YES/NO case\n",
    "    yes, no = _yn(\"YES\", \"NO\")\n",
    "    a = answer.strip().lower()\n",
    "    if \"yes\" in a and \"no\" not in a:\n",
    "        answer = yes\n",
    "    elif \"no\" in a and \"yes\" not in a:\n",
    "        answer = no\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        G, rels = parse_question_to_graph_generic(parser, q)\n",
    "        text = build_relationship_text(q, G, rels)  # Output [QUESTION][GRAPH][TRIPLES]\n",
    "\n",
    "        # Get LLM answer\n",
    "        answer = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,\n",
    "            \"num_nodes\": G.number_of_nodes(),\n",
    "            \"num_edges\": G.number_of_edges(),\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            metadata[\"prompt_snapshot\"] = make_graph_qa_prompt(q, G, rels)\n",
    "\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_faiss_index(docs: List[Document]) -> FAISS:\n",
    "    vectordb = FAISS.from_documents(docs, emb)\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691a20c",
   "metadata": {},
   "source": [
    "### Answer questions in bulk and load them into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n",
      "FAISS index ready. docs=10\n"
     ]
    }
   ],
   "source": [
    "# 1) Parser\n",
    "parser = RelationshipGraphParser()   # or CausalQuestionGraphParser()\n",
    "\n",
    "# 2) Load Phi-4-mini-reasoning\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                # Automatically select appropriate precision\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,           # Control randomness\n",
    ")\n",
    "\n",
    "# 3) Question set\n",
    "questions = [\n",
    "    \"Is the Great Wall of China located in China?\",\n",
    "    \"Does the Great Wall span over 13000 miles?\", \n",
    "    \"Was the Great Wall built during the Ming Dynasty?\",\n",
    "    \"Can the Great Wall be seen from space?\",\n",
    "    \"Is the Great Wall made of stone and brick?\",\n",
    "    \"Does the Great Wall have watchtowers?\",\n",
    "    \"Was the Great Wall constructed over 2000 years?\",\n",
    "    \"Is the Great Wall a UNESCO World Heritage Site?\",\n",
    "    \"Does the Great Wall stretch across northern China?\",\n",
    "    \"Are millions of tourists visiting the Great Wall annually?\"\n",
    "]\n",
    "\n",
    "# 4) Build documents (including LLM answers in metadata)\n",
    "docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe, add_prompt_snapshot=False\n",
    ")\n",
    "\n",
    "# 5) Vectorization & Save\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_db = FAISS.from_documents(docs, emb)\n",
    "faiss_db.save_local(\"graph_rag_faiss_index\")\n",
    "print(f\"FAISS index ready. docs={len(docs)}\")\n",
    "\n",
    "\n",
    "# To load later:\n",
    "# faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2cf0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Is the Great Wall of China located in China?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590585}, page_content='the Great Wall of China -> subj -> locate\\nlocate -> prep_in -> China'), Document(metadata={'graph_id': 'Q2', 'question': 'Does the Great Wall span over 13000 miles?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590586}, page_content='Great Wall span -> subj -> do\\ndo -> prep_over -> 13000 miles'), Document(metadata={'graph_id': 'Q3', 'question': 'Was the Great Wall built during the Ming Dynasty?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590586}, page_content='Great Wall -> subj -> build\\nbuild -> prep_during -> the Ming Dynasty'), Document(metadata={'graph_id': 'Q4', 'question': 'Can the Great Wall be seen from space?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590586}, page_content='see -> prep_from -> space\\nGreat Wall -> subj -> see'), Document(metadata={'graph_id': 'Q5', 'question': 'Is the Great Wall made of stone and brick?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590587}, page_content='Great Wall -> subj -> make\\nmake -> prep_of -> stone'), Document(metadata={'graph_id': 'Q6', 'question': 'Does the Great Wall have watchtowers?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590587}, page_content='have -> obj -> watchtowers\\nthe Great Wall -> subj -> have'), Document(metadata={'graph_id': 'Q7', 'question': 'Was the Great Wall constructed over 2000 years?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590587}, page_content='construct -> obj -> over 2000 years\\nGreat Wall -> subj -> construct'), Document(metadata={'graph_id': 'Q8', 'question': 'Is the Great Wall a UNESCO World Heritage Site?', 'num_nodes': 2, 'num_edges': 1, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590588}, page_content='Great Wall -> isa -> a'), Document(metadata={'graph_id': 'Q9', 'question': 'Does the Great Wall stretch across northern China?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590588}, page_content='Great Wall -> subj -> stretch\\nstretch -> prep_across -> China'), Document(metadata={'graph_id': 'Q10', 'question': 'Are millions of tourists visiting the Great Wall annually?', 'num_nodes': 3, 'num_edges': 2, 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'NO', 'created_at': 1755590588}, page_content='tourists -> subj -> visit\\nvisit -> obj -> the Great Wall')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94036c",
   "metadata": {},
   "source": [
    "### Test for answering individual questions (adjust prompt with no database context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a04bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n",
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "parser = RelationshipGraphParser()   #\n",
    "\n",
    "gen_pipe, _ = load_llm_pipeline(\n",
    "    model_id=\"microsoft/Phi-4-mini-reasoning\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=None,                #\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "questions = \"Is the Great Wall visible from low Earth orbit?\"\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "#answer = answer_with_llm(questions, gen_pipe, parser)\n",
    "answer = answer_with_llm(questions, gen_pipe, parser, faiss_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400a67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Dependency: CONFIG / make_graph_qa_prompt / parse_question_to_graph_generic\n",
    "# and the retrieval function similarity_search_graph_docs\n",
    "# already defined in your current file, no modification needed\n",
    "\n",
    "def _get_retrieved_docs_for_prompt(\n",
    "    question: str,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    k: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"Decide whether to retrieve based on CONFIG['include_retrieved_context'], return hits ([(Document, score), ...]).\"\"\"\n",
    "    if not faiss_db or not CONFIG.get(\"include_retrieved_context\", True):\n",
    "        return None\n",
    "    k = k or CONFIG.get(\"faiss_search_k\", 3)\n",
    "    _, hits = similarity_search_graph_docs(question, parser, faiss_db, k=k)\n",
    "    return hits if hits else None\n",
    "\n",
    "def _count_tokens(tokenizer, text: str) -> int:\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "def measure_once(\n",
    "    question: str,\n",
    "    gen_pipe,              # pipeline from load_llm_pipeline\n",
    "    tokenizer,             # tokenizer from load_llm_pipeline (used for counting tokens)\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    According to current CONFIG, construct the prompt (controlled by include_retrieved_context / include_current_triples),\n",
    "    then call LLM, measuring once:\n",
    "      - input_tokens / output_tokens / total_tokens\n",
    "      - latency_sec\n",
    "      - (optional) peak_vram_MiB\n",
    "      - record whether retrieval and triples are used\n",
    "    \"\"\"\n",
    "    # 1) Retrieval (if enabled)\n",
    "    retrieved_docs = _get_retrieved_docs_for_prompt(\n",
    "        question, parser, faiss_db=faiss_db, k=CONFIG.get(\"faiss_search_k\", 3)\n",
    "    )\n",
    "\n",
    "    # 2) Parse the current question into graph/triples\n",
    "    G, rels = parse_question_to_graph_generic(parser, question)\n",
    "\n",
    "    # 3) Construct prompt (internally decides whether to include triples based on CONFIG['include_current_triples'])\n",
    "    prompt = make_graph_qa_prompt(\n",
    "        question=question,\n",
    "        G=G,\n",
    "        relations=rels,\n",
    "        retrieved_docs=retrieved_docs\n",
    "    )\n",
    "\n",
    "    # 4) Count input tokens\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # 5) Timing & generation (optional: peak GPU memory)\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    answer = answer_with_llm(question, gen_pipe, parser, faiss_db, prompt)\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # 7) Count output tokens\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    # 8) Peak GPU memory usage\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # 9) Mark whether retrieval/triples were used\n",
    "    used_retrieval = bool(retrieved_docs)\n",
    "    used_triples = bool(rels) and CONFIG.get(\"include_current_triples\", True)\n",
    "\n",
    "    return {\n",
    "        \"label\": label or (\"with_graph_ctx\" if used_triples or used_retrieval else \"no_graph_ctx\"),\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": dt,\n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"used_retrieval\": used_retrieval,\n",
    "        \"used_current_triples\": used_triples,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "# ===== Batch evaluation & summary (optional) =====\n",
    "def batch_measure(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    *,\n",
    "    flip_configs: List[Dict] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run multiple CONFIG combinations (e.g. with/without retrieval, with/without triples) on a question set,\n",
    "    return a summary DataFrame.\n",
    "    flip_configs: each element is a local override of CONFIG, for example:\n",
    "        [{\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "         {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"}]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if not flip_configs:\n",
    "        flip_configs = [ {\"label\": \"current_CONFIG\"} ]\n",
    "\n",
    "    for cfg in flip_configs:\n",
    "        # Save old values, temporarily override\n",
    "        old_retrieve = CONFIG.get(\"include_retrieved_context\", True)\n",
    "        old_triples  = CONFIG.get(\"include_current_triples\", True)\n",
    "        if \"include_retrieved_context\" in cfg:\n",
    "            CONFIG[\"include_retrieved_context\"] = cfg[\"include_retrieved_context\"]\n",
    "        if \"include_current_triples\" in cfg:\n",
    "            CONFIG[\"include_current_triples\"] = cfg[\"include_current_triples\"]\n",
    "\n",
    "        for q in questions:\n",
    "            try:\n",
    "                rec = measure_once(\n",
    "                    question=q,\n",
    "                    gen_pipe=gen_pipe,\n",
    "                    tokenizer=tokenizer,\n",
    "                    parser=parser,\n",
    "                    faiss_db=faiss_db,\n",
    "                    label=cfg.get(\"label\")\n",
    "                )\n",
    "                rows.append(rec)\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"label\": cfg.get(\"label\"),\n",
    "                    \"question\": q,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        # Restore CONFIG\n",
    "        CONFIG[\"include_retrieved_context\"] = old_retrieve\n",
    "        CONFIG[\"include_current_triples\"]   = old_triples\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    \"\"\"Compare average cost of two configurations and print relative changes (%).\"\"\"\n",
    "    A = df[df[\"label\"]==base_label]\n",
    "    B = df[df[\"label\"]==target_label]\n",
    "    if A.empty or B.empty:\n",
    "        print(\"Not enough data for comparison.\")\n",
    "        return\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b-a)/max(1e-9, a)\n",
    "\n",
    "    for col in [\"input_tokens\",\"output_tokens\",\"total_tokens\",\"latency_sec\",\"peak_vram_MiB\",\"prompt_chars\"]:\n",
    "        if col in df.columns:\n",
    "            a,b,d = avg(col)\n",
    "            print(f\"{col:>15s} | {base_label}: {a:8.2f} | {target_label}: {b:8.2f} | Δ%: {d*100:7.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d6d6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "{'label': 'current_CONFIG', 'question': 'Is the Great Wall visible from low Earth orbit?', 'input_tokens': 188, 'output_tokens': 1, 'total_tokens': 189, 'latency_sec': 0.7058252499999966, 'peak_vram_MiB': None, 'used_retrieval': True, 'used_current_triples': True, 'prompt_chars': 797, 'answer': 'NO'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n",
      "       label                                           question  input_tokens  \\\n",
      "0     no_ctx    Is the Great Wall visible from low Earth orbit?            97   \n",
      "1     no_ctx  Was the Great Wall built during the Ming Dynasty?            97   \n",
      "2     no_ctx              Does the Great Wall have watchtowers?            96   \n",
      "3  with_both    Is the Great Wall visible from low Earth orbit?           188   \n",
      "4  with_both  Was the Great Wall built during the Ming Dynasty?           197   \n",
      "\n",
      "   output_tokens  total_tokens  latency_sec peak_vram_MiB  used_retrieval  \\\n",
      "0              1            98     0.397436          None           False   \n",
      "1              1            98     0.278040          None           False   \n",
      "2              1            97     0.303288          None           False   \n",
      "3              1           189     0.601151          None            True   \n",
      "4              1           198     0.709760          None            True   \n",
      "\n",
      "   used_current_triples  prompt_chars answer  \n",
      "0                 False           417     NO  \n",
      "1                 False           419     NO  \n",
      "2                 False           407     NO  \n",
      "3                  True           797     NO  \n",
      "4                  True           830     NO  \n",
      "\n",
      "=== Summary ===\n",
      "   input_tokens | no_ctx:    96.67 | with_both:   193.00 | Δ%:   99.66%\n",
      "  output_tokens | no_ctx:     1.00 | with_both:     1.00 | Δ%:    0.00%\n",
      "   total_tokens | no_ctx:    97.67 | with_both:   194.00 | Δ%:   98.63%\n",
      "    latency_sec | no_ctx:     0.33 | with_both:     0.67 | Δ%:  104.45%\n",
      "  peak_vram_MiB | no_ctx:      nan | with_both:      nan | Δ%:     nan%\n",
      "   prompt_chars | no_ctx:   414.33 | with_both:   807.67 | Δ%:   94.93%\n"
     ]
    }
   ],
   "source": [
    "# 1) Load\n",
    "gen_pipe, tokenizer = load_llm_pipeline()   # Use your loader above\n",
    "parser = RelationshipGraphParser()\n",
    "faiss_db = FAISS.load_local(\"graph_rag_faiss_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 2) Single-question measurement (under current CONFIG)\n",
    "rec = measure_once(\n",
    "    \"Is the Great Wall visible from low Earth orbit?\",\n",
    "    gen_pipe, tokenizer, parser, faiss_db, label=\"current_CONFIG\"\n",
    ")\n",
    "print(rec)\n",
    "\n",
    "# 3) Batch A/B comparison (no context vs. both retrieval & triples)\n",
    "questions = [\n",
    "    \"Is the Great Wall visible from low Earth orbit?\",\n",
    "    \"Was the Great Wall built during the Ming Dynasty?\",\n",
    "    \"Does the Great Wall have watchtowers?\"\n",
    "]\n",
    "df = batch_measure(\n",
    "    questions, gen_pipe, tokenizer, parser, faiss_db,\n",
    "    flip_configs=[\n",
    "        {\"include_retrieved_context\": False, \"include_current_triples\": False, \"label\": \"no_ctx\"},\n",
    "        {\"include_retrieved_context\": True,  \"include_current_triples\": True,  \"label\": \"with_both\"},\n",
    "    ]\n",
    ")\n",
    "print(df.head())\n",
    "print(\"\\n=== Summary ===\")\n",
    "summarize_cost(df, base_label=\"no_ctx\", target_label=\"with_both\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
