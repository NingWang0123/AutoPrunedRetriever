{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from groupwords import *\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID  Phi-4-mini-reasoning Phi-4-mini-instruct\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": True,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.4,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"short\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "    \"use_cached_text_embeddings\": True, # Whether text rag reuse embedding to search\n",
    "    \"use_cached_graph_embeddings\": True  #Whether graph rag reuse embedding to search\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2dd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_3247/3364762243.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class WordAvgEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str = \"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model.vectors.npy\"):\n",
    "        self.kv = KeyedVectors.load(model_path, mmap='r')\n",
    "        self.dim = self.kv.vector_size\n",
    "        self.token_pat = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        toks = [t.lower() for t in self.token_pat.findall(text)]\n",
    "        vecs = [self.kv[w] for w in toks if w in self.kv]\n",
    "        if not vecs:\n",
    "            return np.zeros(self.dim, dtype=np.float32)\n",
    "        return np.mean(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed_text(text)\n",
    "\n",
    "word_emb = WordAvgEmbeddings(model_path=\"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model\")\n",
    "sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "def _avg_pool(mat: np.ndarray) -> np.ndarray:\n",
    "    if mat is None or len(mat) == 0:\n",
    "        return None\n",
    "    m = np.asarray(mat, dtype=np.float32)\n",
    "    if m.ndim == 1:\n",
    "        return m.astype(np.float32)\n",
    "    return m.mean(axis=0).astype(np.float32)\n",
    "\n",
    "def _normalize(v: np.ndarray) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    v /= (np.linalg.norm(v) + 1e-12)\n",
    "    return v\n",
    "\n",
    "def _graph_doc_vec_from_cached_or_embed(\n",
    "    er_e: list[str],\n",
    "    er_r: list[str],\n",
    "    e_embeds: list | None,\n",
    "    r_embeds: list | None,\n",
    "    emb_model,                     \n",
    "    use_cache: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    文档向量 = concat( avg(e_embeds), avg(r_embeds) ) 或者两者的平均。\n",
    "    这里用简单且稳定的做法：取 e,r 的平均再做均值融合。\n",
    "    \"\"\"\n",
    "    if use_cache and e_embeds is not None and len(e_embeds) and r_embeds is not None and len(r_embeds):\n",
    "        e_mean = _avg_pool(np.asarray(e_embeds, dtype=np.float32))\n",
    "        r_mean = _avg_pool(np.asarray(r_embeds, dtype=np.float32))\n",
    "        v = (e_mean + r_mean) / 2.0\n",
    "        return _normalize(v)\n",
    "\n",
    "    # 缓存不可用 → 现算：对每个实体/关系分别用词向量平均，再整体平均\n",
    "    e_vecs = []\n",
    "    for e in (er_e or []):\n",
    "        e_vecs.append(np.asarray(emb_model.embed_query(e), dtype=np.float32))\n",
    "    r_vecs = []\n",
    "    for r in (er_r or []):\n",
    "        r_vecs.append(np.asarray(emb_model.embed_query(r), dtype=np.float32))\n",
    "\n",
    "    e_mean = _avg_pool(np.stack(e_vecs, axis=0)) if e_vecs else None\n",
    "    r_mean = _avg_pool(np.stack(r_vecs, axis=0)) if r_vecs else None\n",
    "\n",
    "    if e_mean is None and r_mean is None:\n",
    "        # 两边都空，退化为零向量（用实体任意词兜底也可）\n",
    "        dim = getattr(emb_model, \"dim\", None) or len(emb_model.embed_query(\"a\"))\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "\n",
    "    if e_mean is None: v = r_mean\n",
    "    elif r_mean is None: v = e_mean\n",
    "    else: v = (e_mean + r_mean) / 2.0\n",
    "    return _normalize(v)\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "#emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document], emb) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "\n",
    "def print_graph(title: str, G):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"Nodes:\")\n",
    "    for n, d in G.nodes(data=True):\n",
    "        print(f\"  - {n!r} :: {d}\")\n",
    "    print(\"Edges:\")\n",
    "    if G.is_multigraph():\n",
    "        for u, v, k, d in G.edges(keys=True, data=True):\n",
    "            print(f\"  - {u!r} -[{k}]-> {v!r} :: {d}\")\n",
    "    else:\n",
    "        arrow = \"->\" if G.is_directed() else \"--\"\n",
    "        for u, v, d in G.edges(data=True):\n",
    "            print(f\"  - {u!r} {arrow} {v!r} :: {d}\")\n",
    "\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        G, rels = parser.question_to_graph(question)\n",
    "        G = merge_graph_nodes_by_canonical(G, normalizer=normalize_text, merge_edge_attrs=(\"relation\",))\n",
    "        return G, rels\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        G, rels = parser.question_to_causal_graph(question)\n",
    "        G = merge_graph_nodes_by_canonical(G, normalizer=normalize_text, merge_edge_attrs=(\"relation\",))\n",
    "        return G, rels\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "import ast\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "\n",
    "        # Get codebook and decode question list\n",
    "        metadata = doc0.metadata or {}\n",
    "        er = ast.literal_eval(doc0.page_content)\n",
    "        codebook_main = {\n",
    "            \"e\": er['e'],\n",
    "            \"r\": er['r'],\n",
    "            \"edge_matrix\": metadata[\"edge_matrix\"],\n",
    "            \"questions_lst\": metadata[\"questions_lst\"],\n",
    "            \"e_embeddings\": metadata[\"e_embeddings\"],\n",
    "            \"r_embeddings\": metadata[\"r_embeddings\"],\n",
    "        }\n",
    "\n",
    "        # 循环遍历所有 question\n",
    "        query_chains = []\n",
    "        for group_idx, group in enumerate(codebook_main[\"questions_lst\"]):\n",
    "            for q_idx, question_chain in enumerate(group):\n",
    "                # 1) 用于检索的输入：必须收集“边索引链”\n",
    "                query_chains.append(question_chain)    \n",
    "\n",
    "        related_triples = \"__EMPTY_JSON__\"\n",
    "        if query_chains:\n",
    "            wrapper_res = coarse_filter(\n",
    "                questions=query_chains,\n",
    "                codebook_main=codebook_main,\n",
    "                emb=sentence_emb,\n",
    "                top_k=3,\n",
    "                question_batch_size=2,\n",
    "                questions_db_batch_size=8,\n",
    "                top_m=2,\n",
    "            )\n",
    "\n",
    "            if isinstance(wrapper_res, dict) and wrapper_res:\n",
    "        \n",
    "                first_non_empty = next((lst for lst in wrapper_res.values() if lst), [])\n",
    "                if first_non_empty:\n",
    "                    related_triples = first_non_empty[0].get(\"text\", \"__EMPTY_JSON__\")\n",
    "\n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        \n",
    "        if related_triples != \"__EMPTY_JSON__\":\n",
    "            sections.append(\n",
    "                \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "                \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "                \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "                f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "                f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "                \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "            )\n",
    "\n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    mode = _mode()\n",
    "\n",
    "    if mode in {\"yes_no\", \"binary\"}:\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        rules = (\n",
    "            \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "            f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "            \"- Do NOT copy or summarize any context.\\n\"\n",
    "            \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "    else:\n",
    "        style_line = {\n",
    "            \"short\":    \"- Give a short, direct answer in 2–3 sentences.\\n\",\n",
    "            \"detail\":   \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"long\":     \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"reasoning\":\"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "            \"explain\":  \"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "        }.get(mode, \"- Provide a clear and helpful answer.\\n\")\n",
    "\n",
    "        rules = (\n",
    "            \"[TASK]: You are a QA assistant for open-ended questions.\\n\"\n",
    "            f\"{style_line}\"\n",
    "            \"- Do NOT restrict to yes/no.\\n\"\n",
    "            \"[FORMAT]: Write complete sentences (not a single word).\"\n",
    "            \"Avoid starting with just 'Yes.' or 'No.'; if the question is yes/no-style, state the conclusion AND 1–2 short reasons.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "\n",
    "    sections.append(rules)\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def _mode():\n",
    "    return (CONFIG.get(\"answer_mode\") or \"short\").lower()\n",
    "\n",
    "def _gen(gen_pipe, prompt):\n",
    "    # 显式传 generation 参数；有些 pipeline 会忽略默认 config\n",
    "    kwargs = dict(\n",
    "        do_sample=CONFIG.get(\"do_sample\", True),\n",
    "        temperature=CONFIG.get(\"temperature\", 0.4),\n",
    "        top_p=CONFIG.get(\"top_p\", 1.0),\n",
    "        max_new_tokens=CONFIG.get(\"max_new_tokens\", 256),\n",
    "        return_full_text=CONFIG.get(\"return_full_text\", False),\n",
    "    )\n",
    "    # 兼容 pad/eos（部分 Phi 模型需要）\n",
    "    try:\n",
    "        tok = gen_pipe.tokenizer\n",
    "        if tok is not None:\n",
    "            if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "                tok.pad_token_id = tok.eos_token_id\n",
    "            kwargs.setdefault(\"eos_token_id\", tok.eos_token_id)\n",
    "            kwargs.setdefault(\"pad_token_id\", tok.pad_token_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out = gen_pipe(prompt, **kwargs)\n",
    "    return out[0][\"generated_text\"]\n",
    "\n",
    "def _extract_answer_text(prompt, text):\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        return text[len(prompt):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def strip_think(s: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    提取并去掉 <think> ... </think> 段，同时识别尾部残缺 <think>...（无 </think>）。\n",
    "    返回:\n",
    "        clean_text: 去掉所有 think 段后的文本\n",
    "        thinks: 提取到的所有 think 内容（含尾部残缺内容）\n",
    "        has_dangling: 是否存在尾部残缺的 <think>\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\", [], False\n",
    "\n",
    "    s_lower = s.lower()\n",
    "    thinks: List[str] = []\n",
    "    spans: List[Tuple[int, int]] = []  # 需要从原文删除的区间 [start, end)\n",
    "\n",
    "    # 1) 提取所有完整的 <think>...</think>\n",
    "    for m in re.finditer(r\"<think>(.*?)</think>\", s, flags=re.S | re.I):\n",
    "        thinks.append(m.group(1).strip())\n",
    "        spans.append((m.start(), m.end()))\n",
    "\n",
    "    # 2) 检测尾部残缺的 <think>...\n",
    "    has_dangling = False\n",
    "    last_open = s_lower.rfind(\"<think>\")\n",
    "    if last_open != -1:\n",
    "        # 若在 last_open 之后找不到 </think>，则视为残缺\n",
    "        if s_lower.find(\"</think>\", last_open) == -1:\n",
    "            has_dangling = True\n",
    "            # 提取残缺内容：从 <think> 后面到字符串末尾\n",
    "            content_start = last_open + len(\"<think>\")\n",
    "            dangling_text = s[content_start:].strip()\n",
    "            if dangling_text:\n",
    "                thinks.append(dangling_text)\n",
    "            spans.append((last_open, len(s)))  # 删除从 <think> 到末尾\n",
    "\n",
    "    # 3) 从原文删除所有 spans（可能存在重叠，先排序再合并）\n",
    "    if spans:\n",
    "        spans.sort()\n",
    "        merged = []\n",
    "        cur_s, cur_e = spans[0]\n",
    "        for st, en in spans[1:]:\n",
    "            if st <= cur_e:  # 重叠或相接\n",
    "                cur_e = max(cur_e, en)\n",
    "            else:\n",
    "                merged.append((cur_s, cur_e))\n",
    "                cur_s, cur_e = st, en\n",
    "        merged.append((cur_s, cur_e))\n",
    "    else:\n",
    "        merged = []\n",
    "\n",
    "    # 4) 拼接非 think 的文本片段\n",
    "    parts = []\n",
    "    prev = 0\n",
    "    for st, en in merged:\n",
    "        if prev < st:\n",
    "            parts.append(s[prev:st])\n",
    "        prev = en\n",
    "    if prev < len(s):\n",
    "        parts.append(s[prev:])\n",
    "\n",
    "    clean_text = \"\".join(parts)\n",
    "\n",
    "    # 5) 额外清理常见的分析前缀（可选）\n",
    "    clean_text = re.sub(r\"(?:^|\\n)\\s*(Okay,|Let’s|Let's|Step by step|Thought:).*\", \"\", clean_text, flags=re.I)\n",
    "\n",
    "    return clean_text.strip(), thinks\n",
    "\n",
    "\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    prompt=None,\n",
    "    max_retries: int = 5,   \n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    if faiss_db:\n",
    "        _, hits = similarity_search_graph_docs(\n",
    "            question, parser, faiss_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "            emb_model=word_emb,\n",
    "            use_cache=CONFIG.get(\"use_cached_graph_embeddings\", True),\n",
    "        )\n",
    "        retrieved_docs = hits\n",
    "\n",
    "    if prompt is None:\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    mode = _mode()\n",
    "    YES_RE = re.compile(r\"^\\s*(yes|y|true|correct|affirmative)\\s*\\.?\\s*$\", re.I)\n",
    "    NO_RE  = re.compile(r\"^\\s*(no|n|false|incorrect|negative)\\s*\\.?\\s*$\", re.I)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "        raw = _gen(gen_pipe, prompt)\n",
    "        print(f\"----- RAW (try {attempt}):\", raw)\n",
    "\n",
    "        answer = _extract_answer_text(prompt, raw)\n",
    "        answer, thinking = strip_think(raw)  # thinking 以后可用\n",
    "        print(\"----- ANS:\", answer)\n",
    "\n",
    "        # 如果生成了空字符串 → 继续下一次循环\n",
    "        if not answer.strip():\n",
    "            continue  \n",
    "\n",
    "        if mode in {\"yes_no\", \"binary\"}:\n",
    "            if YES_RE.match(answer) and not NO_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(answer) and not YES_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "\n",
    "            # 再尝试加严格格式后缀\n",
    "            strict_suffix = (\n",
    "                \"\\n\\n[FORMAT]: Answer with exactly ONE token: \"\n",
    "                + (\"YES or NO.\" if CONFIG.get(\"answer_uppercase\", True) else \"yes or no.\")\n",
    "            )\n",
    "            raw2 = _gen(gen_pipe, prompt + strict_suffix)\n",
    "            ans2 = _extract_answer_text(prompt + strict_suffix, raw2)\n",
    "\n",
    "            if YES_RE.match(ans2) and not NO_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(ans2) and not YES_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "            return ans2  # 兜底返回\n",
    "        else:\n",
    "            # 开放式模式 → 避免单词回答\n",
    "            if YES_RE.match(answer) or NO_RE.match(answer) or len(answer.split()) <= 2:\n",
    "                format_suffix = (\n",
    "                    \"\\n\\n[FORMAT]: Provide a 2–3 sentence explanation; do not answer with a single word.\"\n",
    "                )\n",
    "                raw2 = _gen(gen_pipe, prompt + format_suffix)\n",
    "                ans2 = _extract_answer_text(prompt + format_suffix, raw2)\n",
    "                if len(ans2.strip()) > len(answer.strip()):\n",
    "                    return ans2.strip()\n",
    "            return answer.strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "def build_graph_faiss_index_from_cached(\n",
    "    docs: list[Document],\n",
    "    emb_model,   \n",
    ") -> FAISS:\n",
    "    use_cache = CONFIG.get(\"use_cached_graph_embeddings\", True)\n",
    "    texts, metas, vecs = [], [], []\n",
    "\n",
    "    for d in docs:\n",
    "        texts.append(d.page_content)\n",
    "        metas.append(d.metadata or {})\n",
    "\n",
    "\n",
    "        import ast\n",
    "        er = ast.literal_eval(d.page_content) if isinstance(d.page_content, str) else d.page_content\n",
    "        er_e = er.get(\"e\", []) if isinstance(er, dict) else []\n",
    "        er_r = er.get(\"r\", []) if isinstance(er, dict) else []\n",
    "\n",
    "        e_embeds = (d.metadata or {}).get(\"e_embeddings\")\n",
    "        r_embeds = (d.metadata or {}).get(\"r_embeddings\")\n",
    "\n",
    "        v = _graph_doc_vec_from_cached_or_embed(\n",
    "            er_e, er_r, e_embeds, r_embeds, emb_model, use_cache=use_cache\n",
    "        )\n",
    "        vecs.append(v.tolist())\n",
    "\n",
    "\n",
    "        d.metadata[\"graph_vec\"] = v.tolist()\n",
    "\n",
    "    X = np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "\n",
    "    text_embeddings = [(texts[i], X[i].tolist()) for i in range(len(texts))]\n",
    "    try:\n",
    "        return FAISS.from_embeddings(text_embeddings, embedding=emb_model, metadatas=metas)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return FAISS.from_embeddings(\n",
    "                embeddings=X.tolist(), metadatas=metas, texts=texts, embedding=emb_model\n",
    "            )\n",
    "        except Exception:\n",
    "            vs = FAISS.from_texts(texts=[], embedding=emb_model)\n",
    "            if hasattr(vs, \"add_embeddings\"):\n",
    "                vs.add_embeddings(embeddings=X, metadatas=metas, texts=texts)\n",
    "            else:\n",
    "                vs.add_texts(texts=texts, metadatas=metas)  \n",
    "            return vs\n",
    "        \n",
    "_GRAPH_QVEC_CACHE = {}\n",
    "\n",
    "def _faiss_search_by_vec_graph(vs, qv, k):\n",
    "    if hasattr(vs, \"similarity_search_by_vector_with_score\"):\n",
    "        return vs.similarity_search_by_vector_with_score(qv, k=k)\n",
    "    if hasattr(vs, \"similarity_search_by_vector\"):\n",
    "        docs = vs.similarity_search_by_vector(qv, k=k)\n",
    "        return [(d, None) for d in docs]\n",
    "    index = getattr(vs, \"index\", None)\n",
    "    id_map = getattr(vs, \"index_to_docstore_id\", None)\n",
    "    store  = getattr(vs, \"docstore\", None)\n",
    "    if index is None or id_map is None or store is None:\n",
    "        raise AttributeError(\"FAISS vectorstore has no by-vector APIs and no accessible index/docstore.\")\n",
    "    q = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(q, k)\n",
    "    out = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx == -1: continue\n",
    "        doc_id = id_map[idx]\n",
    "        doc = store.search(doc_id)\n",
    "        out.append((doc, float(dist)))\n",
    "    return out\n",
    "\n",
    "def similarity_search_graph_docs(\n",
    "    user_question: str,\n",
    "    parser,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "    emb_model=None,                    # word_emb\n",
    "    use_cache: Optional[bool] = None,  # 是否缓存查询向量（不影响库文档）\n",
    "):\n",
    "    if use_cache is None:\n",
    "        use_cache = CONFIG.get(\"use_cached_graph_embeddings\", True)\n",
    "    if emb_model is None:\n",
    "        emb_model = globals().get(\"word_emb\", None)\n",
    "    if emb_model is None:\n",
    "        raise ValueError(\"similarity_search_graph_docs: need `emb_model` for query embedding.\")\n",
    "\n",
    "    # 1) 缓存命中\n",
    "    if use_cache and user_question in _GRAPH_QVEC_CACHE:\n",
    "        qv = _GRAPH_QVEC_CACHE[user_question]\n",
    "        return user_question, _faiss_search_by_vec_graph(vectordb, qv, k)\n",
    "\n",
    "    # 2) 现算查询向量：解析成 triples → 拿出 e/r → 算平均\n",
    "    G, rels = parse_question_to_graph_generic(parser, user_question)\n",
    "    # 你已有的 linearization/抽取逻辑不变，这里只要 e/r\n",
    "    # 用简单抽取：把图里节点名当作实体，把边的 relation 当作关系\n",
    "    er_e = list({str(n) for n in G.nodes})          # 去重\n",
    "    er_r = []\n",
    "    if G.is_multigraph():\n",
    "        for _, _, _, data in G.edges(keys=True, data=True):\n",
    "            rel = data.get(\"relation\")\n",
    "            if rel: er_r.append(str(rel))\n",
    "    else:\n",
    "        for _, _, data in G.edges(data=True):\n",
    "            rel = data.get(\"relation\")\n",
    "            if rel: er_r.append(str(rel))\n",
    "\n",
    "    qv = _graph_doc_vec_from_cached_or_embed(er_e, er_r, None, None, emb_model, use_cache=False)\n",
    "    qv = _normalize(qv)\n",
    "\n",
    "    if use_cache:\n",
    "        _GRAPH_QVEC_CACHE[user_question] = qv\n",
    "\n",
    "    return user_question, _faiss_search_by_vec_graph(vectordb, qv, k)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        G, rels = parse_question_to_graph_generic(parser, q)        \n",
    "        codebook_main = build_relationship_text(q, G, rels, json_style=\"codebook_main\")  # Output [QUESTION][GRAPH][TRIPLES]\n",
    "        er = {\n",
    "                \"e\": codebook_main['e'],  \n",
    "                \"r\": codebook_main['r']\n",
    "            }\n",
    "\n",
    "        er = str(er)\n",
    "        # Get LLM answer\n",
    "        answer = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "        answers_tuple = (answer)\n",
    "        codebook_answer = get_code_book(answers_tuple, type='answers')\n",
    "        codebook_main = merging_codebook(codebook_main, codebook_answer, type='answers', word_emb=word_emb)\n",
    "\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "            \"edge_matrix\": codebook_main['edge_matrix'],\n",
    "            \"questions_lst\": codebook_main['questions_lst'],\n",
    "            \"answers_lst\": codebook_main['answers_lst'],\n",
    "            \"e_embeddings\": codebook_main[\"e_embeddings\"],\n",
    "            \"r_embeddings\": codebook_main[\"r_embeddings\"],\n",
    "        }\n",
    "   \n",
    "        if add_prompt_snapshot:\n",
    "            metadata[\"prompt_snapshot\"] = make_graph_qa_prompt(q, G, rels)\n",
    "\n",
    "        docs.append(Document(page_content=er, metadata=metadata))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20306aa8",
   "metadata": {},
   "source": [
    "## Text RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295d34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "       # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\",\n",
    "    #\"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\",\n",
    "    #\"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\",\n",
    "    #\"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\",\n",
    "    #\"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", \n",
    "\n",
    "    #\"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\",\n",
    "    #\"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\",\n",
    "    #\"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\",\n",
    "    #\"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\",\n",
    "    #\"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba0620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\": \"YES\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\": \"YES\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\": \"YES\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\": \"YES\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\": \"YES\",\n",
    "\n",
    "    # --- 原始 NO 类 ---\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\": \"NO\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\": \"NO\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\": \"NO\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\": \"NO\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\": \"NO\",\n",
    "\n",
    "    # --- Follow-up Questions (YES/NO balanced) ---\n",
    "    # Earth shape\n",
    "    \"Despite the Earth's slightly flattened poles, is its shape closer to a sphere than to a flat surface?\": \"YES\",\n",
    "    \"Is the Earth perfectly flat with no curvature anywhere on its surface?\": \"NO\",\n",
    "\n",
    "    # Sun rotation\n",
    "    \"Given the Earth's rotation, is the apparent motion of the Sun consistent with the Sun rising in the east?\": \"YES\",\n",
    "    \"If the Earth did not rotate on its axis, would the Sun still rise and set in the same pattern as it does now?\": \"NO\",\n",
    "\n",
    "    # Paris as capital\n",
    "    \"Considering France's administrative structure, is Paris recognized as the political and economic capital of the nation?\": \"YES\",\n",
    "    \"Is Berlin, rather than Paris, designated as the official capital of France in any historical or legal record?\": \"NO\",\n",
    "\n",
    "    # Oxygen necessity\n",
    "    \"Since oxygen is vital for human life, is it correct to say that humans cannot survive without breathing air containing oxygen?\": \"YES\",\n",
    "    \"Can humans live indefinitely without any access to oxygen in their environment?\": \"NO\",\n",
    "\n",
    "    # Moon as satellite\n",
    "    \"Is the Moon the only large natural body that consistently orbits Earth in the solar system?\": \"YES\",\n",
    "    \"Do humans have multiple moons orbiting the Earth, similar to Jupiter or Saturn?\": \"NO\",\n",
    "\n",
    "    # Sahara Desert\n",
    "    \"Is the Sahara Desert geographically located across multiple countries in northern Africa?\": \"YES\",\n",
    "    \"Is the Sahara Desert primarily located in the continent of South America?\": \"NO\",\n",
    "\n",
    "    # Amazon River\n",
    "    \"Does the Nile River surpass the Amazon River in length when measured by the most widely accepted geographical data?\": \"YES\",\n",
    "    \"Is the Amazon River considered to originate in Europe according to global mapping authorities?\": \"NO\",\n",
    "\n",
    "    # Tokyo\n",
    "    \"Is Tokyo the capital city of Japan and a major economic center in Asia?\": \"YES\",\n",
    "    \"Is Tokyo officially listed as the capital city of South Korea in government documents?\": \"NO\",\n",
    "\n",
    "    # Penguins\n",
    "    \"Do penguins naturally inhabit regions in the Southern Hemisphere, particularly Antarctica?\": \"YES\",\n",
    "    \"Do penguins live alongside polar bears in the Arctic region as part of their natural habitat?\": \"NO\",\n",
    "\n",
    "    # Gold vs Lead\n",
    "    \"Is gold denser than most metals but still slightly less dense than lead?\": \"NO\",\n",
    "    \"Is gold classified as a metal due to its physical and chemical properties?\": \"YES\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399a983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text RAG: 仅用问题文本入库 ===\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_text_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    生成仅文本 RAG 的文档，并使 metadata 字段与图 RAG 对齐：\n",
    "    - graph_id / question / num_nodes / num_edges / llm_model / llm_answer / created_at / prompt_snapshot(可选)\n",
    "    - 其中 num_nodes/num_edges 统一置 0，保持同名键方便评测与对比\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        page_content = f\"{q}\"\n",
    "        q_vec = sentence_emb.embed_query(q)\n",
    "\n",
    "        answer = answer_with_llm_text(q, gen_pipe, text_db=text_db, q_vec=q_vec)\n",
    "        \n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,                  \n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "            \"q_embeddings\": q_vec\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            prompt_snapshot = make_text_qa_prompt(q, None if not text_db else similarity_search_text_docs(q, text_db, k=CONFIG.get(\"faiss_search_k\",3))[1])\n",
    "            metadata[\"prompt_snapshot\"] = prompt_snapshot\n",
    "\n",
    "        docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "    return docs, q_vec\n",
    "\n",
    "_QVEC_CACHE = {}\n",
    "\n",
    "def _faiss_search_by_vec(vs, qv, k):\n",
    "    \"\"\"兼容不同 langchain-community 版本的 FAISS 向量检索。返回 [(doc, score_or_None), ...]\"\"\"\n",
    "    if hasattr(vs, \"similarity_search_by_vector_with_score\"):\n",
    "        return vs.similarity_search_by_vector_with_score(qv, k=k)\n",
    "\n",
    "    if hasattr(vs, \"similarity_search_by_vector\"):\n",
    "        docs = vs.similarity_search_by_vector(qv, k=k)\n",
    "        return [(d, None) for d in docs]\n",
    "\n",
    "    index = getattr(vs, \"index\", None)\n",
    "    id_map = getattr(vs, \"index_to_docstore_id\", None)\n",
    "    store  = getattr(vs, \"docstore\", None)\n",
    "    if index is None or id_map is None or store is None:\n",
    "        raise AttributeError(\"FAISS vectorstore has no by-vector APIs and no accessible index/docstore.\")\n",
    "\n",
    "    import numpy as np\n",
    "    q = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(q, k)\n",
    "    out = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        doc_id = id_map[idx]\n",
    "        doc = store.search(doc_id)\n",
    "        out.append((doc, float(dist)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def similarity_search_text_docs(\n",
    "    user_question: str,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "    query_vec: Optional[List[float]] = None,\n",
    "    emb=None,\n",
    "    use_cache: Optional[bool] = None,   \n",
    "):\n",
    "    import numpy as np\n",
    "    if use_cache is None:\n",
    "        use_cache = CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "\n",
    "    if query_vec is not None and use_cache:\n",
    "        qv = np.asarray(query_vec, dtype=np.float32)\n",
    "        qv /= (np.linalg.norm(qv) + 1e-12)\n",
    "        results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "        return user_question, results\n",
    "\n",
    "    if use_cache and user_question in _QVEC_CACHE:\n",
    "        qv = _QVEC_CACHE[user_question]\n",
    "        results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "        return user_question, results\n",
    "\n",
    "    if emb is None:\n",
    "        emb = globals().get(\"sentence_emb\", None)\n",
    "    if emb is None:\n",
    "        raise ValueError(\"similarity_search_text_docs: need `emb` when no cache/vec provided.\")\n",
    "\n",
    "    qv = np.asarray(emb.embed_query(user_question), dtype=np.float32)\n",
    "    qv /= (np.linalg.norm(qv) + 1e-12)\n",
    "\n",
    "    if use_cache:\n",
    "        _QVEC_CACHE[user_question] = qv  \n",
    "\n",
    "    results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "    return user_question, results\n",
    "\n",
    "\n",
    "def make_text_qa_prompt(\n",
    "    question: str,\n",
    "    retrieved_docs=None\n",
    ") -> str:\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, _ = retrieved_docs[0]\n",
    "        related_q_txt = doc0.page_content.strip()\n",
    "        related_answer = (doc0.metadata or {}).get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION TEXT]:\\n{related_q_txt}\\n\"\n",
    "            f\"[RELATED ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    sections.append(f\"[CURRENT QUESTION]: {question}\")\n",
    "\n",
    "    mode = _mode()\n",
    "\n",
    "    if mode in {\"yes_no\", \"binary\"}:\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        rules = (\n",
    "            \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "            f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "            \"- Do NOT copy or summarize any context.\\n\"\n",
    "            \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "    else:\n",
    "        style_line = {\n",
    "            \"short\":    \"- Give a short, direct answer in 2–3 sentences.\\n\",\n",
    "            \"detail\":   \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"long\":     \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"reasoning\":\"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "            \"explain\":  \"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "        }.get(mode, \"- Provide a clear and helpful answer.\\n\")\n",
    "\n",
    "        rules = (\n",
    "            \"[TASK]: You are a QA assistant for open-ended questions.\\n\"\n",
    "            f\"{style_line}\"\n",
    "            \"- Do NOT restrict to yes/no.\\n\"\n",
    "            \"[FORMAT]: Write complete sentences (not a single word).\"\n",
    "            \"Avoid starting with just 'Yes.' or 'No.'; if the question is yes/no-style, state the conclusion AND 1–2 short reasons.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "\n",
    "    sections.append(rules)\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "def answer_with_llm_text(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    q_vec=None, \n",
    "    *,\n",
    "    text_db: Optional[\"FAISS\"] = None,\n",
    "    max_retries: int = 3,  \n",
    ") -> str:\n",
    "  \n",
    "    retrieved_docs = None\n",
    "    if text_db:\n",
    "        _, hits = similarity_search_text_docs(\n",
    "            question, text_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "            emb=sentence_emb,\n",
    "            use_cache=CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "        )\n",
    "        retrieved_docs = hits\n",
    "\n",
    "   \n",
    "    prompt = make_text_qa_prompt(question, retrieved_docs)\n",
    "    mode = _mode()\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "\n",
    "\n",
    "        raw = _gen(gen_pipe, prompt)\n",
    "    \n",
    "        print(f\"----- RAW (try {attempt}):\", raw)\n",
    "\n",
    "        text = _extract_answer_text(prompt, raw)\n",
    "        answer, think = strip_think(text)  \n",
    "        answer = (answer or \"\").strip()\n",
    "        print(\"----- ANS:\", answer)\n",
    "\n",
    "        if not answer:\n",
    "            continue\n",
    "\n",
    "        YES_RE = re.compile(r\"^\\s*(yes|y|true|correct|affirmative)\\s*\\.?\\s*$\", re.I)\n",
    "        NO_RE  = re.compile(r\"^\\s*(no|n|false|incorrect|negative)\\s*\\.?\\s*$\", re.I)\n",
    "        if mode in {\"yes_no\", \"binary\"}:\n",
    "            if YES_RE.match(answer) and not NO_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(answer) and not YES_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "\n",
    "            strict_suffix = (\n",
    "                \"\\n\\n[FORMAT]: Answer with exactly ONE token: \"\n",
    "                + (\"YES or NO.\" if CONFIG.get(\"answer_uppercase\", True) else \"yes or no.\")\n",
    "            )\n",
    "            raw2 = _gen(gen_pipe, prompt + strict_suffix)\n",
    "            ans2 = _extract_answer_text(prompt + strict_suffix, raw2)\n",
    "            ans2, _ = strip_think(ans2)\n",
    "            if YES_RE.match(ans2) and not NO_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(ans2) and not YES_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "            return ans2.strip() if ans2 else answer\n",
    "\n",
    "        else:\n",
    "            if YES_RE.match(answer) or NO_RE.match(answer) or len(answer.split()) <= 2:\n",
    "                format_suffix = (\n",
    "                    \"\\n\\n[FORMAT]: Provide a 2–3 sentence explanation; \"\n",
    "                    \"do not answer with a single word.\"\n",
    "                )\n",
    "                raw2 = _gen(gen_pipe, prompt + format_suffix)\n",
    "                ans2 = _extract_answer_text(prompt + format_suffix, raw2)\n",
    "                ans2, _ = strip_think(ans2)\n",
    "                if ans2 and len(ans2.strip()) > len(answer):\n",
    "                    return ans2.strip()\n",
    "            return answer\n",
    "\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_index_from_cached(\n",
    "    docs: List[Document],\n",
    "    emb,\n",
    ") -> FAISS:\n",
    "    use_cache = CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "    texts, metas, vecs = [], [], []\n",
    "\n",
    "    for d in docs:\n",
    "        texts.append(d.page_content)\n",
    "        metas.append(d.metadata)\n",
    "        if use_cache:\n",
    "            v = d.metadata.get(\"q_embeddings\", None) or d.metadata.get(\"q_vec\", None)\n",
    "            if v is None:\n",
    "                v = emb.embed_query(d.page_content)\n",
    "        else:\n",
    "            # 强制现算（不读 metadata 中存的）\n",
    "            v = emb.embed_query(d.page_content)\n",
    "        vecs.append(v)\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No docs provided to build_faiss_index_from_cached().\")\n",
    "\n",
    "    X = np.asarray(vecs, dtype=np.float32)\n",
    "    # 内积索引建议归一化；L2 也可以保留\n",
    "    X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # ① 新签名：from_embeddings(text_embeddings=[(text, vec), ...], embedding=..., metadatas=[...])\n",
    "    text_embeddings = [(t, X[i].tolist()) for i, t in enumerate(texts)]\n",
    "    try:\n",
    "        return FAISS.from_embeddings(\n",
    "            text_embeddings, embedding=emb, metadatas=metas\n",
    "        )\n",
    "    except TypeError:\n",
    "        # ② 旧签名：from_embeddings(embeddings=[vec...], metadatas=[...], texts=[...], embedding=...)\n",
    "        try:\n",
    "            return FAISS.from_embeddings(\n",
    "                embeddings=X.tolist(), metadatas=metas, texts=texts, embedding=emb\n",
    "            )\n",
    "        except Exception:\n",
    "            # ③ 回退路径：from_texts 或 add_embeddings\n",
    "            try:\n",
    "                vs = FAISS.from_texts(texts=[], embedding=emb)\n",
    "                if hasattr(vs, \"add_embeddings\"):\n",
    "                    vs.add_embeddings(embeddings=X, metadatas=metas, texts=texts)\n",
    "                else:\n",
    "                    # 最老的版本只能 add_texts（会重算嵌入，缓存用不上）\n",
    "                    vs.add_texts(texts=texts, metadatas=metas)\n",
    "                return vs\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to build FAISS index with cached vectors: {e}\")\n",
    "\n",
    "def build_text_faiss_index_with_answers(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    bootstrap_db: Optional[FAISS] = None\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    用文本 RAG 路线生成答案并入库，然后返回 FAISS 向量库。\n",
    "    bootstrap_db: 若传入，文本检索会优先引用该库的历史问答作为 retrieved context（冷启动可传 None）。\n",
    "    \"\"\"\n",
    "    docs, q_vec = build_text_docs_with_answer(\n",
    "        questions=questions,\n",
    "        gen_pipe=gen_pipe,\n",
    "        add_prompt_snapshot=add_prompt_snapshot,\n",
    "        text_db=bootstrap_db,\n",
    "    )\n",
    "    print(docs)\n",
    "    return build_faiss_index_from_cached(docs, sentence_emb), q_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3966e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_once_mode(\n",
    "    question: str,\n",
    "    mode: str,                 # \"text\" or \"graph\"\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser=None,\n",
    "    text_db: Optional[FAISS] = None,\n",
    "    graph_db: Optional[FAISS] = None,\n",
    "    q_vecs = None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    assert mode in (\"text\", \"graph\")\n",
    "\n",
    "    retrieved_docs = None\n",
    "    retrieval_latency = 0.0\n",
    "    retrieved_count = 0\n",
    "\n",
    "    # ---- 1) 检索 + 计时（仅检索耗时）----\n",
    "    if mode == \"text\":\n",
    "        if text_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_text_docs(\n",
    "                question, text_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "                emb=sentence_emb,\n",
    "                use_cache=CONFIG.get(\"use_cached_text_embeddings\", True)  # 显式\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        prompt = make_text_qa_prompt(question, retrieved_docs=retrieved_docs)\n",
    "    else:\n",
    "        if graph_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_graph_docs(\n",
    "                question, parser, graph_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "                emb_model=word_emb,\n",
    "                use_cache=CONFIG.get(\"use_cached_graph_embeddings\", True),\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    # 小工具：token 计数\n",
    "    def _count_tokens(tokenizer, text: str) -> int:\n",
    "        return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # ---- 2) 推理计时（生成部分）----\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_g0 = time.perf_counter()\n",
    "    out = gen_pipe(prompt)\n",
    "    gen_latency = time.perf_counter() - t_g0  # 仅生成耗时\n",
    "\n",
    "    text = out[0][\"generated_text\"]\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # ---- 3) 汇总 ----\n",
    "    return {\n",
    "        \"label\": label or f\"{mode}_rag\",\n",
    "        \"mode\": mode,\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": retrieval_latency + gen_latency,  \n",
    "        \"gen_latency_sec\": gen_latency,                   \n",
    "        \"retrieval_latency_sec\": retrieval_latency,       \n",
    "        \"retrieved_count\": retrieved_count,               \n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "        \"used_retrieval\": bool(retrieved_docs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compare_text_vs_graph(\n",
    "    questions: List[str],\n",
    "    gen_pipe, tokenizer, parser,\n",
    "    text_db: Optional[FAISS],\n",
    "    graph_db: Optional[FAISS],\n",
    "    q_vecs = None,\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q in questions:\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"text\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"text_rag\", q_vecs=q_vecs)\n",
    "        )\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"graph\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"graph_rag\", q_vecs=q_vecs)\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _normalize_yesno(text: str) -> str:\n",
    "    if text is None: return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\" or (\"yes\" in t and \"no\" not in t): return \"YES\"\n",
    "    if t == \"no\"  or (\"no\"  in t and \"yes\" not in t): return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def _norm_q(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"把 gold label 合并到 df，并生成 pred / correct 列。一定要 return DataFrame。\"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"attach_gold: input df is None or not a DataFrame\")\n",
    "\n",
    "    g = pd.DataFrame(list(gold_map.items()), columns=[\"question\",\"gold\"])\n",
    "    g[\"question_norm\"] = g[\"question\"].map(_norm_q)\n",
    "    g[\"gold\"] = g[\"gold\"].map(lambda x: \"YES\" if str(x).upper()==\"YES\" else \"NO\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"question_norm\"] = out[\"question\"].map(_norm_q)\n",
    "    out = out.merge(g[[\"question_norm\",\"gold\"]], on=\"question_norm\", how=\"left\")\n",
    "\n",
    "    out[\"pred\"] = out[\"answer\"].map(_normalize_yesno)\n",
    "    out[\"correct\"] = (out[\"pred\"] == out[\"gold\"]).astype(int)\n",
    "\n",
    "    # 便于排错：提示没有 gold 命中的题\n",
    "    miss = out[out[\"gold\"].isna()]\n",
    "    if len(miss):\n",
    "        print(f\"⚠️ {len(miss)} questions had no gold match. Showing a few:\")\n",
    "        print(miss[[\"question\",\"label\"]].head(5))\n",
    "\n",
    "    return out  # ←←← 关键：确保返回\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame):\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for k, sub in df_with_gold.groupby(\"label\"):\n",
    "        n = len(sub[sub[\"gold\"].notna()])\n",
    "        acc = sub[\"correct\"].mean() if n else float(\"nan\")\n",
    "        print(f\"{k:<10s} acc={acc:.3f} (n={n})\")\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    A = df[df[\"label\"] == base_label]\n",
    "    B = df[df[\"label\"] == target_label]\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b - a) / max(1e-9, a)\n",
    "\n",
    "    print(\"\\n== Cost (avg) ==\")\n",
    "    for col in [\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"latency_sec\",             \n",
    "        \"retrieval_latency_sec\",   \n",
    "        \"gen_latency_sec\",        \n",
    "        \"retrieved_count\",         \n",
    "        \"peak_vram_MiB\",\n",
    "        \"prompt_chars\",\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            a, b, d = avg(col)\n",
    "            print(f\"{col:>22s} | {base_label}: {a:8.4f} | {target_label}: {b:8.4f} | Δ%: {d*100:7.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9916527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_size_bytes(path: str) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            fp = os.path.join(root, f)\n",
    "            try: total += os.path.getsize(fp)\n",
    "            except OSError: pass\n",
    "    return total\n",
    "\n",
    "def save_and_report_sizes(text_db: FAISS, graph_db: FAISS, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\"):\n",
    "    text_db.save_local(text_dir)\n",
    "    graph_db.save_local(graph_dir)\n",
    "    def human(n):\n",
    "        u=[\"B\",\"KB\",\"MB\",\"GB\"]; i=0; x=float(n)\n",
    "        while x>=1024 and i<len(u)-1: x/=1024.0; i+=1\n",
    "        return f\"{x:.2f} {u[i]}\"\n",
    "    s_text  = dir_size_bytes(text_dir)\n",
    "    s_graph = dir_size_bytes(graph_dir)\n",
    "    print(f\"[Index size] text_rag  = {human(s_text)}  ({text_dir})\")\n",
    "    print(f\"[Index size] graph_rag = {human(s_graph)}  ({graph_dir})\")\n",
    "    return s_text, s_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4e9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use mps\n",
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RAW (try 1):  The geocentric model...<think>\n",
      "Okay, let's tackle this question. The user is asking if Earth is generally considered round despite its equatorial bulge. First, I need to recall what the equatorial bulge means. Earth isn't a perfect sphere; it's an oblate spheroid. That means it's slightly wider around the equator than through the poles. But the key here is the term \"generally considered round.\" I should check common perceptions. Most people, even if they know a bit about the bulge, still refer to Earth as a sphere for simplicity, especially in educational contexts. Scientific classifications like \"oblate spheroid\" are more precise, but the general consensus is that it's round. Also, the geocentric model mention in the prompt might be a red herring, but the answer should clarify that the question isn't about that model. Wait, the FORMAT mentions the geocentric model... but the user's actual question is about Earth's shape. Maybe that part is a hint to address a common misconception. So, putting it all together: Earth's shape is technically an oblate spheroid, but it's commonly referred to as round. The bulge is a well-known feature, but the general understanding doesn't contradict the spherical approximation for most purposes\n",
      "----- ANS: The geocentric model...\n",
      "[Document(metadata={'graph_id': 'Q1', 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'The geocentric model...', 'created_at': 1757081227, 'edge_matrix': [[0, 0, 1]], 'questions_lst': [[[0]]], 'answers_lst': [[]], 'e_embeddings': [array([-0.04133078,  0.12045681,  0.16340296,  0.07953241,  0.00281928,\n",
      "       -0.04604143,  0.02934419,  0.03239037, -0.05704931,  0.05387106,\n",
      "        0.07525693, -0.06266754,  0.12119845, -0.04496451,  0.04772114,\n",
      "       -0.18962815,  0.08505752,  0.1742195 , -0.019293  , -0.00371298,\n",
      "        0.05644143, -0.03584292, -0.04177272, -0.00956963,  0.14327857,\n",
      "        0.14419462, -0.00358836,  0.09421128,  0.13154258, -0.01902208,\n",
      "        0.03093247, -0.02059343, -0.07147927,  0.03520795,  0.00341243,\n",
      "        0.00047286, -0.01764885,  0.01146134, -0.01279359, -0.06595246,\n",
      "       -0.2638268 , -0.01307535, -0.08108682,  0.07289144, -0.06156185,\n",
      "        0.06303667, -0.10670077,  0.13114128, -0.02227145,  0.00478058,\n",
      "        0.07221922,  0.01635164,  0.1126949 ,  0.18410812, -0.00998549,\n",
      "       -0.35998702,  0.06055097,  0.03900085,  0.267552  ,  0.10015293,\n",
      "        0.00428987,  0.24010423, -0.06603882, -0.02456581,  0.17076525,\n",
      "        0.10218315,  0.00107392, -0.05564729,  0.09148853, -0.12731959,\n",
      "        0.07338757, -0.0162641 ,  0.07240887,  0.06943551, -0.02860085,\n",
      "       -0.1488223 ,  0.10808247, -0.10265049, -0.00869184,  0.13032682,\n",
      "        0.01323012,  0.19496192, -0.18056923,  0.04715051, -0.04242462,\n",
      "        0.05719832,  0.13192187, -0.04736894,  0.07746325, -0.02944748,\n",
      "        0.08384176,  0.04153566,  0.06023941,  0.08300699, -0.03516054,\n",
      "       -0.05677331, -0.15340425, -0.04055865,  0.01556648, -0.04942625],\n",
      "      dtype=float32), array([-0.08490037,  0.03239898,  0.04945718,  0.03578956, -0.02220758,\n",
      "        0.03960995,  0.08009425,  0.08027262, -0.11974717,  0.04294418,\n",
      "       -0.02302487, -0.02976905,  0.08561696,  0.05444304,  0.00287928,\n",
      "        0.06678126, -0.04884211,  0.06508688,  0.00145609, -0.0853819 ,\n",
      "        0.03762599,  0.0543016 ,  0.07778646,  0.06172772,  0.10635652,\n",
      "       -0.03478695,  0.05561185, -0.1067351 ,  0.02164343, -0.05637434,\n",
      "       -0.09012754,  0.03477546,  0.05480266, -0.08069087,  0.04496384,\n",
      "        0.00271333, -0.0623362 ,  0.09723349, -0.03869739, -0.02672886,\n",
      "       -0.0499635 , -0.17106985,  0.03643564, -0.05726514, -0.03047213,\n",
      "       -0.04055556,  0.06864718, -0.05032125, -0.03544915, -0.15892199,\n",
      "       -0.05811297, -0.00317245,  0.03463465,  0.24254745,  0.0077377 ,\n",
      "       -0.6014154 ,  0.01927492, -0.05524864,  0.2740004 ,  0.10284649,\n",
      "       -0.14186041,  0.16720925, -0.06199209,  0.07090721,  0.118666  ,\n",
      "       -0.04279763,  0.00118174,  0.07727258, -0.04284864, -0.10698166,\n",
      "        0.03093642, -0.04420516, -0.05863086, -0.05301446,  0.0554232 ,\n",
      "       -0.03079346, -0.07513116, -0.03686947, -0.08853129,  0.02463695,\n",
      "        0.10218775,  0.02636018, -0.14365277,  0.03194273, -0.16125026,\n",
      "       -0.06622566, -0.0407603 , -0.080739  , -0.0149323 , -0.00501625,\n",
      "        0.0021734 ,  0.02268867, -0.01020462,  0.12267026, -0.0585927 ,\n",
      "        0.05285858, -0.05591026, -0.02271547,  0.19864208, -0.03465103],\n",
      "      dtype=float32)], 'r_embeddings': [array([ 0.13378012, -0.05760282, -0.00130663,  0.08330501,  0.00688825,\n",
      "        0.01164046, -0.1853098 , -0.06166784,  0.14242698, -0.01164238,\n",
      "        0.04585325,  0.03980619,  0.04807544, -0.06888603, -0.08853245,\n",
      "       -0.04106433,  0.0592699 ,  0.10756981, -0.08584042,  0.09156555,\n",
      "        0.07329212, -0.01941846,  0.01561325,  0.01447866, -0.0084283 ,\n",
      "       -0.13016929, -0.03986188, -0.11585125, -0.23025815, -0.03526089,\n",
      "        0.04920654,  0.07274571,  0.06347761,  0.06064811, -0.01100478,\n",
      "        0.00045383,  0.04411135, -0.02366794, -0.03742391, -0.05593401,\n",
      "        0.06120669, -0.18647568,  0.07662627, -0.06592775,  0.06746954,\n",
      "       -0.02722309, -0.01115705,  0.06583552, -0.00113993, -0.13801916,\n",
      "        0.01603941, -0.01546725,  0.06319048,  0.1030715 ,  0.08156832,\n",
      "       -0.23861092, -0.01209917, -0.1368515 ,  0.38795155,  0.01362755,\n",
      "       -0.00109778,  0.05567298, -0.04311772,  0.01433284,  0.18736318,\n",
      "       -0.0450841 ,  0.11614011, -0.01385586,  0.14335623, -0.08318668,\n",
      "       -0.15288362,  0.02719524,  0.02723352, -0.04002197,  0.01923748,\n",
      "       -0.12354449, -0.01491701,  0.07171553, -0.24806   ,  0.0972002 ,\n",
      "        0.1584504 ,  0.11366734,  0.00223941,  0.0821913 , -0.02626947,\n",
      "       -0.02420912, -0.05731744, -0.15189172,  0.09761958, -0.10604543,\n",
      "       -0.10835811, -0.18912074,  0.11484368,  0.00350347, -0.09144374,\n",
      "        0.06130762,  0.10247811, -0.05963185,  0.11528395, -0.16078395],\n",
      "      dtype=float32)]}, page_content=\"{'e': ['Earth', 'round in shape'], 'r': ['property']}\")]\n",
      "----- RAW (try 1):  The Earth is generally considered round, though it is an oblate spheroid due to its rotation. This shape is confirmed by satellite photos, circumnavigation travels, and observations by astronauts.<think>\n",
      "Okay, let's tackle this question. The user is asking if Earth is considered round despite its slight equatorial bulge. First, I need to recall what an oblate spheroid is. From what I remember, because the Earth spins, the centrifugal force causes the equator to bulge out a bit bit. So it's not a perfect sphere, but it's still mostly spherical.\n",
      "\n",
      "Then, how do people know it's round? Well, circumnavigation means traveling around the Earth along a line of longitude, which requires the Earth to be roughly spherical. Also, satellite images probably show the round shape. Astronauts in space might have seen the Earth from orbit, which would definitely show its curvature.\n",
      "\n",
      "Wait, the question mentions \"generally considered round.\" So even though it's technically an oblate spheroid, the common perception is that it's a sphere or round. That makes sense because the bulge isn't huge compared to the overall size. Most people don't notice the slight flattening unless they're looking at topographical maps or images from space.\n",
      "\n",
      "I should also think about\n",
      "----- ANS: The Earth is generally considered round, though it is an oblate spheroid due to its rotation. This shape is confirmed by satellite photos, circumnavigation travels, and observations by astronauts.\n",
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?', 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'The Earth is generally considered round, though it is an oblate spheroid due to its rotation. This shape is confirmed by satellite photos, circumnavigation travels, and observations by astronauts.', 'created_at': 1757081249, 'q_embeddings': [0.06542075425386429, -0.014430391602218151, -0.018086358904838562, -0.014991224743425846, 0.01676652580499649, -0.06757217645645142, -0.009964372962713242, 0.04057123884558678, 0.046724963933229446, 0.007202856242656708, -0.04560878127813339, -0.011130229569971561, -0.029934514313936234, -0.005765230394899845, 0.10064125806093216, -0.1872469037771225, -0.08053438365459442, -0.05268656089901924, 0.0744856745004654, 0.044800397008657455, -0.056824274361133575, 0.030281415209174156, 0.031367264688014984, 0.09395118802785873, -0.007329029031097889, -0.02491253986954689, 0.03077881969511509, -0.037623219192028046, -0.07332856953144073, -0.058687105774879456, -0.06748991459608078, 0.0685398280620575, -0.009645778685808182, -0.024280408397316933, -0.03764333948493004, -0.009335151873528957, 0.030721653252840042, 0.02179000899195671, 0.01986871100962162, 0.010883891955018044, 0.034092482179403305, -0.019201675429940224, 0.1001710519194603, 0.038083042949438095, -0.027212996035814285, -0.019198007881641388, -0.09353438019752502, 0.013230457901954651, 0.02172282710671425, -0.03411518782377243, 0.08047931641340256, -0.10348403453826904, -0.05585544556379318, 0.051338374614715576, -0.056574661284685135, -0.03898410126566887, -0.004007314331829548, -0.030923059210181236, 0.06842052191495895, -0.0017821870278567076, 0.049198418855667114, -0.059234220534563065, -0.024465229362249374, 0.06328897178173065, -0.0481225810945034, 0.02219015173614025, -0.03131762146949768, -0.08929338306188583, -0.1087387204170227, -0.045089300721883774, -0.02673696167767048, 0.015236623585224152, -0.08332531154155731, -0.06985262036323547, 0.01101217046380043, 0.0189246516674757, 0.015416685491800308, 0.027726175263524055, -0.023263033479452133, 0.0056232186034321785, -0.03328915685415268, 0.118981271982193, -0.017964079976081848, 0.06439021974802017, -0.039911992847919464, -0.066289521753788, -0.04219970852136612, -0.051615871489048004, -0.031110869720578194, -0.020036309957504272, -0.013301806524395943, 0.062365662306547165, 0.009858524426817894, -0.004559061489999294, -0.035305894911289215, 0.06350944936275482, 0.01903628557920456, -0.05611179396510124, 0.02509717456996441, -0.006962779443711042, 0.04144595190882683, 0.06174908205866814, 0.03876348212361336, 0.016144415363669395, 0.04379544407129288, 0.06549662351608276, 0.013735910877585411, -0.0018381986301392317, 0.02584477886557579, -0.07691558450460434, -0.08831297606229782, 0.04152350500226021, 0.05239245295524597, -0.007808252237737179, -0.006640582345426083, -0.03776084631681442, 0.028710011392831802, 0.02577335014939308, -0.09520390629768372, -0.01725289411842823, -0.024209752678871155, 0.030802978202700615, 0.07638406753540039, 0.08433720469474792, 0.08817622810602188, 0.01968533731997013, -0.023006197065114975, -1.730688597910308e-33, 0.013077212497591972, 0.06721542030572891, 0.08100231736898422, 0.024515625089406967, -0.06854503601789474, 0.018323136493563652, -0.0056765759363770485, 0.03357704356312752, 0.02367384545505047, -0.0719379410147667, -0.06801832467317581, -0.03936314582824707, 0.0745881050825119, -0.02229251153767109, 0.05882708728313446, 0.022234130650758743, 0.029007574543356895, 0.0935809388756752, -0.02798147313296795, -0.005991792306303978, -0.09558990597724915, -0.002000095322728157, 0.022795138880610466, -0.0789325088262558, -0.07986985892057419, 0.00033006741432473063, 0.009036975912749767, -0.014598245732486248, -0.06490974873304367, -0.0036359375808387995, 0.05314653366804123, -0.04043601453304291, 0.04380303993821144, 0.06497499346733093, 0.02613372728228569, -0.01858905330300331, 0.006514766253530979, 0.07224921137094498, 0.0011794216698035598, 0.011040201410651207, -0.021111587062478065, -0.01145181618630886, 0.01239546574652195, 0.02257627248764038, 0.03849472850561142, 0.06222790852189064, 0.0372113361954689, -0.06309938430786133, -0.014775401912629604, 0.014645887538790703, 0.01569734700024128, 0.017223678529262543, 0.035711996257305145, -0.006725745275616646, 0.06559184193611145, -0.0025028411764651537, 0.006596233695745468, -0.03287944197654724, -0.0937613993883133, 0.0328063890337944, 0.14904002845287323, -0.018386580049991608, -0.022701401263475418, -0.056399114429950714, 0.020021013915538788, 0.028359517455101013, 0.018710236996412277, -0.01750471070408821, -0.016675014048814774, -0.009937374852597713, 0.053729575127363205, 0.04722920432686806, 0.031173715367913246, 0.1224491223692894, 0.03322848305106163, 0.035818278789520264, 0.06905853748321533, -0.03556734696030617, 0.016759617254137993, 0.08946263790130615, -0.0320582278072834, -0.009426604956388474, -0.03170815855264664, -0.0661376342177391, -0.04271269217133522, -0.014607200399041176, -0.004013272933661938, 0.015808450058102608, 0.03631596267223358, 0.023200811818242073, -0.013836358673870564, -0.053956758230924606, 0.01937195286154747, 0.060121696442365646, -0.12716947495937347, -8.20330728733019e-35, -0.08066537231206894, -0.01545698381960392, -0.04602224752306938, 0.010435989126563072, 0.03369175270199776, 0.02628834918141365, -0.049015432596206665, 0.07723827660083771, -0.10796883702278137, -0.015673724934458733, 0.05596829205751419, 0.027238639071583748, 0.013382339850068092, -0.08952008932828903, 0.06330560147762299, 0.030434899032115936, -0.027924662455916405, -0.008808346465229988, -0.0622200146317482, 0.009348281659185886, 0.049821119755506516, -0.11445072293281555, -0.01617974415421486, -0.0024084236938506365, 0.053669992834329605, 0.04018454626202583, -0.06578603386878967, -0.04084031656384468, -0.0015409947372972965, 0.09263856709003448, -0.02278146706521511, -0.046609461307525635, -0.03663245961070061, -0.04493989422917366, -0.019731083884835243, -0.02461951971054077, -0.03933260217308998, -0.026153868064284325, -0.01811322569847107, 0.012302903458476067, -0.09737702459096909, -0.03301510959863663, 0.025879470631480217, -0.003306790953502059, 0.02002006582915783, -0.15236502885818481, 0.08923696726560593, 0.14038686454296112, -0.04837346822023392, 0.10203997790813446, -0.09147225320339203, 0.04281827062368393, 0.04091447964310646, 0.08235223591327667, -0.012447556480765343, 0.06230970844626427, -0.046568650752305984, 0.06411930173635483, -0.017105136066675186, -0.03274594619870186, 0.021926041692495346, -0.012977506965398788, 0.04518650099635124, -0.12019401043653488, 0.009310913272202015, -0.004616734106093645, -0.0072496733628213406, 0.011264736764132977, -0.04779358208179474, 0.04197615757584572, -0.0026414894964545965, -0.07543329149484634, -0.051555149257183075, 0.01991504617035389, 0.06339653581380844, -0.04030786082148552, 0.12550204992294312, -0.07167152315378189, -0.014737440273165703, 0.05130148306488991, -0.014117660000920296, -0.001122637651860714, 0.05391814932227135, -0.0223163440823555, -0.061626799404621124, 0.010457728058099747, 0.03316982090473175, -0.004644106607884169, -0.03482661023736, 0.1225762888789177, -0.010322832502424717, -0.005325291771441698, 0.02627125009894371, 0.044896405190229416, -0.045712318271398544, -2.4205810689181817e-08, 0.03333791345357895, -0.03862835094332695, 0.04956555739045143, 0.03983752802014351, -0.0635344460606575, 0.03153986111283302, 0.045332007110118866, -0.02125292830169201, -0.08542105555534363, -0.010851306840777397, -0.002553594531491399, -0.007342382334172726, 0.049966003745794296, -0.003926665056496859, -0.04524504393339157, -0.052054066210985184, -0.0007079971255734563, 0.09585142880678177, -0.006990235764533281, 0.08813624083995819, -0.00853783916682005, -0.03226854279637337, -0.0444849357008934, -0.06479892134666443, 0.0082198865711689, -0.04428539797663689, -0.025398725643754005, 0.10677175968885422, -0.041835084557533264, -0.09008990973234177, -0.005559274461120367, 0.028867287561297417, 0.018398914486169815, -0.05889801308512688, -0.06581767648458481, 0.019539235159754753, 0.0071434564888477325, 0.03296918421983719, 0.02263159491121769, -0.009836997836828232, 0.012763071805238724, 0.04070437327027321, 0.0848914161324501, 0.04909476265311241, 0.041821692138910294, 0.06300928443670273, 0.041623398661613464, 0.03268375247716904, 0.04486740380525589, -0.08801893889904022, -0.052553437650203705, 0.012072556652128696, 0.04224124550819397, 0.023770490661263466, -0.013869422487914562, -0.018761835992336273, -0.0532124824821949, -0.07043259590864182, 0.04486360400915146, 0.054040610790252686, -0.044239699840545654, -0.05985434725880623, 0.002100657904520631, 0.02980027347803116]}, page_content='Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?')]\n",
      "[Index size] text_rag  = 5.73 KB  (faiss_text_idx)\n",
      "[Index size] graph_rag = 3.31 KB  (faiss_graph_idx)\n",
      "\n",
      "== Cost (avg) ==\n",
      "          input_tokens | text_rag: 260.0000 | graph_rag: 206.0000 | Δ%:  -20.77%\n",
      "         output_tokens | text_rag:  41.0000 | graph_rag:   6.0000 | Δ%:  -85.37%\n",
      "          total_tokens | text_rag: 301.0000 | graph_rag: 212.0000 | Δ%:  -29.57%\n",
      "           latency_sec | text_rag:   4.1226 | graph_rag:   1.0106 | Δ%:  -75.49%\n",
      " retrieval_latency_sec | text_rag:   0.0091 | graph_rag:   0.0071 | Δ%:  -22.55%\n",
      "       gen_latency_sec | text_rag:   4.1135 | graph_rag:   1.0035 | Δ%:  -75.60%\n",
      "       retrieved_count | text_rag:   1.0000 | graph_rag:   1.0000 | Δ%:    0.00%\n",
      "         peak_vram_MiB | text_rag:      nan | graph_rag:      nan | Δ%:     nan%\n",
      "          prompt_chars | text_rag: 1176.0000 | graph_rag: 889.0000 | Δ%:  -24.40%\n"
     ]
    }
   ],
   "source": [
    "gen_pipe, tokenizer = load_llm_pipeline()\n",
    "parser = RelationshipGraphParser()\n",
    "\n",
    "graph_docs = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    faiss_db=None\n",
    ")\n",
    "print(graph_docs)\n",
    "graph_db = build_graph_faiss_index_from_cached(graph_docs, word_emb)\n",
    "\n",
    "text_db, q_vecs = build_text_faiss_index_with_answers(\n",
    "    questions,\n",
    "    gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    bootstrap_db=None  \n",
    ")\n",
    "\n",
    "save_and_report_sizes(text_db, graph_db, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\")\n",
    "\n",
    "eval_questions = list(GOLD_LABELS.keys())[:1]\n",
    "df = batch_compare_text_vs_graph(\n",
    "    eval_questions, gen_pipe, tokenizer, parser, text_db, graph_db, q_vecs\n",
    ")\n",
    "df_ab_gold = attach_gold(df, GOLD_LABELS)\n",
    "#evaluate_accuracy(df_ab_gold)\n",
    "summarize_cost(df_ab_gold, base_label=\"text_rag\", target_label=\"graph_rag\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
