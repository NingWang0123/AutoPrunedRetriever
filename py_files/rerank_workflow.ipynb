{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from graph_generator.graphparsers import RelationshipGraphParser\n",
    "from groupwords import *\n",
    "from linearization_utils import *\n",
    "from retrieval_utils import similarity_search_graph_docs\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529b5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # === Embedding & VectorStore ===\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Embedding model for documents/questions\n",
    "    \"faiss_search_k\": 3,  # Number of nearest neighbors to retrieve from FAISS\n",
    "\n",
    "    # === LLM (text generation) ===\n",
    "    \"llm_model_id\": \"microsoft/Phi-4-mini-reasoning\",  # HuggingFace model ID  Phi-4-mini-reasoning Phi-4-mini-instruct\n",
    "    \"device_map\": \"auto\",  # Device placement: \"cuda\", \"mps\", \"cpu\", or \"auto\"\n",
    "    \"dtype_policy\": \"auto\",  # Precision: \"auto\", \"bf16\", \"fp16\", or \"fp32\"\n",
    "    \"max_new_tokens\": 256,  # Maximum tokens generated per response\n",
    "    \"do_sample\": True,  # Whether to use sampling (True) or greedy decoding (False)\n",
    "    \"temperature\": 0.4,  # Randomness control for sampling; lower = more deterministic\n",
    "    \"top_p\": 1.0,  # Nucleus sampling threshold; 1.0 = no restriction\n",
    "    \"return_full_text\": False,  # Return full text (input+output) if True, only output if False\n",
    "    \"seed\": None,  # Random seed for reproducibility; set to int or None\n",
    "\n",
    "    # === Prompt / Answer ===\n",
    "    \"answer_mode\": \"short\",  # Answer format mode, e.g., YES/NO\n",
    "    \"answer_uppercase\": True,  # If True → \"YES\"/\"NO\", else \"yes\"/\"no\"\n",
    "\n",
    "    # === Prompt construction ===\n",
    "    \"include_retrieved_context\": True,  # Include retrieved Q&A in prompt\n",
    "    \"include_current_triples\": True,  # Include graph triples in prompt\n",
    "    \"use_cached_text_embeddings\": True, # Whether text rag reuse embedding to search\n",
    "    \"use_cached_graph_embeddings\": True  #Whether graph rag reuse embedding to search\n",
    "}\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed  # Utility for reproducibility\n",
    "except Exception:\n",
    "    set_seed = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2dd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/xzmgrvdj0gj6gtqph130n1fr0000gn/T/ipykernel_24533/3364762243.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import re\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class WordAvgEmbeddings(Embeddings):\n",
    "    def __init__(self, model_path: str = \"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model.vectors.npy\"):\n",
    "        self.kv = KeyedVectors.load(model_path, mmap='r')\n",
    "        self.dim = self.kv.vector_size\n",
    "        self.token_pat = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        toks = [t.lower() for t in self.token_pat.findall(text)]\n",
    "        vecs = [self.kv[w] for w in toks if w in self.kv]\n",
    "        if not vecs:\n",
    "            return np.zeros(self.dim, dtype=np.float32)\n",
    "        return np.mean(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed_text(t) for t in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed_text(text)\n",
    "\n",
    "word_emb = WordAvgEmbeddings(model_path=\"gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.model\")\n",
    "sentence_emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6946a",
   "metadata": {},
   "source": [
    "## RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_dtype() -> torch.dtype:\n",
    "    \"\"\"Choose dtype based on CONFIG['dtype_policy'] and hardware.\"\"\"\n",
    "    policy = CONFIG.get(\"dtype_policy\", \"auto\")\n",
    "    if policy == \"bf16\":\n",
    "        return torch.bfloat16\n",
    "    if policy == \"fp16\":\n",
    "        return torch.float16\n",
    "    if policy == \"fp32\":\n",
    "        return torch.float32\n",
    "\n",
    "    # auto mode\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    # MPS backend works more reliably with fp32\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.float32\n",
    "    return torch.float32\n",
    "\n",
    "def _yn(text_yes=\"YES\", text_no=\"NO\"):\n",
    "    return (text_yes, text_no) if CONFIG.get(\"answer_uppercase\", True) else (text_yes.lower(), text_no.lower())\n",
    "\n",
    "def _avg_pool(mat: np.ndarray) -> np.ndarray:\n",
    "    if mat is None or len(mat) == 0:\n",
    "        return None\n",
    "    m = np.asarray(mat, dtype=np.float32)\n",
    "    if m.ndim == 1:\n",
    "        return m.astype(np.float32)\n",
    "    return m.mean(axis=0).astype(np.float32)\n",
    "\n",
    "def _normalize(v: np.ndarray) -> np.ndarray:\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    v /= (np.linalg.norm(v) + 1e-12)\n",
    "    return v\n",
    "\n",
    "def _graph_doc_vec_from_cached_or_embed(\n",
    "    er_e: list[str],\n",
    "    er_r: list[str],\n",
    "    e_embeds: list | None,\n",
    "    r_embeds: list | None,\n",
    "    emb_model,                     \n",
    "    use_cache: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    文档向量 = concat( avg(e_embeds), avg(r_embeds) ) 或者两者的平均。\n",
    "    这里用简单且稳定的做法：取 e,r 的平均再做均值融合。\n",
    "    \"\"\"\n",
    "    if use_cache and e_embeds is not None and len(e_embeds) and r_embeds is not None and len(r_embeds):\n",
    "        e_mean = _avg_pool(np.asarray(e_embeds, dtype=np.float32))\n",
    "        r_mean = _avg_pool(np.asarray(r_embeds, dtype=np.float32))\n",
    "        v = (e_mean + r_mean) / 2.0\n",
    "        return _normalize(v)\n",
    "\n",
    "    # 缓存不可用 → 现算：对每个实体/关系分别用词向量平均，再整体平均\n",
    "    e_vecs = []\n",
    "    for e in (er_e or []):\n",
    "        e_vecs.append(np.asarray(emb_model.embed_query(e), dtype=np.float32))\n",
    "    r_vecs = []\n",
    "    for r in (er_r or []):\n",
    "        r_vecs.append(np.asarray(emb_model.embed_query(r), dtype=np.float32))\n",
    "\n",
    "    e_mean = _avg_pool(np.stack(e_vecs, axis=0)) if e_vecs else None\n",
    "    r_mean = _avg_pool(np.stack(r_vecs, axis=0)) if r_vecs else None\n",
    "\n",
    "    if e_mean is None and r_mean is None:\n",
    "        # 两边都空，退化为零向量（用实体任意词兜底也可）\n",
    "        dim = getattr(emb_model, \"dim\", None) or len(emb_model.embed_query(\"a\"))\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "\n",
    "    if e_mean is None: v = r_mean\n",
    "    elif r_mean is None: v = e_mean\n",
    "    else: v = (e_mean + r_mean) / 2.0\n",
    "    return _normalize(v)\n",
    "\n",
    "# =========================\n",
    "# Embeddings / Vectorstore\n",
    "# =========================\n",
    "#emb = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])  # Local embedding model (MiniLM-L6-v2, 384 dim)\n",
    "\n",
    "def build_faiss_index(docs: List[Document], emb) -> FAISS:\n",
    "    return FAISS.from_documents(docs, emb)\n",
    "\n",
    "# =========================\n",
    "# LLM Loader\n",
    "# =========================\n",
    "def load_llm_pipeline(\n",
    "    model_id: Optional[str] = None,       # HuggingFace model id\n",
    "    device_map: Optional[str] = None,     # Device placement\n",
    "    dtype: Optional[torch.dtype] = None,  # Torch dtype\n",
    "    max_new_tokens: Optional[int] = None, # Max tokens per generation\n",
    "    temperature: Optional[float] = None,  # Sampling temperature\n",
    "    top_p: Optional[float] = None,        # Nucleus sampling threshold\n",
    "    do_sample: Optional[bool] = None,     # Sampling vs greedy\n",
    "    return_full_text: Optional[bool] = None,  # Return input+output if True\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a text-generation pipeline for QA generation.\n",
    "    All defaults pull from CONFIG; any arg here will override CONFIG.\n",
    "    \"\"\"\n",
    "    model_id = model_id or CONFIG[\"llm_model_id\"]\n",
    "    device_map = device_map or CONFIG[\"device_map\"]\n",
    "    dtype = dtype or _select_dtype()\n",
    "    max_new_tokens = max_new_tokens or CONFIG[\"max_new_tokens\"]\n",
    "    temperature = CONFIG[\"temperature\"] if temperature is None else temperature\n",
    "    top_p = CONFIG[\"top_p\"] if top_p is None else top_p\n",
    "    do_sample = CONFIG[\"do_sample\"] if do_sample is None else do_sample\n",
    "    return_full_text = CONFIG[\"return_full_text\"] if return_full_text is None else return_full_text\n",
    "\n",
    "    if set_seed and isinstance(CONFIG.get(\"seed\"), int):\n",
    "        set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=dtype,\n",
    "        return_full_text=return_full_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return gen_pipe, tokenizer\n",
    "\n",
    "# =========================\n",
    "# Question → Graph (generic)\n",
    "# =========================\n",
    "\n",
    "def print_graph(title: str, G):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"Nodes:\")\n",
    "    for n, d in G.nodes(data=True):\n",
    "        print(f\"  - {n!r} :: {d}\")\n",
    "    print(\"Edges:\")\n",
    "    if G.is_multigraph():\n",
    "        for u, v, k, d in G.edges(keys=True, data=True):\n",
    "            print(f\"  - {u!r} -[{k}]-> {v!r} :: {d}\")\n",
    "    else:\n",
    "        arrow = \"->\" if G.is_directed() else \"--\"\n",
    "        for u, v, d in G.edges(data=True):\n",
    "            print(f\"  - {u!r} {arrow} {v!r} :: {d}\")\n",
    "\n",
    "def parse_question_to_graph_generic(parser, question: str) -> Tuple[nx.Graph, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Compatible with RelationshipGraphParser.question_to_graph\n",
    "    and CausalQuestionGraphParser.question_to_causal_graph\n",
    "    \"\"\"\n",
    "    if hasattr(parser, \"question_to_graph\"):\n",
    "        G, rels = parser.question_to_graph(question)\n",
    "        G = merge_graph_nodes_by_canonical(G, normalizer=normalize_text, merge_edge_attrs=(\"relation\",))\n",
    "        return G, rels\n",
    "    elif hasattr(parser, \"question_to_causal_graph\"):\n",
    "        G, rels = parser.question_to_causal_graph(question)\n",
    "        G = merge_graph_nodes_by_canonical(G, normalizer=normalize_text, merge_edge_attrs=(\"relation\",))\n",
    "        return G, rels\n",
    "    else:\n",
    "        raise AttributeError(\"Parser must provide question_to_graph or question_to_causal_graph\")\n",
    "\n",
    "import ast\n",
    "# =========================\n",
    "# Prompt Builder\n",
    "# =========================\n",
    "def make_graph_qa_prompt(\n",
    "    question: str,\n",
    "    G: nx.Graph,\n",
    "    relations: Optional[List[Dict]] = None,\n",
    "    retrieved_docs = None\n",
    ") -> str:\n",
    "    # 1) retrieved context (if any)\n",
    "    sections = []\n",
    "    compact_json = {}\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, score0 = retrieved_docs[0]\n",
    "\n",
    "        # Get codebook and decode question list\n",
    "        metadata = doc0.metadata or {}\n",
    "        codebook_main = (metadata.get(\"codebook_main\") or {})\n",
    "\n",
    "        query_chains = []\n",
    "        for group_idx, group in enumerate(codebook_main[\"questions_lst\"]):\n",
    "            for q_idx, question_chain in enumerate(group):\n",
    "                query_chains.append(question_chain)    \n",
    "\n",
    "        related_triples = \"__EMPTY_JSON__\"\n",
    "        if query_chains:\n",
    "            wrapper_res = coarse_filter(\n",
    "                questions=query_chains,\n",
    "                codebook_main=codebook_main,\n",
    "                emb=sentence_emb,\n",
    "                top_k=3,\n",
    "                question_batch_size=2,\n",
    "                questions_db_batch_size=8,\n",
    "                top_m=3,\n",
    "            )\n",
    "\n",
    "            if isinstance(wrapper_res, dict) and wrapper_res:\n",
    "                first_non_empty = next((lst for lst in wrapper_res.values() if lst), [])\n",
    "                if first_non_empty:\n",
    "                    related_triples = first_non_empty[0].get(\"text\", \"__EMPTY_JSON__\")\n",
    "                    topm_with_answers = add_answers_to_filtered_lst(wrapper_res, codebook_main)\n",
    "                    flat_lists = get_flat_answers_lsts([it['answers(edges[i])'] for it in topm_with_answers[0]])\n",
    "                    overlaps   = common_contiguous_overlaps(flat_lists, min_len=2)\n",
    "                    uniq_res   = get_unique_knowledge({'overlaps': overlaps}, flat_lists)\n",
    "                    query_chains_flat = [[idx for chain in query_chains for idx in chain]]\n",
    "\n",
    "                    compact_json = get_json_with_given_knowledge(\n",
    "                        flat_answers_lsts = uniq_res['out_answers'],\n",
    "                        codebook_main     = codebook_main,\n",
    "                        codebook_sub_q    = {\n",
    "                            'e': codebook_main['e'],\n",
    "                            'r': codebook_main['r'],\n",
    "                            'edge_matrix': codebook_main['edge_matrix'],\n",
    "                            'questions(edges[i])': query_chains_flat, \n",
    "                            'rule': codebook_main.get('rule', ''),\n",
    "                        },\n",
    "                        decode = True\n",
    "                    )\n",
    "                    \n",
    "                    \n",
    "        related_answer  = doc0.metadata.get(\"llm_answer\", \"\")\n",
    "        \n",
    "        if related_triples != \"__EMPTY_JSON__\":\n",
    "            sections.append(\n",
    "                \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "                \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "                \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "                f\"[RELATED QUESTION'S GRAPH TRIPLES]:\\n{related_triples}\\n\"\n",
    "                f\"[RELATED QUESTION'S ANSWER]: {related_answer}\\n\"\n",
    "                \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "            )\n",
    "\n",
    "    q_block = f\"[CURRENT QUESTION]: {question}\"\n",
    "    sections.append(q_block)\n",
    "\n",
    "    mode = _mode()\n",
    "\n",
    "    if mode in {\"yes_no\", \"binary\"}:\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        rules = (\n",
    "            \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "            f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "            \"- Do NOT copy or summarize any context.\\n\"\n",
    "            \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "    else:\n",
    "        style_line = {\n",
    "            \"short\":    \"- Give a short, direct answer in 2–3 sentences.\\n\",\n",
    "            \"detail\":   \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"long\":     \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"reasoning\":\"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "            \"explain\":  \"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "        }.get(mode, \"- Provide a clear and helpful answer.\\n\")\n",
    "\n",
    "        rules = (\n",
    "            \"[TASK]: You are a QA assistant for open-ended questions.\\n\"\n",
    "            f\"{style_line}\"\n",
    "            \"- Do NOT restrict to yes/no.\\n\"\n",
    "            \"[FORMAT]: Write complete sentences (not a single word).\"\n",
    "            \"Avoid starting with just 'Yes.' or 'No.'; if the question is yes/no-style, state the conclusion AND 1–2 short reasons.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "\n",
    "    sections.append(rules)\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt, compact_json\n",
    "\n",
    "# =========================\n",
    "# LLM Answerer\n",
    "# =========================\n",
    "def _mode():\n",
    "    return (CONFIG.get(\"answer_mode\") or \"short\").lower()\n",
    "\n",
    "def _gen(gen_pipe, prompt):\n",
    "    # 显式传 generation 参数；有些 pipeline 会忽略默认 config\n",
    "    kwargs = dict(\n",
    "        do_sample=CONFIG.get(\"do_sample\", True),\n",
    "        temperature=CONFIG.get(\"temperature\", 0.4),\n",
    "        top_p=CONFIG.get(\"top_p\", 1.0),\n",
    "        max_new_tokens=CONFIG.get(\"max_new_tokens\", 256),\n",
    "        return_full_text=CONFIG.get(\"return_full_text\", False),\n",
    "    )\n",
    "    # 兼容 pad/eos（部分 Phi 模型需要）\n",
    "    try:\n",
    "        tok = gen_pipe.tokenizer\n",
    "        if tok is not None:\n",
    "            if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "                tok.pad_token_id = tok.eos_token_id\n",
    "            kwargs.setdefault(\"eos_token_id\", tok.eos_token_id)\n",
    "            kwargs.setdefault(\"pad_token_id\", tok.pad_token_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out = gen_pipe(prompt, **kwargs)\n",
    "    return out[0][\"generated_text\"]\n",
    "\n",
    "def _extract_answer_text(prompt, text):\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        return text[len(prompt):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def strip_think(s: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    提取并去掉 <think> ... </think> 段，同时识别尾部残缺 <think>...（无 </think>）。\n",
    "    返回:\n",
    "        clean_text: 去掉所有 think 段后的文本\n",
    "        thinks: 提取到的所有 think 内容（含尾部残缺内容）\n",
    "        has_dangling: 是否存在尾部残缺的 <think>\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\", [], False\n",
    "\n",
    "    s_lower = s.lower()\n",
    "    thinks: List[str] = []\n",
    "    spans: List[Tuple[int, int]] = []  # 需要从原文删除的区间 [start, end)\n",
    "\n",
    "    # 1) 提取所有完整的 <think>...</think>\n",
    "    for m in re.finditer(r\"<think>(.*?)</think>\", s, flags=re.S | re.I):\n",
    "        thinks.append(m.group(1).strip())\n",
    "        spans.append((m.start(), m.end()))\n",
    "\n",
    "    # 2) 检测尾部残缺的 <think>...\n",
    "    has_dangling = False\n",
    "    last_open = s_lower.rfind(\"<think>\")\n",
    "    if last_open != -1:\n",
    "        # 若在 last_open 之后找不到 </think>，则视为残缺\n",
    "        if s_lower.find(\"</think>\", last_open) == -1:\n",
    "            has_dangling = True\n",
    "            # 提取残缺内容：从 <think> 后面到字符串末尾\n",
    "            content_start = last_open + len(\"<think>\")\n",
    "            dangling_text = s[content_start:].strip()\n",
    "            if dangling_text:\n",
    "                thinks.append(dangling_text)\n",
    "            spans.append((last_open, len(s)))  # 删除从 <think> 到末尾\n",
    "\n",
    "    # 3) 从原文删除所有 spans（可能存在重叠，先排序再合并）\n",
    "    if spans:\n",
    "        spans.sort()\n",
    "        merged = []\n",
    "        cur_s, cur_e = spans[0]\n",
    "        for st, en in spans[1:]:\n",
    "            if st <= cur_e:  # 重叠或相接\n",
    "                cur_e = max(cur_e, en)\n",
    "            else:\n",
    "                merged.append((cur_s, cur_e))\n",
    "                cur_s, cur_e = st, en\n",
    "        merged.append((cur_s, cur_e))\n",
    "    else:\n",
    "        merged = []\n",
    "\n",
    "    # 4) 拼接非 think 的文本片段\n",
    "    parts = []\n",
    "    prev = 0\n",
    "    for st, en in merged:\n",
    "        if prev < st:\n",
    "            parts.append(s[prev:st])\n",
    "        prev = en\n",
    "    if prev < len(s):\n",
    "        parts.append(s[prev:])\n",
    "\n",
    "    clean_text = \"\".join(parts)\n",
    "\n",
    "    # 5) 额外清理常见的分析前缀（可选）\n",
    "    clean_text = re.sub(r\"(?:^|\\n)\\s*(Okay,|Let’s|Let's|Step by step|Thought:).*\", \"\", clean_text, flags=re.I)\n",
    "\n",
    "    return clean_text.strip(), thinks\n",
    "\n",
    "\n",
    "def answer_with_llm(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    parser,\n",
    "    faiss_db=None,\n",
    "    prompt=None,\n",
    "    max_retries: int = 5,   \n",
    ") -> str:\n",
    "    retrieved_docs = None\n",
    "    compact_json = {}\n",
    "\n",
    "    if faiss_db:\n",
    "        _, hits = similarity_search_graph_docs(\n",
    "            question, parser, faiss_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "            emb_model=word_emb,\n",
    "            use_cache=CONFIG.get(\"use_cached_graph_embeddings\", True),\n",
    "        )\n",
    "        retrieved_docs = hits\n",
    "\n",
    "    if prompt is None:\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt, compact_json = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "        print(compact_json)\n",
    "\n",
    "    mode = _mode()\n",
    "    YES_RE = re.compile(r\"^\\s*(yes|y|true|correct|affirmative)\\s*\\.?\\s*$\", re.I)\n",
    "    NO_RE  = re.compile(r\"^\\s*(no|n|false|incorrect|negative)\\s*\\.?\\s*$\", re.I)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "        raw = _gen(gen_pipe, prompt)\n",
    "        print(f\"----- RAW (try {attempt}):\", raw)\n",
    "\n",
    "        answer = _extract_answer_text(prompt, raw)\n",
    "        answer, thinking = strip_think(raw)  # thinking 以后可用\n",
    "        print(\"----- ANS:\", answer)\n",
    "\n",
    "        if not answer.strip():\n",
    "            continue  \n",
    "\n",
    "        if mode in {\"yes_no\", \"binary\"}:\n",
    "            if YES_RE.match(answer) and not NO_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y, compact_json\n",
    "            if NO_RE.match(answer) and not YES_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n, compact_json\n",
    "\n",
    "            strict_suffix = (\n",
    "                \"\\n\\n[FORMAT]: Answer with exactly ONE token: \"\n",
    "                + (\"YES or NO.\" if CONFIG.get(\"answer_uppercase\", True) else \"yes or no.\")\n",
    "            )\n",
    "            raw2 = _gen(gen_pipe, prompt + strict_suffix)\n",
    "            ans2 = _extract_answer_text(prompt + strict_suffix, raw2)\n",
    "\n",
    "            if YES_RE.match(ans2) and not NO_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y, compact_json\n",
    "            if NO_RE.match(ans2) and not YES_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n, compact_json\n",
    "            return ans2, compact_json  #\n",
    "        else:\n",
    "            if YES_RE.match(answer) or NO_RE.match(answer) or len(answer.split()) <= 2:\n",
    "                format_suffix = (\n",
    "                    \"\\n\\n[FORMAT]: Provide a 2–3 sentence explanation; do not answer with a single word.\"\n",
    "                )\n",
    "                raw2 = _gen(gen_pipe, prompt + format_suffix)\n",
    "                ans2 = _extract_answer_text(prompt + format_suffix, raw2)\n",
    "                if len(ans2.strip()) > len(answer.strip()):\n",
    "                    return ans2.strip(), compact_json\n",
    "            return answer.strip(), compact_json\n",
    "\n",
    "    return answer, compact_json\n",
    "\n",
    "\n",
    "\n",
    "def build_graph_faiss_index_from_cached(\n",
    "    docs: list[Document],\n",
    "    emb_model,   \n",
    ") -> FAISS:\n",
    "    use_cache = CONFIG.get(\"use_cached_graph_embeddings\", True)\n",
    "    texts, metas, vecs = [], [], []\n",
    "\n",
    "    for d in docs:\n",
    "        texts.append(d.page_content)\n",
    "        metas.append(d.metadata or {})\n",
    "\n",
    "\n",
    "        import ast\n",
    "        er = ast.literal_eval(d.page_content) if isinstance(d.page_content, str) else d.page_content\n",
    "        er_e = er.get(\"e\", []) if isinstance(er, dict) else []\n",
    "        er_r = er.get(\"r\", []) if isinstance(er, dict) else []\n",
    "\n",
    "        e_embeds = (d.metadata or {}).get(\"e_embeddings\")\n",
    "        r_embeds = (d.metadata or {}).get(\"r_embeddings\")\n",
    "\n",
    "        v = _graph_doc_vec_from_cached_or_embed(\n",
    "            er_e, er_r, e_embeds, r_embeds, emb_model, use_cache=use_cache\n",
    "        )\n",
    "        vecs.append(v.tolist())\n",
    "\n",
    "\n",
    "        d.metadata[\"graph_vec\"] = v.tolist()\n",
    "\n",
    "    X = np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "\n",
    "    text_embeddings = [(texts[i], X[i].tolist()) for i in range(len(texts))]\n",
    "    try:\n",
    "        return FAISS.from_embeddings(text_embeddings, embedding=emb_model, metadatas=metas)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return FAISS.from_embeddings(\n",
    "                embeddings=X.tolist(), metadatas=metas, texts=texts, embedding=emb_model\n",
    "            )\n",
    "        except Exception:\n",
    "            vs = FAISS.from_texts(texts=[], embedding=emb_model)\n",
    "            if hasattr(vs, \"add_embeddings\"):\n",
    "                vs.add_embeddings(embeddings=X, metadatas=metas, texts=texts)\n",
    "            else:\n",
    "                vs.add_texts(texts=texts, metadatas=metas)  \n",
    "            return vs\n",
    "        \n",
    "_GRAPH_QVEC_CACHE = {}\n",
    "\n",
    "def _faiss_search_by_vec_graph(vs, qv, k):\n",
    "    if hasattr(vs, \"similarity_search_by_vector_with_score\"):\n",
    "        return vs.similarity_search_by_vector_with_score(qv, k=k)\n",
    "    if hasattr(vs, \"similarity_search_by_vector\"):\n",
    "        docs = vs.similarity_search_by_vector(qv, k=k)\n",
    "        return [(d, None) for d in docs]\n",
    "    index = getattr(vs, \"index\", None)\n",
    "    id_map = getattr(vs, \"index_to_docstore_id\", None)\n",
    "    store  = getattr(vs, \"docstore\", None)\n",
    "    if index is None or id_map is None or store is None:\n",
    "        raise AttributeError(\"FAISS vectorstore has no by-vector APIs and no accessible index/docstore.\")\n",
    "    q = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(q, k)\n",
    "    out = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx == -1: continue\n",
    "        doc_id = id_map[idx]\n",
    "        doc = store.search(doc_id)\n",
    "        out.append((doc, float(dist)))\n",
    "    return out\n",
    "\n",
    "def similarity_search_graph_docs(\n",
    "    user_question: str,\n",
    "    parser,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "    emb_model=None,                    \n",
    "    use_cache: Optional[bool] = None,  \n",
    "):\n",
    "    if use_cache is None:\n",
    "        use_cache = CONFIG.get(\"use_cached_graph_embeddings\", True)\n",
    "    if emb_model is None:\n",
    "        emb_model = globals().get(\"word_emb\", None)\n",
    "    if emb_model is None:\n",
    "        raise ValueError(\"similarity_search_graph_docs: need `emb_model` for query embedding.\")\n",
    "\n",
    "    if use_cache and user_question in _GRAPH_QVEC_CACHE:\n",
    "        qv = _GRAPH_QVEC_CACHE[user_question]\n",
    "        return user_question, _faiss_search_by_vec_graph(vectordb, qv, k)\n",
    "\n",
    "    G, rels = parse_question_to_graph_generic(parser, user_question)\n",
    "    er_e = list({str(n) for n in G.nodes})          \n",
    "    er_r = []\n",
    "    if G.is_multigraph():\n",
    "        for _, _, _, data in G.edges(keys=True, data=True):\n",
    "            rel = data.get(\"relation\")\n",
    "            if rel: er_r.append(str(rel))\n",
    "    else:\n",
    "        for _, _, data in G.edges(data=True):\n",
    "            rel = data.get(\"relation\")\n",
    "            if rel: er_r.append(str(rel))\n",
    "\n",
    "    qv = _graph_doc_vec_from_cached_or_embed(er_e, er_r, None, None, emb_model, use_cache=False)\n",
    "    qv = _normalize(qv)\n",
    "\n",
    "    if use_cache:\n",
    "        _GRAPH_QVEC_CACHE[user_question] = qv\n",
    "\n",
    "    return user_question, _faiss_search_by_vec_graph(vectordb, qv, k)\n",
    "\n",
    "def _has_param(fn, name: str) -> bool:\n",
    "    try:\n",
    "        return name in inspect.signature(fn).parameters\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def upsert_graph_docs_into_faiss(\n",
    "    new_docs: List[Document],\n",
    "    emb_model,                  # 例如 word_emb\n",
    "    faiss_db: Optional[FAISS] = None,\n",
    ") -> FAISS:\n",
    "    texts, metas, vecs = [], [], []\n",
    "\n",
    "    for d in new_docs:\n",
    "        texts.append(d.page_content)\n",
    "        meta = d.metadata or {}\n",
    "        metas.append(meta)\n",
    "\n",
    "        er = ast.literal_eval(d.page_content) if isinstance(d.page_content, str) else d.page_content\n",
    "        er_e = er.get(\"e\", []) if isinstance(er, dict) else []\n",
    "        er_r = er.get(\"r\", []) if isinstance(er, dict) else []\n",
    "\n",
    "        cbm = (meta.get(\"codebook_main\") or {})\n",
    "        e_embeds = cbm.get(\"e_embeddings\")\n",
    "        r_embeds = cbm.get(\"r_embeddings\")\n",
    "\n",
    "        v = _graph_doc_vec_from_cached_or_embed(\n",
    "            er_e, er_r, e_embeds, r_embeds, emb_model, use_cache=True\n",
    "        )\n",
    "        vecs.append(v.tolist())\n",
    "        meta[\"graph_vec\"] = v.tolist()\n",
    "\n",
    "    X = np.asarray(vecs, dtype=np.float32)\n",
    "\n",
    "    # === 初始化 ===\n",
    "    if faiss_db is None:\n",
    "        text_embeddings = [(texts[i], X[i].tolist()) for i in range(len(texts))]\n",
    "        try:\n",
    "            # 新签名\n",
    "            faiss_db = FAISS.from_embeddings(text_embeddings, embedding=emb_model, metadatas=metas)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                # 旧签名\n",
    "                faiss_db = FAISS.from_embeddings(\n",
    "                    embeddings=X.tolist(), metadatas=metas, texts=texts, embedding=emb_model\n",
    "                )\n",
    "            except Exception:\n",
    "                # 最老兜底\n",
    "                faiss_db = FAISS.from_texts(texts=[], embedding=emb_model)\n",
    "                if hasattr(faiss_db, \"add_embeddings\"):\n",
    "                    # 兼容 add_embeddings 的两种签名\n",
    "                    add_fn = faiss_db.add_embeddings\n",
    "                    if _has_param(add_fn, \"text_embeddings\"):\n",
    "                        faiss_db.add_embeddings(text_embeddings=text_embeddings, metadatas=metas)\n",
    "                    else:\n",
    "                        faiss_db.add_embeddings(embeddings=X.tolist(), metadatas=metas, texts=texts)\n",
    "                else:\n",
    "                    faiss_db.add_texts(texts=texts, metadatas=metas)\n",
    "        return faiss_db\n",
    "\n",
    "    # === 已有库 → 追加 ===\n",
    "    if hasattr(faiss_db, \"add_embeddings\"):\n",
    "        add_fn = faiss_db.add_embeddings\n",
    "        if _has_param(add_fn, \"text_embeddings\"):\n",
    "            # 新签名：add_embeddings(text_embeddings=[(text, vec), ...], metadatas=[...])\n",
    "            text_embeddings = [(texts[i], X[i].tolist()) for i in range(len(texts))]\n",
    "            faiss_db.add_embeddings(text_embeddings=text_embeddings, metadatas=metas)\n",
    "        elif _has_param(add_fn, \"embeddings\") and _has_param(add_fn, \"texts\"):\n",
    "            # 旧签名：add_embeddings(embeddings=[...], metadatas=[...], texts=[...])\n",
    "            faiss_db.add_embeddings(embeddings=X.tolist(), metadatas=metas, texts=texts)\n",
    "        else:\n",
    "            # 保险兜底，优先尝试新签名\n",
    "            text_embeddings = [(texts[i], X[i].tolist()) for i in range(len(texts))]\n",
    "            try:\n",
    "                faiss_db.add_embeddings(text_embeddings=text_embeddings, metadatas=metas)\n",
    "            except TypeError:\n",
    "                faiss_db.add_embeddings(embeddings=X.tolist(), metadatas=metas, texts=texts)\n",
    "    elif hasattr(faiss_db, \"add_texts\"):\n",
    "        # 会重算嵌入，但兼容性最好\n",
    "        faiss_db.add_texts(texts=texts, metadatas=metas)\n",
    "    elif hasattr(faiss_db, \"add_documents\"):\n",
    "        # 退而求其次（同样可能重算）\n",
    "        faiss_db.add_documents(new_docs)\n",
    "    else:\n",
    "        raise AttributeError(\"FAISS vectorstore has no add_* method to append data.\")\n",
    "\n",
    "    return faiss_db\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build Docs with LLM Answer\n",
    "# =========================\n",
    "def build_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    parser,\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    faiss_db = None\n",
    ") -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    codebook_main = {}\n",
    "    compact_json = {}\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        codebook_question = get_code_book(q, type=\"questions\")\n",
    "        # Get LLM answer\n",
    "        answer, compact_json = answer_with_llm(q, gen_pipe, parser, faiss_db)\n",
    "        codebook_answer = get_code_book(answer, type='answers')\n",
    "        if compact_json != {}:\n",
    "            codebook_main[\"e\"] = compact_json[\"e\"]\n",
    "            codebook_main[\"r\"] = compact_json[\"r\"]\n",
    "            codebook_main[\"edge_matrix\"] = compact_json[\"edge_matrix\"]\n",
    "            codebook_main[\"questions([[e,r,e], ...])\"] = compact_json[\"questions([[e,r,e], ...])\"]\n",
    "            codebook_main[\"given knowledge([[e,r,e], ...]\"] = compact_json[\"given knowledge([[e,r,e], ...])\"]\n",
    "\n",
    "        codebook_main = merging_codebook(codebook_main,codebook_question,type='questions')\n",
    "        codebook_main = merging_codebook(codebook_main,codebook_answer,type='answers')\n",
    "        codebook_main = combine_ents(codebook_main, min_exp_num=2, max_exp_num=8, random_state=0, use_thinking=False)\n",
    "\n",
    "        er = {\n",
    "                \"e\": codebook_main['e'],  \n",
    "                \"r\": codebook_main['r']\n",
    "            }\n",
    "        er = str(er)\n",
    "        codebook_main.pop(\"rule\", None)\n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "            \"codebook_main\": codebook_main,\n",
    "        }\n",
    "\n",
    "        doc = Document(page_content=er, metadata=metadata)\n",
    "        faiss_db = upsert_graph_docs_into_faiss([doc], word_emb, faiss_db)\n",
    "    return faiss_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20306aa8",
   "metadata": {},
   "source": [
    "## Text RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295d34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "       # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\",\n",
    "    #\"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\",\n",
    "    #\"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\",\n",
    "    #\"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\", \n",
    "\n",
    "    #\"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\",\n",
    "    #\"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\",\n",
    "    #\"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\",\n",
    "    #\"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\",\n",
    "    #\"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba0620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_LABELS = {\n",
    "    # --- 原始 YES 类 ---\n",
    "    \"Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?\": \"YES\",\n",
    "    \"Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?\": \"YES\",\n",
    "    \"Considering that Paris is the administrative and cultural center of France, is it also the official capital city of the country according to its constitution?\": \"YES\",\n",
    "    \"Based on basic human biology, which requires oxygen for cellular respiration and energy production, do humans need oxygen to survive under normal conditions?\": \"YES\",\n",
    "    \"Since the Moon is gravitationally bound to Earth and completes an orbit approximately every 27 days, is it classified as Earth's only natural satellite?\": \"YES\",\n",
    "\n",
    "    # --- 原始 NO 类 ---\n",
    "    \"Although the Sahara Desert is one of the largest deserts in the world, is it located in the South American continent instead of Africa?\": \"NO\",\n",
    "    \"Even though the Amazon River is among the longest rivers globally, is it actually longer than the Nile River when measured by official geographical surveys?\": \"NO\",\n",
    "    \"Despite Tokyo being the largest city in Japan and a major global hub, is it the capital city of South Korea?\": \"NO\",\n",
    "    \"Considering the natural habitats of penguins, which are mainly located in the Southern Hemisphere, do penguins naturally live in the Arctic region alongside polar bears?\": \"NO\",\n",
    "    \"Although gold is a dense and valuable metal, is its density greater than that of lead, making it heavier per cubic centimeter?\": \"NO\",\n",
    "\n",
    "    # --- Follow-up Questions (YES/NO balanced) ---\n",
    "    # Earth shape\n",
    "    \"Despite the Earth's slightly flattened poles, is its shape closer to a sphere than to a flat surface?\": \"YES\",\n",
    "    \"Is the Earth perfectly flat with no curvature anywhere on its surface?\": \"NO\",\n",
    "\n",
    "    # Sun rotation\n",
    "    \"Given the Earth's rotation, is the apparent motion of the Sun consistent with the Sun rising in the east?\": \"YES\",\n",
    "    \"If the Earth did not rotate on its axis, would the Sun still rise and set in the same pattern as it does now?\": \"NO\",\n",
    "\n",
    "    # Paris as capital\n",
    "    \"Considering France's administrative structure, is Paris recognized as the political and economic capital of the nation?\": \"YES\",\n",
    "    \"Is Berlin, rather than Paris, designated as the official capital of France in any historical or legal record?\": \"NO\",\n",
    "\n",
    "    # Oxygen necessity\n",
    "    \"Since oxygen is vital for human life, is it correct to say that humans cannot survive without breathing air containing oxygen?\": \"YES\",\n",
    "    \"Can humans live indefinitely without any access to oxygen in their environment?\": \"NO\",\n",
    "\n",
    "    # Moon as satellite\n",
    "    \"Is the Moon the only large natural body that consistently orbits Earth in the solar system?\": \"YES\",\n",
    "    \"Do humans have multiple moons orbiting the Earth, similar to Jupiter or Saturn?\": \"NO\",\n",
    "\n",
    "    # Sahara Desert\n",
    "    \"Is the Sahara Desert geographically located across multiple countries in northern Africa?\": \"YES\",\n",
    "    \"Is the Sahara Desert primarily located in the continent of South America?\": \"NO\",\n",
    "\n",
    "    # Amazon River\n",
    "    \"Does the Nile River surpass the Amazon River in length when measured by the most widely accepted geographical data?\": \"YES\",\n",
    "    \"Is the Amazon River considered to originate in Europe according to global mapping authorities?\": \"NO\",\n",
    "\n",
    "    # Tokyo\n",
    "    \"Is Tokyo the capital city of Japan and a major economic center in Asia?\": \"YES\",\n",
    "    \"Is Tokyo officially listed as the capital city of South Korea in government documents?\": \"NO\",\n",
    "\n",
    "    # Penguins\n",
    "    \"Do penguins naturally inhabit regions in the Southern Hemisphere, particularly Antarctica?\": \"YES\",\n",
    "    \"Do penguins live alongside polar bears in the Arctic region as part of their natural habitat?\": \"NO\",\n",
    "\n",
    "    # Gold vs Lead\n",
    "    \"Is gold denser than most metals but still slightly less dense than lead?\": \"NO\",\n",
    "    \"Is gold classified as a metal due to its physical and chemical properties?\": \"YES\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "399a983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text RAG: 仅用问题文本入库 ===\n",
    "from langchain.schema import Document\n",
    "\n",
    "def build_text_docs_with_answer(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    text_db: Optional[FAISS] = None\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    生成仅文本 RAG 的文档，并使 metadata 字段与图 RAG 对齐：\n",
    "    - graph_id / question / num_nodes / num_edges / llm_model / llm_answer / created_at / prompt_snapshot(可选)\n",
    "    - 其中 num_nodes/num_edges 统一置 0，保持同名键方便评测与对比\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for qid, q in enumerate(questions, start=1):\n",
    "        page_content = f\"{q}\"\n",
    "        q_vec = sentence_emb.embed_query(q)\n",
    "\n",
    "        answer = answer_with_llm_text(q, gen_pipe, text_db=text_db, q_vec=q_vec)\n",
    "        \n",
    "        metadata = {\n",
    "            \"graph_id\": f\"Q{qid}\",\n",
    "            \"question\": q,                  \n",
    "            \"llm_model\": CONFIG[\"llm_model_id\"],\n",
    "            \"llm_answer\": answer,\n",
    "            \"created_at\": int(time.time()),\n",
    "            \"q_embeddings\": q_vec\n",
    "        }\n",
    "        if add_prompt_snapshot:\n",
    "            prompt_snapshot = make_text_qa_prompt(q, None if not text_db else similarity_search_text_docs(q, text_db, k=CONFIG.get(\"faiss_search_k\",3))[1])\n",
    "            metadata[\"prompt_snapshot\"] = prompt_snapshot\n",
    "\n",
    "        docs.append(Document(page_content=page_content, metadata=metadata))\n",
    "    return docs, q_vec\n",
    "\n",
    "_QVEC_CACHE = {}\n",
    "\n",
    "def _faiss_search_by_vec(vs, qv, k):\n",
    "    \"\"\"兼容不同 langchain-community 版本的 FAISS 向量检索。返回 [(doc, score_or_None), ...]\"\"\"\n",
    "    if hasattr(vs, \"similarity_search_by_vector_with_score\"):\n",
    "        return vs.similarity_search_by_vector_with_score(qv, k=k)\n",
    "\n",
    "    if hasattr(vs, \"similarity_search_by_vector\"):\n",
    "        docs = vs.similarity_search_by_vector(qv, k=k)\n",
    "        return [(d, None) for d in docs]\n",
    "\n",
    "    index = getattr(vs, \"index\", None)\n",
    "    id_map = getattr(vs, \"index_to_docstore_id\", None)\n",
    "    store  = getattr(vs, \"docstore\", None)\n",
    "    if index is None or id_map is None or store is None:\n",
    "        raise AttributeError(\"FAISS vectorstore has no by-vector APIs and no accessible index/docstore.\")\n",
    "\n",
    "    import numpy as np\n",
    "    q = np.asarray(qv, dtype=np.float32).reshape(1, -1)\n",
    "    D, I = index.search(q, k)\n",
    "    out = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        doc_id = id_map[idx]\n",
    "        doc = store.search(doc_id)\n",
    "        out.append((doc, float(dist)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def similarity_search_text_docs(\n",
    "    user_question: str,\n",
    "    vectordb: FAISS,\n",
    "    k: int = 5,\n",
    "    query_vec: Optional[List[float]] = None,\n",
    "    emb=None,\n",
    "    use_cache: Optional[bool] = None,   \n",
    "):\n",
    "    import numpy as np\n",
    "    if use_cache is None:\n",
    "        use_cache = CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "\n",
    "    if query_vec is not None and use_cache:\n",
    "        qv = np.asarray(query_vec, dtype=np.float32)\n",
    "        qv /= (np.linalg.norm(qv) + 1e-12)\n",
    "        results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "        return user_question, results\n",
    "\n",
    "    if use_cache and user_question in _QVEC_CACHE:\n",
    "        qv = _QVEC_CACHE[user_question]\n",
    "        results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "        return user_question, results\n",
    "\n",
    "    if emb is None:\n",
    "        emb = globals().get(\"sentence_emb\", None)\n",
    "    if emb is None:\n",
    "        raise ValueError(\"similarity_search_text_docs: need `emb` when no cache/vec provided.\")\n",
    "\n",
    "    qv = np.asarray(emb.embed_query(user_question), dtype=np.float32)\n",
    "    qv /= (np.linalg.norm(qv) + 1e-12)\n",
    "\n",
    "    if use_cache:\n",
    "        _QVEC_CACHE[user_question] = qv  \n",
    "\n",
    "    results = _faiss_search_by_vec(vectordb, qv, k)\n",
    "    return user_question, results\n",
    "\n",
    "\n",
    "def make_text_qa_prompt(\n",
    "    question: str,\n",
    "    retrieved_docs=None\n",
    ") -> str:\n",
    "    sections = []\n",
    "    if retrieved_docs and CONFIG.get(\"include_retrieved_context\", True):\n",
    "        doc0, _ = retrieved_docs[0]\n",
    "        related_q_txt = doc0.page_content.strip()\n",
    "        related_answer = (doc0.metadata or {}).get(\"llm_answer\", \"\")\n",
    "        sections.append(\n",
    "            \"<<<RETRIEVED_CONTEXT_START>>>\\n\"\n",
    "            \"The system searched for a related question in the database. Below are related question's graph triples and its prior answer as reference. \" \\\n",
    "            \"You don't have to follow it completely, just use it as a reference.\\n\"\n",
    "            f\"[RELATED QUESTION TEXT]:\\n{related_q_txt}\\n\"\n",
    "            f\"[RELATED ANSWER]: {related_answer}\\n\"\n",
    "            \"<<<RETRIEVED_CONTEXT_END>>>\"\n",
    "        )\n",
    "\n",
    "    sections.append(f\"[CURRENT QUESTION]: {question}\")\n",
    "\n",
    "    mode = _mode()\n",
    "\n",
    "    if mode in {\"yes_no\", \"binary\"}:\n",
    "        yes, no = _yn(\"YES\", \"NO\")\n",
    "        rules = (\n",
    "            \"[TASK]: You are a precise QA assistant for binary (yes/no) questions.\\n\"\n",
    "            f\"- Output ONLY one token: {yes} or {no}.\\n\"\n",
    "            \"- Do NOT copy or summarize any context.\\n\"\n",
    "            \"- Do NOT show reasoning, steps, or extra words.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "    else:\n",
    "        style_line = {\n",
    "            \"short\":    \"- Give a short, direct answer in 2–3 sentences.\\n\",\n",
    "            \"detail\":   \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"long\":     \"- Provide a clear, detailed, and structured answer.\\n\",\n",
    "            \"reasoning\":\"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "            \"explain\":  \"- Provide a well-structured explanation with logical reasoning flow.\\n- If useful, break the answer into brief sections.\\n\",\n",
    "        }.get(mode, \"- Provide a clear and helpful answer.\\n\")\n",
    "\n",
    "        rules = (\n",
    "            \"[TASK]: You are a QA assistant for open-ended questions.\\n\"\n",
    "            f\"{style_line}\"\n",
    "            \"- Do NOT restrict to yes/no.\\n\"\n",
    "            \"[FORMAT]: Write complete sentences (not a single word).\"\n",
    "            \"Avoid starting with just 'Yes.' or 'No.'; if the question is yes/no-style, state the conclusion AND 1–2 short reasons.\\n\"\n",
    "            \"[ANSWER]: \"\n",
    "        )\n",
    "\n",
    "    sections.append(rules)\n",
    "    prompt = \"\\n\\n\".join(sections)\n",
    "    return prompt\n",
    "\n",
    "def answer_with_llm_text(\n",
    "    question: str,\n",
    "    gen_pipe,\n",
    "    q_vec=None, \n",
    "    *,\n",
    "    text_db: Optional[\"FAISS\"] = None,\n",
    "    max_retries: int = 3,  \n",
    ") -> str:\n",
    "  \n",
    "    retrieved_docs = None\n",
    "    if text_db:\n",
    "        _, hits = similarity_search_text_docs(\n",
    "            question, text_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "            emb=sentence_emb,\n",
    "            use_cache=CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "        )\n",
    "        retrieved_docs = hits\n",
    "\n",
    "   \n",
    "    prompt = make_text_qa_prompt(question, retrieved_docs)\n",
    "    mode = _mode()\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "\n",
    "\n",
    "        raw = _gen(gen_pipe, prompt)\n",
    "    \n",
    "        print(f\"----- RAW (try {attempt}):\", raw)\n",
    "\n",
    "        text = _extract_answer_text(prompt, raw)\n",
    "        answer, think = strip_think(text)  \n",
    "        answer = (answer or \"\").strip()\n",
    "        print(\"----- ANS:\", answer)\n",
    "\n",
    "        if not answer:\n",
    "            continue\n",
    "\n",
    "        YES_RE = re.compile(r\"^\\s*(yes|y|true|correct|affirmative)\\s*\\.?\\s*$\", re.I)\n",
    "        NO_RE  = re.compile(r\"^\\s*(no|n|false|incorrect|negative)\\s*\\.?\\s*$\", re.I)\n",
    "        if mode in {\"yes_no\", \"binary\"}:\n",
    "            if YES_RE.match(answer) and not NO_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(answer) and not YES_RE.match(answer):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "\n",
    "            strict_suffix = (\n",
    "                \"\\n\\n[FORMAT]: Answer with exactly ONE token: \"\n",
    "                + (\"YES or NO.\" if CONFIG.get(\"answer_uppercase\", True) else \"yes or no.\")\n",
    "            )\n",
    "            raw2 = _gen(gen_pipe, prompt + strict_suffix)\n",
    "            ans2 = _extract_answer_text(prompt + strict_suffix, raw2)\n",
    "            ans2, _ = strip_think(ans2)\n",
    "            if YES_RE.match(ans2) and not NO_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return y\n",
    "            if NO_RE.match(ans2) and not YES_RE.match(ans2):\n",
    "                y, n = _yn(\"YES\", \"NO\") if CONFIG.get(\"answer_uppercase\", True) else _yn(\"yes\", \"no\")\n",
    "                return n\n",
    "            return ans2.strip() if ans2 else answer\n",
    "\n",
    "        else:\n",
    "            if YES_RE.match(answer) or NO_RE.match(answer) or len(answer.split()) <= 2:\n",
    "                format_suffix = (\n",
    "                    \"\\n\\n[FORMAT]: Provide a 2–3 sentence explanation; \"\n",
    "                    \"do not answer with a single word.\"\n",
    "                )\n",
    "                raw2 = _gen(gen_pipe, prompt + format_suffix)\n",
    "                ans2 = _extract_answer_text(prompt + format_suffix, raw2)\n",
    "                ans2, _ = strip_think(ans2)\n",
    "                if ans2 and len(ans2.strip()) > len(answer):\n",
    "                    return ans2.strip()\n",
    "            return answer\n",
    "\n",
    "    return answer\n",
    "\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_index_from_cached(\n",
    "    docs: List[Document],\n",
    "    emb,\n",
    ") -> FAISS:\n",
    "    use_cache = CONFIG.get(\"use_cached_text_embeddings\", True)\n",
    "    texts, metas, vecs = [], [], []\n",
    "\n",
    "    for d in docs:\n",
    "        texts.append(d.page_content)\n",
    "        metas.append(d.metadata)\n",
    "        if use_cache:\n",
    "            v = d.metadata.get(\"q_embeddings\", None) or d.metadata.get(\"q_vec\", None)\n",
    "            if v is None:\n",
    "                v = emb.embed_query(d.page_content)\n",
    "        else:\n",
    "            # 强制现算（不读 metadata 中存的）\n",
    "            v = emb.embed_query(d.page_content)\n",
    "        vecs.append(v)\n",
    "\n",
    "    if not texts:\n",
    "        raise ValueError(\"No docs provided to build_faiss_index_from_cached().\")\n",
    "\n",
    "    X = np.asarray(vecs, dtype=np.float32)\n",
    "    # 内积索引建议归一化；L2 也可以保留\n",
    "    X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # ① 新签名：from_embeddings(text_embeddings=[(text, vec), ...], embedding=..., metadatas=[...])\n",
    "    text_embeddings = [(t, X[i].tolist()) for i, t in enumerate(texts)]\n",
    "    try:\n",
    "        return FAISS.from_embeddings(\n",
    "            text_embeddings, embedding=emb, metadatas=metas\n",
    "        )\n",
    "    except TypeError:\n",
    "        # ② 旧签名：from_embeddings(embeddings=[vec...], metadatas=[...], texts=[...], embedding=...)\n",
    "        try:\n",
    "            return FAISS.from_embeddings(\n",
    "                embeddings=X.tolist(), metadatas=metas, texts=texts, embedding=emb\n",
    "            )\n",
    "        except Exception:\n",
    "            # ③ 回退路径：from_texts 或 add_embeddings\n",
    "            try:\n",
    "                vs = FAISS.from_texts(texts=[], embedding=emb)\n",
    "                if hasattr(vs, \"add_embeddings\"):\n",
    "                    vs.add_embeddings(embeddings=X, metadatas=metas, texts=texts)\n",
    "                else:\n",
    "                    # 最老的版本只能 add_texts（会重算嵌入，缓存用不上）\n",
    "                    vs.add_texts(texts=texts, metadatas=metas)\n",
    "                return vs\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to build FAISS index with cached vectors: {e}\")\n",
    "\n",
    "def build_text_faiss_index_with_answers(\n",
    "    questions: List[str],\n",
    "    gen_pipe,\n",
    "    *,\n",
    "    add_prompt_snapshot: bool = False,\n",
    "    bootstrap_db: Optional[FAISS] = None\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    用文本 RAG 路线生成答案并入库，然后返回 FAISS 向量库。\n",
    "    bootstrap_db: 若传入，文本检索会优先引用该库的历史问答作为 retrieved context（冷启动可传 None）。\n",
    "    \"\"\"\n",
    "    docs, q_vec = build_text_docs_with_answer(\n",
    "        questions=questions,\n",
    "        gen_pipe=gen_pipe,\n",
    "        add_prompt_snapshot=add_prompt_snapshot,\n",
    "        text_db=bootstrap_db,\n",
    "    )\n",
    "    print(docs)\n",
    "    return build_faiss_index_from_cached(docs, sentence_emb), q_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3966e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_once_mode(\n",
    "    question: str,\n",
    "    mode: str,                 # \"text\" or \"graph\"\n",
    "    gen_pipe,\n",
    "    tokenizer,\n",
    "    parser=None,\n",
    "    text_db: Optional[FAISS] = None,\n",
    "    graph_db: Optional[FAISS] = None,\n",
    "    q_vecs = None,\n",
    "    *,\n",
    "    label: Optional[str] = None,\n",
    "    use_cuda_mem: bool = True,\n",
    ") -> Dict:\n",
    "    assert mode in (\"text\", \"graph\")\n",
    "\n",
    "    retrieved_docs = None\n",
    "    retrieval_latency = 0.0\n",
    "    retrieved_count = 0\n",
    "\n",
    "    # ---- 1) 检索 + 计时（仅检索耗时）----\n",
    "    if mode == \"text\":\n",
    "        if text_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_text_docs(\n",
    "                question, text_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "                emb=sentence_emb,\n",
    "                use_cache=CONFIG.get(\"use_cached_text_embeddings\", True)  # 显式\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        prompt= make_text_qa_prompt(question, retrieved_docs=retrieved_docs)\n",
    "    else:\n",
    "        if graph_db and CONFIG.get(\"include_retrieved_context\", True):\n",
    "            t_r0 = time.perf_counter()\n",
    "            _, hits = similarity_search_graph_docs(\n",
    "                question, parser, graph_db, k=CONFIG.get(\"faiss_search_k\", 3),\n",
    "                emb_model=word_emb,\n",
    "                use_cache=CONFIG.get(\"use_cached_graph_embeddings\", True),\n",
    "            )\n",
    "            retrieval_latency = time.perf_counter() - t_r0\n",
    "            retrieved_docs = hits if hits else None\n",
    "            retrieved_count = len(hits) if hits else 0\n",
    "        G, rels = parse_question_to_graph_generic(parser, question)\n",
    "        prompt, compact_json = make_graph_qa_prompt(question, G, rels, retrieved_docs)\n",
    "\n",
    "    # 小工具：token 计数\n",
    "    def _count_tokens(tokenizer, text: str) -> int:\n",
    "        return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    in_tok = _count_tokens(tokenizer, prompt)\n",
    "\n",
    "    # ---- 2) 推理计时（生成部分）----\n",
    "    peak_mem = None\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_g0 = time.perf_counter()\n",
    "    out = gen_pipe(prompt)\n",
    "    gen_latency = time.perf_counter() - t_g0  # 仅生成耗时\n",
    "\n",
    "    text = out[0][\"generated_text\"]\n",
    "    if CONFIG.get(\"return_full_text\", False):\n",
    "        answer = text[len(prompt):].strip()\n",
    "    else:\n",
    "        answer = text.strip()\n",
    "\n",
    "    out_tok = _count_tokens(tokenizer, answer)\n",
    "\n",
    "    if use_cuda_mem and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # ---- 3) 汇总 ----\n",
    "    return {\n",
    "        \"label\": label or f\"{mode}_rag\",\n",
    "        \"mode\": mode,\n",
    "        \"question\": question,\n",
    "        \"input_tokens\": in_tok,\n",
    "        \"output_tokens\": out_tok,\n",
    "        \"total_tokens\": in_tok + out_tok,\n",
    "        \"latency_sec\": retrieval_latency + gen_latency,  \n",
    "        \"gen_latency_sec\": gen_latency,                   \n",
    "        \"retrieval_latency_sec\": retrieval_latency,       \n",
    "        \"retrieved_count\": retrieved_count,               \n",
    "        \"peak_vram_MiB\": peak_mem,\n",
    "        \"prompt_chars\": len(prompt),\n",
    "        \"answer\": answer,\n",
    "        \"used_retrieval\": bool(retrieved_docs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d847f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_compare_text_vs_graph(\n",
    "    questions: List[str],\n",
    "    gen_pipe, tokenizer, parser,\n",
    "    text_db: Optional[FAISS],\n",
    "    graph_db: Optional[FAISS],\n",
    "    q_vecs = None,\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q in questions:\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"text\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"text_rag\", q_vecs=q_vecs)\n",
    "        )\n",
    "        rows.append(\n",
    "            measure_once_mode(q, \"graph\", gen_pipe, tokenizer, parser, text_db, graph_db, label=\"graph_rag\", q_vecs=q_vecs)\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _normalize_yesno(text: str) -> str:\n",
    "    if text is None: return \"NO\"\n",
    "    t = str(text).strip().lower()\n",
    "    if t == \"yes\" or (\"yes\" in t and \"no\" not in t): return \"YES\"\n",
    "    if t == \"no\"  or (\"no\"  in t and \"yes\" not in t): return \"NO\"\n",
    "    return \"NO\"\n",
    "\n",
    "def _norm_q(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip().lower()\n",
    "\n",
    "def attach_gold(df: pd.DataFrame, gold_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"把 gold label 合并到 df，并生成 pred / correct 列。一定要 return DataFrame。\"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"attach_gold: input df is None or not a DataFrame\")\n",
    "\n",
    "    g = pd.DataFrame(list(gold_map.items()), columns=[\"question\",\"gold\"])\n",
    "    g[\"question_norm\"] = g[\"question\"].map(_norm_q)\n",
    "    g[\"gold\"] = g[\"gold\"].map(lambda x: \"YES\" if str(x).upper()==\"YES\" else \"NO\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"question_norm\"] = out[\"question\"].map(_norm_q)\n",
    "    out = out.merge(g[[\"question_norm\",\"gold\"]], on=\"question_norm\", how=\"left\")\n",
    "\n",
    "    out[\"pred\"] = out[\"answer\"].map(_normalize_yesno)\n",
    "    out[\"correct\"] = (out[\"pred\"] == out[\"gold\"]).astype(int)\n",
    "\n",
    "    # 便于排错：提示没有 gold 命中的题\n",
    "    miss = out[out[\"gold\"].isna()]\n",
    "    if len(miss):\n",
    "        print(f\"⚠️ {len(miss)} questions had no gold match. Showing a few:\")\n",
    "        print(miss[[\"question\",\"label\"]].head(5))\n",
    "\n",
    "    return out  # ←←← 关键：确保返回\n",
    "\n",
    "def evaluate_accuracy(df_with_gold: pd.DataFrame):\n",
    "    print(\"\\n== Accuracy by config ==\")\n",
    "    for k, sub in df_with_gold.groupby(\"label\"):\n",
    "        n = len(sub[sub[\"gold\"].notna()])\n",
    "        acc = sub[\"correct\"].mean() if n else float(\"nan\")\n",
    "        print(f\"{k:<10s} acc={acc:.3f} (n={n})\")\n",
    "\n",
    "def summarize_cost(df: pd.DataFrame, base_label: str, target_label: str):\n",
    "    A = df[df[\"label\"] == base_label]\n",
    "    B = df[df[\"label\"] == target_label]\n",
    "\n",
    "    def avg(col):\n",
    "        a, b = A[col].mean(), B[col].mean()\n",
    "        return a, b, (b - a) / max(1e-9, a)\n",
    "\n",
    "    print(\"\\n== Cost (avg) ==\")\n",
    "    for col in [\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"latency_sec\",             \n",
    "        \"retrieval_latency_sec\",   \n",
    "        \"gen_latency_sec\",        \n",
    "        \"retrieved_count\",         \n",
    "        \"peak_vram_MiB\",\n",
    "        \"prompt_chars\",\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            a, b, d = avg(col)\n",
    "            print(f\"{col:>22s} | {base_label}: {a:8.4f} | {target_label}: {b:8.4f} | Δ%: {d*100:7.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9916527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_size_bytes(path: str) -> int:\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            fp = os.path.join(root, f)\n",
    "            try: total += os.path.getsize(fp)\n",
    "            except OSError: pass\n",
    "    return total\n",
    "\n",
    "def save_and_report_sizes(text_db: FAISS, graph_db: FAISS, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\"):\n",
    "    text_db.save_local(text_dir)\n",
    "    graph_db.save_local(graph_dir)\n",
    "    def human(n):\n",
    "        u=[\"B\",\"KB\",\"MB\",\"GB\"]; i=0; x=float(n)\n",
    "        while x>=1024 and i<len(u)-1: x/=1024.0; i+=1\n",
    "        return f\"{x:.2f} {u[i]}\"\n",
    "    s_text  = dir_size_bytes(text_dir)\n",
    "    s_graph = dir_size_bytes(graph_dir)\n",
    "    print(f\"[Index size] text_rag  = {human(s_text)}  ({text_dir})\")\n",
    "    print(f\"[Index size] graph_rag = {human(s_graph)}  ({graph_dir})\")\n",
    "    return s_text, s_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e4e9e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compact_json in make prompt {}\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RAW (try 1):  The Earth is generally considered round, or \"approximately spherical,\" with a slight equatorial bulge due to its rotation. This understanding is based on extensive observations, gravitational studies, and measurements that account for the balance between centrifugal force and gravitational compression.<think>\n",
      "Okay, let's tackle this question. The user is asking if Earth is considered round despite the equatorial bulge. First, I need to recall what the standard scientific consensus is on Earth's shape. I remember that planets are generally spherical due to gravity pulling everything into a rounded shape. But the bulge mentioned is from rotation, right? So even though it's not a perfect sphere, it's still called round or oval-shaped in many contexts.\n",
      "\n",
      "Wait, the question specifies \"generally considered round.\" So I should confirm that in common terminology, Earth is taught as a sphere in schools, even though technically it's an oblate spheroid. The term \"round\" might be sufficient for general use, while the bulge is a more precise detail. Also, gravitational studies and satellite data like from GPS support the slight flattening. So the answer should mention that the Earth is approximately spherical, acknowledge the bulge, and reference the reasons why it's still considered round despite that minor deformation. Need to keep it concise as per\n",
      "----- ANS: The Earth is generally considered round, or \"approximately spherical,\" with a slight equatorial bulge due to its rotation. This understanding is based on extensive observations, gravitational studies, and measurements that account for the balance between centrifugal force and gravitational compression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compact_json in make prompt {'e': ['round in shape', 'balance', 'base', 'extensive observations'], 'r': ['prep_for', 'subj', 'prep_on', 'obj', 'property'], 'edge_matrix': [[1, 0, 1], [1, 1, 2], [2, 2, 3], [3, 1, 1], [1, 3, 0], [1, 1, 1], [3, 3, 3], [1, 2, 2], [2, 3, 3], [3, 2, 1], [1, 4, 3], [1, 2, 1]], 'questions([[e,r,e], ...])': [[[1, 2, 1]]], 'given knowledge([[e,r,e], ...])': [[[1, 1, 2], [2, 2, 3], [3, 1, 1], [1, 3, 0], [1, 1, 1], [1, 0, 1]]], 'rule': ''}\n",
      "{'e': ['round in shape', 'balance', 'base', 'extensive observations'], 'r': ['prep_for', 'subj', 'prep_on', 'obj', 'property'], 'edge_matrix': [[1, 0, 1], [1, 1, 2], [2, 2, 3], [3, 1, 1], [1, 3, 0], [1, 1, 1], [3, 3, 3], [1, 2, 2], [2, 3, 3], [3, 2, 1], [1, 4, 3], [1, 2, 1]], 'questions([[e,r,e], ...])': [[[1, 2, 1]]], 'given knowledge([[e,r,e], ...])': [[[1, 1, 2], [2, 2, 3], [3, 1, 1], [1, 3, 0], [1, 1, 1], [1, 0, 1]]], 'rule': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RAW (try 1): ....<think>\n",
      "Okay, let's tackle this question. The user is asking if Earth's rotation causes the Sun to appear to rise in the east and set in the west from an observer's perspective on the surface. Hmm, I remember that the Earth's rotation is the main factor here. But wait, the Sun's apparent movement is due to the rotation, right? So when the Earth spins, the observer sees the Sun moving across the sky. But wait, the Sun itself is stationary relative to the solar system, and the Earth is rotating beneath it. So the rotation makes the Sun seem to move, but actually, it's the opposite. The Earth turns east to the west, making the Sun appear to move west to east. Wait, no, the observer sees the Sun moving from east to west. Wait, no, when you look at the Sun rise in the east and set in the west, that's because the Earth is rotating eastward. So the rotation causes the Sun to appear to move westward across the sky. But I need to make sure. Let me think again. If the Earth wasn't rotating, the Sun would stay in one place. But since we're on a rotating Earth, as we move into the day, the Sun appears to rise in the east\n",
      "----- ANS: ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/relation/lib/python3.10/site-packages/transformers/pytorch_utils.py:339: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RAW (try 1):  The Earth is generally considered round, or “oblate spheroid,” due to its rotation and gravitational shape.  The discovery of other planets with similar properties and the principles of physics (e.g., uniform gravitational field around a sphere of equal density) have reinforced this understanding.<think>\n",
      "Okay, let's tackle this question. The user is asking if Earth is considered round despite its slight equatorial bulge. First, I need to recall what I know about Earth's shape. I remember that because the Earth spins, the centrifugal force causes the equator to bulge out a bit bit. So it's not a perfect sphere, but an oblate spheroid. Then, the question mentions other planets, so maybe I should think about whether other planets are similarly shaped. From what I've studied, planets like Mars and Jupiter are also oblate. The term \"round\" is often used informally even when the scientific term is \"oblate.\" Also, physics principles like how gravity works for a rotating sphere would support that they're roughly spherical. Plus, when we talk about the solar system's planets in textbooks, they're usually depicted as round, even if slightly flattened at the poles. So putting this all together, the answer should confirm that Earth is considered round (oblate)\n",
      "----- ANS: The Earth is generally considered round, or “oblate spheroid,” due to its rotation and gravitational shape.  The discovery of other planets with similar properties and the principles of physics (e.g., uniform gravitational field around a sphere of equal density) have reinforced this understanding.\n",
      "----- RAW (try 1): <think>\n",
      "\n",
      "Okay, so the question is whether Earth's rotation causes the Sun to appear to rise in the east and set in the west. Let me start by recalling what I know about Earth's rotation and how we see the Sun from our perspective.\n",
      "\n",
      "First, Earth rotates on its axis. I remember that this rotation is from west to east, which is the same direction that the Sun appears to move across the sky. Wait, but why does the Sun rise in the east? Well, because as Earth spins, different parts of the planet move into the light from the Sun. The eastern hemisphere would first face the Sun, making it appear to rise. Then, as the Earth keeps rotating, the Sun would get higher in the sky until it reaches the zenith at solar noon, and then it sets in the western sky.\n",
      "\n",
      "But wait, the actual Sun is fixed in the sky relative to the stars due to Earth's rotation. So it's more about the observer's perspective changing as the Earth turns. So the apparent movement of the Sun is just an effect of the Earth's rotation, not the Sun's actual movement. That makes sense. So the rotation of Earth causes the diurnal motion, making the Sun rise and set.\n",
      "\n",
      "But I should also consider if there are any other\n",
      "----- ANS: \n",
      "----- RAW (try 2): <think>\n",
      "\n",
      "Okay, so the question is whether Earth's rotation causes the Sun to appear to rise in the east and set in the west. Let me start by recalling what I know about Earth's rotation and how we see the Sun.\n",
      "\n",
      "First, Earth spins on its axis. I remember that this rotation is from west to east, which is similar to how the Sun moves across the sky. Wait, but why does the Earth rotate that way? Maybe it's because when life forms on Earth evolved, they adapted to this direction of rotation. But how does that relate to the Sun's apparent movement?\n",
      "\n",
      "The Sun rising and setting in the east and west is due to the rotation of the Earth. As the Earth turns, different parts of the planet move into the Sun's light. So, for an observer on the surface, the Sun seems to move from east to west across the sky. But why east to west and not the other way? Hmm, maybe it's because of the direction of rotation. If Earth spins west to east, then the eastern hemisphere would face the Sun first as it rotates into the daylight, making the Sun rise, and then the western hemisphere enters daylight, causing the Sun to set. \n",
      "\n",
      "But wait, the Sun's actual position isn't moving; it's the\n",
      "----- ANS: \n",
      "----- RAW (try 3):  Earth rotates eastward, making the Sun appear to rise and set.<think>\n",
      "Okay, so the question is whether Earth's rotation causes the Sun to appear to rise in the east and set in the west. Let me start by recalling what I know about Earth's rotation and how we perceive the Sun.\n",
      "\n",
      "First, Earth spins on its axis. I remember that this rotation is from west to east, which is opposite to the way we usually read directions. So, if Earth is rotating eastward, then when facing the Sun, it would be moving towards the east. But how does this make the Sun rise and set?\n",
      "\n",
      "From what I've observed, the Sun does come up in the east, reaches its highest point in the south (or maybe that's the star Polaris?), and then sets in the west. But why does that happen? It's because as the Earth rotates, different parts of the Earth turn into the Sun's position. So, an observer on the surface would see the Sun rise when their location turns towards the Sun relative to the stars. Since the stars are fixed in the sky, the Sun moving from a position below the horizon to above must be due to Earth's rotation.\n",
      "\n",
      "Wait, but the stars are fixed because they're so far away, right? So the Sun\n",
      "----- ANS: Earth rotates eastward, making the Sun appear to rise and set.\n",
      "[Document(metadata={'graph_id': 'Q1', 'question': 'Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?', 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'The Earth is generally considered round, or “oblate spheroid,” due to its rotation and gravitational shape.  The discovery of other planets with similar properties and the principles of physics (e.g., uniform gravitational field around a sphere of equal density) have reinforced this understanding.', 'created_at': 1757245599, 'q_embeddings': [0.06542075425386429, -0.014430391602218151, -0.018086358904838562, -0.014991224743425846, 0.01676652580499649, -0.06757217645645142, -0.009964372962713242, 0.04057123884558678, 0.046724963933229446, 0.007202856242656708, -0.04560878127813339, -0.011130229569971561, -0.029934514313936234, -0.005765230394899845, 0.10064125806093216, -0.1872469037771225, -0.08053438365459442, -0.05268656089901924, 0.0744856745004654, 0.044800397008657455, -0.056824274361133575, 0.030281415209174156, 0.031367264688014984, 0.09395118802785873, -0.007329029031097889, -0.02491253986954689, 0.03077881969511509, -0.037623219192028046, -0.07332856953144073, -0.058687105774879456, -0.06748991459608078, 0.0685398280620575, -0.009645778685808182, -0.024280408397316933, -0.03764333948493004, -0.009335151873528957, 0.030721653252840042, 0.02179000899195671, 0.01986871100962162, 0.010883891955018044, 0.034092482179403305, -0.019201675429940224, 0.1001710519194603, 0.038083042949438095, -0.027212996035814285, -0.019198007881641388, -0.09353438019752502, 0.013230457901954651, 0.02172282710671425, -0.03411518782377243, 0.08047931641340256, -0.10348403453826904, -0.05585544556379318, 0.051338374614715576, -0.056574661284685135, -0.03898410126566887, -0.004007314331829548, -0.030923059210181236, 0.06842052191495895, -0.0017821870278567076, 0.049198418855667114, -0.059234220534563065, -0.024465229362249374, 0.06328897178173065, -0.0481225810945034, 0.02219015173614025, -0.03131762146949768, -0.08929338306188583, -0.1087387204170227, -0.045089300721883774, -0.02673696167767048, 0.015236623585224152, -0.08332531154155731, -0.06985262036323547, 0.01101217046380043, 0.0189246516674757, 0.015416685491800308, 0.027726175263524055, -0.023263033479452133, 0.0056232186034321785, -0.03328915685415268, 0.118981271982193, -0.017964079976081848, 0.06439021974802017, -0.039911992847919464, -0.066289521753788, -0.04219970852136612, -0.051615871489048004, -0.031110869720578194, -0.020036309957504272, -0.013301806524395943, 0.062365662306547165, 0.009858524426817894, -0.004559061489999294, -0.035305894911289215, 0.06350944936275482, 0.01903628557920456, -0.05611179396510124, 0.02509717456996441, -0.006962779443711042, 0.04144595190882683, 0.06174908205866814, 0.03876348212361336, 0.016144415363669395, 0.04379544407129288, 0.06549662351608276, 0.013735910877585411, -0.0018381986301392317, 0.02584477886557579, -0.07691558450460434, -0.08831297606229782, 0.04152350500226021, 0.05239245295524597, -0.007808252237737179, -0.006640582345426083, -0.03776084631681442, 0.028710011392831802, 0.02577335014939308, -0.09520390629768372, -0.01725289411842823, -0.024209752678871155, 0.030802978202700615, 0.07638406753540039, 0.08433720469474792, 0.08817622810602188, 0.01968533731997013, -0.023006197065114975, -1.730688597910308e-33, 0.013077212497591972, 0.06721542030572891, 0.08100231736898422, 0.024515625089406967, -0.06854503601789474, 0.018323136493563652, -0.0056765759363770485, 0.03357704356312752, 0.02367384545505047, -0.0719379410147667, -0.06801832467317581, -0.03936314582824707, 0.0745881050825119, -0.02229251153767109, 0.05882708728313446, 0.022234130650758743, 0.029007574543356895, 0.0935809388756752, -0.02798147313296795, -0.005991792306303978, -0.09558990597724915, -0.002000095322728157, 0.022795138880610466, -0.0789325088262558, -0.07986985892057419, 0.00033006741432473063, 0.009036975912749767, -0.014598245732486248, -0.06490974873304367, -0.0036359375808387995, 0.05314653366804123, -0.04043601453304291, 0.04380303993821144, 0.06497499346733093, 0.02613372728228569, -0.01858905330300331, 0.006514766253530979, 0.07224921137094498, 0.0011794216698035598, 0.011040201410651207, -0.021111587062478065, -0.01145181618630886, 0.01239546574652195, 0.02257627248764038, 0.03849472850561142, 0.06222790852189064, 0.0372113361954689, -0.06309938430786133, -0.014775401912629604, 0.014645887538790703, 0.01569734700024128, 0.017223678529262543, 0.035711996257305145, -0.006725745275616646, 0.06559184193611145, -0.0025028411764651537, 0.006596233695745468, -0.03287944197654724, -0.0937613993883133, 0.0328063890337944, 0.14904002845287323, -0.018386580049991608, -0.022701401263475418, -0.056399114429950714, 0.020021013915538788, 0.028359517455101013, 0.018710236996412277, -0.01750471070408821, -0.016675014048814774, -0.009937374852597713, 0.053729575127363205, 0.04722920432686806, 0.031173715367913246, 0.1224491223692894, 0.03322848305106163, 0.035818278789520264, 0.06905853748321533, -0.03556734696030617, 0.016759617254137993, 0.08946263790130615, -0.0320582278072834, -0.009426604956388474, -0.03170815855264664, -0.0661376342177391, -0.04271269217133522, -0.014607200399041176, -0.004013272933661938, 0.015808450058102608, 0.03631596267223358, 0.023200811818242073, -0.013836358673870564, -0.053956758230924606, 0.01937195286154747, 0.060121696442365646, -0.12716947495937347, -8.20330728733019e-35, -0.08066537231206894, -0.01545698381960392, -0.04602224752306938, 0.010435989126563072, 0.03369175270199776, 0.02628834918141365, -0.049015432596206665, 0.07723827660083771, -0.10796883702278137, -0.015673724934458733, 0.05596829205751419, 0.027238639071583748, 0.013382339850068092, -0.08952008932828903, 0.06330560147762299, 0.030434899032115936, -0.027924662455916405, -0.008808346465229988, -0.0622200146317482, 0.009348281659185886, 0.049821119755506516, -0.11445072293281555, -0.01617974415421486, -0.0024084236938506365, 0.053669992834329605, 0.04018454626202583, -0.06578603386878967, -0.04084031656384468, -0.0015409947372972965, 0.09263856709003448, -0.02278146706521511, -0.046609461307525635, -0.03663245961070061, -0.04493989422917366, -0.019731083884835243, -0.02461951971054077, -0.03933260217308998, -0.026153868064284325, -0.01811322569847107, 0.012302903458476067, -0.09737702459096909, -0.03301510959863663, 0.025879470631480217, -0.003306790953502059, 0.02002006582915783, -0.15236502885818481, 0.08923696726560593, 0.14038686454296112, -0.04837346822023392, 0.10203997790813446, -0.09147225320339203, 0.04281827062368393, 0.04091447964310646, 0.08235223591327667, -0.012447556480765343, 0.06230970844626427, -0.046568650752305984, 0.06411930173635483, -0.017105136066675186, -0.03274594619870186, 0.021926041692495346, -0.012977506965398788, 0.04518650099635124, -0.12019401043653488, 0.009310913272202015, -0.004616734106093645, -0.0072496733628213406, 0.011264736764132977, -0.04779358208179474, 0.04197615757584572, -0.0026414894964545965, -0.07543329149484634, -0.051555149257183075, 0.01991504617035389, 0.06339653581380844, -0.04030786082148552, 0.12550204992294312, -0.07167152315378189, -0.014737440273165703, 0.05130148306488991, -0.014117660000920296, -0.001122637651860714, 0.05391814932227135, -0.0223163440823555, -0.061626799404621124, 0.010457728058099747, 0.03316982090473175, -0.004644106607884169, -0.03482661023736, 0.1225762888789177, -0.010322832502424717, -0.005325291771441698, 0.02627125009894371, 0.044896405190229416, -0.045712318271398544, -2.4205810689181817e-08, 0.03333791345357895, -0.03862835094332695, 0.04956555739045143, 0.03983752802014351, -0.0635344460606575, 0.03153986111283302, 0.045332007110118866, -0.02125292830169201, -0.08542105555534363, -0.010851306840777397, -0.002553594531491399, -0.007342382334172726, 0.049966003745794296, -0.003926665056496859, -0.04524504393339157, -0.052054066210985184, -0.0007079971255734563, 0.09585142880678177, -0.006990235764533281, 0.08813624083995819, -0.00853783916682005, -0.03226854279637337, -0.0444849357008934, -0.06479892134666443, 0.0082198865711689, -0.04428539797663689, -0.025398725643754005, 0.10677175968885422, -0.041835084557533264, -0.09008990973234177, -0.005559274461120367, 0.028867287561297417, 0.018398914486169815, -0.05889801308512688, -0.06581767648458481, 0.019539235159754753, 0.0071434564888477325, 0.03296918421983719, 0.02263159491121769, -0.009836997836828232, 0.012763071805238724, 0.04070437327027321, 0.0848914161324501, 0.04909476265311241, 0.041821692138910294, 0.06300928443670273, 0.041623398661613464, 0.03268375247716904, 0.04486740380525589, -0.08801893889904022, -0.052553437650203705, 0.012072556652128696, 0.04224124550819397, 0.023770490661263466, -0.013869422487914562, -0.018761835992336273, -0.0532124824821949, -0.07043259590864182, 0.04486360400915146, 0.054040610790252686, -0.044239699840545654, -0.05985434725880623, 0.002100657904520631, 0.02980027347803116]}, page_content='Is the Earth, which orbits the Sun along with seven other planets in the solar system, generally considered to be round in shape despite its slight equatorial bulge?'), Document(metadata={'graph_id': 'Q2', 'question': 'Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?', 'llm_model': 'microsoft/Phi-4-mini-reasoning', 'llm_answer': 'Earth rotates eastward, making the Sun appear to rise and set.', 'created_at': 1757245663, 'q_embeddings': [-0.011772827245295048, 0.03634904697537422, 0.08662214130163193, -0.04265142232179642, -0.03554951027035713, -0.021594446152448654, -0.05356825515627861, -0.06157520040869713, 0.070494644343853, -0.01968705654144287, 0.037737827748060226, 0.056409358978271484, 0.018629156053066254, -0.010401655919849873, 0.06232714280486107, -0.02009040303528309, -0.039399996399879456, 0.022597311064600945, 0.020688872784376144, -0.03750623017549515, -0.04648561403155327, -0.032492924481630325, -0.017418215051293373, 0.0709427073597908, -0.016142146661877632, -0.01287265494465828, 0.02588803879916668, -0.03842109814286232, -0.01569795049726963, -0.0025445660576224327, -0.08490310609340668, 0.0016786223277449608, -0.008176237344741821, 0.028748318552970886, -0.031585052609443665, 0.012206505052745342, -0.010878132656216621, -0.04799681156873703, -0.015290801413357258, 0.024765117093920708, 0.03922414779663086, -0.040844738483428955, -0.0126561988145113, 0.0032366011291742325, -0.019360538572072983, -0.01249668002128601, 0.04546528309583664, 0.05038067325949669, 0.012238637544214725, 0.08253053575754166, -0.01421357225626707, 0.0030293804593384266, 0.010007725097239017, -0.04886339604854584, -0.0012399983825162053, 0.12692692875862122, 0.023472098633646965, 0.009062371216714382, 0.11040959507226944, 0.05583851784467697, 0.042374081909656525, -0.0012277825735509396, -0.07332505285739899, 0.03994638845324516, 0.01980971358716488, -0.0019331378862261772, -0.061895135790109634, -0.10123393684625626, -0.04218600317835808, -0.04591194912791252, -0.04569090157747269, 0.035977624356746674, -0.0058855595998466015, -0.09111553430557251, -0.04576005041599274, 0.04644574970006943, 0.023267941549420357, -0.04280586540699005, 0.019505377858877182, -0.019572526216506958, 0.10220445692539215, 0.08694225549697876, 0.046177398413419724, 0.050616033375263214, -0.015496531501412392, -0.07812986522912979, -0.04732755571603775, 0.1149948388338089, 0.04993169382214546, 0.02924479730427265, -0.03773796558380127, -0.09677132964134216, -0.08895736932754517, -0.00456587877124548, 0.007169123273342848, 0.04418814554810524, 0.013906994834542274, -0.05853642523288727, 0.008917523548007011, -0.046057041734457016, -0.021958963945508003, 0.045657236129045486, -0.030444668605923653, -0.00864576268941164, 0.012939294800162315, -0.021328512579202652, -0.014339686371386051, -0.01766332797706127, -0.08674605935811996, -0.05363461747765541, -0.02516855299472809, 0.04797491803765297, 0.059367042034864426, 0.0269943680614233, 0.032481566071510315, 0.05084090679883957, 0.06336592882871628, 0.011357991024851799, -0.1127503290772438, -0.07529199868440628, 0.05374373495578766, 0.034613024443387985, 0.09445536881685257, 0.07395250350236893, 0.05594852939248085, -0.008638233877718449, 0.08391543477773666, -5.025976339812405e-33, 0.010187285952270031, 0.03836636617779732, -0.01723058521747589, 0.01798936352133751, 0.014865652658045292, 0.040658432990312576, -0.004963317885994911, 0.07610838115215302, 0.02065749652683735, -0.08004600554704666, -0.05289599671959877, 0.03453461453318596, 0.04544489085674286, 0.039541635662317276, -0.09015285968780518, 0.009824325330555439, 0.06911677867174149, 0.0913766622543335, -0.057752061635255814, -0.01551137026399374, -0.0646476298570633, -0.03873521089553833, -0.025314051657915115, -0.10493339598178864, -0.1720777302980423, -0.052095022052526474, 0.06748469918966293, 0.012322233989834785, -0.028002677485346794, -0.023098725825548172, 0.038901958614587784, -0.07957489043474197, -0.022720368579030037, 0.003613410983234644, 0.049718111753463745, 0.013129303231835365, 0.0019272915087640285, 0.03238971158862114, -0.002362900646403432, -0.0011148271150887012, -0.007616101764142513, 0.07666201144456863, -0.07790011912584305, -0.028258031234145164, 0.040009308606386185, -0.006404581945389509, 0.02642739936709404, -0.015329854562878609, -0.04522240534424782, 0.08699610084295273, -0.005838004406541586, 0.033783916383981705, -0.016628369688987732, -0.09617576003074646, 0.019328352063894272, 0.06392660737037659, 0.019308151677250862, -0.052181731909513474, -0.07955732196569443, 0.07341604679822922, 0.04453999921679497, -0.0387594997882843, -0.010363098233938217, -0.034497760236263275, 0.006672934163361788, 0.021520690992474556, 0.06572145223617554, -0.0006570354453288019, -0.008855669759213924, 0.03878668323159218, 0.06789033114910126, -0.0359337218105793, 0.041093483567237854, 0.08455350250005722, -0.058857206255197525, 0.0684942677617073, -0.0005374070024117827, 0.08004230260848999, -0.042147718369960785, 0.12421979010105133, 0.0035223986487835646, 0.034775953739881516, 0.05251218006014824, -0.07139616459608078, 0.04356418177485466, -0.008423488587141037, -0.015317772515118122, 0.051733601838350296, 0.035415951162576675, -0.04170897975564003, 0.003050442785024643, -0.05472718924283981, 0.03585290163755417, 0.00032357085729017854, -0.07167708873748779, 1.683823107510962e-33, -0.0009042593883350492, -0.07915393263101578, -0.024233222007751465, -0.04513676464557648, -0.01221353281289339, -0.08341702073812485, -0.035500481724739075, 0.09599646925926208, -0.08265627175569534, -0.014464162290096283, 0.03890160471200943, 0.029280684888362885, 0.017519574612379074, 0.016696477308869362, 0.006412545219063759, -0.0703936368227005, 0.01675346866250038, 0.08772628009319305, -0.06071243807673454, 0.05004691332578659, -0.004654726013541222, -0.07934097945690155, -0.0014967410825192928, 0.03457734361290932, 0.04432263970375061, 0.059486012905836105, 0.07452422380447388, 0.08343415707349777, -0.03780772536993027, 0.04965364187955856, -0.05135057866573334, -0.05163835734128952, -0.01531994342803955, 0.03213471546769142, -0.022437382489442825, 0.04097454994916916, -0.07601390779018402, -0.03100036084651947, -0.020885728299617767, 0.05383064225316048, -0.06646975874900818, -0.018537072464823723, 0.0020028678700327873, -0.008719638921320438, -0.04988458380103111, -0.04186307638883591, 0.006336423568427563, 0.14208441972732544, 0.02557695284485817, 0.03926758095622063, -0.07793735712766647, 0.053903281688690186, 0.031008120626211166, 0.021768804639577866, 0.001531120273284614, 0.015702275559306145, 0.027108963578939438, -0.007059155032038689, -0.021551191806793213, 0.04465968906879425, -0.01769007369875908, 0.008125504478812218, 0.052208684384822845, -0.10932696610689163, 0.0032312925904989243, -0.021009182557463646, 0.060842111706733704, 0.0052863480523228645, 0.06579172611236572, 0.011638378724455833, 0.12044252455234528, -0.04510476440191269, -0.0777733251452446, -0.049275804311037064, 0.01977233774960041, 0.034978237003088, 0.060038771480321884, 0.008411293849349022, -0.06671487540006638, -0.02595069445669651, -0.13221675157546997, -0.07356514036655426, -0.0034702010452747345, 0.010055315680801868, 0.005354003980755806, -0.0975935235619545, -0.10382916033267975, -0.08155795186758041, 0.02806766703724861, 0.037707410752773285, -0.035136740654706955, 0.026244325563311577, -0.05669880285859108, 0.025816600769758224, 0.016885023564100266, -2.615320404686372e-08, -0.054337214678525925, 0.03906439617276192, 0.07538723945617676, 0.08595526963472366, -0.08202206343412399, 0.01754661090672016, 0.11837399005889893, -0.05158069729804993, -0.10190849006175995, -0.07735802233219147, -0.019869793206453323, -0.03994127735495567, 0.07762348651885986, -0.01948048360645771, -0.029665201902389526, -0.0581827238202095, 0.00855989009141922, 0.010830878280103207, -0.00455431966111064, 0.011792332865297794, 0.046490516513586044, -0.005287723615765572, 0.04946780577301979, -0.0009577226592227817, 0.03525955229997635, -0.005116716958582401, -0.035650625824928284, 0.0626164972782135, -0.01960776373744011, 0.018931448459625244, 0.06841544806957245, 0.003580657998099923, 0.06260700523853302, -0.11284414678812027, -0.014482095837593079, -0.051410991698503494, -0.059220992028713226, 0.02449014224112034, 0.005522807594388723, -0.005890214815735817, -0.00968233123421669, 0.04372866451740265, -0.016212807968258858, 0.025508113205432892, 0.008411684073507786, 0.025995377451181412, 0.053477752953767776, 0.004652618430554867, 0.0030253997538238764, -0.07915136963129044, -0.021305188536643982, -0.01718820445239544, 0.05407411605119705, -0.03911788389086723, 0.016050372272729874, 0.06989510357379913, 0.04205816239118576, -0.12374017387628555, 0.07972444593906403, 0.030903786420822144, -0.045110248029232025, -0.024787360802292824, -0.05351103097200394, -0.05948074907064438]}, page_content='Given that Earth completes a rotation approximately every 24 hours, does this rotation cause the Sun to appear to rise in the east and set in the west from the perspective of an observer on the surface?')]\n",
      "[Index size] text_rag  = 11.09 KB  (faiss_text_idx)\n",
      "[Index size] graph_rag = 20.21 KB  (faiss_graph_idx)\n",
      "compact_json in make prompt {'e': ['balance', 'base', 'extensive observations', 'east', 'set', 'perspective', 'rise', 'rotation', 'cause', \"Earth 's rotation\", 'let', 'just distant star', 'tackle', 'rotate', 'user', 'sky', 'seem', 'spin', 'such', 'make'], 'r': ['subj', 'prep_on', 'obj', 'prep_from', 'prep_in', 'isa', 'prep_to', 'prep_across', 'prep_with', 'prep_into', 'prep_for', 'property'], 'edge_matrix': [[0, 0, 1], [1, 1, 2], [2, 0, 0], [0, 2, 4], [0, 0, 0], [2, 2, 2], [9, 0, 4], [4, 3, 5], [4, 4, 3], [6, 4, 3], [7, 0, 8], [15, 0, 6], [15, 0, 16], [9, 0, 8], [9, 0, 6], [9, 0, 10], [15, 5, 11], [19, 6, 3], [12, 0, 12], [9, 0, 13], [9, 0, 19], [14, 0, 10], [19, 7, 15], [19, 8, 9], [9, 0, 17], [10, 0, 10], [19, 0, 19], [17, 1, 9], [19, 3, 3], [15, 0, 19], [15, 0, 4], [11, 0, 19], [7, 5, 18], [18, 0, 19], [12, 2, 10], [19, 9, 5], [19, 9, 19], [19, 1, 1], [1, 2, 2], [2, 1, 19], [19, 3, 4], [19, 1, 19], [2, 3, 2], [19, 2, 1], [1, 3, 2], [2, 2, 19], [19, 4, 2], [19, 2, 19], [3, 1, 4], [9, 1, 4], [4, 5, 5], [4, 6, 3], [6, 6, 3], [7, 1, 8], [15, 1, 6], [4, 3, 7], [15, 1, 16], [9, 1, 8], [9, 1, 6], [9, 1, 10], [15, 7, 11], [19, 8, 3], [12, 1, 12], [9, 1, 13], [9, 1, 19], [14, 1, 10], [19, 9, 15], [19, 10, 9], [9, 1, 17], [10, 1, 10], [17, 2, 9], [19, 5, 3], [15, 1, 19], [15, 1, 4], [11, 1, 19], [7, 7, 18], [18, 1, 19], [12, 3, 10], [19, 11, 5]], 'questions([[e,r,e], ...])': [[[19, 11, 5], [12, 0, 12], [15, 5, 11], [15, 0, 16], [9, 0, 6], [12, 0, 12], [15, 5, 11], [15, 0, 16], [9, 0, 10], [9, 0, 8], [9, 0, 13], [19, 6, 3], [9, 0, 19]]], 'given knowledge([[e,r,e], ...])': [[[1, 1, 2], [2, 0, 0], [0, 2, 4], [0, 0, 0], [2, 2, 2], [0, 0, 1]], [[9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 13], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 19], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 13], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 19], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 6, 3], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 3, 3], [9, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 6, 3], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 3, 3], [11, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [9, 0, 19], [19, 7, 15], [15, 0, 16], [9, 0, 19], [19, 8, 9], [9, 0, 13], [9, 0, 19], [19, 8, 9], [9, 0, 19], [15, 0, 19], [19, 7, 15], [15, 0, 16], [11, 0, 19], [19, 7, 15], [15, 0, 16], [11, 0, 19], [19, 8, 9], [9, 0, 13], [11, 0, 19], [19, 8, 9], [9, 0, 19], [9, 0, 6], [6, 4, 3], [12, 0, 12], [12, 2, 10], [9, 0, 19], [19, 6, 3], [9, 0, 19], [19, 3, 3], [15, 0, 6], [6, 4, 3], [9, 0, 4], [4, 4, 3], [9, 0, 4], [4, 3, 5], [9, 0, 19], [19, 0, 19], [15, 0, 4], [4, 4, 3], [15, 0, 4], [4, 3, 5], [11, 0, 19], [19, 6, 3], [11, 0, 19], [19, 3, 3], [18, 0, 19], [19, 9, 5], [9, 0, 8], [9, 0, 10], [15, 5, 11], [14, 0, 10], [15, 0, 16], [15, 0, 16], [9, 0, 10], [10, 0, 10], [7, 5, 18], [7, 0, 8], [19, 0, 19]], [[9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 13], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 19], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 13], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 8, 9], [9, 0, 19], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 6, 3], [9, 0, 19], [19, 7, 15], [15, 0, 19], [19, 3, 3], [9, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 6, 3], [11, 0, 19], [19, 7, 15], [15, 0, 19], [19, 3, 3], [11, 0, 19], [19, 8, 9], [9, 0, 17], [17, 1, 9], [9, 0, 19], [19, 7, 15], [15, 0, 16], [9, 0, 19], [19, 8, 9], [9, 0, 13], [9, 0, 19], [19, 8, 9], [9, 0, 19], [15, 0, 19], [19, 7, 15], [15, 0, 16], [11, 0, 19], [19, 7, 15], [15, 0, 16], [11, 0, 19], [19, 8, 9], [9, 0, 13], [11, 0, 19], [19, 8, 9], [9, 0, 19], [9, 0, 6], [6, 4, 3], [12, 0, 12], [12, 2, 10], [9, 0, 19], [19, 6, 3], [9, 0, 19], [19, 3, 3], [15, 0, 6], [6, 4, 3], [9, 0, 4], [4, 4, 3], [9, 0, 4], [4, 3, 5], [9, 0, 19], [19, 0, 19], [15, 0, 4], [4, 4, 3], [15, 0, 4], [4, 3, 5], [11, 0, 19], [19, 6, 3], [11, 0, 19], [19, 3, 3], [18, 0, 19], [19, 9, 5], [9, 0, 8], [9, 0, 10], [15, 5, 11], [14, 0, 10], [15, 0, 16], [15, 0, 16], [9, 0, 10], [10, 0, 10], [7, 5, 18], [7, 0, 8], [19, 0, 19]]], 'rule': ''}\n",
      "\n",
      "== Cost (avg) ==\n",
      "          input_tokens | text_rag: 277.0000 | graph_rag: 454.0000 | Δ%:   63.90%\n",
      "         output_tokens | text_rag:  31.0000 | graph_rag:  80.0000 | Δ%:  158.06%\n",
      "          total_tokens | text_rag: 308.0000 | graph_rag: 534.0000 | Δ%:   73.38%\n",
      "           latency_sec | text_rag:   3.1805 | graph_rag:   8.6335 | Δ%:  171.45%\n",
      " retrieval_latency_sec | text_rag:   0.0077 | graph_rag:   0.0064 | Δ%:  -17.27%\n",
      "       gen_latency_sec | text_rag:   3.1728 | graph_rag:   8.6271 | Δ%:  171.91%\n",
      "       retrieved_count | text_rag:   2.0000 | graph_rag:   2.0000 | Δ%:    0.00%\n",
      "         peak_vram_MiB | text_rag:      nan | graph_rag:      nan | Δ%:     nan%\n",
      "          prompt_chars | text_rag: 1278.0000 | graph_rag: 2021.0000 | Δ%:   58.14%\n"
     ]
    }
   ],
   "source": [
    "gen_pipe, tokenizer = load_llm_pipeline()\n",
    "parser = RelationshipGraphParser()\n",
    "\n",
    "graph_db = build_docs_with_answer(\n",
    "    questions, parser, gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    faiss_db=None\n",
    ")\n",
    "\n",
    "text_db, q_vecs = build_text_faiss_index_with_answers(\n",
    "    questions,\n",
    "    gen_pipe,\n",
    "    add_prompt_snapshot=False,\n",
    "    bootstrap_db=None  \n",
    ")\n",
    "\n",
    "save_and_report_sizes(text_db, graph_db, text_dir=\"faiss_text_idx\", graph_dir=\"faiss_graph_idx\")\n",
    "\n",
    "eval_questions = list(GOLD_LABELS.keys())[:1]\n",
    "df = batch_compare_text_vs_graph(\n",
    "    eval_questions, gen_pipe, tokenizer, parser, text_db, graph_db, q_vecs\n",
    ")\n",
    "df_ab_gold = attach_gold(df, GOLD_LABELS)\n",
    "#evaluate_accuracy(df_ab_gold)\n",
    "summarize_cost(df_ab_gold, base_label=\"text_rag\", target_label=\"graph_rag\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
